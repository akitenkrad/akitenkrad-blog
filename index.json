[{"categories":null,"contents":"Este archivo existe únicamente para responder a la URL /search con la plantilla de diseño search relacionada.\nNo se muestra ningún contenido aquí, todo el contenido se basa en la plantilla layouts/page/search.html\nEstablecer una prioridad muy baja en el mapa del sitio le dirá a los motores de búsqueda que éste no es un contenido importante.\nEsta implementación utiliza Fusejs, jquery y mark.js\nConfiguración inicial La búsqueda depende del tipo de contenido de salida adicional de JSON en config.toml\n``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nBúsqueda de archivos adicionales Para buscar campos adicionales definidos en el front matter, debes añadirlo en 2 lugares.\nEditar layouts/_default/index.JSON Esto expone los valores en /index.json: por ejemplo, para agregar categories ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEditar las opciones de fuse.js para buscar static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"Este archivo existe únicamente para responder a la URL /search con la plantilla de diseño search relacionada.\nNo se muestra ningún contenido aquí, todo el contenido se basa en la plantilla layouts/page/search.html\nEstablecer una prioridad muy baja en el mapa del sitio le dirá a los motores de búsqueda que éste no es un contenido importante.\nEsta implementación utiliza Fusejs, jquery y mark.js\nConfiguración inicial La búsqueda depende del tipo de contenido de salida adicional de JSON en config.","tags":null,"title":"Resultados de Búsqueda"},{"categories":null,"contents":"Este archivo existe únicamente para responder a la URL /search con la plantilla de diseño search relacionada.\nNo se muestra ningún contenido aquí, todo el contenido se basa en la plantilla layouts/page/search.html\nEstablecer una prioridad muy baja en el mapa del sitio le dirá a los motores de búsqueda que éste no es un contenido importante.\nEsta implementación utiliza Fusejs, jquery y mark.js\nConfiguración inicial La búsqueda depende del tipo de contenido de salida adicional de JSON en config.toml\n``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nBúsqueda de archivos adicionales Para buscar campos adicionales definidos en el front matter, debes añadirlo en 2 lugares.\nEditar layouts/_default/index.JSON Esto expone los valores en /index.json: por ejemplo, para agregar categories ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEditar las opciones de fuse.js para buscar static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"Este archivo existe únicamente para responder a la URL /search con la plantilla de diseño search relacionada.\nNo se muestra ningún contenido aquí, todo el contenido se basa en la plantilla layouts/page/search.html\nEstablecer una prioridad muy baja en el mapa del sitio le dirá a los motores de búsqueda que éste no es un contenido importante.\nEsta implementación utiliza Fusejs, jquery y mark.js\nConfiguración inicial La búsqueda depende del tipo de contenido de salida adicional de JSON en config.","tags":null,"title":"Resultados de Búsqueda"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"অনুসন্ধানের ফলাফল"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"June 8, 2010","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"অনুসন্ধানের ফলাফল"},{"categories":null,"contents":" Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments  Citation  Rogers, A., Kovaleva, O., \u0026amp; Rumshisky, A. (2020).\nA Primer in BERTology: What We Know About How BERT Works.\nTransactions of the Association for Computational Linguistics, 8, 842–866.\nPaper Link\n Abstract  Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.\n Model History gantt dateFormat YYYY-MM title Model History axisFormat %Y-%-m section Model BERT :active, des201701, 2016-06, 2017-06 DeBERTa :active, des202001, 2019-06, 2020-06  What Knowledge Does BERT Have? Syntactic Knowledge   Lin et al. (2019)  Yongjie Lin, Y. Tan, R. Frank. (2019)\nOpen Sesame: Getting inside BERT’s Linguistic Knowledge\nBlackboxNLP@ACL\nPaper Link\nInfluential Citation Count (18), SS-ID (165d51a547cd920e6ac55660ad5c404dcb9562ed)\nABSTRACT\nHow and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT’s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT’s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT’s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  BERTの分散表現は線形というよりは階層的である． BERTの分散表現には単語の順番に関する情報以外に文法構造の階層的な情報が含まれている可能性がある．    Tenney et al. (2019), Liu et al. (2019)  Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, Ellie Pavlick. (2019)\nWhat do you learn from context? Probing for sentence structure in contextualized word representations\nICLR\nPaper Link\nInfluential Citation Count (45), SS-ID (e2587eddd57bc4ba286d91b27c185083f16f40ee)\nABSTRACT\nContextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.\n  Nelson F. Liu, Matt Gardner, Y. Belinkov, Matthew E. Peters, Noah A. Smith. (2019)\nLinguistic Knowledge and Transferability of Contextual Representations\nNAACL\nPaper Link\nInfluential Citation Count (108), SS-ID (f6fbb6809374ca57205bd2cf1421d4f4fa04f975)\nABSTRACT\nContextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  BERTには品詞，文法的なチャンク・格に関する情報が一部含まれている． ただし，文法階層的に遠く離れた親ノードのラベルを復元することはできなかったので，分散表現が保持しているのは単語の近辺の情報に限られる．    Htut et al. (2019), Jawahar et al. (2019)  Phu Mon Htut, Jason Phang, Shikha Bordia, Samuel R. Bowman. (2019)\nDo Attention Heads in BERT Track Syntactic Dependencies?\nArXiv\nPaper Link\nInfluential Citation Count (12), SS-ID (ba8215e77f35b0d947c7cec39c45df4516e93421)\nABSTRACT\nWe investigate the extent to which individual attention heads in pretrained transformer language models, such as BERT and RoBERTa, implicitly capture syntactic dependency relations. We employ two methods\u0026mdash;taking the maximum attention weight and computing the maximum spanning tree\u0026mdash;to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency (UD) trees. We show that, for some UD relation types, there exist heads that can recover the dependency type significantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze BERT fine-tuned on two datasets\u0026mdash;the syntax-oriented CoLA and the semantics-oriented MNLI\u0026mdash;to investigate whether fine-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing significantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that BERT-style models are known to learn.\n  Ganesh Jawahar, Benoît Sagot, Djamé Seddah. (2019)\nWhat Does BERT Learn about the Structure of Language?\nACL\nPaper Link\nInfluential Citation Count (44), SS-ID (335613303ebc5eac98de757ed02a56377d99e03a)\nABSTRACT\nBERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT’s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  文法構造がどのように埋め込まれるかということに関して， Attention Weights には文法的な構造は直接的には含まれていない． Htut et al. (2019)によれば，文法ツリーのルートの正解データを与えたとしても，Attention Weightsから完全な文法ツリーを構成することはできなかった． Jawahar et al. (2019)は，Attention Weightsから文法ツリーを抽出したとするが，定量的な結果は示されていない．    Hewitt and Manning (2019)  John Hewitt, Christopher D. Manning. (2019)\nA Structural Probe for Finding Syntax in Word Representations\nNAACL\nPaper Link\nInfluential Citation Count (30), SS-ID (455a8838cde44f288d456d01c76ede95b56dc675)\nABSTRACT\nRecent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network’s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models’ vector geometry.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  文法の情報はBERTの トークンの分散表現 から復元することができる可能性がある． Hewitt and Manning (2019)では，PenTreebankデータセットを使用して，BERTのトークンの分散表現から文法の依存構造を復元する変換行列を学習することに成功した．    Wu et al. (2020)  Zhiyong Wu, Yun Chen, B. Kao, Qun Liu. (2020)\nPerturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT\nACL\nPaper Link\nInfluential Citation Count (10), SS-ID (3aaa8aaad5ef36550a6b47d6ee000f0b346a5a1f)\nABSTRACT\nBy introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  MLM (Masked Language Model) タスクにおいて，ある単語が他の単語とどの程度関連性があるかを検証し，BERTは文法的な情報をある程度学習するが，それは正解データに近しいものとは限らないと結論づけた．     Figure 1: Parameter-free probe for syntactic know-ledge: words sharing syntactic subtrees have largerimpact on each other in the MLM prediction (Wu et al.,2020).\n  Goldberg (2019), van Schijndel et al., (2019)  Yoav Goldberg. (2019)\nAssessing BERT\u0026rsquo;s Syntactic Abilities\nArXiv\nPaper Link\nInfluential Citation Count (17), SS-ID (efeab0dcdb4c1cce5e537e57745d84774be99b9a)\nABSTRACT\nI assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) \u0026ldquo;coloreless green ideas\u0026rdquo; subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.\n  Marten van Schijndel, Aaron Mueller, Tal Linzen. (2019)\nQuantity doesn’t buy quality syntax with neural language models\nEMNLP\nPaper Link\nInfluential Citation Count (0), SS-ID (356645552f8f40adf5a99b4e3a69f47699399010)\nABSTRACT\nRecurrent neural networks can learn to predict upcoming words remarkably well on average; in syntactically complex contexts, however, they often assign unexpectedly high probabilities to ungrammatical words. We investigate to what extent these shortcomings can be mitigated by increasing the size of the network and the corpus on which it is trained. We find that gains from increasing network size are minimal beyond a certain point. Likewise, expanding the training corpus yields diminishing returns; we estimate that the training corpus would need to be unrealistically large for the models to match human performance. A comparison to GPT and BERT, Transformer-based models trained on billions of words, reveals that these models perform even more poorly than our LSTMs in some constructions. Our results make the case for more data efficient architectures.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  BERTはClozeタスクにおいて，主語-述語の関係を加味しながら学習しているが，意味のない文章や曖昧な文章に関しても一律に主語と動詞を関連づけようとしてしまう．    Warstadt et al. (2019)  Alex Warstadt, Samuel R. Bowman. (2020)\nCan neural networks acquire a structural bias from raw linguistic data?\nCogSci\nPaper Link\nInfluential Citation Count (3), SS-ID (0e012c2bd18236445cfbc6e3e409eb02df4691fe)\nABSTRACT\nWe evaluate whether BERT, a widely used neural network for sentence processing, acquires an inductive bias towards forming structural generalizations through pretraining on raw data. We conduct four experiments testing its preference for structural vs. linear generalizations in different structure-dependent phenomena. We find that BERT makes a structural generalization in 3 out of 4 empirical domains\u0026mdash;subject-auxiliary inversion, reflexive binding, and verb tense detection in embedded clauses\u0026mdash;but makes a linear generalization when tested on NPI licensing. We argue that these results are the strongest evidence so far from artificial learners supporting the proposition that a structural bias can be acquired from raw data. If this conclusion is correct, it is tentative evidence that some linguistic universals can be acquired by learners without innate biases. However, the precise implications for human language acquisition are unclear, as humans learn language from significantly less data than BERT.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  BERTは文法のscope violationsを検知するよりも，NPIs (Negative Polarity Items) の存在やNPIsに関連する単語を検知するほうが得意である．    Ettinger (2019), Vulic (2020)  Allyson Ettinger. (2019)\nWhat BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models\nTACL\nPaper Link\nInfluential Citation Count (10), SS-ID (a0e49f65b6847437f262c59d0d399255101d0b75)\nABSTRACT\nPre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction— and, in particular, it shows clear insensitivity to the contextual impacts of negation.\n  Goran Glavas, Ivan Vulic. (2020)\nIs Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation\nEACL\nPaper Link\nInfluential Citation Count (0), SS-ID (575ac3f36e9fddeb258e2f639e26a6a7ec35160a)\nABSTRACT\nTraditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the model for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experiments (with intermediate target-language parsing) show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers’ representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models?\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  BERTは「否定」を理解できず，不正な入力の検知に対しても脆弱である． 単語の語順を入れ替えたり，文章を一部削除したり守護や目的語を取り除いてもBERTの出力結果が変わらなかったため．これが意味するところは，BERTが学習している文法は不完全であるか，またはタスクが文法的な知識に依存していないかのどちらかである． タスクにおける中間段階での教師ありFine-Tuningは下流タスクの性能に大きく影響しないため，後者である可能性が高いとのこと．   Semantic Knowledge  Tenney et al. (2019)  Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, Ellie Pavlick. (2019)\nWhat do you learn from context? Probing for sentence structure in contextualized word representations\nICLR\nPaper Link\nInfluential Citation Count (45), SS-ID (e2587eddd57bc4ba286d91b27c185083f16f40ee)\nABSTRACT\nContextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  Classifierを分析することで，BERTがentity type，relations，semantic roles，proto-rolesに関する情報をエンコードしていることを示した   Wallace et al. (2019)  Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner. (2019)\nDo NLP Models Know Numbers? Probing Numeracy in Embeddings\nEMNLP\nPaper Link\nInfluential Citation Count (18), SS-ID (0427110f0e79f41e69a8eb00a3ec8868bac26a4f)\nABSTRACT\nThe ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens—they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  BERTは数字に関する分散表現が苦手である． 加算や数値のエンコード・デコードに関するタスクではBERTはあまり良い性能を発揮できなかった． BERTのTokenizerであるWordpieceでは，似た数値であってもベクトル空間上では離れた位置に写像されることがありうるため，これが原因の一部である可能性が考えられる．   Balasubramanian et al. (2020), Broscheit (2019)  S. Balasubramanian, Naman Jain, G. Jindal, Abhijeet Awasthi, Sunita Sarawagi. (2020)\nWhat’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?\nREPL4NLP\nPaper Link\nInfluential Citation Count (1), SS-ID (167f52d369b0979f27282af0f3a1a4be9c9be84b)\nABSTRACT\nWe evaluate named entity representations of BERT-based NLP models by investigating their robustness to replacements from the same typed class in the input. We highlight that on several tasks while such perturbations are natural, state of the art trained models are surprisingly brittle. The brittleness continues even with the recent entity-aware BERT models. We also try to discern the cause of this non-robustness, considering factors such as tokenization and frequency of occurrence. Then we provide a simple method that ensembles predictions from multiple replacements while jointly modeling the uncertainty of type annotations and label predictions. Experiments on three NLP tasks shows that our method enhances robustness and increases accuracy on both natural and adversarial datasets.\n  Samuel Broscheit. (2019)\nInvestigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking\nCoNLL\nPaper Link\nInfluential Citation Count (6), SS-ID (399308fa54ade9b1362d56628132323489ce50cd)\nABSTRACT\nA typical architecture for end-to-end entity linking systems consists of three steps: mention detection, candidate generation and entity disambiguation. In this study we investigate the following questions: (a) Can all those steps be learned jointly with a model for contextualized text-representations, i.e. BERT? (b) How much entity knowledge is already contained in pretrained BERT? (c) Does additional entity knowledge improve BERT’s performance in downstream tasks? To this end we propose an extreme simplification of the entity linking setup that works surprisingly well: simply cast it as a per token classification over the entire entity vocabulary (over 700K classes in our case). We show on an entity linking benchmark that (i) this model improves the entity representations over plain BERT, (ii) that it outperforms entity linking architectures that optimize the tasks separately and (iii) that it only comes second to the current state-of-the-art that does mention detection and entity disambiguation jointly. Additionally, we investigate the usefulness of entity-aware token-representations in the text-understanding benchmark GLUE, as well as the question answering benchmarks SQUAD~V2 and SWAG and also the EN-DE WMT14 machine translation benchmark. To our surprise, we find that most of those benchmarks do not benefit from additional entity knowledge, except for a task with very small training data, the RTE task in GLUE, which improves by 2%.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  BERTは固有名詞に関する分散表現が苦手である． Balasubramanian et al. (2020) によれば，Coreference Taskにおいて名詞を置換すると，予測結果の85%が変化する．これによれば，NERタスクのF1スコアは高く出るものの，BERTは固有名詞の一般的な概念を理解しているとは言い難い． Broscheit (2019) によれば，Wikipediaのentitiy linkingにおけるBERTのfine-tuningでは，entityに関して追加情報を与えるのみで，entityに関連する情報を全て学習しているわけではない．  World Knowledge  Ettinger (2019), Da and Kasai (2019)  Allyson Ettinger. (2019)\nWhat BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models\nTACL\nPaper Link\nInfluential Citation Count (10), SS-ID (a0e49f65b6847437f262c59d0d399255101d0b75)\nABSTRACT\nPre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction— and, in particular, it shows clear insensitivity to the contextual impacts of negation.\n  Jeff Da, Jungo Kasai. (2019)\nUnderstanding Commonsense Inference Aptitude of Deep Contextual Representations\nProceedings of the First Workshop on Commonsense Inference in Natural Language Processing\nPaper Link\nInfluential Citation Count (0), SS-ID (80dc7b0e6dbc26571672d9be57a0ae589689e410)\nABSTRACT\nPretrained deep contextual representations have advanced the state-of-the-art on various commonsense NLP tasks, but we lack a concrete understanding of the capability of these models. Thus, we investigate and challenge several aspects of BERT’s commonsense representation abilities. First, we probe BERT’s ability to classify various object attributes, demonstrating that BERT shows a strong ability in encoding various commonsense features in its embedding space, but is still deficient in many areas. Next, we show that, by augmenting BERT’s pretraining data with additional data related to the deficient attributes, we are able to improve performance on a downstream commonsense reasoning task while using a minimal amount of data. Finally, we develop a method of fine-tuning knowledge graphs embeddings alongside BERT and show the continued importance of explicit knowledge graphs.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  BERTは実用的な推論や現実のイベントなどに関する知識の扱いが不得手である． BERTは抽象的な物事や見た目，感覚的な特徴などの扱いも得意ではない．   Petroni et al. (2019), Roberts et al. (2020), Davison et al. (2019)  Fabio Petroni, Tim Rocktäschel, Patrick Lewis, A. Bakhtin, Yuxiang Wu, Alexander H. Miller, S. Riedel. (2019)\nLanguage Models as Knowledge Bases?\nEMNLP\nPaper Link\nInfluential Citation Count (115), SS-ID (d0086b86103a620a86bc918746df0aa642e2a8a3)\nABSTRACT\nRecent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.\n  Adam Roberts, Colin Raffel, Noam M. Shazeer. (2020)\nHow Much Knowledge Can You Pack into the Parameters of a Language Model?\nEMNLP\nPaper Link\nInfluential Citation Count (38), SS-ID (80376bdec5f534be78ba82821f540590ebce5559)\nABSTRACT\nIt has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales surprisingly well with model size and outperforms models that explicitly look up knowledge on the open-domain variants of Natural Questions and WebQuestions. To facilitate reproducibility and future work, we release our code and trained models.\n  Joshua Feldman, Joe Davison, Alexander M. Rush. (2019)\nCommonsense Knowledge Mining from Pretrained Models\nEMNLP\nPaper Link\nInfluential Citation Count (17), SS-ID (f98e135986414cccf29aec593d547c0656e4d82c)\nABSTRACT\nInferring commonsense knowledge is a key challenge in machine learning. Due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple’s validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though we do worse on a held-out test set than models explicitly trained on a corresponding training set, our approach outperforms these methods when mining commonsense knowledge from new sources, suggesting that our unsupervised technique generalizes better than current supervised approaches.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  BERTは関係性の抽出には長けており，シンプルなBERTモデルであってもKnowledge Baseによる手法と同等の精度を発揮することができる． Roberts et al. (2019)はT5を使ってopen-domain QAでも同様の結果が得られることを示した． Davison et al. (2020)によれば，BERTは関係性の推論に関しては汎化性能が高く，未知のデータにも良く対応できる．   Forbes et al. (2019), Zhou et al. (2019), Richardson and Sabharwal (2019), Poerner et al., (2019)  Maxwell Forbes, Ari Holtzman, Yejin Choi. (2019)\nDo Neural Language Representations Learn Physical Commonsense?\nCogSci\nPaper Link\nInfluential Citation Count (3), SS-ID (cc02386375b1262c3a1d5525154eaea24c761d15)\nABSTRACT\nHumans understand language based on the rich background knowledge about how the physical world works, which in turn allows us to reason about the physical world through language. In addition to the properties of objects (e.g., boats require fuel) and their affordances, i.e., the actions that are applicable to them (e.g., boats can be driven), we can also reason about if-then inferences between what properties of objects imply the kind of actions that are applicable to them (e.g., that if we can drive something then it likely requires fuel). In this paper, we investigate the extent to which state-of-the-art neural language representations, trained on a vast amount of natural language text, demonstrate physical commonsense reasoning. While recent advancements of neural language models have demonstrated strong performance on various types of natural language inference tasks, our study based on a dataset of over 200k newly collected annotations suggests that neural language representations still only learn associations that are explicitly written down.\n  Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang. (2019)\nEvaluating Commonsense in Pre-trained Language Models\nAAAI\nPaper Link\nInfluential Citation Count (5), SS-ID (01f2b214962997260020279bd1fd1f8f372249d4)\nABSTRACT\nContextualized representations trained over large raw text data have given remarkable improvements for NLP tasks including question answering and reading comprehension. There have been works showing that syntactic, semantic and word sense knowledge are contained in such representations, which explains why they benefit such tasks. However, relatively little work has been done investigating commonsense knowledge contained in contextualized representations, which is crucial for human question answering and reading comprehension. We study the commonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven challenging benchmarks, finding that language modeling and its variants are effective objectives for promoting models\u0026rsquo; commonsense ability while bi-directional context and larger training set are bonuses. We additionally find that current models do poorly on tasks require more necessary inference steps. Finally, we test the robustness of models by making dual test cases, which are correlated so that the correct prediction of one sample should lead to correct prediction of the other. Interestingly, the models show confusion on these test cases, which suggests that they learn commonsense at the surface rather than the deep level. We release a test set, named CATs publicly, for future research.\n  Kyle Richardson, Ashish Sabharwal. (2019)\nWhat Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge\nTransactions of the Association for Computational Linguistics\nPaper Link\nInfluential Citation Count (2), SS-ID (5a9001cdccdb8b1de227a45eccc503d32d1a2464)\nABSTRACT\nAbstract Open-domain question answering (QA) involves many knowledge and reasoning challenges, but are successful QA models actually learning such knowledge when trained on benchmark QA tasks? We investigate this via several new diagnostic tasks probing whether multiple-choice QA models know definitions and taxonomic reasoning—two skills widespread in existing benchmarks and fundamental to more complex reasoning. We introduce a methodology for automatically building probe datasets from expert knowledge sources, allowing for systematic control and a comprehensive evaluation. We include ways to carefully control for artifacts that may arise during this process. Our evaluation confirms that transformer-based multiple-choice QA models are already predisposed to recognize certain types of structural linguistic knowledge. However, it also reveals a more nuanced picture: their performance notably degrades even with a slight increase in the number of “hops” in the underlying taxonomic hierarchy, and with more challenging distractor candidates. Further, existing models are far from perfect when assessed at the level of clusters of semantically connected probes, such as all hypernym questions about a single concept.\n  Nina Poerner, Ulli Waltinger, Hinrich Schütze. (2019)\nBERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA\nArXiv\nPaper Link\nInfluential Citation Count (8), SS-ID (7c62ac7aedacc39ca417a48f8134e0514dc6a523)\nABSTRACT\nThe BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts. Petroni et al. (2019) take this as evidence that BERT memorizes factual knowledge during pre-training. We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian. More specifically, we show that BERT\u0026rsquo;s precision drops dramatically when we filter certain easy-to-guess facts. As a remedy, we propose E-BERT, an extension of BERT that replaces entity mentions with symbolic entity embeddings. E-BERT outperforms both BERT and ERNIE (Zhang et al., 2019) on hard-to-guess queries. We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT.\n   ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  BERTはWorld Knowledgeに関する因果推論を行うことはできない． BERTは関係性を推論することはできるが，その関係性の因果については感知しない．例えば，BERTは「人間が家に入る」ということと，「家が大きい」ということはそれぞれ学習できるが，「家が人間よりも大きいかどうか」ということは判断できない． BERTのWorld Knowledgeに関する成功は，現実世界のステレオタイプに依存している．例えば，BERTモデルではアメリカ人であろうとドイツ人であろうと，イタリア風な名前の持ち主はイタリア人であると判断される．  Limitations Localizing Linguistic Knowledge BERT Embeddings Self-Attention Heads BERT Layers Training BERT How Big Should BERT Be? Directions for Further Research References  Evaluating Commonsense in Pre-trained Language Models  Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang. (2019)\nEvaluating Commonsense in Pre-trained Language Models\nAAAI\nPaper Link\nInfluential Citation Count (5), SS-ID (01f2b214962997260020279bd1fd1f8f372249d4)\nABSTRACT\nContextualized representations trained over large raw text data have given remarkable improvements for NLP tasks including question answering and reading comprehension. There have been works showing that syntactic, semantic and word sense knowledge are contained in such representations, which explains why they benefit such tasks. However, relatively little work has been done investigating commonsense knowledge contained in contextualized representations, which is crucial for human question answering and reading comprehension. We study the commonsense ability of GPT, BERT, XLNet, and RoBERTa by testing them on seven challenging benchmarks, finding that language modeling and its variants are effective objectives for promoting models\u0026rsquo; commonsense ability while bi-directional context and larger training set are bonuses. We additionally find that current models do poorly on tasks require more necessary inference steps. Finally, we test the robustness of models by making dual test cases, which are correlated so that the correct prediction of one sample should lead to correct prediction of the other. Interestingly, the models show confusion on these test cases, which suggests that they learn commonsense at the surface rather than the deep level. We release a test set, named CATs publicly, for future research.\n   ERNIE: Enhanced Representation through Knowledge Integration  Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu. (2019)\nERNIE: Enhanced Representation through Knowledge Integration\nArXiv\nPaper Link\nInfluential Citation Count (63), SS-ID (031e4e43aaffd7a479738dcea69a2d5be7957aa3)\nABSTRACT\nWe present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT, ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words.Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit.Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.\n   Do NLP Models Know Numbers? Probing Numeracy in Embeddings  Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner. (2019)\nDo NLP Models Know Numbers? Probing Numeracy in Embeddings\nEMNLP\nPaper Link\nInfluential Citation Count (18), SS-ID (0427110f0e79f41e69a8eb00a3ec8868bac26a4f)\nABSTRACT\nThe ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens—they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact.\n   Emergent linguistic structure in artificial neural networks trained by self-supervision  Christopher D. Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, Omer Levy. (2020)\nEmergent linguistic structure in artificial neural networks trained by self-supervision\nProceedings of the National Academy of Sciences\nPaper Link\nInfluential Citation Count (5), SS-ID (04ef54bd467d5e03dee7b0be601cf06d420bffa0)\nABSTRACT\nThis paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.\n   RoBERTa: A Robustly Optimized BERT Pretraining Approach  Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, Veselin Stoyanov. (2019)\nRoBERTa: A Robustly Optimized BERT Pretraining Approach\nArXiv\nPaper Link\nInfluential Citation Count (2014), SS-ID (077f8329a7b6fa3b7c877a57b81eb6c18b5f87de)\nABSTRACT\nLanguage model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.\n   Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned  Elena Voita, David Talbot, F. Moiseev, Rico Sennrich, Ivan Titov. (2019)\nAnalyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned\nACL\nPaper Link\nInfluential Citation Count (49), SS-ID (07a64686ce8e43ac475a8d820a8a9f1d87989583)\nABSTRACT\nMulti-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.\n   Distilling the Knowledge in a Neural Network  Geoffrey E. Hinton, Oriol Vinyals, J. Dean. (2015)\nDistilling the Knowledge in a Neural Network\nArXiv\nPaper Link\nInfluential Citation Count (1210), SS-ID (0c908739fbff75f03469d13d4a1a07de3414ee19)\nABSTRACT\nA very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.\n   TinyBERT: Distilling BERT for Natural Language Understanding  Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, F. Wang, Qun Liu. (2019)\nTinyBERT: Distilling BERT for Natural Language Understanding\nFINDINGS\nPaper Link\nInfluential Citation Count (131), SS-ID (0cbf97173391b0430140117027edcaf1a37968c7)\nABSTRACT\nLanguage model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large “teacher” BERT can be effectively transferred to a small “student” TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28% parameters and ~31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.\n   MPNet: Masked and Permuted Pre-training for Language Understanding  Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu. (2020)\nMPNet: Masked and Permuted Pre-training for Language Understanding\nNeurIPS\nPaper Link\nInfluential Citation Count (16), SS-ID (0e002114cd379efaca0ec5cda6d262b5fe0be104)\nABSTRACT\nBERT adopts masked language modeling (MLM) for pre-training and is one of the most successful pre-training models. Since BERT neglects dependency among predicted tokens, XLNet introduces permuted language modeling (PLM) for pre-training to address this problem. We argue that XLNet does not leverage the full position information of a sentence and thus suffers from position discrepancy between pre-training and fine-tuning. In this paper, we propose MPNet, a novel pre-training method that inherits the advantages of BERT and XLNet and avoids their limitations. MPNet leverages the dependency among predicted tokens through permuted language modeling (vs. MLM in BERT), and takes auxiliary position information as input to make the model see a full sentence and thus reducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a large-scale dataset (over 160GB text corpora) and fine-tune on a variety of down-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet outperforms MLM and PLM by a large margin, and achieves better results on these tasks compared with previous state-of-the-art pre-trained methods (e.g., BERT, XLNet, RoBERTa) under the same model setting. We release the code and pre-trained model in GitHub\\footnote{\\url{this https URL}}.\n   Can neural networks acquire a structural bias from raw linguistic data?  Alex Warstadt, Samuel R. Bowman. (2020)\nCan neural networks acquire a structural bias from raw linguistic data?\nCogSci\nPaper Link\nInfluential Citation Count (3), SS-ID (0e012c2bd18236445cfbc6e3e409eb02df4691fe)\nABSTRACT\nWe evaluate whether BERT, a widely used neural network for sentence processing, acquires an inductive bias towards forming structural generalizations through pretraining on raw data. We conduct four experiments testing its preference for structural vs. linear generalizations in different structure-dependent phenomena. We find that BERT makes a structural generalization in 3 out of 4 empirical domains\u0026mdash;subject-auxiliary inversion, reflexive binding, and verb tense detection in embedded clauses\u0026mdash;but makes a linear generalization when tested on NPI licensing. We argue that these results are the strongest evidence so far from artificial learners supporting the proposition that a structural bias can be acquired from raw data. If this conclusion is correct, it is tentative evidence that some linguistic universals can be acquired by learners without innate biases. However, the precise implications for human language acquisition are unclear, as humans learn language from significantly less data than BERT.\n   The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives  Elena Voita, Rico Sennrich, Ivan Titov. (2019)\nThe Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives\nEMNLP\nPaper Link\nInfluential Citation Count (10), SS-ID (112fd54ee193237b24f2ce7fce79e399609a29c5)\nABSTRACT\nWe seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We chose the Transformers for our analysis as they have been shown effective with various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and observe that the choice of the objective determines this process. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers.\n   Using Dynamic Embeddings to Improve Static Embeddings  Yile Wang, Leyang Cui, Yue Zhang. (2019)\nUsing Dynamic Embeddings to Improve Static Embeddings\nArXiv\nPaper Link\nInfluential Citation Count (1), SS-ID (1257f59bd9b6bc3f3823125408c7b6e63db4a158)\nABSTRACT\nHow to build high-quality word embeddings is a fundamental research question in the field of natural language processing. Traditional methods such as Skip-Gram and Continuous Bag-of-Words learn {\\it static} embeddings by training lookup tables that translate words into dense vectors. Static embeddings are directly useful for solving lexical semantics tasks, and can be used as input representations for downstream problems. Recently, contextualized embeddings such as BERT have been shown more effective than static embeddings as NLP input embeddings. Such embeddings are {\\it dynamic}, calculated according to a sentential context using a network structure. One limitation of dynamic embeddings, however, is that they cannot be used without a sentence-level context. We explore the advantages of dynamic embeddings for training static embeddings, by using contextualized embeddings to facilitate training of static embedding lookup tables. Results show that the resulting embeddings outperform existing static embedding methods on various lexical semantics tasks.\n   Is Attention Interpretable?  Sofia Serrano, Noah A. Smith. (2019)\nIs Attention Interpretable?\nACL\nPaper Link\nInfluential Citation Count (16), SS-ID (135112c7ba1762d65f39b1a61777f26ae4dfd8ad)\nABSTRACT\nAttention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components’ representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components’ overall importance to a model, it is by no means a fail-safe indicator.1\n   Open Sesame: Getting inside BERT’s Linguistic Knowledge  Yongjie Lin, Y. Tan, R. Frank. (2019)\nOpen Sesame: Getting inside BERT’s Linguistic Knowledge\nBlackboxNLP@ACL\nPaper Link\nInfluential Citation Count (18), SS-ID (165d51a547cd920e6ac55660ad5c404dcb9562ed)\nABSTRACT\nHow and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT’s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT’s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT’s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.\n   What’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?  S. Balasubramanian, Naman Jain, G. Jindal, Abhijeet Awasthi, Sunita Sarawagi. (2020)\nWhat’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?\nREPL4NLP\nPaper Link\nInfluential Citation Count (1), SS-ID (167f52d369b0979f27282af0f3a1a4be9c9be84b)\nABSTRACT\nWe evaluate named entity representations of BERT-based NLP models by investigating their robustness to replacements from the same typed class in the input. We highlight that on several tasks while such perturbations are natural, state of the art trained models are surprisingly brittle. The brittleness continues even with the recent entity-aware BERT models. We also try to discern the cause of this non-robustness, considering factors such as tokenization and frequency of occurrence. Then we provide a simple method that ensembles predictions from multiple replacements while jointly modeling the uncertainty of type annotations and label predictions. Experiments on three NLP tasks shows that our method enhances robustness and increases accuracy on both natural and adversarial datasets.\n   Conditional BERT Contextual Augmentation  Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, Songlin Hu. (2018)\nConditional BERT Contextual Augmentation\nICCS\nPaper Link\nInfluential Citation Count (21), SS-ID (188024469a2443f262b3cbb5c5d4a96851949d68)\nABSTRACT\nData augmentation methods are often applied to prevent overfitting and improve generalization of deep neural network models. Recently proposed contextual augmentation augments labeled sentences by randomly replacing words with more varied substitutions predicted by language model. Bidirectional Encoder Representations from Transformers (BERT) demonstrates that a deep bidirectional language model is more powerful than either an unidirectional language model or the shallow concatenation of a forward and backward model. We propose a novel data augmentation method for labeled sentences called conditional BERT contextual augmentation. We retrofit BERT to conditional BERT by introducing a new conditional masked language model (The term “conditional masked language model” appeared once in original BERT paper, which indicates context-conditional, is equivalent to term “masked language model”. In our paper, “conditional masked language model” indicates we apply extra label-conditional constraint to the “masked language model”.) task. The well trained conditional BERT can be applied to enhance contextual augmentation. Experiments on six various different text classification tasks show that our method can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement.\n   Universal Adversarial Triggers for Attacking and Analyzing NLP  Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh. (2019)\nUniversal Adversarial Triggers for Attacking and Analyzing NLP\nEMNLP\nPaper Link\nInfluential Citation Count (41), SS-ID (18a1c21f35153c45d0ef30c564bffb7d70a13ccc)\nABSTRACT\nAdversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of “why” questions in SQuAD to be answered “to kill american people”, and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.\n   Learning and Evaluating General Linguistic Intelligence  Dani Yogatama, Cyprien de Masson d\u0026rsquo;Autume, Jerome T. Connor, Tomás Kociský, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, P. Blunsom. (2019)\nLearning and Evaluating General Linguistic Intelligence\nArXiv\nPaper Link\nInfluential Citation Count (6), SS-ID (19281b9ecdb5c07a93423a506627ab9d9b0cf039)\nABSTRACT\nWe define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language\u0026rsquo;s lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.\n   GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference  Ali Hadi Zadeh, A. Moshovos. (2020)\nGOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference\n2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)\nPaper Link\nInfluential Citation Count (4), SS-ID (1b0c8b26affd13e10ace5770e85478d60dcc368e)\nABSTRACT\nAttention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present GOBO, a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not require fine-tuning nor retraining to compensate for the quantization error. We present two practical hardware applications of GOBO. In the first GOBO reduces memory storage and traffic and as a result inference latency and energy consumption. This GOBO memory compression mechanism is plug-in compatible with many architectures; we demonstrate it with the TPU, Eyeriss, and an architecture using Tensor Cores-like units. Second, we present a co-designed hardware architecture that also reduces computation. Uniquely, the GOBO architecture maintains most of the weights in 3b even during computation, a property that: (i) makes the processing elements area efficient, allowing us to pack more compute power per unit area, (ii) replaces most multiply-accumulations with additions, and (iii) reduces the off-chip traffic by amplifying on-chip memory capacity.\n   SegaBERT: Pre-training of Segment-aware BERT for Language Understanding  He Bai, Peng Shi, Jimmy J. Lin, Luchen Tan, Kun Xiong, Wen Gao, Ming Li. (2020)\nSegaBERT: Pre-training of Segment-aware BERT for Language Understanding\nArXiv\nPaper Link\nInfluential Citation Count (2), SS-ID (1cc0b98b938b984e5da85f86c1a24099b9b4b582)\nABSTRACT\nPre-trained language models have achieved state-of-the-art results in various natural language processing tasks. Most of them are based on the Transformer architecture, which distinguishes tokens with the token position index of the input sequence. However, sentence index and paragraph index are also important to indicate the token position in a document. We hypothesize that better contextual representations can be generated from the text encoder with richer positional information. To verify this, we propose a segment-aware BERT, by replacing the token position embedding of Transformer with a combination of paragraph index, sentence index, and token index embeddings. We pre-trained the SegaBERT on the masked language modeling task in BERT but without any affiliated tasks. Experimental results show that our pre-trained model can outperform the original BERT model on various NLP tasks.\n   Attention is not Explanation  Sarthak Jain, Byron C. Wallace. (2019)\nAttention is not Explanation\nNAACL\nPaper Link\nInfluential Citation Count (36), SS-ID (1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f)\nABSTRACT\nAttention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations” for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.\n   HuggingFace's Transformers: State-of-the-art Natural Language Processing  Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, T. Rault, Rémi Louf, Morgan Funtowicz, Jamie Brew. (2019)\nHuggingFace\u0026rsquo;s Transformers: State-of-the-art Natural Language Processing\nArXiv\nPaper Link\nInfluential Citation Count (207), SS-ID (1fa9ed2bea208511ae698a967875e943049f16b6)\nABSTRACT\nRecent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.\n   Attention is All you Need  Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. (2017)\nAttention is All you Need\nNIPS\nPaper Link\nInfluential Citation Count (7560), SS-ID (204e3073870fae3d05bcbc2f6a8e263d9b72e776)\nABSTRACT\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n   The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks  Jonathan Frankle, Michael Carbin. (2018)\nThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\nICLR\nPaper Link\nInfluential Citation Count (229), SS-ID (21937ecd9d66567184b83eca3d3e09eb4e6fbd60)\nABSTRACT\nNeural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \u0026ldquo;lottery ticket hypothesis:\u0026rdquo; dense, randomly-initialized, feed-forward networks contain subnetworks (\u0026ldquo;winning tickets\u0026rdquo;) that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.\n   Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models  Cheolhyoung Lee, Kyunghyun Cho, Wanmo Kang. (2019)\nMixout: Effective Regularization to Finetune Large-scale Pretrained Language Models\nICLR\nPaper Link\nInfluential Citation Count (12), SS-ID (222b9a7b8038120671a1610e857d3edbc7ac5550)\nABSTRACT\nIn natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as \u0026ldquo;mixout\u0026rdquo;, motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE.\n   Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers  Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, K. Keutzer, D. Klein, Joseph Gonzalez. (2020)\nTrain Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers\nICML\nPaper Link\nInfluential Citation Count (6), SS-ID (2356781b8a98bf94e6fc73798c6cb65ac35e5f97)\nABSTRACT\nSince hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.\n   Parameter-Efficient Transfer Learning for NLP  N. Houlsby, A. Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, S. Gelly. (2019)\nParameter-Efficient Transfer Learning for NLP\nICML\nPaper Link\nInfluential Citation Count (112), SS-ID (29ddc1f43f28af7c846515e32cc167bc66886d0c)\nABSTRACT\nFine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter\u0026rsquo;s effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task.\n   Attention Module is Not Only a Weight: Analyzing Transformers with Vector Norms  Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui. (2020)\nAttention Module is Not Only a Weight: Analyzing Transformers with Vector Norms\nArXiv\nPaper Link\nInfluential Citation Count (0), SS-ID (2a8e42995caaedadc9dc739d85bed2c57fc78568)\nABSTRACT\nAttention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.\n   BERT-of-Theseus: Compressing BERT by Progressive Module Replacing  Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou. (2020)\nBERT-of-Theseus: Compressing BERT by Progressive Module Replacing\nEMNLP\nPaper Link\nInfluential Citation Count (14), SS-ID (2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7)\nABSTRACT\nIn this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.\n   Small and Practical BERT Models for Sequence Labeling  Henry Tsai, Jason Riesa, Melvin Johnson, N. Arivazhagan, Xin Li, Amelia Archer. (2019)\nSmall and Practical BERT Models for Sequence Labeling\nEMNLP\nPaper Link\nInfluential Citation Count (6), SS-ID (2f9d4887d0022400fc40c774c4c78350c3bc5390)\nABSTRACT\nWe propose a practical scheme to train a single multilingual sequence labeling model that yields state of the art results and is small and fast enough to run on a single CPU. Starting from a public multilingual BERT checkpoint, our final model is 6x smaller and 27x faster, and has higher accuracy than a state-of-the-art multilingual baseline. We show that our model especially outperforms on low-resource languages, and works on codemixed input text without being explicitly trained on codemixed examples. We showcase the effectiveness of our method by reporting on part-of-speech tagging and morphological prediction on 70 treebanks and 48 languages.\n   Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT  Shijie Wu, Mark Dredze. (2019)\nBeto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT\nEMNLP\nPaper Link\nInfluential Citation Count (28), SS-ID (2fa3f7ce620a1c7155daef6620dd6bb0e01934f3)\nABSTRACT\nPretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.\n   Pre-Training With Whole Word Masking for Chinese BERT  Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu. (2019)\nPre-Training With Whole Word Masking for Chinese BERT\nIEEE/ACM Transactions on Audio, Speech, and Language Processing\nPaper Link\nInfluential Citation Count (61), SS-ID (2ff41a463a374b138bb5a012e5a32bc4beefec20)\nABSTRACT\nBidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and its consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we aim to first introduce the whole word masking (wwm) strategy for Chinese BERT, along with a series of Chinese pre-trained language models. Then we also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways. Especially, we propose a new masking strategy called MLM as correction (Mac). To demonstrate the effectiveness of these models, we create a series of Chinese pre-trained language models as our baselines, including BERT, RoBERTa, ELECTRA, RBT, etc. We carried out extensive experiments on ten Chinese NLP tasks to evaluate the created Chinese pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. We open-source our pre-trained language models for further facilitating our research community.1\n   Whatcha lookin' at? DeepLIFTing BERT's Attention in Question Answering  Ekaterina Arkhangelskaia, Sourav Dutta. (2019)\nWhatcha lookin\u0026rsquo; at? DeepLIFTing BERT\u0026rsquo;s Attention in Question Answering\nArXiv\nPaper Link\nInfluential Citation Count (0), SS-ID (304b7c87e5c6e76ffcfdaa59fbd0656f9dab47d8)\nABSTRACT\nThere has been great success recently in tackling challenging NLP tasks by neural networks which have been pre-trained and fine-tuned on large amounts of task data. In this paper, we investigate one such model, BERT for question-answering, with the aim to analyze why it is able to achieve significantly better results than other models. We run DeepLIFT on the model predictions and test the outcomes to monitor shift in the attention values for input. We also cluster the results to analyze any possible patterns similar to human reasoning depending on the kind of input paragraph and question the model is trying to answer.\n   Understanding Multi-Head Attention in Abstractive Summarization  Joris Baan, Maartje ter Hoeve, M. V. D. Wees, Anne Schuth, M. de Rijke. (2019)\nUnderstanding Multi-Head Attention in Abstractive Summarization\nArXiv\nPaper Link\nInfluential Citation Count (1), SS-ID (317d2ac530e1db49229d6c442f50722db85afbb7)\nABSTRACT\nAttention mechanisms in deep learning architectures have often been used as a means of transparency and, as such, to shed light on the inner workings of the architectures. Recently, there has been a growing interest in whether or not this assumption is correct. In this paper we investigate the interpretability of multi-head attention in abstractive summarization, a sequence-to-sequence task for which attention does not have an intuitive alignment role, such as in machine translation. We first introduce three metrics to gain insight in the focus of attention heads and observe that these heads specialize towards relative positions, specific part-of-speech tags, and named entities. However, we also find that ablating and pruning these heads does not lead to a significant drop in performance, indicating redundancy. By replacing the softmax activation functions with sparsemax activation functions, we find that attention heads behave seemingly more transparent: we can ablate fewer heads and heads score higher on our interpretability metrics. However, if we apply pruning to the sparsemax model we find that we can prune even more heads, raising the question whether enforced sparsity actually improves transparency. Finally, we find that relative positions heads seem integral to summarization performance and persistently remain after pruning.\n   75 Languages, 1 Model: Parsing Universal Dependencies Universally  D. Kondratyuk. (2019)\n75 Languages, 1 Model: Parsing Universal Dependencies Universally\nEMNLP\nPaper Link\nInfluential Citation Count (33), SS-ID (31c872514c28a172f7f0221c8596aa5bfcdb9e98)\nABSTRACT\nWe present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging a multilingual BERT self-attention model pretrained on 104 languages, we found that fine-tuning it on all datasets concatenated together with simple softmax classifiers for each UD task can meet or exceed state-of-the-art UPOS, UFeats, Lemmas, (and especially) UAS, and LAS scores, without requiring any recurrent or language-specific components. We evaluate UDify for multilingual learning, showing that low-resource languages benefit the most from cross-linguistic annotations. We also evaluate for zero-shot learning, with results suggesting that multilingual training provides strong UD predictions even for languages that neither UDify nor BERT have ever been trained on.\n   exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models  Benjamin Hoover, Hendrik Strobelt, Sebastian Gehrmann. (2019)\nexBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models\nACL\nPaper Link\nInfluential Citation Count (4), SS-ID (327d7e55d64cb34d55bd3a3fe58233c238a312cd)\nABSTRACT\nLarge Transformer-based language models can route and reshape complex information via their multi-headed attention mechanism. Although the attention never receives explicit supervision, it can exhibit recognizable patterns following linguistic or positional information. Analyzing the learned representations and attentions is paramount to furthering our understanding of the inner workings of these models. However, analyses have to catch up with the rapid release of new models and the growing diversity of investigation techniques. To support analysis for a wide variety of models, we introduce exBERT, a tool to help humans conduct flexible, interactive investigations and formulate hypotheses for the model-internal reasoning process. exBERT provides insights into the meaning of the contextual representations and attention by matching a human-specified input to similar contexts in large annotated datasets. By aggregating the annotations of the matched contexts, exBERT can quickly replicate findings from literature and extend them to previously not analyzed models.\n   Data Augmentation using Pre-trained Transformer Models  Varun Kumar, Ashutosh Choudhary, Eunah Cho. (2020)\nData Augmentation using Pre-trained Transformer Models\nLIFELONGNLP\nPaper Link\nInfluential Citation Count (17), SS-ID (33496cb3a5623925267528fa6b726f015e4dcda2)\nABSTRACT\nLanguage model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of transformer based pre-trained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classification benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information.\n   What Does BERT Learn about the Structure of Language?  Ganesh Jawahar, Benoît Sagot, Djamé Seddah. (2019)\nWhat Does BERT Learn about the Structure of Language?\nACL\nPaper Link\nInfluential Citation Count (44), SS-ID (335613303ebc5eac98de757ed02a56377d99e03a)\nABSTRACT\nBERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT’s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.\n   Beyond Accuracy: Behavioral Testing of NLP Models with CheckList  Marco Tulio Ribeiro, Tongshuang Sherry Wu, Carlos Guestrin, Sameer Singh. (2020)\nBeyond Accuracy: Behavioral Testing of NLP Models with CheckList\nACL\nPaper Link\nInfluential Citation Count (61), SS-ID (33ec7eb2168e37e3007d1059aa96b9a63254b4da)\nABSTRACT\nAlthough measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.\n   Quantity doesn’t buy quality syntax with neural language models  Marten van Schijndel, Aaron Mueller, Tal Linzen. (2019)\nQuantity doesn’t buy quality syntax with neural language models\nEMNLP\nPaper Link\nInfluential Citation Count (0), SS-ID (356645552f8f40adf5a99b4e3a69f47699399010)\nABSTRACT\nRecurrent neural networks can learn to predict upcoming words remarkably well on average; in syntactically complex contexts, however, they often assign unexpectedly high probabilities to ungrammatical words. We investigate to what extent these shortcomings can be mitigated by increasing the size of the network and the corpus on which it is trained. We find that gains from increasing network size are minimal beyond a certain point. Likewise, expanding the training corpus yields diminishing returns; we estimate that the training corpus would need to be unrealistically large for the models to match human performance. A comparison to GPT and BERT, Transformer-based models trained on billions of words, reveals that these models perform even more poorly than our LSTMs in some constructions. Our results make the case for more data efficient architectures.\n   What do you mean?  M. Jackson. (1989)\nWhat do you mean?\nGeriatric nursing\nPaper Link\nInfluential Citation Count (17), SS-ID (357771514cfbbdc0ddafe1dfdf54eda3c42b325e)\nABSTRACT\n   The Lottery Ticket Hypothesis for Pre-trained BERT Networks  Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, Michael Carbin. (2020)\nThe Lottery Ticket Hypothesis for Pre-trained BERT Networks\nNeurIPS\nPaper Link\nInfluential Citation Count (18), SS-ID (389036b1366b64579725457993c1f63a4f3370ba)\nABSTRACT\nIn natural language processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the lottery ticket hypothesis has shown that models for NLP and computer vision contain smaller matching subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed find matching subnetworks at 40% to 90% sparsity. We find these subnetworks at (pre-trained) initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task (the same task used to pre-train the model) transfer universally; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context. Codes available at this https URL.\n   BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension  M. Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer. (2019)\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nACL\nPaper Link\nInfluential Citation Count (481), SS-ID (395de0bd3837fdf4b4b5e5f04835bcc69c279481)\nABSTRACT\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.\n   Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking  Samuel Broscheit. (2019)\nInvestigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking\nCoNLL\nPaper Link\nInfluential Citation Count (6), SS-ID (399308fa54ade9b1362d56628132323489ce50cd)\nABSTRACT\nA typical architecture for end-to-end entity linking systems consists of three steps: mention detection, candidate generation and entity disambiguation. In this study we investigate the following questions: (a) Can all those steps be learned jointly with a model for contextualized text-representations, i.e. BERT? (b) How much entity knowledge is already contained in pretrained BERT? (c) Does additional entity knowledge improve BERT’s performance in downstream tasks? To this end we propose an extreme simplification of the entity linking setup that works surprisingly well: simply cast it as a per token classification over the entire entity vocabulary (over 700K classes in our case). We show on an entity linking benchmark that (i) this model improves the entity representations over plain BERT, (ii) that it outperforms entity linking architectures that optimize the tasks separately and (iii) that it only comes second to the current state-of-the-art that does mention detection and entity disambiguation jointly. Additionally, we investigate the usefulness of entity-aware token-representations in the text-understanding benchmark GLUE, as well as the question answering benchmarks SQUAD~V2 and SWAG and also the EN-DE WMT14 machine translation benchmark. To our surprise, we find that most of those benchmarks do not benefit from additional entity knowledge, except for a task with very small training data, the RTE task in GLUE, which improves by 2%.\n   Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT  Zhiyong Wu, Yun Chen, B. Kao, Qun Liu. (2020)\nPerturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT\nACL\nPaper Link\nInfluential Citation Count (10), SS-ID (3aaa8aaad5ef36550a6b47d6ee000f0b346a5a1f)\nABSTRACT\nBy introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.\n   Cross-Lingual Ability of Multilingual BERT: An Empirical Study  Karthikeyan K, Zihan Wang, Stephen Mayhew, D. Roth. (2019)\nCross-Lingual Ability of Multilingual BERT: An Empirical Study\nICLR\nPaper Link\nInfluential Citation Count (7), SS-ID (3b2538f84812f434c740115c185be3e5e216c526)\nABSTRACT\nRecent work has exhibited the surprising cross-lingual abilities of multilingual BERT (M-BERT) \u0026ndash; surprising since it is trained without any cross-lingual objective and with no aligned data. In this work, we provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability. We study the impact of linguistic properties of the languages, the architecture of the model, and the learning objectives. The experimental study is done in the context of three typologically different languages \u0026ndash; Spanish, Hindi, and Russian \u0026ndash; and using two conceptually different NLP tasks, textual entailment and named entity recognition. Among our key conclusions is the fact that the lexical overlap between languages plays a negligible role in the cross-lingual success, while the depth of the network is an integral part of it. All our models and implementations can be found on our project page: this http URL .\n   Reducing BERT Pre-Training Time from 3 Days to 76 Minutes  Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, J. Demmel, Cho-Jui Hsieh. (2019)\nReducing BERT Pre-Training Time from 3 Days to 76 Minutes\nArXiv\nPaper Link\nInfluential Citation Count (13), SS-ID (3c6dca9041f54583aeab60587c9e6e9272104dc1)\nABSTRACT\nLarge-batch training is key to speeding up deep neural network training in large distributed systems. However, large-batch training is difficult because it produces a generalization gap. Straightforward optimization often leads to accuracy loss on the test set. BERT \\cite{devlin2018bert} is a state-of-the-art deep learning model that builds on top of deep bidirectional transformers for language understanding. Previous large-batch training techniques do not perform well for BERT when we scale the batch size (e.g. beyond 8192). BERT pre-training also takes a long time to finish (around three days on 16 TPUv3 chips). To solve this problem, we propose the LAMB optimizer, which helps us to scale the batch size to 65536 without losing accuracy. LAMB is a general optimizer that works for both small and large batch sizes and does not need hyper-parameter tuning besides the learning rate. The baseline BERT-Large model needs 1 million iterations to finish pre-training, while LAMB with batch size 65536/32768 only needs 8599 iterations. We push the batch size to the memory limit of a TPUv3 pod and can finish BERT training in 76 minutes.\n   Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs  Alex Warstadt, Yuning Cao, Ioana Grosu, Wei Peng, Hagen Blix, Yining Nie, Anna Alsop, Shikha Bordia, Haokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason Phang, Anhad Mohananey, Phu Mon Htut, Paloma Jeretic, Samuel R. Bowman. (2019)\nInvestigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs\nEMNLP\nPaper Link\nInfluential Citation Count (4), SS-ID (3cd331c997e90f737810aad6fcce4d993315189f)\nABSTRACT\nThough state-of-the-art sentence representation models can perform tasks requiring significant knowledge of grammar, it is an open question how best to evaluate their grammatical knowledge. We explore five experimental methods inspired by prior work evaluating pretrained sentence representation models. We use a single linguistic phenomenon, negative polarity item (NPI) licensing, as a case study for our experiments. NPIs like any are grammatical only if they appear in a licensing environment like negation (Sue doesn’t have any cats vs. *Sue has any cats). This phenomenon is challenging because of the variety of NPI licensing environments that exist. We introduce an artificially generated dataset that manipulates key features of NPI licensing for the experiments. We find that BERT has significant knowledge of these features, but its success varies widely across different experimental methods. We conclude that a variety of methods is necessary to reveal all relevant aspects of a model’s grammatical knowledge in a given domain.\n   Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer  Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. (2019)\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nJ. Mach. Learn. Res.\nPaper Link\nInfluential Citation Count (615), SS-ID (3cfb319689f06bf04c2e28399361f414ca32c4b3)\nABSTRACT\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \u0026ldquo;Colossal Clean Crawled Corpus\u0026rdquo;, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.\n   Deepening Hidden Representations from Pre-trained Language Models for Natural Language Understanding  Jie Yang, Hai Zhao. (2019)\nDeepening Hidden Representations from Pre-trained Language Models for Natural Language Understanding\nArXiv\nPaper Link\nInfluential Citation Count (0), SS-ID (3ee7b17cc627ac5bc99632a22ef820dc559393e6)\nABSTRACT\nTransformer-based pre-trained language models have proven to be effective for learning contextualized language representation. However, current approaches only take advantage of the output of the encoder\u0026rsquo;s final layer when fine-tuning the downstream tasks. We argue that only taking single layer\u0026rsquo;s output restricts the power of pre-trained representation. Thus we deepen the representation learned by the model by fusing the hidden representation in terms of an explicit HIdden Representation Extractor (HIRE), which automatically absorbs the complementary representation with respect to the output from the final layer. Utilizing RoBERTa as the backbone encoder, our proposed improvement over the pre-trained models is shown effective on multiple natural language understanding tasks and help our model rival with the state-of-the-art models on the GLUE benchmark.\n   Improving Transformer Models by Reordering their Sublayers  Ofir Press, Noah A. Smith, Omer Levy. (2019)\nImproving Transformer Models by Reordering their Sublayers\nACL\nPaper Link\nInfluential Citation Count (2), SS-ID (3ff8d265f4351e4b1fdac5b586466bee0b5d6fff)\nABSTRACT\nMultilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.\n   Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference  R. Thomas McCoy, Ellie Pavlick, Tal Linzen. (2019)\nRight for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\nACL\nPaper Link\nInfluential Citation Count (103), SS-ID (42ed4a9994e6121a9f325f5b901c5b3d7ce104f5)\nABSTRACT\nA machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.\n   PoWER-BERT: Accelerating BERT inference for Classification Tasks  Saurabh Goyal, Anamitra R. Choudhury, Venkatesan T. Chakaravarthy, Saurabh ManishRaje, Yogish Sabharwal, Ashish Verma. (2020)\nPoWER-BERT: Accelerating BERT inference for Classification Tasks\nArXiv\nPaper Link\nInfluential Citation Count (1), SS-ID (4510d9ad22f474c30c530ae7f886ec4d42402d68)\nABSTRACT\nBERT has emerged as a popular model for natural language understanding. Given its computeintensive nature, even for inference, many recent studies have considered optimization of two important performance characteristics: model size and inference time. We consider classification tasks and propose a novel method, called PoWER-BERT, for improving the inference time for the BERT model without significant loss in the accuracy. The method works by eliminating word-vectors (intermediate vector outputs) from the encoder pipeline. We design a strategy for measuring the significance of the word-vectors based on the self-attention mechanism of the encoders which helps us identify the word-vectors to be eliminated. Experimental evaluation on the standard GLUE benchmark shows that PoWER-BERT achieves up to 4.5x reduction in inference time over BERT with \u0026lt; 1% loss in accuracy. We show that compared to the prior inference time reduction methods, PoWER-BERT offers better trade-off between accuracy and inference time. Lastly, we demonstrate that our scheme can also be used in conjunction with ALBERT (a highly compressed version of BERT) and can attain up to 6.8x factor reduction in inference time with \u0026lt; 1% loss in accuracy.\n   A Structural Probe for Finding Syntax in Word Representations  John Hewitt, Christopher D. Manning. (2019)\nA Structural Probe for Finding Syntax in Word Representations\nNAACL\nPaper Link\nInfluential Citation Count (30), SS-ID (455a8838cde44f288d456d01c76ede95b56dc675)\nABSTRACT\nRecent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network’s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models’ vector geometry.\n   What do you mean, BERT? Assessing BERT as a Distributional Semantics Model  Timothee Mickus, Denis Paperno, Mathieu Constant, Kees van Deemter. (2019)\nWhat do you mean, BERT? Assessing BERT as a Distributional Semantics Model\nArXiv\nPaper Link\nInfluential Citation Count (3), SS-ID (4bff291cf7fa02a0dbac767aba55d43ad8c59055)\nABSTRACT\nContextualized word embeddings, i.e. vector representations for words in context, are naturally seen as an extension of previous noncontextual distributional semantic models. In this work, we focus on BERT, a deep neural network that produces contextualized embeddings and has set the state-of-the-art in several semantic tasks, and study the semantic coherence of its embedding space. While showing a tendency towards coherence, BERT does not fully live up to the natural expectations for a semantic vector space. In particular, we find that the position of the sentence in which a word occurs, while having no meaning correlates, leaves a noticeable trace on the word embeddings and disturbs similarity relationships.\n   Structured Pruning of a BERT-based Question Answering Model  J. Scott McCarley, Rishav Chakravarti, Avirup Sil. (2019)\nStructured Pruning of a BERT-based Question Answering Model\nPaper Link\nInfluential Citation Count (4), SS-ID (4d8a4509753cc91832f80ec35795064e79630ef3)\nABSTRACT\nThe recent trend in industry-setting Natural Language Processing (NLP) research has been to operate large scale pretrained language models like BERT under strict computational limits. While most model compression work has focused on \u0026ldquo;distilling\u0026rdquo; a general-purpose language representation using expensive pretraining distillation, much less attention has been paid to creating smaller task-specific language representations which, arguably, are more useful in an industry setting. In this paper, we investigate compressing BERT- and RoBERTa-based question answering systems by structured pruning of parameters from the underlying trained transformer model. We find that an inexpensive combination of task-specific structured pruning and task-specific distillation, without the expense of pretraining distillation, yields highly-performing models across a range of speed/accuracy tradeoff operating points. We start from full-size models trained for SQuAD 2.0 or Natural Questions and introduce gates that allow selected parts of transformers to be individually eliminated. Specifically, we investigate (1) structured pruning to reduce the number of parameters in each transformer layer, (2) applicability to both BERT- and RoBERTa-based models, (3) applicability to both SQuAD 2.0 and Natural Questions, and (4) combining structured pruning with distillation. We find that pruning a combination of attention heads and the feed-forward layer yields a near-doubling of inference speed on SQuAD 2.0, with less than a 1.5 F1-point loss in accuracy. Furthermore, we find that a combination of distillation and structured pruning almost doubles the inference speed of RoBERTa-large based model for Natural Questions, while losing less than 0.5 F1-point on short answers.\n   K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters  Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, Ming Zhou. (2020)\nK-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters\nFINDINGS\nPaper Link\nInfluential Citation Count (32), SS-ID (4f03e69963b9649950ba29ae864a0de8c14f1f86)\nABSTRACT\nWe study the problem of injecting knowledge into large pre-trained models like BERT and RoBERTa. Existing methods typically update the original parameters of pre-trained models when injecting knowledge. However, when multiple kinds of knowledge are injected, they may suffer from catastrophic forgetting. To address this, we propose K-Adapter, which remains the original parameters of the pre-trained model fixed and supports continual knowledge infusion. Taking RoBERTa as the pre-trained model, K-Adapter has a neural adapter for each kind of infused knowledge, like a plug-in connected to RoBERTa. There is no information flow between different adapters, thus different adapters are efficiently trained in a distributed way. We inject two kinds of knowledge, including factual knowledge obtained from automatically aligned text-triplets on Wikipedia and Wikidata, and linguistic knowledge obtained from dependency parsing. Results on three knowledge-driven tasks (total six datasets) including relation classification, entity typing and question answering demonstrate that each adapter improves the performance, and the combination of both adapters brings further improvements. Probing experiments further indicate that K-Adapter captures richer factual and commonsense knowledge than RoBERTa.\n   Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT  Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Z. Yao, A. Gholami, Michael W. Mahoney, K. Keutzer. (2019)\nQ-BERT: Hessian Based Ultra Low Precision Quantization of BERT\nAAAI\nPaper Link\nInfluential Citation Count (22), SS-ID (4fb8fd55b476909a26a8dc594e0ae98d4923ad4d)\nABSTRACT\nTransformer based architectures have become de-facto models used for a range of Natural Language Processing tasks. In particular, the BERT based models achieved significant accuracy gain for GLUE tasks, CoNLL-03 and SQuAD. However, BERT based models have a prohibitive memory footprint and latency. As a result, deploying BERT based models in resource constrained environments has become a challenging task. In this work, we perform an extensive analysis of fine-tuned BERT models using second order Hessian information, and we use our results to propose a novel method for quantizing BERT models to ultra low precision. In particular, we propose a new group-wise quantization scheme, and we use Hessian-based mix-precision method to compress the model further. We extensively test our proposed method on BERT downstream tasks of SST-2, MNLI, CoNLL-03, and SQuAD. We can achieve comparable performance to baseline with at most 2.3% performance degradation, even with ultra-low precision quantization down to 2 bits, corresponding up to 13× compression of the model parameters, and up to 4× compression of the embedding table as well as activations. Among all tasks, we observed the highest performance loss for BERT fine-tuned on SQuAD. By probing into the Hessian based analysis as well as visualization, we show that this is related to the fact that current training/fine-tuning strategy of BERT does not converge for SQuAD.\n   Parsing as Pretraining  David Vilares, Michalina Strzyz, Anders Søgaard, Carlos G\u0026rsquo;omez-Rodr\u0026rsquo;iguez. (2020)\nParsing as Pretraining\nAAAI\nPaper Link\nInfluential Citation Count (2), SS-ID (50be7d2858523d0e63174d974f380349fca0d666)\nABSTRACT\nRecent analyses suggest that encoders pretrained for language modeling capture certain morpho-syntactic structure. However, probing frameworks for word vectors still do not report results on standard setups such as constituent and dependency parsing. This paper addresses this problem and does full parsing (on English) relying only on pretraining architectures – and no decoding. We first cast constituent and dependency parsing as sequence tagging. We then use a single feed-forward layer to directly map word vectors to labels that encode a linearized tree. This is used to: (i) see how far we can reach on syntax modelling with just pretrained encoders, and (ii) shed some light about the syntax-sensitivity of different word vectors (by freezing the weights of the pretraining network during training). For evaluation, we use bracketing F1-score and las, and analyze in-depth differences across representations for span lengths and dependency displacements. The overall results surpass existing sequence tagging parsers on the ptb (93.5%) and end-to-end en-ewt ud (78.8%).\n   Reweighted Proximal Pruning for Large-Scale Language Representation  Fu-Ming Guo, Sijia Liu, F. Mungall, Xue Lin, Yanzhi Wang. (2019)\nReweighted Proximal Pruning for Large-Scale Language Representation\nArXiv\nPaper Link\nInfluential Citation Count (3), SS-ID (540f074cb6f16563a357741837e41c44c0a38234)\nABSTRACT\nRecently, pre-trained language representation flourishes as the mainstay of the natural language understanding community, e.g., BERT. These pre-trained language representations can create state-of-the-art results on a wide range of downstream tasks. Along with continuous significant performance improvement, the size and complexity of these pre-trained neural models continue to increase rapidly. Is it possible to compress these large-scale language representation models? How will the pruned language representation affect the downstream multi-task transfer learning objectives? In this paper, we propose Reweighted Proximal Pruning (RPP), a new pruning method specifically designed for a large-scale language representation model. Through experiments on SQuAD and the GLUE benchmark suite, we show that proximal pruned BERT keeps high accuracy for both the pre-training task and the downstream multiple fine-tuning tasks at high prune ratio. RPP provides a new perspective to help us analyze what large-scale language representation might learn. Additionally, RPP makes it possible to deploy a large state-of-the-art language representation model such as BERT on a series of distinct devices (e.g., online servers, mobile phones, and edge devices).\n   The Berkeley FrameNet Project  Collin F. Baker, C. Fillmore, J. Lowe. (1998)\nThe Berkeley FrameNet Project\nCOLING-ACL\nPaper Link\nInfluential Citation Count (436), SS-ID (547f23597f9ec8a93f66cedaa6fbfb73960426b1)\nABSTRACT\nFrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, \u0026ldquo;Tools for Lexicon Building\u0026rdquo;). The project\u0026rsquo;s key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between \u0026ldquo;frame elements\u0026rdquo; and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project\u0026rsquo;s goals and workflow, and information about the computational tools that have been adapted or created in-house for this work.\n   SesameBERT: Attention for Anywhere  Ta-Chun Su, Hsiang-Chih Cheng. (2019)\nSesameBERT: Attention for Anywhere\n2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)\nPaper Link\nInfluential Citation Count (0), SS-ID (553c1048e90e84575cad9016f367cf69c52a7fd7)\nABSTRACT\nFine-tuning with pre-trained models has achieved exceptional results for many language tasks. In this study, we focused on one such self-attention network model, namely BERT, which has performed well in terms of stacking layers across diverse language-understanding benchmarks. However, in many downstream tasks, information between layers is ignored by BERT for fine-tuning. In addition, although self-attention networks are well-known for their ability to capture global dependencies, room for improvement remains in terms of emphasizing the importance of local contexts. In light of these advantages and disadvantages, this paper proposes SesameBERT, a generalized fine-tuning method that (1) enables the extraction of global information among all layers through Squeeze and Excitation and (2) enriches local information by capturing neighboring contexts via Gaussian blurring. Furthermore, we demonstrated the effectiveness of our approach in the HANS dataset, which is used to determine whether models have adopted shallow heuristics instead of learning underlying generalizations. The experiments revealed that SesameBERT outperformed BERT with respect to GLUE benchmark and the HANS evaluation set.\n   Association for Computational Linguistics  D. Litman, J. Hirschberg, M. Swerts, Scott Miller, L. Ramshaw, R. Weischedel, Eugene Charniak, Lillian Lee. (2001)\nAssociation for Computational Linguistics\nPaper Link\nInfluential Citation Count (67), SS-ID (566eb7be43b8a2b2daff82b03711098a84859b2a)\nABSTRACT\n   KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation  Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juan-Zi Li, Jian Tang. (2019)\nKEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\nTransactions of the Association for Computational Linguistics\nPaper Link\nInfluential Citation Count (40), SS-ID (56cafbac34f2bb3f6a9828cd228ff281b810d6bb)\nABSTRACT\nAbstract Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG/KEPLER.\n   Semantics-aware BERT for Language Understanding  Zhuosheng Zhang, Yuwei Wu, Zhao Hai, Z. Li, Shuailiang Zhang, Xi Zhou, Xiang Zhou. (2019)\nSemantics-aware BERT for Language Understanding\nAAAI\nPaper Link\nInfluential Citation Count (27), SS-ID (5744f56d3253bd7c4341d36de40a93fceaa266b3)\nABSTRACT\nThe latest work on language representations carefully integrates contextualized features into language model training, which enables a series of success especially in various machine reading comprehension and natural language inference tasks. However, the existing language representation models including ELMo, GPT and BERT only exploit plain context-sensitive features such as character or word embeddings. They rarely consider incorporating structured semantic information which can provide rich semantics for language representation. To promote natural language understanding, we propose to incorporate explicit contextual semantics from pre-trained semantic role labeling, and introduce an improved language representation model, Semantics-aware BERT (SemBERT), which is capable of explicitly absorbing contextual semantics over a BERT backbone. SemBERT keeps the convenient usability of its BERT precursor in a light fine-tuning way without substantial task-specific modifications. Compared with BERT, semantics-aware BERT is as simple in concept but more powerful. It obtains new state-of-the-art or substantially improves results on ten reading comprehension and language inference tasks.\n   Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation  Goran Glavas, Ivan Vulic. (2020)\nIs Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation\nEACL\nPaper Link\nInfluential Citation Count (0), SS-ID (575ac3f36e9fddeb258e2f639e26a6a7ec35160a)\nABSTRACT\nTraditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the model for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experiments (with intermediate target-language parsing) show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers’ representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models?\n   Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation  Alessandro Raganato, Yves Scherrer, J. Tiedemann. (2020)\nFixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation\nFINDINGS\nPaper Link\nInfluential Citation Count (5), SS-ID (57f123c95ecf9d901be3a53291f53302740451e2)\nABSTRACT\nTransformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed – non-learnable – attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.\n   What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?  Chenglei Si, Shuohang Wang, Min-Yen Kan, Jing Jiang. (2019)\nWhat does BERT Learn from Multiple-Choice Reading Comprehension Datasets?\nArXiv\nPaper Link\nInfluential Citation Count (2), SS-ID (59abe3db26b55c8837a1f2babb87350ba95ab1c0)\nABSTRACT\nMultiple-Choice Reading Comprehension (MCRC) requires the model to read the passage and question, and select the correct answer among the given options. Recent state-of-the-art models have achieved impressive performance on multiple MCRC datasets. However, such performance may not reflect the model\u0026rsquo;s true ability of language understanding and reasoning. In this work, we adopt two approaches to investigate what BERT learns from MCRC datasets: 1) an un-readable data attack, in which we add keywords to confuse BERT, leading to a significant performance drop; and 2) an un-answerable data training, in which we train BERT on partial or shuffled input. Under un-answerable data training, BERT achieves unexpectedly high performance. Based on our experiments on the 5 key MCRC datasets - RACE, MCTest, MCScript, MCScript2.0, DREAM - we observe that 1) fine-tuned BERT mainly learns how keywords lead to correct prediction, instead of learning semantic understanding and reasoning; and 2) BERT does not need correct syntactic information to solve the task; 3) there exists artifacts in these datasets such that they can be solved even without the full context.\n   Efficient Training of BERT by Progressively Stacking  Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, Tie-Yan Liu. (2019)\nEfficient Training of BERT by Progressively Stacking\nICML\nPaper Link\nInfluential Citation Count (7), SS-ID (5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88)\nABSTRACT\nUnsupervised pre-training is commonly used in natural language processing: a deep neural network trained with proper unsupervised prediction tasks are shown to be effective in many downstream tasks. Because it is easy to create a large monolingual dataset by collecting data from the Web, we can train high-capacity models. Therefore, training efficiency becomes a critical issue even when using high-performance hardware. In this paper, we explore an efficient training method for the state-of-the-art bidirectional Transformer (BERT) model. By visualizing the self-attention distributions of different layers at different positions in a well-trained BERT model, we find that in most layers, the self-attention distribution will concentrate locally around its position and the start-of-sentence token. Motivated by this, we propose the stacking algorithm to transfer knowledge from a shallow model to a deep model; then we apply stacking progressively to accelerate BERT training. Experiments showed that the models trained by our training strategy achieve similar performance to models trained from scratch, but our algorithm is much faster.\n   What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge  Kyle Richardson, Ashish Sabharwal. (2019)\nWhat Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge\nTransactions of the Association for Computational Linguistics\nPaper Link\nInfluential Citation Count (2), SS-ID (5a9001cdccdb8b1de227a45eccc503d32d1a2464)\nABSTRACT\nAbstract Open-domain question answering (QA) involves many knowledge and reasoning challenges, but are successful QA models actually learning such knowledge when trained on benchmark QA tasks? We investigate this via several new diagnostic tasks probing whether multiple-choice QA models know definitions and taxonomic reasoning—two skills widespread in existing benchmarks and fundamental to more complex reasoning. We introduce a methodology for automatically building probe datasets from expert knowledge sources, allowing for systematic control and a comprehensive evaluation. We include ways to carefully control for artifacts that may arise during this process. Our evaluation confirms that transformer-based multiple-choice QA models are already predisposed to recognize certain types of structural linguistic knowledge. However, it also reveals a more nuanced picture: their performance notably degrades even with a slight increase in the number of “hops” in the underlying taxonomic hierarchy, and with more challenging distractor candidates. Further, existing models are far from perfect when assessed at the level of clusters of semantically connected probes, such as all hypernym questions about a single concept.\n   How Language-Neutral is Multilingual BERT?  Jindřich Libovický, Rudolf Rosa, Alexander M. Fraser. (2019)\nHow Language-Neutral is Multilingual BERT?\nArXiv\nPaper Link\nInfluential Citation Count (12), SS-ID (5d8beeca1a2e3263b2796e74e2f57ffb579737ee)\nABSTRACT\nMultilingual BERT (mBERT) provides sentence representations for 104 languages, which are useful for many multi-lingual tasks. Previous work probed the cross-linguality of mBERT using zero-shot transfer learning on morphological and syntactic tasks. We instead focus on the semantic properties of mBERT. We show that mBERT representations can be split into a language-specific component and a language-neutral component, and that the language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment and sentence retrieval but is not yet good enough for the more difficult task of MT quality estimation. Our work presents interesting challenges which must be solved to build better language-neutral representations, particularly for tasks requiring linguistic transfer of semantics.\n   Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering  Changmao Li, Jinho D. Choi. (2020)\nTransformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering\nACL\nPaper Link\nInfluential Citation Count (3), SS-ID (5dd520b6c92aae3fd76df5bb61014e50fab93817)\nABSTRACT\nWe introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue. First, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts. Then, multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering (QA). Our approach is evaluated on the FriendsQA dataset and shows improvements of 3.8% and 1.4% over the two state-of-the-art transformer models, BERT and RoBERTa, respectively.\n   oLMpics-On What Language Model Pre-training Captures  Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant. (2019)\noLMpics-On What Language Model Pre-training Captures\nTransactions of the Association for Computational Linguistics\nPaper Link\nInfluential Citation Count (17), SS-ID (5e0cffc51e8b64a8f11326f955fa4b4f1803e3be)\nABSTRACT\nAbstract Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models, and objective functions for pre-training.\n   On Measuring Social Biases in Sentence Encoders  Chandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, Rachel Rudinger. (2019)\nOn Measuring Social Biases in Sentence Encoders\nNAACL\nPaper Link\nInfluential Citation Count (25), SS-ID (5e9c85235210b59a16bdd84b444a904ae271f7e7)\nABSTRACT\nThe Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test’s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.\n   ERNIE: Enhanced Language Representation with Informative Entities  Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu. (2019)\nERNIE: Enhanced Language Representation with Informative Entities\nACL\nPaper Link\nInfluential Citation Count (90), SS-ID (5f994dc8cae24ca9d1ed629e517fcc652660ddde)\nABSTRACT\nNeural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.\n   MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers  Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou. (2020)\nMiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nNeurIPS\nPaper Link\nInfluential Citation Count (42), SS-ID (60a4a3a886338d0c8e3579d392cb32f493430255)\nABSTRACT\nPre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.\n   Symmetric Regularization based BERT for Pair-wise Semantic Reasoning  Xingyi Cheng, Weidi Xu, Kunlong Chen, Wei Wang, Bin Bi, Ming Yan, Chen Wu, Luo Si, Wei Chu, Taifeng Wang. (2019)\nSymmetric Regularization based BERT for Pair-wise Semantic Reasoning\nSIGIR\nPaper Link\nInfluential Citation Count (1), SS-ID (63f9e2417563456f91c7e5586d43eb25c00a0c19)\nABSTRACT\nThe ability of semantic reasoning over the sentence pair is essential for many natural language understanding tasks, e.g., natural language inference and machine reading comprehension. A recent significant improvement in these tasks comes from BERT. As reported, the next sentence prediction (NSP) in BERT is of great significance for downstream problems with sentence-pair input. Despite its effectiveness, NSP still lacks the essential signal to distinguish between entailment and shallow correlation. To remedy this, we propose to augment the NSP task to a multi-class categorization task, which includes previous sentence prediction (PSP). This task encourages the model to learn the subtle semantics, thereby improves the ability of semantic understanding. Furthermore, by using a smoothing technique, the scopes of NSP and PSP are expanded into a broader range which includes close but nonsuccessive sentences. This simple method yields remarkable improvement against vanilla BERT. Our method consistently improves the performance on the NLI and MRC benchmarks by a large margin, including the challenging HANS dataset.\n   Does BERT agree? Evaluating knowledge of structure dependence through agreement relations  Geoff Bacon, T. Regier. (2019)\nDoes BERT agree? Evaluating knowledge of structure dependence through agreement relations\nArXiv\nPaper Link\nInfluential Citation Count (0), SS-ID (645a96e5c474d919415850892880005e4ad3fb43)\nABSTRACT\nLearning representations that accurately model semantics is an important goal of natural language processing research. Many semantic phenomena depend on syntactic structure. Recent work examines the extent to which state-of-the-art models for pre-training representations, such as BERT, capture such structure-dependent phenomena, but is largely restricted to one phenomenon in English: number agreement between subjects and verbs. We evaluate BERT\u0026rsquo;s sensitivity to four types of structure-dependent agreement relations in a new semi-automatically curated dataset across 26 languages. We show that both the single-language and multilingual BERT models capture syntax-sensitive agreement patterns well in general, but we also highlight the specific linguistic contexts in which their performance degrades.\n   Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks  Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, M. Zhou. (2019)\nUnicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks\nEMNLP\nPaper Link\nInfluential Citation Count (17), SS-ID (65f788fb964901e3f1149a0a53317535ca85ed7d)\nABSTRACT\nWe present Unicoder, a universal language encoder that is insensitive to different languages. Given an arbitrary NLP task, a model can be trained with Unicoder using training data in one language and directly applied to inputs of the same task in other languages. Comparing to similar efforts such as Multilingual BERT and XLM , three new cross-lingual pre-training tasks are proposed, including cross-lingual word recovery, cross-lingual paraphrase classification and cross-lingual masked language model. These tasks help Unicoder learn the mappings among different languages from more perspectives. We also find that doing fine-tuning on multiple languages together can bring further improvement. Experiments are performed on two tasks: cross-lingual natural language inference (XNLI) and cross-lingual question answering (XQA), where XLM is our baseline. On XNLI, 1.8% averaged accuracy improvement (on 15 languages) is obtained. On XQA, which is a new cross-lingual dataset built by us, 5.5% averaged accuracy improvement (on French and German) is obtained.\n   BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning  Asa Cooper Stickland, Iain Murray. (2019)\nBERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning\nICML\nPaper Link\nInfluential Citation Count (13), SS-ID (660d3472d9c3733dedcf911187b234f2b65561b5)\nABSTRACT\nMulti-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsupervised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a single BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or `projected attention layers\u0026rsquo;, we match the performance of separately fine-tuned models on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.\n   Analysis Methods in Neural Language Processing: A Survey  Y. Belinkov, James R. Glass. (2018)\nAnalysis Methods in Neural Language Processing: A Survey\nTACL\nPaper Link\nInfluential Citation Count (17), SS-ID (668f42a4d4094f0a66d402a16087e14269b31a1f)\nABSTRACT\nThe field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.\n   Movement Pruning: Adaptive Sparsity by Fine-Tuning  Victor Sanh, Thomas Wolf, Alexander M. Rush. (2020)\nMovement Pruning: Adaptive Sparsity by Fine-Tuning\nNeurIPS\nPaper Link\nInfluential Citation Count (20), SS-ID (66f0f35fc78bdf2af9de46093d49a428970cde2e)\nABSTRACT\nMagnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters.\n   Probing Natural Language Inference Models through Semantic Fragments  Kyle Richardson, Hai Hu, L. Moss, Ashish Sabharwal. (2019)\nProbing Natural Language Inference Models through Semantic Fragments\nAAAI\nPaper Link\nInfluential Citation Count (12), SS-ID (681fbcd98acf20df3355eff3585994bd1f9008b7)\nABSTRACT\nDo state-of-the-art models for language understanding already have, or can they easily learn, abilities such as boolean coordination, quantification, conditionals, comparatives, and monotonicity reasoning (i.e., reasoning about word substitutions in sentential contexts)? While such phenomena are involved in natural language inference (NLI) and go beyond basic linguistic understanding, it is unclear the extent to which they are captured in existing NLI benchmarks and effectively learned by models. To investigate this, we propose the use of semantic fragments—systematically generated datasets that each target a different semantic phenomenon—for probing, and efficiently improving, such capabilities of linguistic models. This approach to creating challenge datasets allows direct control over the semantic diversity and complexity of the targeted linguistic phenomena, and results in a more precise characterization of a model\u0026rsquo;s linguistic behavior. Our experiments, using a library of 8 such semantic fragments, reveal two remarkable findings: (a) State-of-the-art models, including BERT, that are pre-trained on existing NLI benchmark datasets perform poorly on these new fragments, even though the phenomena probed here are central to the NLI task; (b) On the other hand, with only a few minutes of additional fine-tuning—with a carefully selected learning rate and a novel variation of “inoculation”—a BERT-based model can master all of these logic and monotonicity fragments while retaining its performance on established NLI benchmarks.\n   Language Models are Few-Shot Learners  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, Rewon Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. (2020)\nLanguage Models are Few-Shot Learners\nNeurIPS\nPaper Link\nInfluential Citation Count (428), SS-ID (6b85b63579a916f705a8e10a49bd8d849d91b1fc)\nABSTRACT\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\u0026rsquo;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n   Span Selection Pre-training for Question Answering  Michael R. Glass, A. Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, G P Shrivatsa Bhargav, Dinesh Garg, Avirup Sil. (2019)\nSpan Selection Pre-training for Question Answering\nACL\nPaper Link\nInfluential Citation Count (2), SS-ID (6c8503803760c5c7790f72437d0f8b874334e6f0)\nABSTRACT\nBERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA). BERT is pretrained on two auxiliary tasks: Masked Language Model and Next Sentence Prediction. In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding. Span Selection PreTraining (SSPT) poses cloze-like training instances, but rather than draw the answer from the model’s parameters, it is selected from a relevant passage. We find significant and consistent improvements over both BERT-BASE and BERT-LARGE on multiple Machine Reading Comprehension (MRC) datasets. Specifically, our proposed model has strong empirical evidence as it obtains SOTA results on Natural Questions, a new benchmark MRC dataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction. We also show significant impact in HotpotQA, improving answer prediction F1 by 4 points and supporting fact prediction F1 by 1 point and outperforming the previous best system. Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount.\n   Unsupervised Cross-lingual Representation Learning at Scale  A. Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. (2019)\nUnsupervised Cross-lingual Representation Learning at Scale\nACL\nPaper Link\nInfluential Citation Count (527), SS-ID (6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6)\nABSTRACT\nThis paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.\n   Inducing Syntactic Trees from BERT Representations  Rudolf Rosa, D. Mareček. (2019)\nInducing Syntactic Trees from BERT Representations\nArXiv\nPaper Link\nInfluential Citation Count (1), SS-ID (71f551f0352b91ab4725c498c68610655d3b5578)\nABSTRACT\nWe use the English model of BERT and explore how a deletion of one word in a sentence changes representations of other words. Our hypothesis is that removing a reducible word (e.g. an adjective) does not affect the representation of other words so much as removing e.g. the main verb, which makes the sentence ungrammatical and of \u0026ldquo;high surprise\u0026rdquo; for the language model. We estimate reducibilities of individual words and also of longer continuous phrases (word n-grams), study their syntax-related properties, and then also use them to induce full dependency trees.\n   Compressing Large-Scale Transformer-Based Models: A Case Study on BERT  Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Y. Yang, Deming Chen, M. Winslett, Hassan Sajjad, Preslav Nakov. (2020)\nCompressing Large-Scale Transformer-Based Models: A Case Study on BERT\nTransactions of the Association for Computational Linguistics\nPaper Link\nInfluential Citation Count (5), SS-ID (738215a396f6eee1709c6b521a6199769f0ce674)\nABSTRACT\nAbstract Pre-trained Transformer-based models have achieved state-of-the-art performance for various Natural Language Processing (NLP) tasks. However, these models often have billions of parameters, and thus are too resource- hungry and computation-intensive to suit low- capability devices or applications with strict latency requirements. One potential remedy for this is model compression, which has attracted considerable research attention. Here, we summarize the research in compressing Transformers, focusing on the especially popular BERT model. In particular, we survey the state of the art in compression for BERT, we clarify the current best practices for compressing large-scale Transformer models, and we provide insights into the workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving lightweight, accurate, and generic NLP models.\n   Information-Theoretic Probing for Linguistic Structure  Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, Ryan Cotterell. (2020)\nInformation-Theoretic Probing for Linguistic Structure\nACL\nPaper Link\nInfluential Citation Count (16), SS-ID (738c6d664aa6c3854e1aa894957bd595f621fc42)\nABSTRACT\nThe success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually “know” about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network’s learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research—plus English—totalling eleven languages. Our implementation is available in https://github.com/rycolab/info-theoretic-probing.\n   WaLDORf: Wasteless Language-model Distillation On Reading-comprehension  J. Tian, A. Kreuzer, Pai-Hung Chen, Hans-Martin Will. (2019)\nWaLDORf: Wasteless Language-model Distillation On Reading-comprehension\nArXiv\nPaper Link\nInfluential Citation Count (0), SS-ID (73dd65e859d2566f3755c11cb12aff518202186a)\nABSTRACT\nTransformer based Very Large Language Models (VLLMs) like BERT, XLNet and RoBERTa, have recently shown tremendous performance on a large variety of Natural Language Understanding (NLU) tasks. However, due to their size, these VLLMs are extremely resource intensive and cumbersome to deploy at production time. Several recent publications have looked into various ways to distil knowledge from a transformer based VLLM (most commonly BERT-Base) into a smaller model which can run much faster at inference time. Here, we propose a novel set of techniques which together produce a task-specific hybrid convolutional and transformer model, WaLDORf, that achieves state-of-the-art inference speed while still being more accurate than previous distilled models.\n   Well-Read Students Learn Better: On the Importance of Pre-training Compact Models  Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. (2019)\nWell-Read Students Learn Better: On the Importance of Pre-training Compact Models\nPaper Link\nInfluential Citation Count (37), SS-ID (7402b604f14b8b91c53ed6eed04af92c59636c97)\nABSTRACT\nRecent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). However, surprisingly, the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, we first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.\n   Extreme Language Model Compression with Optimal Subwords and Shared Projections  Sanqiang Zhao, Raghav Gupta, Yang Song, Denny Zhou. (2019)\nExtreme Language Model Compression with Optimal Subwords and Shared Projections\nArXiv\nPaper Link\nInfluential Citation Count (6), SS-ID (740e4599b0e3113ad804cee4394c7fa7c0e96ca5)\nABSTRACT\nPre-trained deep neural network language models such as ELMo, GPT, BERT and XLNet have recently achieved state-of-the-art performance on a variety of language understanding tasks. However, their size makes them impractical for a number of scenarios, especially on mobile and edge devices. In particular, the input word embedding matrix accounts for a significant proportion of the model\u0026rsquo;s memory footprint, due to the large input vocabulary and embedding dimensions. Knowledge distillation techniques have had success at compressing large neural network models, but they are ineffective at yielding student models with vocabularies different from the original teacher models. We introduce a novel knowledge distillation technique for training a student model with a significantly smaller vocabulary as well as lower embedding and hidden state dimensions. Specifically, we employ a dual-training mechanism that trains the teacher and student models simultaneously to obtain optimal word embeddings for the student vocabulary. We combine this approach with learning shared projection matrices that transfer layer-wise knowledge from the teacher model to the student model. Our method is able to compress the BERT_BASE model by more than 60x, with only a minor drop in downstream task metrics, resulting in a language model with a footprint of under 7MB. Experimental results also demonstrate higher compression efficiency and accuracy when compared with other state-of-the-art compression techniques.\n   ALBERT: A Lite BERT for Self-supervised Learning of Language Representations  Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. (2019)\nALBERT: A Lite BERT for Self-supervised Learning of Language Representations\nICLR\nPaper Link\nInfluential Citation Count (574), SS-ID (7a064df1aeada7e69e5173f7d4c8606f4470365b)\nABSTRACT\nIncreasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.\n   BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA  Nina Poerner, Ulli Waltinger, Hinrich Schütze. (2019)\nBERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA\nArXiv\nPaper Link\nInfluential Citation Count (8), SS-ID (7c62ac7aedacc39ca417a48f8134e0514dc6a523)\nABSTRACT\nThe BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts. Petroni et al. (2019) take this as evidence that BERT memorizes factual knowledge during pre-training. We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian. More specifically, we show that BERT\u0026rsquo;s precision drops dramatically when we filter certain easy-to-guess facts. As a remedy, we propose E-BERT, an extension of BERT that replaces entity mentions with symbolic entity embeddings. E-BERT outperforms both BERT and ERNIE (Zhang et al., 2019) on hard-to-guess queries. We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT.\n   Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction  Taeuk Kim, Jihun Choi, Daniel Edmiston, Sang-goo Lee. (2020)\nAre Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction\nICLR\nPaper Link\nInfluential Citation Count (7), SS-ID (7cf8510d5905bd8a63f1e098e05ab591d689e0fd)\nABSTRACT\nWith the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. In line with such interest, we propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from the pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences.\n   How Much Knowledge Can You Pack into the Parameters of a Language Model?  Adam Roberts, Colin Raffel, Noam M. Shazeer. (2020)\nHow Much Knowledge Can You Pack into the Parameters of a Language Model?\nEMNLP\nPaper Link\nInfluential Citation Count (38), SS-ID (80376bdec5f534be78ba82821f540590ebce5559)\nABSTRACT\nIt has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales surprisingly well with model size and outperforms models that explicitly look up knowledge on the open-domain variants of Natural Questions and WebQuestions. To facilitate reproducibility and future work, we release our code and trained models.\n   How Multilingual is Multilingual BERT?  Telmo J. P. Pires, Eva Schlinger, Dan Garrette. (2019)\nHow Multilingual is Multilingual BERT?\nACL\nPaper Link\nInfluential Citation Count (62), SS-ID (809cc93921e4698bde891475254ad6dfba33d03b)\nABSTRACT\nIn this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.\n   Patient Knowledge Distillation for BERT Model Compression  S. Sun, Yu Cheng, Zhe Gan, Jingjing Liu. (2019)\nPatient Knowledge Distillation for BERT Model Compression\nEMNLP\nPaper Link\nInfluential Citation Count (82), SS-ID (80cf2a6af4200ecfca1c18fc89de16148f1cd4bf)\nABSTRACT\nPre-trained language models such as BERT have proven to be highly effective for natural language processing (NLP) tasks. However, the high demand for computing resources in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training, we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: (i) PKD-Last: learning from the last k layers; and (ii) PKD-Skip: learning from every k layers. These two patient distillation schemes enable the exploitation of rich information in the teacher’s hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multi-layer distillation process. Empirically, this translates into improved results on multiple NLP tasks with a significant gain in training efficiency, without sacrificing model accuracy.\n   Understanding Commonsense Inference Aptitude of Deep Contextual Representations  Jeff Da, Jungo Kasai. (2019)\nUnderstanding Commonsense Inference Aptitude of Deep Contextual Representations\nProceedings of the First Workshop on Commonsense Inference in Natural Language Processing\nPaper Link\nInfluential Citation Count (0), SS-ID (80dc7b0e6dbc26571672d9be57a0ae589689e410)\nABSTRACT\nPretrained deep contextual representations have advanced the state-of-the-art on various commonsense NLP tasks, but we lack a concrete understanding of the capability of these models. Thus, we investigate and challenge several aspects of BERT’s commonsense representation abilities. First, we probe BERT’s ability to classify various object attributes, demonstrating that BERT shows a strong ability in encoding various commonsense features in its embedding space, but is still deficient in many areas. Next, we show that, by augmenting BERT’s pretraining data with additional data related to the deficient attributes, we are able to improve performance on a downstream commonsense reasoning task while using a minimal amount of data. Finally, we develop a method of fine-tuning knowledge graphs embeddings alongside BERT and show the continued importance of explicit knowledge graphs.\n   ERNIE 2.0: A Continual Pre-training Framework for Language Understanding  Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, Haifeng Wang. (2019)\nERNIE 2.0: A Continual Pre-training Framework for Language Understanding\nAAAI\nPaper Link\nInfluential Citation Count (63), SS-ID (80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef)\nABSTRACT\nRecently pre-trained models have achieved state-of-the-art results in various language understanding tasks. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring information, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entities, semantic closeness and discourse relations. In order to extract the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pre-trained models on these constructed tasks via continual multi-task learning. Based on this framework, we construct several tasks and train the ERNIE 2.0 model to capture lexical, syntactic and semantic aspects of information in the training data. Experimental results demonstrate that ERNIE 2.0 model outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several similar tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.\n   From English To Foreign Languages: Transferring Pre-trained Language Models  Ke M. Tran. (2019)\nFrom English To Foreign Languages: Transferring Pre-trained Language Models\nArXiv\nPaper Link\nInfluential Citation Count (2), SS-ID (8199b4c196b09d6176816e4d7db8d6f3d65e07c1)\nABSTRACT\nPre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT base model within a day and a foreign BERT large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.\n   How Can We Know What Language Models Know?  Zhengbao Jiang, Frank F. Xu, J. Araki, Graham Neubig. (2019)\nHow Can We Know What Language Models Know?\nTransactions of the Association for Computational Linguistics\nPaper Link\nInfluential Citation Count (16), SS-ID (81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85)\nABSTRACT\nAbstract Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as “Obama is a __ by profession”. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as “Obama worked as a __ ” may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.\n   SpanBERT: Improving Pre-training by Representing and Predicting Spans  Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy. (2019)\nSpanBERT: Improving Pre-training by Representing and Predicting Spans\nTACL\nPaper Link\nInfluential Citation Count (175), SS-ID (81f5810fbbab9b7203b9556f4ce3c741875407bc)\nABSTRACT\nWe present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1\n   Is Multilingual BERT Fluent in Language Generation?  Samuel Rönnqvist, Jenna Kanerva, T. Salakoski, Filip Ginter. (2019)\nIs Multilingual BERT Fluent in Language Generation?\nArXiv\nPaper Link\nInfluential Citation Count (3), SS-ID (81fbf08beb80b01abaa6ad6a07b48c3034ead8a6)\nABSTRACT\nThe multilingual BERT model is trained on 104 languages and meant to serve as a universal language model and tool for encoding sentences. We explore how well the model performs on several languages across several tasks: a diagnostic classification probing the embeddings for a particular syntactic property, a cloze task testing the language modelling ability to fill in gaps in a sentence, and a natural language generation task testing for the ability to produce coherent text fitting a given context. We find that the currently available multilingual BERT model is clearly inferior to the monolingual counterparts, and cannot in many cases serve as a substitute for a well-trained monolingual model. We find that the English and German models perform well at generation, whereas the multilingual model is lacking, in particular, for Nordic languages.\n   REALM: Retrieval-Augmented Language Model Pre-Training  Kelvin Guu, Kenton Lee, Z. Tung, Panupong Pasupat, Ming-Wei Chang. (2020)\nREALM: Retrieval-Augmented Language Model Pre-Training\nArXiv\nPaper Link\nInfluential Citation Count (64), SS-ID (832fff14d2ed50eb7969c4c4b976c35776548f56)\nABSTRACT\nLanguage model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.\n   How Does BERT Answer Questions?: A Layer-Wise Analysis of Transformer Representations  Betty van Aken, Benjamin Winter, Alexander Löser, F. Gers. (2019)\nHow Does BERT Answer Questions?: A Layer-Wise Analysis of Transformer Representations\nCIKM\nPaper Link\nInfluential Citation Count (4), SS-ID (8380ab11c120a77cbdd2053337aa52525ec0f22e)\nABSTRACT\nBidirectional Encoder Representations from Transformers (BERT) reach state-of-the-art results in a variety of Natural Language Processing tasks. However, understanding of their internal functioning is still insufficient and unsatisfactory. In order to better understand BERT and other Transformer-based models, we present a layer-wise analysis of BERT\u0026rsquo;s hidden states. Unlike previous research, which mainly focuses on explaining Transformer models by their attention weights, we argue that hidden states contain equally valuable information. Specifically, our analysis focuses on models fine-tuned on the task of Question Answering (QA) as an example of a complex downstream task. We inspect how QA models transform token vectors in order to find the correct answer. To this end, we apply a set of general and QA-specific probing tasks that reveal the information stored in each representation layer. Our qualitative analysis of hidden state visualizations provides additional insights into BERT\u0026rsquo;s reasoning process. Our results show that the transformations within BERT go through phases that are related to traditional pipeline tasks. The system can therefore implicitly incorporate task-specific information into its token representations. Furthermore, our analysis reveals that fine-tuning has little impact on the models\u0026rsquo; semantic abilities and that prediction errors can be recognized in the vector representations of even early layers.\n   Universal Text Representation from BERT: An Empirical Study  Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, Bing Xiang. (2019)\nUniversal Text Representation from BERT: An Empirical Study\nArXiv\nPaper Link\nInfluential Citation Count (1), SS-ID (850713961a5aa20812cf952f950f09d491fae281)\nABSTRACT\nWe present a systematic investigation of layer-wise BERT activations for general-purpose text representations to understand what linguistic information they capture and how transferable they are across different tasks. Sentence-level embeddings are evaluated against two state-of-the-art models on downstream and probing tasks from SentEval, while passage-level embeddings are evaluated on four question-answering (QA) datasets under a learning-to-rank problem setting. Embeddings from the pre-trained BERT model perform poorly in semantic similarity and sentence surface information probing tasks. Fine-tuning BERT on natural language inference data greatly improves the quality of the embeddings. Combining embeddings from different BERT layers can further boost performance. BERT embeddings outperform BM25 baseline significantly on factoid QA datasets at the passage level, but fail to perform better than BM25 on non-factoid datasets. For all QA datasets, there is a gap between embedding-based method and in-domain fine-tuned BERT (we report new state-of-the-art results on two datasets), which suggests deep interactions between question and answer pairs are critical for those hard tasks.\n   To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks  Matthew E. Peters, Sebastian Ruder, Noah A. Smith. (2019)\nTo Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks\nRepL4NLP@ACL\nPaper Link\nInfluential Citation Count (21), SS-ID (8659bf379ca8756755125a487c43cfe8611ce842)\nABSTRACT\nWhile most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.\n   How does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope  Yiyun Zhao, Steven Bethard. (2020)\nHow does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope\nACL\nPaper Link\nInfluential Citation Count (0), SS-ID (868349fe969bc7c6b14b5f35e118a26075b7b1f2)\nABSTRACT\nLarge pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. We apply this methodology to test BERT and RoBERTa on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. We find that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.\n   On the Comparability of Pre-trained Language Models  M. Aßenmacher, C. Heumann. (2020)\nOn the Comparability of Pre-trained Language Models\nSwissText/KONVENS\nPaper Link\nInfluential Citation Count (0), SS-ID (86bd570007c863c147eb9c13f00fc6908f6b3fc9)\nABSTRACT\nRecent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP. Mainly three forces are driving the improvements in this area of research: More elaborated architectures are making better use of contextual information. Instead of simply plugging in static pre-trained representations, these are learned based on surrounding context in end-to-end trainable models with more intelligently designed language modelling objectives. Along with this, larger corpora are used as resources for pre-training large language models in a self-supervised fashion which are afterwards fine-tuned on supervised tasks. Advances in parallel computing as well as in cloud computing, made it possible to train these models with growing capacities in the same or even in shorter time than previously established models. These three developments agglomerate in new state-of-the-art (SOTA) results being revealed in a higher and higher frequency. It is not always obvious where these improvements originate from, as it is not possible to completely disentangle the contributions of the three driving forces. We set ourselves to providing a clear and concise overview on several large pre-trained language models, which achieved SOTA results in the last two years, with respect to their use of new architectures and resources. We want to clarify for the reader where the differences between the models are and we furthermore attempt to gain some insight into the single contributions of lexical/computational improvements as well as of architectural changes. We explicitly do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons. Furthermore, we tentatively want to point at potential possibilities for improvement in the field of open-sourcing and reproducible research.\n   Distributed Representations of Words and Phrases and their Compositionality  Tomas Mikolov, Ilya Sutskever, Kai Chen, G. Corrado, J. Dean. (2013)\nDistributed Representations of Words and Phrases and their Compositionality\nNIPS\nPaper Link\nInfluential Citation Count (3587), SS-ID (87f40e6f3022adbc1f1905e3e506abad05a9964f)\nABSTRACT\nThe recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.\n An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \u0026ldquo;Canada\u0026rdquo; and \u0026ldquo;Air\u0026rdquo; cannot be easily combined to obtain \u0026ldquo;Air Canada\u0026rdquo;. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.\n  HellaSwag: Can a Machine Really Finish Your Sentence?  Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi. (2019)\nHellaSwag: Can a Machine Really Finish Your Sentence?\nACL\nPaper Link\nInfluential Citation Count (35), SS-ID (8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad)\nABSTRACT\nRecent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at a piano,” a machine must select the most likely followup: “She sets her fingers on the keys.” With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (\u0026gt;95% accuracy), state-of-the-art models struggle (\u0026lt;48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical ‘Goldilocks’ zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.\n   Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results  M. Crane. (2018)\nQuestionable Answers in Question Answering Research: Reproducibility and Variability of Published Results\nTACL\nPaper Link\nInfluential Citation Count (5), SS-ID (8c0548331e02c2ead48d6c0380f9a80471ea5d80)\nABSTRACT\n“Based on theoretical reasoning it has been suggested that the reliability of findings published in the scientific literature decreases with the popularity of a research field” (Pfeiffer and Hoffmann, 2009). As we know, deep learning is very popular and the ability to reproduce results is an important part of science. There is growing concern within the deep learning community about the reproducibility of results that are presented. In this paper we present a number of controllable, yet unreported, effects that can substantially change the effectiveness of a sample model, and thusly the reproducibility of those results. Through these environmental effects we show that the commonly held belief that distribution of source code is all that is needed for reproducibility is not enough. Source code without a reproducible environment does not mean anything at all. In addition the range of results produced from these effects can be larger than the majority of incremental improvement reported.\n   Transfer Fine-Tuning: A BERT Case Study  Yuki Arase, Junichi Tsujii. (2019)\nTransfer Fine-Tuning: A BERT Case Study\nEMNLP\nPaper Link\nInfluential Citation Count (0), SS-ID (8e00d81ff7b1656c621f64fe72fff2356bacb29f)\nABSTRACT\nA semantic equivalence assessment is defined as a task that assesses semantic equivalence in a sentence pair by binary judgment (i.e., paraphrase identification) or grading (i.e., semantic textual similarity measurement). It constitutes a set of tasks crucial for research on natural language understanding. Recently, BERT realized a breakthrough in sentence representation learning (Devlin et al., 2019), which is broadly transferable to various NLP tasks. While BERT’s performance improves by increasing its model size, the required computational power is an obstacle preventing practical applications from adopting the technology. Herein, we propose to inject phrasal paraphrase relations into BERT in order to generate suitable representations for semantic equivalence assessment instead of increasing the model size. Experiments on standard natural language understanding tasks confirm that our method effectively improves a smaller BERT model while maintaining the model size. The generated model exhibits superior performance compared to a larger BERT model on semantic equivalence assessment tasks. Furthermore, it achieves larger performance gains on tasks with limited training datasets for fine-tuning, which is a property desirable for transfer learning.\n   Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets  Saku Sugawara, Pontus Stenetorp, Kentaro Inui, Akiko Aizawa. (2019)\nAssessing the Benchmarking Capacity of Machine Reading Comprehension Datasets\nAAAI\nPaper Link\nInfluential Citation Count (6), SS-ID (9148f4bb8ebdcc75beaddc875d6de857bbe85ba3)\nABSTRACT\nExisting analysis work in machine reading comprehension (MRC) is largely concerned with evaluating the capabilities of systems. However, the capabilities of datasets are not assessed for benchmarking language understanding precisely. We propose a semi-automated, ablation-based methodology for this challenge; By checking whether questions can be solved even after removing features associated with a skill requisite for language understanding, we evaluate to what degree the questions do not require the skill. Experiments on 10 datasets (e.g., CoQA, SQuAD v2.0, and RACE) with a strong baseline model show that, for example, the relative scores of the baseline model provided with content words only and with shuffled sentence words in the context are on average 89.2% and 78.5% of the original scores, respectively. These results suggest that most of the questions already answered correctly by the model do not necessarily require grammatical and complex reasoning. For precise benchmarking, MRC datasets will need to take extra care in their design to ensure that questions can correctly evaluate the intended skills.\n   When BERT Plays the Lottery, All Tickets Are Winning  Sai Prasanna, Anna Rogers, Anna Rumshisky. (2020)\nWhen BERT Plays the Lottery, All Tickets Are Winning\nEMNLP\nPaper Link\nInfluential Citation Count (7), SS-ID (91ac65431b2dc46919e1673fde67671c29446812)\nABSTRACT\nMuch of the recent success in NLP is due to the large Transformer-based models such as BERT (Devlin et al, 2019). However, these models have been shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis. For fine-tuned BERT, we show that (a) it is possible to find a subnetwork of elements that achieves performance comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. However, the \u0026ldquo;bad\u0026rdquo; subnetworks can be fine-tuned separately to achieve only slightly worse performance than the \u0026ldquo;good\u0026rdquo; ones, indicating that most weights in the pre-trained BERT are potentially useful. We also show that the \u0026ldquo;good\u0026rdquo; subnetworks vary considerably across GLUE tasks, opening up the possibilities to learn what knowledge BERT actually uses at inference time.\n   Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation  Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. (2019)\nWell-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation\nArXiv\nPaper Link\nInfluential Citation Count (18), SS-ID (93ad19fbc85360043988fa9ea7932b7fdf1fa948)\nABSTRACT\nRecent developments in NLP have been accompanied by large, expensive models. Knowledge distillation is the standard method to realize these gains in applications with limited resources: a compact student is trained to recover the outputs of a powerful teacher. While most prior work investigates student architectures and transfer techniques, we focus on an often-neglected aspect\u0026mdash;student initialization. We argue that a random starting point hinders students from fully leveraging the teacher expertise, even in the presence of a large transfer set. We observe that applying language model pre-training to students unlocks their generalization potential, surprisingly even for very compact networks. We conduct experiments on 4 NLP tasks and 24 sizes of Transformer-based students; for sentiment classification on the Amazon Book Reviews dataset, pre-training boosts size reduction and TPU speed-up from 3.1x/1.25x to 31x/16x. Extensive ablation studies dissect the interaction between pre-training and distillation, revealing a compound effect even when they are applied on the same unlabeled dataset.\n   GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding  Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman. (2018)\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\nBlackboxNLP@EMNLP\nPaper Link\nInfluential Citation Count (622), SS-ID (93b8da28d006415866bf48f9a6e06b5242129195)\nABSTRACT\nHuman ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.\n   An Analysis of Encoder Representations in Transformer-Based Machine Translation  Alessandro Raganato, J. Tiedemann. (2018)\nAn Analysis of Encoder Representations in Transformer-Based Machine Translation\nBlackboxNLP@EMNLP\nPaper Link\nInfluential Citation Count (10), SS-ID (94238dead40b12735d79ed63e29ead70730261a2)\nABSTRACT\nThe attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.\n   Knowledge Distillation from Internal Representations  Gustavo Aguilar, Yuan Ling, Y. Zhang, Benjamin Yao, Xing Fan, Edward Guo. (2019)\nKnowledge Distillation from Internal Representations\nAAAI\nPaper Link\nInfluential Citation Count (4), SS-ID (944e7b64903bde89bfea433203d5a0e774cff354)\nABSTRACT\nKnowledge distillation is typically conducted by training a small model (the student) to mimic a large and cumbersome model (the teacher). The idea is to compress the knowledge from the teacher by using its output probabilities as soft-labels to optimize the student. However, when the teacher is considerably large, there is no guarantee that the internal knowledge of the teacher will be transferred into the student; even if the student closely matches the soft-labels, its internal representations may be considerably different. This internal mismatch can undermine the generalization capabilities originally intended to be transferred from the teacher to the student. In this paper, we propose to distill the internal representations of a large model such as BERT into a simplified version of it. We formulate two ways to distill such representations and various algorithms to conduct the distillation. We experiment with datasets from the GLUE benchmark and consistently show that adding knowledge distillation from internal representations is a more powerful method than only using soft-label distillation.\n   PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination  Saurabh Goyal, Anamitra R. Choudhury, S. Raje, Venkatesan T. Chakaravarthy, Yogish Sabharwal, Ashish Verma. (2020)\nPoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination\nICML\nPaper Link\nInfluential Citation Count (13), SS-ID (94f94e8892261d0377159379ca5a166ceae19a14)\nABSTRACT\nWe develop a novel method, called PoWER-BERT, for improving the inference time of the popular BERT model, while maintaining the accuracy. It works by: a) exploiting redundancy pertaining to word-vectors (intermediate encoder outputs) and eliminating the redundant vectors. b) determining which word-vectors to eliminate by developing a strategy for measuring their significance, based on the self-attention mechanism. c) learning how many word-vectors to eliminate by augmenting the BERT model and the loss function. Experiments on the standard GLUE benchmark shows that PoWER-BERT achieves up to 4.5x reduction in inference time over BERT with \u0026lt;1% loss in accuracy. We show that PoWER-BERT offers significantly better trade-off between accuracy and inference time compared to prior methods. We demonstrate that our method attains up to 6.8x reduction in inference time with \u0026lt;1% loss in accuracy when applied over ALBERT, a highly compressed version of BERT. The code for PoWER-BERT is publicly available at this https URL.\n   What Does BERT Look at? An Analysis of BERT’s Attention  Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning. (2019)\nWhat Does BERT Look at? An Analysis of BERT’s Attention\nBlackboxNLP@ACL\nPaper Link\nInfluential Citation Count (74), SS-ID (95a251513853c6032bdecebd4b74e15795662986)\nABSTRACT\nLarge pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT’s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention.\n   BERT Rediscovers the Classical NLP Pipeline  Ian Tenney, Dipanjan Das, Ellie Pavlick. (2019)\nBERT Rediscovers the Classical NLP Pipeline\nACL\nPaper Link\nInfluential Citation Count (59), SS-ID (97906df07855b029b7aae7c2a1c6c5e8df1d531c)\nABSTRACT\nPre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.\n   Contextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation  Alessio Miaschi, F. Dell’Orletta. (2020)\nContextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation\nREPL4NLP\nPaper Link\nInfluential Citation Count (0), SS-ID (9b1933038680b13c06b60dfe810e96a3a0ef9d37)\nABSTRACT\nIn this paper we present a comparison between the linguistic knowledge encoded in the internal representations of a contextual Language Model (BERT) and a contextual-independent one (Word2vec). We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that, although BERT is capable of understanding the full context of each word in an input sequence, the implicit knowledge encoded in its aggregated sentence representations is still comparable to that of a contextual-independent model. We also find that BERT is able to encode sentence-level properties even within single-word embeddings, obtaining comparable or even superior results than those obtained with sentence representations.\n   A Cross-Task Analysis of Text Span Representations  Shubham Toshniwal, Haoyue Shi, Bowen Shi, Lingyu Gao, Karen Livescu, Kevin Gimpel. (2020)\nA Cross-Task Analysis of Text Span Representations\nREPL4NLP\nPaper Link\nInfluential Citation Count (0), SS-ID (9b2b96adf4ec05b086037222a893fa778f83a985)\nABSTRACT\nMany natural language processing (NLP) tasks involve reasoning with textual spans, including question answering, entity recognition, and coreference resolution. While extensive research has focused on functional architectures for representing words and sentences, there is less work on representing arbitrary spans of text within sentences. In this paper, we conduct a comprehensive empirical evaluation of six span representation methods using eight pretrained language representation models across six tasks, including two tasks that we introduce. We find that, although some simple span representations are fairly reliable across tasks, in general the optimal span representation varies by task, and can also vary within different facets of individual tasks. We also find that the choice of span representation has a bigger impact with a fixed pretrained encoder than with a fine-tuned encoder.\n   How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings  Kawin Ethayarajh. (2019)\nHow Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\nEMNLP\nPaper Link\nInfluential Citation Count (29), SS-ID (9d7902e834d5d1d35179962c7a5b9d16623b0d39)\nABSTRACT\nReplacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word’s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.\n   On Identifiability in Transformers  Gino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, Roger Wattenhofer. (2019)\nOn Identifiability in Transformers\nICLR\nPaper Link\nInfluential Citation Count (13), SS-ID (9d7fbdb2e9817a6396992a1c92f75206689852d9)\nABSTRACT\nIn this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models.\n   Products of Random Latent Variable Grammars  Slav Petrov. (2010)\nProducts of Random Latent Variable Grammars\nNAACL\nPaper Link\nInfluential Citation Count (10), SS-ID (9dccaf6ea0fa19772cf8067295b16df3eb7b4dda)\nABSTRACT\nWe show that the automatically induced latent variable grammars of Petrov et al. (2006) vary widely in their underlying representations, depending on their EM initialization point. We use this to our advantage, combining multiple automatically learned grammars into an unweighted product model, which gives significantly improved performance over state-of-the-art individual grammars. In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights. Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.\n   Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?  Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, Samuel R. Bowman. (2020)\nIntermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?\nACL\nPaper Link\nInfluential Citation Count (7), SS-ID (9e594ae4ae9c38b6495810a8872f513ae19be29c)\nABSTRACT\nWhile pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.\n   On the Cross-lingual Transferability of Monolingual Representations  Mikel Artetxe, Sebastian Ruder, Dani Yogatama. (2019)\nOn the Cross-lingual Transferability of Monolingual Representations\nACL\nPaper Link\nInfluential Citation Count (68), SS-ID (9e9d919c1de684ca42c8b581ec62c7aa685f431e)\nABSTRACT\nState-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.\n   BERT is Not an Interlingua and the Bias of Tokenization  Jasdeep Singh, Bryan McCann, R. Socher, Caiming Xiong. (2019)\nBERT is Not an Interlingua and the Bias of Tokenization\nEMNLP\nPaper Link\nInfluential Citation Count (8), SS-ID (9eb4cd1a4b4717c97c47e3dc4563a75779ae9390)\nABSTRACT\nMultilingual transfer learning can benefit both high- and low-resource languages, but the source of these improvements is not well understood. Cananical Correlation Analysis (CCA) of the internal representations of a pre- trained, multilingual BERT model reveals that the model partitions representations for each language rather than using a common, shared, interlingual space. This effect is magnified at deeper layers, suggesting that the model does not progressively abstract semantic con- tent while disregarding languages. Hierarchical clustering based on the CCA similarity scores between languages reveals a tree structure that mirrors the phylogenetic trees hand- designed by linguists. The subword tokenization employed by BERT provides a stronger bias towards such structure than character- and word-level tokenizations. We release a subset of the XNLI dataset translated into an additional 14 languages at https://www.github.com/salesforce/xnli_extension to assist further research into multilingual representations.\n   Cloze-driven Pretraining of Self-attention Networks  Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael Auli. (2019)\nCloze-driven Pretraining of Self-attention Networks\nEMNLP\nPaper Link\nInfluential Citation Count (14), SS-ID (9f1c5777a193b2c3bb2b25e248a156348e5ba56d)\nABSTRACT\nWe present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with BERT. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.\n   BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance  Timo Schick, Hinrich Schütze. (2019)\nBERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance\nACL\nPaper Link\nInfluential Citation Count (3), SS-ID (9f4c37f154946e141a67ae2816c70b19241b3224)\nABSTRACT\nPretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Schütze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.\n   Analyzing the Structure of Attention in a Transformer Language Model  Jesse Vig, Y. Belinkov. (2019)\nAnalyzing the Structure of Attention in a Transformer Language Model\nBlackboxNLP@ACL\nPaper Link\nInfluential Citation Count (8), SS-ID (a039ea239e37f53a2cb60c68e0a1967994353166)\nABSTRACT\nThe Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.\n   Distilling Task-Specific Knowledge from BERT into Simple Neural Networks  Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, Jimmy J. Lin. (2019)\nDistilling Task-Specific Knowledge from BERT into Simple Neural Networks\nArXiv\nPaper Link\nInfluential Citation Count (26), SS-ID (a08293b2c9c5bcddb023cc7eb3354d4d86bfae89)\nABSTRACT\nIn the natural language processing literature, neural networks are becoming increasingly deeper and complex. The recent poster child of this trend is the deep language representation model, which includes BERT, ELMo, and GPT. These developments have led to the conviction that previous-generation, shallower neural networks for language understanding are obsolete. In this paper, however, we demonstrate that rudimentary, lightweight neural networks can still be made competitive without architecture changes, external training data, or additional input features. We propose to distill knowledge from BERT, a state-of-the-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks. Across multiple datasets in paraphrasing, natural language inference, and sentiment classification, we achieve comparable results with ELMo, while using roughly 100 times fewer parameters and 15 times less inference time.\n   What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models  Allyson Ettinger. (2019)\nWhat BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models\nTACL\nPaper Link\nInfluential Citation Count (10), SS-ID (a0e49f65b6847437f262c59d0d399255101d0b75)\nABSTRACT\nPre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction— and, in particular, it shows clear insensitivity to the contextual impacts of negation.\n   A Matter of Framing: The Impact of Linguistic Formalism on Probing Results  Ilia Kuznetsov, Iryna Gurevych. (2020)\nA Matter of Framing: The Impact of Linguistic Formalism on Probing Results\nEMNLP\nPaper Link\nInfluential Citation Count (0), SS-ID (a160dbe78b0546679ec8a3140b3cf4614e3cc485)\nABSTRACT\nDeep pre-trained contextualized encoders like BERT (Delvin et al., 2019) demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms. Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data. Can the choice of formalism affect probing results? To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics. We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism. Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings.\n   All-but-the-Top: Simple and Effective Postprocessing for Word Representations  Jiaqi Mu, S. Bhat, P. Viswanath. (2017)\nAll-but-the-Top: Simple and Effective Postprocessing for Word Representations\nICLR\nPaper Link\nInfluential Citation Count (30), SS-ID (a2d407962bb1f5fcd209114f5687d4c11bf9dfad)\nABSTRACT\nReal-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a {\\em very simple}, and yet counter-intuitive, postprocessing technique \u0026ndash; eliminate the common mean vector and a few top dominating directions from the word vectors \u0026ndash; that renders off-the-shelf representations {\\em even stronger}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and { text classification}) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones.\n   PERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models  Eyal Ben-David, Carmel Rabinovitz, Roi Reichart. (2020)\nPERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models\nTransactions of the Association for Computational Linguistics\nPaper Link\nInfluential Citation Count (4), SS-ID (a33b09d1a41db92ca14185a28f9163056ca2a115)\nABSTRACT\nAbstract Pivot-based neural representation models have led to significant progress in domain adaptation for NLP. However, previous research following this approach utilize only labeled data from the source domain and unlabeled data from the source and target domains, but neglect to incorporate massive unlabeled corpora that are not necessarily drawn from these domains. To alleviate this, we propose PERL: A representation learning model that extends contextualized word embedding models such as BERT (Devlin et al., 2019) with pivot-based fine-tuning. PERL outperforms strong baselines across 22 sentiment classification domain adaptation setups, improves in-domain model performance, yields effective reduced-size models, and increases model stability.1\n   DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter  Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf. (2019)\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\nArXiv\nPaper Link\nInfluential Citation Count (417), SS-ID (a54b56af24bb4873ed0163b77df63b92bd018ddc)\nABSTRACT\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.\n   TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data  Pengcheng Yin, Graham Neubig, Wen-tau Yih, Sebastian Riedel. (2020)\nTaBERT: Pretraining for Joint Understanding of Textual and Tabular Data\nACL\nPaper Link\nInfluential Citation Count (30), SS-ID (a5b1d1cab073cb746a990b37d42dc7b67763f881)\nABSTRACT\nRecent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider.\n   Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks  Anna Rogers, Olga Kovaleva, Matthew Downey, Anna Rumshisky. (2020)\nGetting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks\nAAAI\nPaper Link\nInfluential Citation Count (7), SS-ID (a87f0bac2ed58e8a79d33d0c1cf81c6407cd645f)\nABSTRACT\nThe recent explosion in question answering research produced a wealth of both factoid reading comprehension (RC) and commonsense reasoning datasets. Combining them presents a different kind of task: deciding not simply whether information is present in the text, but also whether a confident guess could be made for the missing information. We present QuAIL, the first RC dataset to combine text-based, world knowledge and unanswerable questions, and to provide question type annotation that would enable diagnostics of the reasoning strategies by a given QA system. QuAIL contains 15K multi-choice questions for 800 texts in 4 domains. Crucially, it offers both general and text-specific questions, unlikely to be found in pretraining data. We show that QuAIL poses substantial challenges to the current state-of-the-art systems, with a 30% drop in accuracy compared to the most similar existing dataset.\n   SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization  Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, T. Zhao. (2019)\nSMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\nACL\nPaper Link\nInfluential Citation Count (24), SS-ID (ab70853cd5912c470f6ff95e95481980f0a2a41b)\nABSTRACT\nTransfer learning has fundamentally changed the landscape of natural language processing (NLP). Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance. The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating. Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI. Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.\n   Thieves on Sesame Street! Model Extraction of BERT-based APIs  Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas Papernot, Mohit Iyyer. (2019)\nThieves on Sesame Street! Model Extraction of BERT-based APIs\nICLR\nPaper Link\nInfluential Citation Count (15), SS-ID (ac713aebdcc06f15f8ea61e1140bb360341fdf27)\nABSTRACT\nWe study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction\u0026mdash;membership classification and API watermarking\u0026mdash;which while successful against naive adversaries, are ineffective against more sophisticated ones.\n   Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment  Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits. (2019)\nIs BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment\nAAAI\nPaper Link\nInfluential Citation Count (68), SS-ID (ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2)\nABSTRACT\nMachine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient—it generates adversarial text with computational complexity linear to the text length.1\n   Are Sixteen Heads Really Better than One?  Paul Michel, Omer Levy, Graham Neubig. (2019)\nAre Sixteen Heads Really Better than One?\nNeurIPS\nPaper Link\nInfluential Citation Count (49), SS-ID (b03c7ff961822183bab66b2e594415e585d3fd09)\nABSTRACT\nAttention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention \u0026ldquo;head\u0026rdquo; potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.\n   A Mutual Information Maximization Perspective of Language Representation Learning  Lingpeng Kong, Cyprien de Masson d\u0026rsquo;Autume, Wang Ling, Lei Yu, Zihang Dai, Dani Yogatama. (2019)\nA Mutual Information Maximization Perspective of Language Representation Learning\nICLR\nPaper Link\nInfluential Citation Count (11), SS-ID (b04889922aae7f799affb2ae6508bc5f5c989567)\nABSTRACT\nWe show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).\n   Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks  Jason Phang, Thibault Févry, Samuel R. Bowman. (2018)\nSentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks\nArXiv\nPaper Link\nInfluential Citation Count (38), SS-ID (b47381e04739ea3f392ba6c8faaf64105493c196)\nABSTRACT\nPretraining sentence encoders with language modeling and related unsupervised tasks has recently been shown to be very effective for language understanding tasks. By supplementing language model-style pretraining with further training on data-rich supervised tasks, such as natural language inference, we obtain additional performance improvements on the GLUE benchmark. Applying supplementary training on BERT (Devlin et al., 2018), we attain a GLUE score of 81.8\u0026mdash;the state of the art (as of 02/24/2019) and a 1.4 point improvement over BERT. We also observe reduced variance across random restarts in this setting. Our approach yields similar improvements when applied to ELMo (Peters et al., 2018a) and Radford et al. (2018)\u0026rsquo;s model. In addition, the benefits of supplementary training are particularly pronounced in data-constrained regimes, as we show in experiments with artificially limited training data.\n   Do Attention Heads in BERT Track Syntactic Dependencies?  Phu Mon Htut, Jason Phang, Shikha Bordia, Samuel R. Bowman. (2019)\nDo Attention Heads in BERT Track Syntactic Dependencies?\nArXiv\nPaper Link\nInfluential Citation Count (12), SS-ID (ba8215e77f35b0d947c7cec39c45df4516e93421)\nABSTRACT\nWe investigate the extent to which individual attention heads in pretrained transformer language models, such as BERT and RoBERTa, implicitly capture syntactic dependency relations. We employ two methods\u0026mdash;taking the maximum attention weight and computing the maximum spanning tree\u0026mdash;to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency (UD) trees. We show that, for some UD relation types, there exist heads that can recover the dependency type significantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze BERT fine-tuned on two datasets\u0026mdash;the syntax-oriented CoLA and the semantics-oriented MNLI\u0026mdash;to investigate whether fine-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing significantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that BERT-style models are known to learn.\n   Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings  Gregor Wiedemann, Steffen Remus, Avi Chawla, Chris Biemann. (2019)\nDoes BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings\nKONVENS\nPaper Link\nInfluential Citation Count (6), SS-ID (ba8b3d0d2b09bc2b56c6d3f153919786d9fc3075)\nABSTRACT\nContextualized word embeddings (CWE) such as provided by ELMo (Peters et al., 2018), Flair NLP (Akbik et al., 2018), or BERT (Devlin et al., 2019) are a major recent innovation in NLP. CWEs provide semantic vector representations of words depending on their respective context. Their advantage over static word embeddings has been shown for a number of tasks, such as text classification, sequence tagging, or machine translation. Since vectors of the same word type can vary depending on the respective context, they implicitly provide a model for word sense disambiguation (WSD). We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs. We compare the performance of different CWE models for the task and can report improvements above the current state of the art for two standard WSD benchmark datasets. We further show that the pre-trained BERT model is able to place polysemic words into distinct \u0026lsquo;sense\u0026rsquo; regions of the embedding space, while ELMo and Flair NLP do not seem to possess this ability.\n   Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping  Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, Noah A. Smith. (2020)\nFine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping\nArXiv\nPaper Link\nInfluential Citation Count (17), SS-ID (baf60d13c98916b77b09bc525ede1cd610ed1db5)\nABSTRACT\nFine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning.\n   Improving BERT Fine-tuning with Embedding Normalization  Wenxuan Zhou, Junyi Du, Xiang Ren. (2019)\nImproving BERT Fine-tuning with Embedding Normalization\nArXiv\nPaper Link\nInfluential Citation Count (0), SS-ID (bb6e205f56f064ae76703b40147422483c438ef6)\nABSTRACT\nLarge pre-trained sentence encoders like BERT start a new chapter in natural language processing. A common practice to apply pre-trained BERT to sequence classification tasks (e.g., classification of sentences or sentence pairs) is by feeding the embedding of [CLS] token (in the last layer) to a task-specific classification layer, and then fine tune the model parameters of BERT and classifier jointly. In this paper, we conduct systematic analysis over several sequence classification datasets to examine the embedding values of [CLS] token before the fine tuning phase, and present the biased embedding distribution issue\u0026mdash;i.e., embedding values of [CLS] concentrate on a few dimensions and are non-zero centered. Such biased embedding brings challenge to the optimization process during fine-tuning as gradients of [CLS] embedding may explode and result in degraded model performance. We further propose several simple yet effective normalization methods to modify the [CLS] embedding during the fine-tuning. Compared with the previous practice, neural classification model with the normalized embedding shows improvements on several text classification tasks, demonstrates the effectiveness of our method.\n   Large Batch Optimization for Deep Learning: Training BERT in 76 minutes  Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, J. Demmel, K. Keutzer, Cho-Jui Hsieh. (2019)\nLarge Batch Optimization for Deep Learning: Training BERT in 76 minutes\nICLR\nPaper Link\nInfluential Citation Count (52), SS-ID (bc789aef715498e79a74f857fa090ece9e383bf1)\nABSTRACT\nTraining large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at this https URL\n   Visualizing Attention in Transformer-Based Language Representation Models  Jesse Vig. (2019)\nVisualizing Attention in Transformer-Based Language Representation Models\nArXiv\nPaper Link\nInfluential Citation Count (5), SS-ID (beb051c652f02c2d5829d783fbc4f3acce99bc3c)\nABSTRACT\nWe present an open-source tool for visualizing multi-head self-attention in Transformer-based language representation models. The tool extends earlier work by visualizing attention at three levels of granularity: the attention-head level, the model level, and the neuron level. We describe how each of these views can help to interpret the model, and we demonstrate the tool on the BERT model and the OpenAI GPT-2 model. We also present three use cases for analyzing GPT-2: detecting model bias, identifying recurring patterns, and linking neurons to model behavior.\n   Syntax-Infused Transformer and BERT models for Machine Translation and Natural Language Understanding  Dhanasekar Sundararaman, Vivek Subramanian, Guoyin Wang, Shijing Si, Dinghan Shen, Dong Wang, L. Carin. (2019)\nSyntax-Infused Transformer and BERT models for Machine Translation and Natural Language Understanding\nArXiv\nPaper Link\nInfluential Citation Count (1), SS-ID (beb91a773677872fc21f08722bdcc737bf5917b5)\nABSTRACT\nAttention-based models have shown significant improvement over traditional algorithms in several NLP tasks. The Transformer, for instance, is an illustrative example that generates abstract representations of tokens inputted to an encoder based on their relationships to all tokens in a sequence. Recent studies have shown that although such models are capable of learning syntactic features purely by seeing examples, explicitly feeding this information to deep learning models can significantly enhance their performance. Leveraging syntactic information like part of speech (POS) may be particularly beneficial in limited training data settings for complex models such as the Transformer. We show that the syntax-infused Transformer with multiple features achieves an improvement of 0.7 BLEU when trained on the full WMT 14 English to German translation dataset and a maximum improvement of 1.99 BLEU points when trained on a fraction of the dataset. In addition, we find that the incorporation of syntax into BERT fine-tuning outperforms baseline on a number of downstream tasks from the GLUE benchmark.\n   Knowledge Enhanced Contextual Word Representations  Matthew E. Peters, Mark Neumann, IV RobertL.Logan, Roy Schwartz, V. Joshi, Sameer Singh, Noah A. Smith. (2019)\nKnowledge Enhanced Contextual Word Representations\nEMNLP\nPaper Link\nInfluential Citation Count (61), SS-ID (bfeb827d06c1a3583b5cc6d25241203a81f6af09)\nABSTRACT\nContextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs.\n   TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection  Siddhant Garg, Thuy Vu, Alessandro Moschitti. (2019)\nTANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection\nAAAI\nPaper Link\nInfluential Citation Count (12), SS-ID (c12e6c65e1de5d3993c5b65d0e234ae1f60c85ae)\nABSTRACT\nWe propose TandA, an effective technique for fine-tuning pre-trained Transformer models for natural language tasks. Specifically, we first transfer a pre-trained model into a model for a general task by fine-tuning it with a large and high-quality dataset. We then perform a second fine-tuning step to adapt the transferred model to the target domain. We demonstrate the benefits of our approach for answer sentence selection, which is a well-known inference task in Question Answering. We built a large scale dataset to enable the transfer step, exploiting the Natural Questions dataset. Our approach establishes the state of the art on two well-known benchmarks, WikiQA and TREC-QA, achieving the impressive MAP scores of 92% and 94.3%, respectively, which largely outperform the the highest scores of 83.4% and 87.5% of previous work. We empirically show that TandA generates more stable and robust models reducing the effort required for selecting optimal hyper-parameters. Additionally, we show that the transfer step of TandA makes the adaptation step more robust to noise. This enables a more effective use of noisy datasets for fine-tuning. Finally, we also confirm the positive impact of TandA in an industrial setting, using domain specific datasets subject to different types of noise.\n   When Bert Forgets How To POS: Amnesic Probing of Linguistic Properties and MLM Predictions  Yanai Elazar, Shauli Ravfogel, Alon Jacovi, Yoav Goldberg. (2020)\nWhen Bert Forgets How To POS: Amnesic Probing of Linguistic Properties and MLM Predictions\nArXiv\nPaper Link\nInfluential Citation Count (1), SS-ID (c8b00d4706fc8979a9c5f410addccbcfe1c0d894)\nABSTRACT\nA growing body of work makes use of probing in order to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results, and offer an alternative method which is focused on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention which removes it from the representation. Equipped with this new analysis tool, we can now ask questions that were not possible before, e.g. is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.\n   On the use of BERT for Neural Machine Translation  S. Clinchant, K. Jung, Vassilina Nikoulina. (2019)\nOn the use of BERT for Neural Machine Translation\nEMNLP\nPaper Link\nInfluential Citation Count (9), SS-ID (c93b2d64fce8737506757bbce51e17b533f9285b)\nABSTRACT\nExploiting large pretrained models for various NMT tasks have gained a lot of visibility recently. In this work we study how BERT pretrained models could be exploited for supervised Neural Machine Translation. We compare various ways to integrate pretrained BERT model with NMT model and study the impact of the monolingual data used for BERT training on the final translation quality. We use WMT-14 English-German, IWSLT15 English-German and IWSLT14 English-Russian datasets for these experiments. In addition to standard task test set evaluation, we perform evaluation on out-of-domain test sets and noise injected test sets, in order to assess how BERT pretrained representations affect model robustness.\n   Do Neural Language Representations Learn Physical Commonsense?  Maxwell Forbes, Ari Holtzman, Yejin Choi. (2019)\nDo Neural Language Representations Learn Physical Commonsense?\nCogSci\nPaper Link\nInfluential Citation Count (3), SS-ID (cc02386375b1262c3a1d5525154eaea24c761d15)\nABSTRACT\nHumans understand language based on the rich background knowledge about how the physical world works, which in turn allows us to reason about the physical world through language. In addition to the properties of objects (e.g., boats require fuel) and their affordances, i.e., the actions that are applicable to them (e.g., boats can be driven), we can also reason about if-then inferences between what properties of objects imply the kind of actions that are applicable to them (e.g., that if we can drive something then it likely requires fuel). In this paper, we investigate the extent to which state-of-the-art neural language representations, trained on a vast amount of natural language text, demonstrate physical commonsense reasoning. While recent advancements of neural language models have demonstrated strong performance on various types of natural language inference tasks, our study based on a dataset of over 200k newly collected annotations suggests that neural language representations still only learn associations that are explicitly written down.\n   Q8BERT: Quantized 8Bit BERT  Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat. (2019)\nQ8BERT: Quantized 8Bit BERT\n2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)\nPaper Link\nInfluential Citation Count (17), SS-ID (ce106590145e89ea4b621c99665862967ccf5dac)\nABSTRACT\nRecently, pre-trained Transformer [1] based language models such as BERT [2] and GPT [3], have shown great improvement in many Natural Language Processing (NLP) tasks. However, these models contain a large amount of parameters. The emergence of even larger and more accurate models such as GPT2 [4] and Megatron11https://github.com/NVIDIA/Megatron-LM, suggest a trend of large pre-trained Transformer models. However, using these large models in production environments is a complex task requiring a large amount of compute, memory and power resources. In this work we show how to perform quantization-aware training during the fine-tuning phase of BERT in order to compress BERT by 4x with minimal accuracy loss. Furthermore, the produced quantized model can accelerate inference speed if it is optimized for 8bit Integer supporting hardware.\n   Attention is not not Explanation  Sarah Wiegreffe, Yuval Pinter. (2019)\nAttention is not not Explanation\nEMNLP\nPaper Link\nInfluential Citation Count (12), SS-ID (ce177672b00ddf46e4906157a7e997ca9338b8b9)\nABSTRACT\nAttention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model’s prediction, and consequently reach insights regarding the model’s decision-making process. A recent paper claims that ‘Attention is not Explanation’ (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one’s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don’t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.\n   Language Models as Knowledge Bases?  Fabio Petroni, Tim Rocktäschel, Patrick Lewis, A. Bakhtin, Yuxiang Wu, Alexander H. Miller, S. Riedel. (2019)\nLanguage Models as Knowledge Bases?\nEMNLP\nPaper Link\nInfluential Citation Count (115), SS-ID (d0086b86103a620a86bc918746df0aa642e2a8a3)\nABSTRACT\nRecent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.\n   FreeLB: Enhanced Adversarial Training for Natural Language Understanding  Chen Zhu, Yu Cheng, Zhe Gan, S. Sun, T. Goldstein, Jingjing Liu. (2019)\nFreeLB: Enhanced Adversarial Training for Natural Language Understanding\nICLR\nPaper Link\nInfluential Citation Count (34), SS-ID (d01fa0311e8e15b8b874b376123530c815f52852)\nABSTRACT\nAdversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44% and 67.75% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as well. Code is available at \\url{this https URL .\n   FreeLB: Enhanced Adversarial Training for Language Understanding  Chen Zhu, Yu Cheng, Zhe Gan, S. Sun, T. Goldstein, Jingjing Liu. (2019)\nFreeLB: Enhanced Adversarial Training for Language Understanding\nICLR 2020\nPaper Link\nInfluential Citation Count (16), SS-ID (d2038ced371e45aee3651c7a595c4566f4826b9f)\nABSTRACT\nAdversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm - FreeLB, that promotes higher robustness and invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-based model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art test accuracies of 85.39% and 67.32% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as well.\n   Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings  Rishi Bommasani, Kelly Davis, Claire Cardie. (2020)\nInterpreting Pretrained Contextualized Representations via Reductions to Static Embeddings\nACL\nPaper Link\nInfluential Citation Count (17), SS-ID (d34580c522c79d5cde620331dd9ffb18643a8090)\nABSTRACT\nContextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings — while more diverse and mature than those available for their dynamic counterparts — are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.\n   Visualizing and Understanding the Effectiveness of BERT  Y. Hao, Li Dong, Furu Wei, Ke Xu. (2019)\nVisualizing and Understanding the Effectiveness of BERT\nEMNLP\nPaper Link\nInfluential Citation Count (3), SS-ID (d3cacb4806886eb2fe59c90d4b6f822c24ff1822)\nABSTRACT\nLanguage model pre-training, such as BERT, has achieved remarkable results in many NLP tasks. However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets. First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch. We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language.\n   RNNs Implicitly Implement Tensor Product Representations  R. Thomas McCoy, Tal Linzen, Ewan Dunbar, P. Smolensky. (2018)\nRNNs Implicitly Implement Tensor Product Representations\nICLR\nPaper Link\nInfluential Citation Count (2), SS-ID (d3ded34ff3378aadaa9a7c10e51cef6d04391a86)\nABSTRACT\nRecurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing fillers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structure-sensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag of words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated by TPRs; at the same time, existing training tasks for sentence representation learning may not be sufficient for inducing robust structural representations.\n   StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding  Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, Luo Si. (2019)\nStructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\nICLR\nPaper Link\nInfluential Citation Count (21), SS-ID (d56c1fc337fb07ec004dc846f80582c327af717c)\nABSTRACT\nRecently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.\n   Multilingual Alignment of Contextual Word Representations  Steven Cao, Nikita Kitaev, D. Klein. (2020)\nMultilingual Alignment of Contextual Word Representations\nICLR\nPaper Link\nInfluential Citation Count (17), SS-ID (d592007d1c106fe1217604eb35664c7a5f07cb32)\nABSTRACT\nWe propose procedures for evaluating and strengthening contextual embedding alignment and show that they are useful in analyzing and improving multilingual BERT. In particular, after our proposed alignment procedure, BERT exhibits significantly improved zero-shot performance on XNLI compared to the base model, remarkably matching pseudo-fully-supervised translate-train models for Bulgarian and Greek. Further, to measure the degree of alignment, we introduce a contextual version of word retrieval and show that it correlates well with downstream zero-shot transfer. Using this word retrieval task, we also analyze BERT and find that it exhibits systematic deficiencies, e.g. worse alignment for open-class parts-of-speech and word pairs written in different scripts, that are corrected by the alignment procedure. These results support contextual alignment as a useful concept for understanding large multilingual pre-trained models.\n   Energy and Policy Considerations for Deep Learning in NLP  Emma Strubell, Ananya Ganesh, A. McCallum. (2019)\nEnergy and Policy Considerations for Deep Learning in NLP\nACL\nPaper Link\nInfluential Citation Count (64), SS-ID (d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea)\nABSTRACT\nRecent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.\n   Revealing the Dark Secrets of BERT  Olga Kovaleva, Alexey Romanov, Anna Rogers, Anna Rumshisky. (2019)\nRevealing the Dark Secrets of BERT\nEMNLP\nPaper Link\nInfluential Citation Count (34), SS-ID (d78aed1dac6656affa4a04cbf225ced11a83d103)\nABSTRACT\nBERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT’s heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.\n   Does BERT Solve Commonsense Task via Commonsense Knowledge?  Leyang Cui, Sijie Cheng, Yu Wu, Yue Zhang. (2020)\nDoes BERT Solve Commonsense Task via Commonsense Knowledge?\nArXiv\nPaper Link\nInfluential Citation Count (1), SS-ID (d8ea988072efb115ee8c85e159c1fa4a816360b5)\nABSTRACT\nThe success of pre-trained contextualized language models such as BERT motivates a line of work that investigates linguistic knowledge inside such models in order to explain the huge improvement in downstream tasks. While previous work shows syntactic, semantic and word sense knowledge in BERT, little work has been done on investigating how BERT solves CommonsenseQA tasks. In particular, it is an interesting research question whether BERT relies on shallow syntactic patterns or deeper commonsense knowledge for disambiguation. We propose two attention-based methods to analyze commonsense knowledge inside BERT, and the contribution of such knowledge for the model prediction. We find that attention heads successfully capture the structured commonsense knowledge encoded in ConceptNet, which helps BERT solve commonsense tasks directly. Fine-tuning further makes BERT learn to use the commonsense knowledge on higher layers.\n   Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning  Mitchell A. Gordon, Kevin Duh, Nicholas Andrews. (2020)\nCompressing BERT: Studying the Effects of Weight Pruning on Transfer Learning\nREPL4NLP\nPaper Link\nInfluential Citation Count (8), SS-ID (d9b824dbecbe3a1f0b1489f9e4521a532a63818d)\nABSTRACT\nPre-trained universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.\n   Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation  Yonghui Wu, M. Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, Yuan Cao, Qin Gao, Klaus Macherey, J. Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Y. Kato, Taku Kudo, H. Kazawa, K. Stevens, George Kurian, Nishant Patil, Wei Wang, C. Young, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, G. Corrado, Macduff Hughes, J. Dean. (2016)\nGoogle\u0026rsquo;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\nArXiv\nPaper Link\nInfluential Citation Count (344), SS-ID (dbde7dfa6cae81df8ac19ef500c42db96c3d1edd)\nABSTRACT\nNeural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT\u0026rsquo;s use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google\u0026rsquo;s Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\u0026ldquo;wordpieces\u0026rdquo;) for both input and output. This method provides a good balance between the flexibility of \u0026ldquo;character\u0026rdquo;-delimited models and the efficiency of \u0026ldquo;word\u0026rdquo;-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google\u0026rsquo;s phrase-based production system.\n   BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward  Florian Schmidt, T. Hofmann. (2020)\nBERT as a Teacher: Contextual Embeddings for Sequence-Level Reward\nArXiv\nPaper Link\nInfluential Citation Count (0), SS-ID (de9e7d6319b26c0d9f0da20c79403e9b9367fff4)\nABSTRACT\nMeasuring the quality of a generated sequence against a set of references is a central problem in many learning frameworks, be it to compute a score, to assign a reward, or to perform discrimination. Despite great advances in model architectures, metrics that scale independently of the number of references are still based on n-gram estimates. We show that the underlying operations, counting words and comparing counts, can be lifted to embedding words and comparing embeddings. An in-depth analysis of BERT embeddings shows empirically that contextual embeddings can be employed to capture the required dependencies while maintaining the necessary scalability through appropriate pruning and smoothing techniques. We cast unconditional generation as a reinforcement learning problem and show that our reward function indeed provides a more effective learning signal than n-gram reward in this challenging setting.\n   BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. (2019)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nNAACL\nPaper Link\nInfluential Citation Count (9858), SS-ID (df2b0e26d0599ce3e70df8a9da02e51594e0e992)\nABSTRACT\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n   Constructions at Work: The Nature of Generalization in Language  A. Goldberg. (2006)\nConstructions at Work: The Nature of Generalization in Language\nPaper Link\nInfluential Citation Count (229), SS-ID (dfc79017e52efb270155ce8b93337467804cb697)\nABSTRACT\nPart One: Constructions 1. Overview 2. Surface Generalizations 3. Item Specific Knowledge and Generalizations Part Two: Learning Generalizations 4. How Generalizations are Learned 5. How Generalizations are Constrained 6. Why Generalizations are Learned Part Three: Explaining Generalizations 7. Island Constraints and Scope 8. Grammatical Categorization: Subject Auxiliary Inversion 9. Cross-linguistic Generalizations in Argument Realization 10. Variations on a Constructionist Theme 11. Conclusion References Index\n   XLNet: Generalized Autoregressive Pretraining for Language Understanding  Zhilin Yang, Zihang Dai, Yiming Yang, J. Carbonell, R. Salakhutdinov, Quoc V. Le. (2019)\nXLNet: Generalized Autoregressive Pretraining for Language Understanding\nNeurIPS\nPaper Link\nInfluential Citation Count (631), SS-ID (e0c6abdbdecf04ffac65c440da77fb9d66bb474c)\nABSTRACT\nWith the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.\n   What do you learn from context? Probing for sentence structure in contextualized word representations  Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, Ellie Pavlick. (2019)\nWhat do you learn from context? Probing for sentence structure in contextualized word representations\nICLR\nPaper Link\nInfluential Citation Count (45), SS-ID (e2587eddd57bc4ba286d91b27c185083f16f40ee)\nABSTRACT\nContextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.\n   Cross-lingual Language Model Pretraining  Guillaume Lample, A. Conneau. (2019)\nCross-lingual Language Model Pretraining\nNeurIPS\nPaper Link\nInfluential Citation Count (345), SS-ID (ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc)\nABSTRACT\nRecent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT’16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.\n   Pooled Contextualized Embeddings for Named Entity Recognition  A. Akbik, Tanja Bergmann, Roland Vollgraf. (2019)\nPooled Contextualized Embeddings for Named Entity Recognition\nNAACL\nPaper Link\nInfluential Citation Count (40), SS-ID (edfe9dd16316618e694cd087d0d418dac91eb48c)\nABSTRACT\nContextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.\n   Document Classification by Word Embeddings of BERT  Hirotaka Tanaka, Hiroyuki Shinnou, Rui Cao, Jing Bai, Wen Ma. (2019)\nDocument Classification by Word Embeddings of BERT\nPACLING\nPaper Link\nInfluential Citation Count (0), SS-ID (ef1041ff14c02dc9e35317916561b904d7ef8433)\nABSTRACT\nBidirectional Encoder Representations from Transformers (BERT) is a pre-training model that uses the encoder component of a bidirectional transformer and converts an input sentence or input sentence pair into word enbeddings. The performance of various natural language processing systems has been greatly improved by BERT. However, for a real task, it is necessary to consider how BERT is used based on the type of task. The standerd method for document classification by BERT is to treat the word embedding of special token [CLS] as a feature vector of the document, and to fine-tune the entire model of the classifier, including a pre-training model. However, after normalizing each the feature vector consisting of the mean vector of word embeddings outputted by BERT for the document, and the feature vectors according to the bag-of-words model, we create a vector concatenating them. Our proposed method involves using the concatenated vector as the feature vector of the document.\n   Pay Less Attention with Lightweight and Dynamic Convolutions  Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, Michael Auli. (2019)\nPay Less Attention with Lightweight and Dynamic Convolutions\nICLR\nPaper Link\nInfluential Citation Count (65), SS-ID (ef523bb9437178c50d1b1e3e3ca5fb230ab37e3f)\nABSTRACT\nSelf-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.\n   Assessing BERT's Syntactic Abilities  Yoav Goldberg. (2019)\nAssessing BERT\u0026rsquo;s Syntactic Abilities\nArXiv\nPaper Link\nInfluential Citation Count (17), SS-ID (efeab0dcdb4c1cce5e537e57745d84774be99b9a)\nABSTRACT\nI assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) \u0026ldquo;coloreless green ideas\u0026rdquo; subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.\n   Further Boosting BERT-based Models by Duplicating Existing Layers: Some Intriguing Phenomena inside BERT  Wei-Tsung Kao, Tsung-Han Wu, Po-Han Chi, Chun-Cheng Hsieh, Hung-yi Lee. (2020)\nFurther Boosting BERT-based Models by Duplicating Existing Layers: Some Intriguing Phenomena inside BERT\nArXiv\nPaper Link\nInfluential Citation Count (1), SS-ID (f18fa3728868af6c44bb1dc3e913925abc37b5c1)\nABSTRACT\nAlthough Bidirectional Encoder Representations from Transformers (BERT) have achieved tremendous success in many natural language processing (NLP) tasks, it remains a black box, so much previous work has tried to lift the veil of BERT and understand the functionality of each layer. In this paper, we found that removing or duplicating most layers in BERT would not change their outputs. This fact remains true across a wide variety of BERT-based models. Based on this observation, we propose a quite simple method to boost the performance of BERT. By duplicating some layers in the BERT-based models to make it deeper (no extra training required in this step), they obtain better performance in the down-stream tasks after fine-tuning.\n   GloVe: Global Vectors for Word Representation  Jeffrey Pennington, R. Socher, Christopher D. Manning. (2014)\nGloVe: Global Vectors for Word Representation\nEMNLP\nPaper Link\nInfluential Citation Count (3447), SS-ID (f37e1b62a767a307c046404ca96bc140b3e68cb5)\nABSTRACT\nRecent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.\n   Probing Neural Network Comprehension of Natural Language Arguments  Timothy Niven, Hung-Yu Kao. (2019)\nProbing Neural Network Comprehension of Natural Language Arguments\nACL\nPaper Link\nInfluential Citation Count (12), SS-ID (f3b89e9a2b8ce1b6058e6984c3556bc2dded0938)\nABSTRACT\nWe are surprised to find that BERT’s peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.\n   Reducing Transformer Depth on Demand with Structured Dropout  Angela Fan, Edouard Grave, Armand Joulin. (2019)\nReducing Transformer Depth on Demand with Structured Dropout\nICLR\nPaper Link\nInfluential Citation Count (47), SS-ID (f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1)\nABSTRACT\nOverparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.\n   Information-Theoretic Probing with Minimum Description Length  Elena Voita, Ivan Titov. (2020)\nInformation-Theoretic Probing with Minimum Description Length\nEMNLP\nPaper Link\nInfluential Citation Count (13), SS-ID (f4b585c9a79dfce0807b445a09036ea0f9cbcdce)\nABSTRACT\nTo measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates \u0026ldquo;the amount of effort\u0026rdquo; needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.\n   Adaptively Sparse Transformers  Gonçalo M. Correia, Vlad Niculae, André F. T. Martins. (2019)\nAdaptively Sparse Transformers\nEMNLP\nPaper Link\nInfluential Citation Count (18), SS-ID (f6390beca54411b06f3bde424fb983a451789733)\nABSTRACT\nAttention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter – which controls the shape and sparsity of alpha-entmax – allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.\n   UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training  Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, H. Hon. (2020)\nUniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training\nICML\nPaper Link\nInfluential Citation Count (23), SS-ID (f64e1d6bc13aae99aab5449fc9ae742a9ba7761e)\nABSTRACT\nWe propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With well-designed position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global masking information, so that all the position embeddings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art results on a wide range of natural language understanding and generation tasks across several widely used benchmarks.\n   Inducing Relational Knowledge from BERT  Zied Bouraoui, José Camacho-Collados, S. Schockaert. (2019)\nInducing Relational Knowledge from BERT\nAAAI\nPaper Link\nInfluential Citation Count (10), SS-ID (f67fcbb1aec92ae293998ddfd904f61a31bef334)\nABSTRACT\nOne of the most remarkable properties of word embeddings is the fact that they capture certain types of semantic and syntactic relationships. Recently, pre-trained language models such as BERT have achieved groundbreaking results across a wide range of Natural Language Processing tasks. However, it is unclear to what extent such models capture relational knowledge beyond what is already captured by standard word embeddings. To explore this question, we propose a methodology for distilling relational knowledge from a pre-trained language model. Starting from a few seed instances of a given relation, we first use a large text corpus to find sentences that are likely to express this relation. We then use a subset of these extracted sentences as templates. Finally, we fine-tune a language model to predict whether a given word pair is likely to be an instance of some relation, when given an instantiated template for that relation as input.\n   Linguistic Knowledge and Transferability of Contextual Representations  Nelson F. Liu, Matt Gardner, Y. Belinkov, Matthew E. Peters, Noah A. Smith. (2019)\nLinguistic Knowledge and Transferability of Contextual Representations\nNAACL\nPaper Link\nInfluential Citation Count (108), SS-ID (f6fbb6809374ca57205bd2cf1421d4f4fa04f975)\nABSTRACT\nContextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.\n   Commonsense Knowledge Mining from Pretrained Models  Joshua Feldman, Joe Davison, Alexander M. Rush. (2019)\nCommonsense Knowledge Mining from Pretrained Models\nEMNLP\nPaper Link\nInfluential Citation Count (17), SS-ID (f98e135986414cccf29aec593d547c0656e4d82c)\nABSTRACT\nInferring commonsense knowledge is a key challenge in machine learning. Due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple’s validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though we do worse on a held-out test set than models explicitly trained on a corresponding training set, our approach outperforms these methods when mining commonsense knowledge from new sources, suggesting that our unsupervised technique generalizes better than current supervised approaches.\n   Green AI  Roy Schwartz, Jesse Dodge, Noah Smith, Oren Etzioni. (2019)\nGreen AI\nCommun. ACM\nPaper Link\nInfluential Citation Count (17), SS-ID (fb73b93de3734a996829caf31e4310e0054e9c6b)\nABSTRACT\nCreating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.\n  ","date":"May 6, 2022","hero":"/blog-akitenkrad/posts/papers/20220506021208/hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/posts/papers/20220506021208/","summary":"Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments  Citation  Rogers, A., Kovaleva, O., \u0026amp; Rumshisky, A. (2020).\nA Primer in BERTology: What We Know About How BERT Works.\nTransactions of the Association for Computational Linguistics, 8, 842–866.\nPaper Link\n Abstract  Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model.","tags":["Round-1","Survey","BERT","BERTology"],"title":"A Primer in BERTology: What We Know About How BERT Works"},{"categories":null,"contents":" Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments  Citation  Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., \u0026amp; Yih, W. (2020).\nDense Passage Retrieval for Open-Domain Question Answering.\nPaper Link\n Abstract  Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.\n What\u0026rsquo;s New  QAタスクにおいて，追加で事前学習せずにDense Embeddingを学習する方法（Deep Passage Retriever）を提案 QuestionとPassageを別々にEmbeddingするDual Encoderモデルにおいて，効率的な学習方法を採用 Deep Passage Retrieverで，既存手法（TF-IDF，BM25）を上回る精度を達成 上記の観点を複数のQAデータセットで検証  Dataset  Natural Questions  Kwiatkowski, T. et al. (2019).\nNatural Questions: A Benchmark for Question Answering Research.\nTransactions of the Association for Computational Linguistics, 7, 453–466.\nhttps://doi.org/10.1162/TACL_A_00276\n   TriviaQA  Joshi, M., Choi, E., Weld, D. S., \u0026amp; Zettlemoyer, L. (2017).\nTriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.\nhttps://doi.org/10.48550/arxiv.1705.03551\n   WebQuestions  Berant, J., Chou, A. K., Frostig, R., \u0026amp; Liang, P. (2013).\nSemantic Parsing on Freebase from Question-Answer Pairs.\nEMNLP.\n   CuratedTRECK  Baudiš, P., \u0026amp; Šedivý, J. (2015).\nModeling of the question answering task in the YodaQA system.\nLecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 9283, 222–228.\nhttps://doi.org/10.1007/978-3-319-24027-5_20\n   SQuAD v1.1  Rajpurkar, P., Zhang, J., Lopyrev, K., \u0026amp; Liang, P. (2016).\nSQuAD: 100,000+ Questions for Machine Comprehension of Text.\nEMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings, 2383–2392.\nhttps://doi.org/10.18653/V1/D16-1264\n   Dataset Summary\n   Dataset Train(original) Train(for DPR) Dev Test     Natural Questions 79,168 58,880 8,757 3,610   TriviaQA 78,785 60,413 8,837 11,313   WebQuestions 3,417 2,474 361 2,032   CuratedTREC 1,353 1,125 133 694   SQuAD 78,713 70,096 8,886 10,570    Model Description TBD\nTraining Settings Results  SQuADを除いて，DPRは軒並みBM25よりも良い精度を出している  SQuADでは，アノテーターはPassageを見た後にQuestionを記載する手順になっているため，PassageとQuestionが非常に似たものになり，計算上，BM25で精度が高くなりやすいと考えられる SQuADではWikipediaの記事中から500件強を収集して構築されたもので，学習データにかなりバイアスがあると考えられる    References  Scaling question answering to the web  Cody C. T. Kwok, Oren Etzioni, Daniel S. Weld. (2001)\nScaling question answering to the web\nTOIS\nPaper Link\nInfluential Citation Count (22), SS-ID (016e9cc85c658c6a69710b4c617609ad2a5d3a74)\nABSTRACT\nThe wealth of information on the web makes it an attractive resource for seeking quick answers to simple, factual questions such as \u0026amp;quote;who was the first American in space?\u0026amp;quote; or \u0026amp;quote;what is the second tallest mountain in the world?\u0026amp;quote; Yet today\u0026rsquo;s most advanced web search services (e.g., Google and AskJeeves) make it surprisingly tedious to locate answers to such questions. In this paper, we extend question-answering techniques, first studied in the information retrieval literature, to the web and experimentally evaluate their performance.First we introduce Mulder, which we believe to be the first general-purpose, fully-automated question-answering system available on the web. Second, we describe Mulder\u0026rsquo;s architecture, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall. Finally, we compare Mulder\u0026rsquo;s performance to that of Google and AskJeeves on questions drawn from the TREC-8 question answering track. We find that Mulder\u0026rsquo;s recall is more than a factor of three higher than that of AskJeeves. In addition, we find that Google requires 6.6 times as much user effort to achieve the same level of recall as Mulder.\n   SQuAD: 100,000+ Questions for Machine Comprehension of Text  Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang. (2016)\nSQuAD: 100,000+ Questions for Machine Comprehension of Text\nEMNLP\nPaper Link\nInfluential Citation Count (1062), SS-ID (05dd7254b632376973f3a1b4d39485da17814df5)\nABSTRACT\nWe present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at this https URL\n   Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering  Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, Bing Xiang. (2019)\nMulti-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering\nEMNLP\nPaper Link\nInfluential Citation Count (16), SS-ID (0cf535110808d33fdf4db3ffa1621dea16e29c0d)\nABSTRACT\nBERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% F1 over all non-BERT models, and 5.8% EM and 6.5% F1 over BERT-based models.\n   Reading Wikipedia to Answer Open-Domain Questions  Danqi Chen, Adam Fisch, J. Weston, Antoine Bordes. (2017)\nReading Wikipedia to Answer Open-Domain Questions\nACL\nPaper Link\nInfluential Citation Count (280), SS-ID (104715e1097b7ebee436058bfd9f45540f269845)\nABSTRACT\nThis paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.\n   Performance Issues and Error Analysis in an Open-Domain Question Answering System  D. Moldovan, Marius Pasca, S. Harabagiu, M. Surdeanu. (2002)\nPerformance Issues and Error Analysis in an Open-Domain Question Answering System\nACL\nPaper Link\nInfluential Citation Count (12), SS-ID (1503e5c5adb0a3063d09b0f398f724d7dd26a979)\nABSTRACT\nThis paper presents an in-depth analysis of a state-of-the-art Question Answering system. Several scenarios are examined: (1) the performance of each module in a serial baseline system, (2) the impact of feedbacks and the insertion of a logic prover, and (3) the impact of various lexical resources. The main conclusion is that the overall performance depends on the depth of natural language processing resources and the tools used for answer finding.\n   Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering  Sewon Min, Danqi Chen, Luke Zettlemoyer, Hannaneh Hajishirzi. (2019)\nKnowledge Guided Text Retrieval and Reading for Open Domain Question Answering\nArXiv\nPaper Link\nInfluential Citation Count (4), SS-ID (1715aa36ccc851310308630d4db61dcecf49a50d)\nABSTRACT\nWe introduce an approach for open-domain question answering (QA) that retrieves and reads a passage graph, where vertices are passages of text and edges represent relationships that are derived from an external knowledge base or co-occurrence in the same article. Our goals are to boost coverage by using knowledge-guided retrieval to find more relevant passages than text-matching methods, and to improve accuracy by allowing for better knowledge-guided fusion of information across related passages. Our graph retrieval method expands a set of seed keyword-retrieved passages by traversing the graph structure of the knowledge base. Our reader extends a BERT-based architecture and updates passage representations by propagating information from related passages and their relations, instead of reading each passage in isolation. Experiments on three open-domain QA datasets, WebQuestions, Natural Questions and TriviaQA, show improved performance over non-graph baselines by 2-11% absolute. Our approach also matches or exceeds the state-of-the-art in every case, without using an expensive end-to-end training regime.\n   Natural Questions: A Benchmark for Question Answering Research  T. Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, D. Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc V. Le, Slav Petrov. (2019)\nNatural Questions: A Benchmark for Question Answering Research\nTACL\nPaper Link\nInfluential Citation Count (121), SS-ID (17dbd7b72029181327732e4d11b52a08ed4630d0)\nABSTRACT\nWe present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.\n   Learning and Inference via Maximum Inner Product Search  Stephen Mussmann, S. Ermon. (2016)\nLearning and Inference via Maximum Inner Product Search\nICML\nPaper Link\nInfluential Citation Count (2), SS-ID (2040c5de2344b9b14f1bbfff372f708639cca739)\nABSTRACT\nA large class of commonly used probabilistic models known as log-linear models are defined up to a normalization constant. Typical learning algorithms for such models require solving a sequence of probabilistic inference queries. These inferences are typically intractable, and are a major bottleneck for learning models with large output spaces. In this paper, we provide a new approach for amortizing the cost of a sequence of related inference queries, such as the ones arising during learning. Our technique relies on a surprising connection with algorithms developed in the past two decades for similarity search in large data bases. Our approach achieves improved running times with provable approximation guarantees. We show that it performs well both on synthetic data and neural language models with large output spaces.\n   Indexing by Latent Semantic Analysis  S. Deerwester, S. Dumais, T. Landauer, G. Furnas, R. Harshman. (1990)\nIndexing by Latent Semantic Analysis\nJ. Am. Soc. Inf. Sci.\nPaper Link\nInfluential Citation Count (770), SS-ID (20a80a7356859daa4170fb4da6b87b84adbb547f)\nABSTRACT\nA new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. initial tests find this completely automatic method for retrieval to be promising.\n   Billion-Scale Similarity Search with GPUs  Jeff Johnson, M. Douze, H. Jégou. (2017)\nBillion-Scale Similarity Search with GPUs\nIEEE Transactions on Big Data\nPaper Link\nInfluential Citation Count (166), SS-ID (2cbb8de53759e75411bc528518947a3094fbce3a)\nABSTRACT\nSimilarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as $k$mml:mathmml:mik\u0026lt;/mml:mi\u0026gt;\u0026lt;/mml:math\u0026gt;-min selection, or make poor use of the memory hierarchy. We propose a novel design for $k$mml:mathmml:mik\u0026lt;/mml:mi\u0026gt;\u0026lt;/mml:math\u0026gt;-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 × faster than prior GPU state of the art. It enables the construction of a high accuracy $k$mml:mathmml:mik\u0026lt;/mml:mi\u0026gt;\u0026lt;/mml:math\u0026gt;-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.\n   End-to-End Open-Domain Question Answering with BERTserini  Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, Jimmy J. Lin. (2019)\nEnd-to-End Open-Domain Question Answering with BERTserini\nNAACL\nPaper Link\nInfluential Citation Count (45), SS-ID (2fe7dba5a58aee5156594b4d78634ecd6c7dcabd)\nABSTRACT\nWe demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.\n   A Discrete Hard EM Approach for Weakly Supervised Question Answering  Sewon Min, Danqi Chen, Hannaneh Hajishirzi, Luke Zettlemoyer. (2019)\nA Discrete Hard EM Approach for Weakly Supervised Question Answering\nEMNLP\nPaper Link\nInfluential Citation Count (19), SS-ID (30eff53e981695c7296d258b8dc44b4c7b482a0c)\nABSTRACT\nMany question answering (QA) tasks only provide weak supervision for how the answer should be computed. For example, TriviaQA answers are entities that can be mentioned multiple times in supporting documents, while DROP answers can be computed by deriving many different equations from numbers in the reference text. In this paper, we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible solutions (e.g. different mentions or equations) that contains one correct option. We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update. Despite its simplicity, we show that this approach significantly outperforms previous methods on six QA tasks, including absolute gains of 2–10%, and achieves the state-of-the-art on five of them. Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer, which we show through detailed qualitative analysis.\n   Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring  Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, J. Weston. (2020)\nPoly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring\nICLR\nPaper Link\nInfluential Citation Count (28), SS-ID (3511facfa0230b8c3ba5b72d9c11bc58f6ed6ecc)\nABSTRACT\nThe use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks.\n   BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension  M. Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer. (2019)\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\nACL\nPaper Link\nInfluential Citation Count (481), SS-ID (395de0bd3837fdf4b4b5e5f04835bcc69c279481)\nABSTRACT\nWe present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.\n   Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer  Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. (2019)\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nJ. Mach. Learn. Res.\nPaper Link\nInfluential Citation Count (615), SS-ID (3cfb319689f06bf04c2e28399361f414ca32c4b3)\nABSTRACT\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \u0026ldquo;Colossal Clean Crawled Corpus\u0026rdquo;, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.\n   Efficient Natural Language Response Suggestion for Smart Reply  Matthew Henderson, Rami Al-Rfou, B. Strope, Yun-Hsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, R. Kurzweil. (2017)\nEfficient Natural Language Response Suggestion for Smart Reply\nArXiv\nPaper Link\nInfluential Citation Count (13), SS-ID (435553998fbef790b5bed3491a8f634d9ec5cfa2)\nABSTRACT\nThis paper presents a computationally efficient machine-learned method for natural language response suggestion. Feed-forward neural networks using n-gram embedding features encode messages into vectors which are optimized to give message-response pairs a high dot-product value. An optimized search finds response suggestions. The method is evaluated in a large-scale commercial e-mail application, Inbox by Gmail. Compared to a sequence-to-sequence approach, the new system achieves the same quality at a small fraction of the computational requirements and latency.\n   Progressively Pretrained Dense Corpus Index for Open-Domain Question Answering  Wenhan Xiong, Hong Wang, W. Wang. (2020)\nProgressively Pretrained Dense Corpus Index for Open-Domain Question Answering\nEACL\nPaper Link\nInfluential Citation Count (1), SS-ID (469d92f195aebfa09e9b411ad92b3c879bcd1eba)\nABSTRACT\nCommonly used information retrieval methods such as TF-IDF in open-domain question answering (QA) systems are insufficient to capture deep semantic matching that goes beyond lexical overlaps. Some recent studies consider the retrieval process as maximum inner product search (MIPS) using dense question and paragraph representations, achieving promising results on several information-seeking QA datasets. However, the pretraining of the dense vector representations is highly resource-demanding, e.g., requires a very large batch size and lots of training steps. In this work, we propose a sample-efficient method to pretrain the paragraph encoder. First, instead of using heuristically created pseudo question-paragraph pairs for pretraining, we use an existing pretrained sequence-to-sequence model to build a strong question generator that creates high-quality pretraining data. Second, we propose a simple progressive pretraining algorithm to ensure the existence of effective negative samples in each batch. Across three open-domain QA datasets, our method consistently outperforms a strong dense retrieval baseline that uses 6 times more computation for training. On two of the datasets, our method achieves more than 4-point absolute improvement in terms of answer exact match.\n   The Probabilistic Relevance Framework: BM25 and Beyond  S. Robertson, H. Zaragoza. (2009)\nThe Probabilistic Relevance Framework: BM25 and Beyond\nFound. Trends Inf. Retr.\nPaper Link\nInfluential Citation Count (278), SS-ID (47ced790a563344efae66588b5fb7fe6cca29ed3)\nABSTRACT\nThe Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.\n   Revealing the Importance of Semantic Retrieval for Machine Reading at Scale  Yixin Nie, Songhe Wang, Mohit Bansal. (2019)\nRevealing the Importance of Semantic Retrieval for Machine Reading at Scale\nEMNLP\nPaper Link\nInfluential Citation Count (20), SS-ID (4bf61dab8ad195e87b6f0496ec7bada5d37c476f)\nABSTRACT\nMachine Reading at Scale (MRS) is a challenging task in which a system is given an input query and is asked to produce a precise output by “reading” information from a large knowledge base. The task has gained popularity with its natural combination of information retrieval (IR) and machine comprehension (MC). Advancements in representation learning have led to separated progress in both IR and MC; however, very few studies have examined the relationship and combined design of retrieval and comprehension at different levels of granularity, for development of MRS systems. In this work, we give general guidelines on system design for MRS by proposing a simple yet effective pipeline system with special consideration on hierarchical semantic retrieval at both paragraph and sentence level, and their potential effects on the downstream task. The system is evaluated on both fact verification and open-domain multihop QA, achieving state-of-the-art results on the leaderboard test sets of both FEVER and HOTPOTQA. To further demonstrate the importance of semantic retrieval, we present ablation and analysis studies to quantify the contribution of neural retrieval modules at both paragraph-level and sentence-level, and illustrate that intermediate semantic retrieval modules are vital for not only effectively filtering upstream information and thus saving downstream computation, but also for shaping upstream data distribution and providing better data for downstream modeling.\n   Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks  Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, M. Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela. (2020)\nRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\nNeurIPS\nPaper Link\nInfluential Citation Count (52), SS-ID (58ed1fbaabe027345f7bb3a6312d41c5aac63e22)\nABSTRACT\nLarge pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) \u0026ndash; models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\n   ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT  O. Khattab, M. Zaharia. (2020)\nColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\nSIGIR\nPaper Link\nInfluential Citation Count (52), SS-ID (60b8ad6177230ad5402af409a6edb5af441baeb4)\nABSTRACT\nRecent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT\u0026rsquo;s pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT\u0026rsquo;s effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.\n   Learning to rank using gradient descent  C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, Greg Hullender. (2005)\nLearning to rank using gradient descent\nICML\nPaper Link\nInfluential Citation Count (290), SS-ID (63aaf12163fe9735dfe9a69114937c4fa34f303a)\nABSTRACT\nWe investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.\n   The TREC-8 Question Answering Track Report  E. Voorhees. (1999)\nThe TREC-8 Question Answering Track Report\nTREC\nPaper Link\nInfluential Citation Count (157), SS-ID (646d4888871aca2a25111eb2520e4c47e253b014)\nABSTRACT\nThe TREC-8 Question Answering track was the first large-scale evaluation of domain-independent question answering systems. This paper summarizes the results of the track by giving a brief overview of the different approaches taken to solve the problem. The most accurate systems found a correct response for more than 2/3 of the questions. Relatively simple bag-of-words approaches were adequate for finding answers when responses could be as long as a paragraph (250 bytes), but more sophisticated processing was necessary for more direct responses (50 bytes).\n The TREC-8 Question Answering track was an initial e\u000bort to bring the bene\u000cts of large-scale evaluation to bear on a question answering (QA) task. The goal in the QA task is to retrieve small snippets of text that contain the actual answer to a question rather than the document lists traditionally returned by text retrieval systems. The assumption is that users would usually prefer to be given the answer rather than and the answer themselves in a document.\nThis paper summarizes the retrieval results of the track; a companion paper (\\The TREC-8 Question Answering Track Evaluation\u0026quot;) gives details about how the evaluation was implemented. By necessity, a track report can give only an overview of the different approaches used in the track. Readers are urged to consult the participants\u0026rsquo; papers elsewhere in the Proceedings for details regarding a particular approach.\n  Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering  Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, R. Socher, Caiming Xiong. (2019)\nLearning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\nICLR\nPaper Link\nInfluential Citation Count (26), SS-ID (6580bf92ca01a403ac58f6764dc1dd7a771579d0)\nABSTRACT\nAnswering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. Our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path. Experimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of our method. Notably, our method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.\n   Learning Dense Representations for Entity Retrieval  D. Gillick, Sayali Kulkarni, L. Lansing, A. Presta, Jason Baldridge, Eugene Ie, Diego Garcia-Olano. (2019)\nLearning Dense Representations for Entity Retrieval\nCoNLL\nPaper Link\nInfluential Citation Count (10), SS-ID (6b5cb3b85fb247256b264c2732916cf129015a92)\nABSTRACT\nWe show that it is feasible to perform entity linking by training a dual encoder (two-tower) model that encodes mentions and entities in the same dense vector space, where candidate entities are retrieved by approximate nearest neighbor search. Unlike prior work, this setup does not rely on an alias table followed by a re-ranker, and is thus the first fully learned entity retrieval model. We show that our dual encoder, trained using only anchor-text links in Wikipedia, outperforms discrete alias table and BM25 baselines, and is competitive with the best comparable results on the standard TACKBP-2010 dataset. In addition, it can retrieve candidates extremely fast, and generalizes well to a new dataset derived from Wikinews. On the modeling side, we demonstrate the dramatic value of an unsupervised negative mining algorithm for this task.\n   Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)  Anshumali Shrivastava, Ping Li. (2014)\nAsymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)\nNIPS\nPaper Link\nInfluential Citation Count (44), SS-ID (6dbffa57b3c6c5645cf701b9b444984a4b61bb57)\nABSTRACT\nWe present the first provably sublinear time algorithm for approximate \\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first hashing algorithm for searching with (un-normalized) inner product as the underlying similarity measure. Finding hashing schemes for MIPS was considered hard. We formally show that the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, and then we extend the existing LSH framework to allow asymmetric hashing schemes. Our proposal is based on an interesting mathematical phenomenon in which inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search. This key observation makes efficient sublinear hashing scheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we provide an explicit construction of provably fast hashing scheme for MIPS. The proposed construction and the extended LSH framework could be of independent theoretical interest. Our proposed algorithm is simple and easy to implement. We evaluate the method, for retrieving inner products, in the collaborative filtering task of item recommendations on Netflix and Movielens datasets.\n   Break It Down: A Question Understanding Benchmark  Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, Jonathan Berant. (2020)\nBreak It Down: A Question Understanding Benchmark\nTACL\nPaper Link\nInfluential Citation Count (10), SS-ID (71c908529b12ef6ee8d735127a63d48b1fc5c43c)\nABSTRACT\nUnderstanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the Break dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the HotpotQA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use Break to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines.\n   Modeling of the Question Answering Task in the YodaQA System  P. Baudis, J. Sedivý. (2015)\nModeling of the Question Answering Task in the YodaQA System\nCLEF\nPaper Link\nInfluential Citation Count (17), SS-ID (7e5955481e9d197cc1cd1b64a90fbd245b88c886)\nABSTRACT\nWe briefly survey the current state of art in the field of Question Answering and present the YodaQA system, an open source framework for this task and a baseline pipeline with reasonable performance. We take a holistic approach, reviewing and aiming to integrate many different question answering task definitions and approaches concerning classes of knowledge bases, question representation and answer generation. To ease performance comparisons of general-purpose QA systems, we also propose an effort in building a new reference QA testing corpus which is a curated and extended version of the TREC corpus.\n   How Much Knowledge Can You Pack into the Parameters of a Language Model?  Adam Roberts, Colin Raffel, Noam M. Shazeer. (2020)\nHow Much Knowledge Can You Pack into the Parameters of a Language Model?\nEMNLP\nPaper Link\nInfluential Citation Count (38), SS-ID (80376bdec5f534be78ba82821f540590ebce5559)\nABSTRACT\nIt has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales surprisingly well with model size and outperforms models that explicitly look up knowledge on the open-domain variants of Natural Questions and WebQuestions. To facilitate reproducibility and future work, we release our code and trained models.\n   REALM: Retrieval-Augmented Language Model Pre-Training  Kelvin Guu, Kenton Lee, Z. Tung, Panupong Pasupat, Ming-Wei Chang. (2020)\nREALM: Retrieval-Augmented Language Model Pre-Training\nArXiv\nPaper Link\nInfluential Citation Count (64), SS-ID (832fff14d2ed50eb7969c4c4b976c35776548f56)\nABSTRACT\nLanguage model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.\n   Passage Re-ranking with BERT  Rodrigo Nogueira, Kyunghyun Cho. (2019)\nPassage Re-ranking with BERT\nArXiv\nPaper Link\nInfluential Citation Count (89), SS-ID (85e07116316e686bf787114ba10ca60f4ea7c5b2)\nABSTRACT\nRecently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10. The code to reproduce our results is available at this https URL\n   Metric Learning: A Survey  B. Kulis. (2013)\nMetric Learning: A Survey\nFound. Trends Mach. Learn.\nPaper Link\nInfluential Citation Count (33), SS-ID (8916521f7d4c97befa30b06e17a7a39cda274552)\nABSTRACT\nThe metric learning problem is concerned with learning a distance function tuned to a particular task, and has been shown to be useful when used in conjunction with nearest-neighbor methods and other techniques that rely on distances or similarities. Metric Learning: A Review presents an overview of existing research in this topic, including recent progress on scaling to high-dimensional feature spaces and to data sets with an extremely large number of data points. It presents as unified a framework as possible under which existing research on metric learning can be cast. The monograph starts out by focusing on linear metric learning approaches, and mainly concentrates on the class of Mahalanobis distance learning methods. It then discusses nonlinear metric learning approaches, focusing on the connections between the non-linear and linear approaches. Finally, it discusses extensions of metric learning, as well as applications to a variety of problems in computer vision, text analysis, program analysis, and multimedia. Metric Learning: A Review is an ideal reference for anyone interested in the metric learning problem. It synthesizes much of the recent work in the area and it is hoped that it will inspire new algorithms and applications.\n   Introduction to  H. Hung, G. Toderici. (2015)\nIntroduction to\nACM Trans. Multim. Comput. Commun. Appl.\nPaper Link\nInfluential Citation Count (1), SS-ID (969dfac1f7f3f500b3976cec07ff1cc7d1a391cd)\nABSTRACT\nThis special issue continues the tradition of inviting the best papers from ACM Multimedia to extend their work to a journal article. In 2015, the conference was held in Orlando, FL, USA. A number of new areas were introduced this year. The two articles presented in this special issue came from the Deep Learning for Multimedia area and the Emotional and Social Signals in Multimedia area. As usual, a rigorous review process was carried out followed by an intense two-day colocated technical program committee meeting. Selecting the final set of best-paper candidates was a very intense process for all concerned, sparking a lot of debate about how important it is to have best paper candidates that are multimodal and take a fresh perspective on new topics. The following best paper extensions underwent a rigorous review procedure to ensure that the work was sufficiently extended compared to their respective conference paper versions. We thank the anonymous reviewers who helped to ensure the quality of these two extended papers. The first article, “Emotion Recognition During Speech Using Dynamics of Multiple Regions of Face” by Yelin Kim and Emily Mower-Provost, addresses the challenging task of performing automated facial emotion recognition when someone is speaking simultaneously. In this article, the authors exploit the context of the speech to disambiguate facial behavior that is caused by speech production from true expressions of facial emotion. They investigate an unsupervised method of segmenting the facial movements due to speech, demonstrating an improvement in facial-emotion recognition performance on the IEMOCAP and SAVEE datasets. Importantly, they describe the correspondence of their experimental findings in relation to existing emotion perception studies. This work is particularly valuable in the development of more naturalistic human-centered and emotionally aware multimedia interfaces. The second article, “Correspondence Autoencoders for Cross-Modal Retrieval” by Fangxiang Feng, Xiaojie Wang, and Ruifan Li, tackles the task of cross-modal retrieval by using correspondence autoencoder which connects the text and image modality. This enables users to issue text queries and have images retrieved using their shared representations. The authors present three distinct architectures for achieving this. A correspondence cross-modal autoencoder reconstructs its input, which may consist of text phrases or images, while using a shared bottleneck layer (with the text and image belonging to the same entity). In the second variant, the full-modal architecture, both inputs must be reconstructed given a single modality. The final deep architecture employs restricted Boltzmann machines. Experimental results show that the described architectures improve upon previously published literature in this domain on the Wikipedia, Pascal, and NUS-WIDE-10k datasets. Moreover, the authors\n   Signature Verification Using A \"Siamese\" Time Delay Neural Network  J. Bromley, James W. Bentz, L. Bottou, I. Guyon, Yann LeCun, C. Moore, Eduard Säckinger, Roopak Shah. (1993)\nSignature Verification Using A \u0026ldquo;Siamese\u0026rdquo; Time Delay Neural Network\nInt. J. Pattern Recognit. Artif. Intell.\nPaper Link\nInfluential Citation Count (153), SS-ID (997dc5d9a058753f034422afe7bd0cc0b8ad808b)\nABSTRACT\nThis paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a \u0026ldquo;Siamese\u0026rdquo; neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector with a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries.\n   An Analysis of the AskMSR Question-Answering System  E. Brill, S. Dumais, Michele Banko. (2002)\nAn Analysis of the AskMSR Question-Answering System\nEMNLP\nPaper Link\nInfluential Citation Count (18), SS-ID (9c99620d7511c83a402ff3b4b3a2348a669e61e3)\nABSTRACT\nWe describe the architecture of the AskMSR question answering system and systematically evaluate contributions of different system components to accuracy. The system differs from most question answering systems in its dependency on data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers. Because a wrong answer is often worse than no answer, we also explore strategies for predicting when the question answering system is likely to give an incorrect answer.\n   Latent Retrieval for Weakly Supervised Open Domain Question Answering  Kenton Lee, Ming-Wei Chang, Kristina Toutanova. (2019)\nLatent Retrieval for Weakly Supervised Open Domain Question Answering\nACL\nPaper Link\nInfluential Citation Count (100), SS-ID (a81874b4a651a740fffbfc47ef96515e8c7f782f)\nABSTRACT\nRecent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.\n   Semantic Parsing on Freebase from Question-Answer Pairs  Jonathan Berant, A. Chou, Roy Frostig, Percy Liang. (2013)\nSemantic Parsing on Freebase from Question-Answer Pairs\nEMNLP\nPaper Link\nInfluential Citation Count (222), SS-ID (b29447ba499507a259ae9d8f685d60cc1597d7d3)\nABSTRACT\nIn this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.\n   Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index  Minjoon Seo, Jinhyuk Lee, T. Kwiatkowski, Ankur P. Parikh, Ali Farhadi, Hannaneh Hajishirzi. (2019)\nReal-Time Open-Domain Question Answering with Dense-Sparse Phrase Index\nACL\nPaper Link\nInfluential Citation Count (12), SS-ID (b29db655a18e7417e1188ba392a06b6314f0cb87)\nABSTRACT\nExisting open-domain question answering (QA) models are not suitable for real-time usage because they need to process several long documents on-demand for every input query, which is computationally prohibitive. In this paper, we introduce query-agnostic indexable representations of document phrases that can drastically speed up open-domain QA. In particular, our dense-sparse phrase encoding effectively captures syntactic, semantic, and lexical information of the phrases and eliminates the pipeline filtering of context documents. Leveraging strategies for optimizing training and inference time, our model can be trained and deployed even in a single 4-GPU server. Moreover, by representing phrases as pointers to their start and end tokens, our model indexes phrases in the entire English Wikipedia (up to 60 billion phrases) using under 2TB. Our experiments on SQuAD-Open show that our model is on par with or more accurate than previous models with 6000x reduced computational cost, which translates into at least 68x faster end-to-end inference benchmark on CPUs. Code and demo are available at nlp.cs.washington.edu/denspi\n   Denoising Distantly Supervised Open-Domain Question Answering  Yankai Lin, Haozhe Ji, Zhiyuan Liu, Maosong Sun. (2018)\nDenoising Distantly Supervised Open-Domain Question Answering\nACL\nPaper Link\nInfluential Citation Count (21), SS-ID (ba1382a0574baa0345fd727f259bc86797fe1381)\nABSTRACT\nDistantly supervised open-domain question answering (DS-QA) aims to find answers in collections of unlabeled text. Existing DS-QA models usually retrieve related paragraphs from a large-scale corpus and apply reading comprehension technique to extract answers from the most relevant paragraph. They ignore the rich information contained in other paragraphs. Moreover, distant supervision data inevitably accompanies with the wrong labeling problem, and these noisy data will substantially degrade the performance of DS-QA. To address these issues, we propose a novel DS-QA model which employs a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines.\n   Real-time Inference in Multi-sentence Tasks with Deep Pretrained Transformers  Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, J. Weston. (2019)\nReal-time Inference in Multi-sentence Tasks with Deep Pretrained Transformers\nArXiv\nPaper Link\nInfluential Citation Count (2), SS-ID (bb2afd8172469fef7276e9789b306e085ed6e650)\nABSTRACT\nThe use of deep pretrained bidirectional transformers has led to remarkable progress in learning multi-sentence representations for downstream language understanding tasks (Devlin et al., 2018). For tasks that make pairwise comparisons, e.g. matching a given context with a corresponding response, two approaches have permeated the literature. A Cross-encoder performs full self-attention over the pair; a Bi-encoder performs self-attention for each sequence separately, and the final representation is a function of the pair. While Cross-encoders nearly always outperform Bi-encoders on various tasks, both in our work and others\u0026rsquo; (Urbanek et al., 2019), they are orders of magnitude slower, which hampers their ability to perform real-time inference. In this work, we develop a new architecture, the Poly-encoder, that is designed to approach the performance of the Cross-encoder while maintaining reasonable computation time. Additionally, we explore two pretraining schemes with different datasets to determine how these affect the performance on our chosen dialogue tasks: ConvAI2 and DSTC7 Track 1. We show that our models achieve state-of-the-art results on both tasks; that the Poly-encoder is a suitable replacement for Bi-encoders and Cross-encoders; and that even better results can be obtained by pretraining on a large dialogue dataset.\n   Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering  Gautier Izacard, Edouard Grave. (2020)\nLeveraging Passage Retrieval with Generative Models for Open Domain Question Answering\nEACL\nPaper Link\nInfluential Citation Count (44), SS-ID (bde0c85ed3d61de2a8874ddad70497b3d68bc8ad)\nABSTRACT\nGenerative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.\n   Introduction to \"This is Watson\"  D. Ferrucci. (2012)\nIntroduction to \u0026ldquo;This is Watson\u0026rdquo;\nIBM J. Res. Dev.\nPaper Link\nInfluential Citation Count (36), SS-ID (bde2d81eaf6086e664af87c04241d4ce4e1bd01b)\nABSTRACT\nIn 2007, IBM Research took on the grand challenge of building a computer system that could compete with champions at the game of Jeopardy!™. In 2011, the open-domain question-answering (QA) system, dubbed Watson, beat the two highest ranked players in a nationally televised two-game Jeopardy! match. This paper provides a brief history of the events and ideas that positioned our team to take on the Jeopardy! challenge, build Watson, IBM Watson™, and ultimately triumph. It describes both the nature of the QA challenge represented by Jeopardy! and our overarching technical approach. The main body of this paper provides a narrative of the DeepQA processing pipeline to introduce the articles in this special issue and put them in context of the overall system. Finally, this paper summarizes our main results, describing how the system, as a holistic combination of many diverse algorithmic techniques, performed at champion levels, and it briefly discusses the team\u0026rsquo;s future research plans.\n   Information science as \"Little Science\":The implications of a bibliometric analysis of theJournal of the American Society for Information Science  W. Koehler. (2001)\nInformation science as \u0026ldquo;Little Science\u0026rdquo;:The implications of a bibliometric analysis of theJournal of the American Society for Information Science\nScientometrics\nPaper Link\nInfluential Citation Count (20), SS-ID (c69317340f1684cd27af2cccdc5f3285402f8d7e)\nABSTRACT\nThis paper considers the status of information science as science through an exploration ofone of the leading journals in the field – the Journal of the American Society for InformationScience (JASIS) from its initial publication as American Documentation (AD) in 1950 through theclosing issue of its Silver Anniversary year in December 1999. It is a bibliometric examination ofAD/JASIS articles. Based on our analysis of articles published in AD and JASIS from 1950 to1999, we find that there has been a slow but perhaps inevitable shift based first on the single nonfundedresearcher and author to a much wider research and publishing participation amongauthors, regions, corporate authors, and countries. This suggests not only cross-fertilization ofideas, but also more complex research questions. A small trend toward greater external fundingfurther reinforces this hypothesis. Information may no longer be \u0026ldquo;little\u0026rdquo; science, but it is also not\u0026quot;big\u0026quot; science.\n   Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval  Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk. (2020)\nApproximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval\nICLR\nPaper Link\nInfluential Citation Count (70), SS-ID (c9b8593db099869fe7254aa1fa53f3c9073b0176)\nABSTRACT\nConducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.\n   Learning a similarity metric discriminatively, with application to face verification  S. Chopra, R. Hadsell, Yann LeCun. (2005)\nLearning a similarity metric discriminatively, with application to face verification\n2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)\nPaper Link\nInfluential Citation Count (178), SS-ID (cfaae9b6857b834043606df3342d8dc97524aa9d)\nABSTRACT\nWe present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L/sub 1/ norm in the target space approximates the \u0026ldquo;semantic\u0026rdquo; distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.\n   Learning Discriminative Projections for Text Similarity Measures  Wen-tau Yih, Kristina Toutanova, John C. Platt, Christopher Meek. (2011)\nLearning Discriminative Projections for Text Similarity Measures\nCoNLL\nPaper Link\nInfluential Citation Count (16), SS-ID (d5b034176d6021bc965fca9aa02f17864d1ccf67)\nABSTRACT\nTraditional text similarity measures consider each term similar only to itself and do not model semantic relatedness of terms. We propose a novel discriminative training method that projects the raw term vectors into a common, low-dimensional vector space. Our approach operates by finding the optimal matrix to minimize the loss of the pre-selected similarity function (e.g., cosine) of the projected vectors, and is able to efficiently handle a large number of training examples in the high-dimensional space. Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient.\n   Maximum inner-product search using cone trees  P. Ram, Alexander G. Gray. (2012)\nMaximum inner-product search using cone trees\nKDD\nPaper Link\nInfluential Citation Count (13), SS-ID (dcd2755f4e7b5ed96571ec6a741b172a217cabe2)\nABSTRACT\nThe problem of efficiently finding the best match for a query in a given set with respect to the Euclidean distance or the cosine similarity has been extensively studied. However, the closely related problem of efficiently finding the best match with respect to the inner-product has never been explored in the general setting to the best of our knowledge. In this paper we consider this problem and contrast it with the previous problems considered. First, we propose a general branch-and-bound algorithm based on a (single) tree data structure. Subsequently, we present a dual-tree algorithm for the case where there are multiple queries. Our proposed branch-and-bound algorithms are based on novel inner-product bounds. Finally we present a new data structure, the cone tree, for increasing the efficiency of the dual-tree algorithm. We evaluate our proposed algorithms on a variety of data sets from various applications, and exhibit up to five orders of magnitude improvement in query time over the naive search technique in some cases.\n   BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. (2019)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nNAACL\nPaper Link\nInfluential Citation Count (9858), SS-ID (df2b0e26d0599ce3e70df8a9da02e51594e0e992)\nABSTRACT\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n   Quantization based Fast Inner Product Search  Ruiqi Guo, Sanjiv Kumar, K. Choromanski, David Simcha. (2015)\nQuantization based Fast Inner Product Search\nAISTATS\nPaper Link\nInfluential Citation Count (6), SS-ID (e15fdad9f7d160e11e9a313bd80ebe99952eff08)\nABSTRACT\nWe propose a quantization based approach for fast approximate Maximum Inner Product Search (MIPS). Each database vector is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a database vector is approximated as the sum of inner products with the subspace quantizers. Different from recently proposed LSH approaches to MIPS, the database vectors and queries do not need to be augmented in a higher dimensional feature space. We also provide a theoretical analysis of the proposed approach, consisting of the concentration results under mild assumptions. Furthermore, if a small sample of example queries is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. Experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art.\n   Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering  R. Das, S. Dhuliawala, M. Zaheer, A. McCallum. (2019)\nMulti-step Retriever-Reader Interaction for Scalable Open-domain Question Answering\nICLR\nPaper Link\nInfluential Citation Count (13), SS-ID (e7512b84e923372ae410d7614e71224d573ed2ef)\nABSTRACT\nThis paper introduces a new framework for open-domain question answering in which the retriever and the reader iteratively interact with each other. The framework is agnostic to the architecture of the machine reading model, only requiring access to the token-level hidden representations of the reader. The retriever uses fast nearest neighbor search to scale to corpora containing millions of paragraphs. A gated recurrent unit updates the query at each step conditioned on the state of the reader and the reformulated query is used to re-rank the paragraphs by the retriever. We conduct analysis and show that iterative interaction helps in retrieving informative paragraphs from the corpus. Finally, we show that our multi-step-reasoning framework brings consistent improvement when applied to two widely used reader architectures DrQA and BiDAF on various large open-domain datasets \u0026mdash; TriviaQA-unfiltered, QuasarT, SearchQA, and SQuAD-Open.\n   TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension  Mandar Joshi, Eunsol Choi, Daniel S. Weld, Luke Zettlemoyer. (2017)\nTriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\nACL\nPaper Link\nInfluential Citation Count (191), SS-ID (f010affab57b5fcf1cd6be23df79d8ec98c7289c)\nABSTRACT\nWe present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at \u0026ndash; this http URL\n   Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering  Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming Li, Jimmy J. Lin. (2019)\nData Augmentation for BERT Fine-Tuning in Open-Domain Question Answering\nArXiv\nPaper Link\nInfluential Citation Count (6), SS-ID (f5eaf727b80240a13e9f631211c9ecec7e3b9feb)\nABSTRACT\nRecently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is \u0026ldquo;furthest\u0026rdquo; from the test data and ending with the \u0026ldquo;closest\u0026rdquo;. Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.\n   R3: Reinforced Ranker-Reader for Open-Domain Question Answering  Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, G. Tesauro, Bowen Zhou, Jing Jiang. (2018)\nR3: Reinforced Ranker-Reader for Open-Domain Question Answering\nAAAI\nPaper Link\nInfluential Citation Count (33), SS-ID (f7df82c5417b9ec7582def05b79ca080a07c4f3b)\nABSTRACT\nIn recent years researchers have achieved considerable success applying neural network methods to question answering (QA). These approaches have achieved state of the art results in simplified closed-domain settings such as the SQuAD (Rajpurkar et al. 2016) dataset, which provides a preselected passage, from which the answer to a given question may be extracted. More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al. 2017a). This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that “reads” the passages to generate an answer to the question. Performance in this setting lags well behind closed-domain performance. In this paper, we present a novel open-domain QA system called Reinforced Ranker-Reader (R), based on two algorithmic innovations. First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of extracting the ground-truth answer to a given question. Second, we propose a novel method that jointly trains the Ranker along with an answer-extraction Reader model, based on reinforcement learning. We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets. 2\n   Learning deep structured semantic models for web search using clickthrough data  Po-Sen Huang, Xiaodong He, Jianfeng Gao, L. Deng, A. Acero, Larry Heck. (2013)\nLearning deep structured semantic models for web search using clickthrough data\nCIKM\nPaper Link\nInfluential Citation Count (243), SS-ID (fdb813d8b927bdd21ae1858cafa6c34b66a36268)\nABSTRACT\nLatent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.\n  ","date":"May 5, 2022","hero":"/blog-akitenkrad/posts/papers/20220505222900/hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/posts/papers/20220505222900/","summary":"Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments  Citation  Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., \u0026amp; Yih, W. (2020).\nDense Passage Retrieval for Open-Domain Question Answering.\nPaper Link\n Abstract  Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework.","tags":["Round-1","Question Answering","Dual Encoder","BERT","Extractive MRC","SQuAD","Natural Questions","TriviaQA","WebQuestions","TREC"],"title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"categories":null,"contents":" Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments  Citation  He, P., Liu, X., Gao, J., \u0026amp; Chen, W. (2020).\nDeBERTa: Decoding-enhanced BERT with Disentangled Attention.\nPaper Link\n Abstract  Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models\u0026rsquo; generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).\n What\u0026rsquo;s New  2つの新しいテクニックを使ってBERTとRoBERTaを精度改善  Disentangled Attention  トークンの分散表現は，その内容だけではなく文書内での相対的な位置にも依存している テキストの内容とトークンの相対的な位置情報をそれぞれベクトル化し，組み合わせて分散表現を構成 Attention Weightは2つのベクトルの共起行列（disentangled matrices）によって計算される   Enhanced Mask Decoder  事前学習のデコーダで絶対的な位置情報を加味してマスクされたトークンを予測する Disentangled Attentionでも相対的な位置情報を利用しているが，予測時には絶対的な位置情報も重要となる     学習には新しいVirtual Adversarial Training (Miyato et al., 2017; Jiang et al., 2020) の学習方法 Scale-invariant-Fine-Tuning (SiFT) を提案 GitHub  https://github.com/microsoft/DeBERTa  Dataset  GLUE  Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman. (2018)\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\nBlackboxNLP@EMNLP\nhttps://www.semanticscholar.org/paper/93b8da28d006415866bf48f9a6e06b5242129195\n   SuperGLUE  Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman. (2019)\nSuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\nNeurIPS\nhttps://www.semanticscholar.org/paper/d9f6ada77448664b71128bb19df15765336974a6\n   SQuAD v1.1  Rajpurkar, P., Zhang, J., Lopyrev, K., \u0026amp; Liang, P. (2016).\nSQuAD: 100,000+ Questions for Machine Comprehension of Text.\nEMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings, 2383–2392.\nhttps://doi.org/10.18653/V1/D16-1264\n   RACE  Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, E. Hovy. (2017)\nRACE: Large-scale ReAding Comprehension Dataset From Examinations\nEMNLP\nhttps://www.semanticscholar.org/paper/636a79420d838eabe4af7fb25d6437de45ab64e8\n   ReCoRD  Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme. (2018)\nReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension\nArXiv\nhttps://www.semanticscholar.org/paper/a5b66ee341cb990f7f70a124b5fab3316d3b7e27\n   SWAG  Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi. (2018)\nSWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\nEMNLP\nhttps://www.semanticscholar.org/paper/af5c4b80fbf847f69a202ba5a780a3dd18c1a027\n   MultiNLI  Adina Williams, Nikita Nangia, Samuel R. Bowman. (2017)\nA Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\nNAACL\nPaper Link\n   CoNLL-2003  Sang, E. F. T. K., \u0026amp; de Meulder, F. (2003).\nIntroduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.\nPaper Link\n  Model Description TBD\nTraining Settings  学習データ  Wikipedia (English Wikipedia dump) BookCorpus  Zhu et al. (2015)  Yukun Zhu, Ryan Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, S. Fidler. (2015) Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books 2015 IEEE International Conference on Computer Vision (ICCV) Paper Link    OPENWEBTEXT  Gokaslan \u0026 Cohen (2019)  Aaron Gokaslan and Vanya Cohen. (2019) Openwebtext corpus. Dataset Link    STORIES  Trinh et al. (2018) Trieu H. Trinh, Quoc V. Le. (2018) A Simple Method for Commonsense Reasoning ArXiv Paper Link      モデルごとの学習データ使用状況 Results Comparison results on the GLUE dev set  RoBERTaやXLNet，ELECTRAなどの事前学習モデルは160GBの学習データを必要としたが，DeBERTaは78GBだけで学習が可能 ほぼ全てのタスクにおいて，DeBERTaが他のモデルよりも良い精度を達成している  Results on NLP Datasets References  ERNIE: Enhanced Representation through Knowledge Integration  Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu. (2019)\nERNIE: Enhanced Representation through Knowledge Integration\nArXiv\nPaper Link\nInfluential Citation Count (63), SS-ID (031e4e43aaffd7a479738dcea69a2d5be7957aa3)\nABSTRACT\nWe present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT, ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words.Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit.Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.\n   Reformer: The Efficient Transformer  Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya. (2020)\nReformer: The Efficient Transformer\nICLR\nPaper Link\nInfluential Citation Count (103), SS-ID (055fd6a9f7293269f1b22c1470e63bd02d8d9500)\nABSTRACT\nLarge Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n   SQuAD: 100,000+ Questions for Machine Comprehension of Text  Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang. (2016)\nSQuAD: 100,000+ Questions for Machine Comprehension of Text\nEMNLP\nPaper Link\nInfluential Citation Count (1062), SS-ID (05dd7254b632376973f3a1b4d39485da17814df5)\nABSTRACT\nWe present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at this https URL\n   RoBERTa: A Robustly Optimized BERT Pretraining Approach  Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, Veselin Stoyanov. (2019)\nRoBERTa: A Robustly Optimized BERT Pretraining Approach\nArXiv\nPaper Link\nInfluential Citation Count (2014), SS-ID (077f8329a7b6fa3b7c877a57b81eb6c18b5f87de)\nABSTRACT\nLanguage model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.\n   Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books  Yukun Zhu, Ryan Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, S. Fidler. (2015)\nAligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books\n2015 IEEE International Conference on Computer Vision (ICCV)\nPaper Link\nInfluential Citation Count (167), SS-ID (0e6824e137847be0599bb0032e37042ed2ef5045)\nABSTRACT\nBooks are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.\n   The Seventh PASCAL Recognizing Textual Entailment Challenge  L. Bentivogli, Peter Clark, Ido Dagan, Danilo Giampiccolo. (2011)\nThe Seventh PASCAL Recognizing Textual Entailment Challenge\nTAC\nPaper Link\nInfluential Citation Count (16), SS-ID (0f8468de03ee9f12d693237bec87916311bf1c24)\nABSTRACT\nThis paper presents the Seventh Recognizing Textual Entailment (RTE-7) challenge. This year’s challenge replicated the exercise proposed in RTE-6, consisting of a Main Task, in which Textual Entailment is performed on a real corpus in the Update Summarization scenario; a Main subtask aimed at detecting novel information; and a KBP Validation Task, in which RTE systems had to validate the output of systems participating in the KBP Slot Filling Task. Thirteen teams participated in the Main Task (submitting 33 runs) and 5 in the Novelty Detection Subtask (submitting 13 runs). The KBP Validation Task was undertaken by 2 participants which submitted 5 runs. The ablation test experiment, introduced in RTE-5 to evaluate the impact of knowledge resources used by the systems participating in the Main Task and extended also to tools in RTE-6, was also repeated in RTE-7.\n   The Winograd Schema Challenge  H. Levesque, E. Davis, L. Morgenstern. (2011)\nThe Winograd Schema Challenge\nKR\nPaper Link\nInfluential Citation Count (146), SS-ID (128cb6b891aee1b5df099acb48e2efecfcff689f)\nABSTRACT\nIn this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. Like the original, it involves responding to typed English sentences, and English-speaking adults will have no difficulty with it. Unlike the original, the subject is not required to engage in a conversation and fool an interrogator into believing she is dealing with a person. Moreover, the test is arranged in such a way that having full access to a large corpus of English text might not help much. Finally, the interrogator or a third party will be able to decide unambiguously after a few minutes whether or not a subject has passed the test.\n   The Second PASCAL Recognising Textual Entailment Challenge  Roy Bar-Haim, Ido Dagan, Bill Dolan, L. Ferro, Danilo Giampiccolo, B. Magnini. (2006)\nThe Second PASCAL Recognising Textual Entailment Challenge\nPaper Link\nInfluential Citation Count (49), SS-ID (136326377c122560768db674e35f5bcd6de3bc40)\nABSTRACT\nThis paper describes the Second PASCAL Recognising Textual Entailment Challenge (RTE-2). 1 We describe the RTE2 dataset and overview the submissions for the challenge. One of the main goals for this year’s dataset was to provide more “realistic” text-hypothesis examples, based mostly on outputs of actual systems. The 23 submissions for the challenge present diverse approaches and research directions, and the best results achieved this year are considerably higher than last year’s state of the art.\n   COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining  Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, Xia Song. (2021)\nCOCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining\nNeurIPS\nPaper Link\nInfluential Citation Count (7), SS-ID (19537be34dbadbcaa4fffcf028a8ada5095b1b5c)\nABSTRACT\nWe present a self-supervised learning framework, COCO-LM, that pretrains Language Models by COrrecting and COntrasting corrupted text sequences. Following ELECTRA-style pretraining, COCO-LM employs an auxiliary language model to corrupt text sequences, upon which it constructs two new tasks for pretraining the main model. The first token-level task, Corrective Language Modeling, is to detect and correct tokens replaced by the auxiliary model, in order to better capture token-level semantics. The second sequence-level task, Sequence Contrastive Learning, is to align text sequences originated from the same source input while ensuring uniformity in the representation space. Experiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms recent state-of-the-art pretrained models in accuracy, but also improves pretraining efficiency. It achieves the MNLI accuracy of ELECTRA with 50% of its pretraining GPU hours. With the same pretraining steps of standard base/large-sized models, COCO-LM outperforms the previous best models by 1+ GLUE average points.\n   Unified Language Model Pre-training for Natural Language Understanding and Generation  Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, M. Zhou, H. Hon. (2019)\nUnified Language Model Pre-training for Natural Language Understanding and Generation\nNeurIPS\nPaper Link\nInfluential Citation Count (106), SS-ID (1c71771c701aadfd72c5866170a9f5d71464bb88)\nABSTRACT\nThis paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.\n   Attention is All you Need  Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. (2017)\nAttention is All you Need\nNIPS\nPaper Link\nInfluential Citation Count (7560), SS-ID (204e3073870fae3d05bcbc2f6a8e263d9b72e776)\nABSTRACT\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n   Natural- to formal-language generation using Tensor Product Representations  Kezhen Chen, Qiuyuan Huang, H. Palangi, P. Smolensky, Kenneth D. Forbus, Jianfeng Gao. (2019)\nNatural- to formal-language generation using Tensor Product Representations\nArXiv\nPaper Link\nInfluential Citation Count (0), SS-ID (219e09984a2a71f54dbd86c15d31c7b0f53e1837)\nABSTRACT\nGenerating formal-language represented by relational tuples, such as Lisp programs or mathematical expressions, from a natural-language input is an extremely challenging task because it requires to explicitly capture discrete symbolic structural information from the input to generate the output. Most state-of-the-art neural sequence models do not explicitly capture such structure information, and thus do not perform well on these tasks. In this paper, we propose a new encoder-decoder model based on Tensor Product Representations (TPRs) for Natural- to Formal-language generation, called TP-N2F. The encoder of TP-N2F employs TPR \u0026lsquo;binding\u0026rsquo; to encode natural-language symbolic structure in vector space and the decoder uses TPR \u0026lsquo;unbinding\u0026rsquo; to generate a sequence of relational tuples, each consisting of a relation (or operation) and a number of arguments, in symbolic space. TP-N2F considerably outperforms LSTM-based Seq2Seq models, creating a new state of the art results on two benchmarks: the MathQA dataset for math problem solving, and the AlgoList dataset for program synthesis. Ablation studies show that improvements are mainly attributed to the use of TPRs in both the encoder and decoder to explicitly capture relational structure information for symbolic reasoning.\n   Generating Long Sequences with Sparse Transformers  Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever. (2019)\nGenerating Long Sequences with Sparse Transformers\nArXiv\nPaper Link\nInfluential Citation Count (79), SS-ID (21da617a0f79aabf94272107184606cefe90ab75)\nABSTRACT\nTransformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.\n   Adversarial Training for Large Neural Language Models  Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, Jianfeng Gao. (2020)\nAdversarial Training for Large Neural Language Models\nArXiv\nPaper Link\nInfluential Citation Count (9), SS-ID (2ffcf8352223c95ae8cef4daaec995525ecc926b)\nABSTRACT\nGeneralization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at this https URL.\n   Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer  Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. (2019)\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nJ. Mach. Learn. Res.\nPaper Link\nInfluential Citation Count (615), SS-ID (3cfb319689f06bf04c2e28399361f414ca32c4b3)\nABSTRACT\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \u0026ldquo;Colossal Clean Crawled Corpus\u0026rdquo;, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.\n   Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity  W. Fedus, Barret Zoph, Noam M. Shazeer. (2021)\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\nArXiv\nPaper Link\nInfluential Citation Count (68), SS-ID (3fd0f34117cf9395130e08c3f02ac2dadcca7206)\nABSTRACT\nIn deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) models defy this and instead select diﬀerent parameters for each incoming example. The result is a sparsely-activated model—with an outrageous number of parameters—but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs, and training instability. We address these with the introduction of the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques mitigate the instabilities, and we show large sparse models may be trained, for the ﬁrst time, with lower precision (bﬂoat16) formats. We design models based oﬀ T5-Base and T5-Large (Raﬀel et al., 2019) to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the “Colossal Clean Crawled Corpus”, and achieve a 4x speedup over the T5-XXL model. 1 fourth axis: increase the parameter count while keeping the ﬂoating point operations (FLOPs) per example constant. Our hypothesis is that the parameter count, independent of total computation performed, is a separately important axis on which to scale. We achieve this by designing a sparsely activated model that eﬃciently uses hardware designed for dense matrix multiplications such as GPUs and TPUs. Our work here focuses on TPU architectures, but these class of models may be similarly trained on GPU clusters. In our distributed training setup, our sparsely activated layers split unique weights on diﬀerent devices. Therefore, the weights of the model increase with the number of devices, all while maintaining a manageable memory and computational footprint on each device.\n   Fixing Weight Decay Regularization in Adam  I. Loshchilov, F. Hutter. (2017)\nFixing Weight Decay Regularization in Adam\nArXiv\nPaper Link\nInfluential Citation Count (93), SS-ID (45dfef0cc1ed96558c1c650432ce39d6a1050b6a)\nABSTRACT\nWe note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam\u0026rsquo;s generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code will become available after the review process.\n   Automatically Constructing a Corpus of Sentential Paraphrases  W. Dolan, Chris Brockett. (2005)\nAutomatically Constructing a Corpus of Sentential Paraphrases\nIJCNLP\nPaper Link\nInfluential Citation Count (132), SS-ID (475354f10798f110d34792b6d88f31d6d5cb099e)\nABSTRACT\nAn obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases. This paper describes the creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase. The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data. These pairs were then submitted to human judges, who confirmed that 67% were in fact semantically equivalent. In addition to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters.\n   Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning  Takeru Miyato, S. Maeda, Masanori Koyama, S. Ishii. (2017)\nVirtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning\nIEEE Transactions on Pattern Analysis and Machine Intelligence\nPaper Link\nInfluential Citation Count (281), SS-ID (4b1c6f6521da545892f3f5dc39461584d4a27ec0)\nABSTRACT\nWe propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only “virtually” adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.\n   Know What You Don’t Know: Unanswerable Questions for SQuAD  Pranav Rajpurkar, Robin Jia, Percy Liang. (2018)\nKnow What You Don’t Know: Unanswerable Questions for SQuAD\nACL\nPaper Link\nInfluential Citation Count (338), SS-ID (4d1c856275744c0284312a3a50efb6ca9dc4cd4c)\nABSTRACT\nExtractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.\n   Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning  Tao Shen, Yi Mao, Pengcheng He, Guodong Long, Adam Trischler, Weizhu Chen. (2020)\nExploiting Structured Knowledge in Text via Graph-Guided Representation Learning\nEMNLP\nPaper Link\nInfluential Citation Count (2), SS-ID (4f42a0782f8b25fff62214e70bc43ce88f914c19)\nABSTRACT\nIn this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs. Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text. This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions. In addition we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective which is optimized jointly with masked language model. In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text. It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples. Experiments show that our proposed model achieves improved performance on five benchmark datasets, including question answering and knowledge base completion tasks.\n   On the Variance of the Adaptive Learning Rate and Beyond  Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han. (2019)\nOn the Variance of the Adaptive Learning Rate and Beyond\nICLR\nPaper Link\nInfluential Citation Count (139), SS-ID (5759a53418ae3fe74ce96c531617914e7656e45e)\nABSTRACT\nThe learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: this https URL.\n   A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference  Adina Williams, Nikita Nangia, Samuel R. Bowman. (2017)\nA Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\nNAACL\nPaper Link\nInfluential Citation Count (438), SS-ID (5ded2b8c64491b4a67f6d39ce473d4b9347a672e)\nABSTRACT\nThis paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.\n   RACE: Large-scale ReAding Comprehension Dataset From Examinations  Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, E. Hovy. (2017)\nRACE: Large-scale ReAding Comprehension Dataset From Examinations\nEMNLP\nPaper Link\nInfluential Citation Count (179), SS-ID (636a79420d838eabe4af7fb25d6437de45ab64e8)\nABSTRACT\nWe present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students’ ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines.\n   Multi-Task Deep Neural Networks for Natural Language Understanding  Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao. (2019)\nMulti-Task Deep Neural Networks for Natural Language Understanding\nACL\nPaper Link\nInfluential Citation Count (168), SS-ID (658721bc13b0fa97366d38c05a96bf0a9f4bb0ac)\nABSTRACT\nIn this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.\n   X-SQL: reinforce schema representation with context  Pengcheng He, Yi Mao, K. Chakrabarti, Weizhu Chen. (2019)\nX-SQL: reinforce schema representation with context\nArXiv\nPaper Link\nInfluential Citation Count (11), SS-ID (658d6885db65a82d6b2eff926f5c4017bf99c9da)\nABSTRACT\nIn this work, we present X-SQL, a new network architecture for the problem of parsing natural language to SQL query. X-SQL proposes to enhance the structural schema representation with the contextual output from BERT-style pre-training model, and together with type information to learn a new schema representation for down-stream tasks. We evaluated X-SQL on the WikiSQL dataset and show its new state-of-the-art performance.\n   Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank  R. Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, Christopher Potts. (2013)\nRecursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\nEMNLP\nPaper Link\nInfluential Citation Count (861), SS-ID (687bac2d3320083eb4530bf18bb8f8f721477600)\nABSTRACT\nSemantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.\n   Language Models are Few-Shot Learners  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, Rewon Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. (2020)\nLanguage Models are Few-Shot Learners\nNeurIPS\nPaper Link\nInfluential Citation Count (428), SS-ID (6b85b63579a916f705a8e10a49bd8d849d91b1fc)\nABSTRACT\nRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\u0026rsquo;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.\n   Longformer: The Long-Document Transformer  Iz Beltagy, Matthew E. Peters, Arman Cohan. (2020)\nLongformer: The Long-Document Transformer\nArXiv\nPaper Link\nInfluential Citation Count (195), SS-ID (71b6394ad5654f5cd0fba763768ba4e523f7bbca)\nABSTRACT\nTransformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer\u0026rsquo;s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.\n   ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators  Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. (2020)\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\nICLR\nPaper Link\nInfluential Citation Count (284), SS-ID (756810258e3419af76aff38c895c20343b0602d0)\nABSTRACT\nWhile masked language modeling (MLM) pre-training methods such as BERT produce excellent results on downstream NLP tasks, they require large amounts of compute to be effective. These approaches corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some input tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the model learns from all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by methods such as BERT and XLNet given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where we match the performance of RoBERTa, the current state-of-the-art pre-trained transformer, while using less than 1/4 of the compute.\n   ALBERT: A Lite BERT for Self-supervised Learning of Language Representations  Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. (2019)\nALBERT: A Lite BERT for Self-supervised Learning of Language Representations\nICLR\nPaper Link\nInfluential Citation Count (574), SS-ID (7a064df1aeada7e69e5173f7d4c8606f4470365b)\nABSTRACT\nIncreasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.\n   SpanBERT: Improving Pre-training by Representing and Predicting Spans  Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, Omer Levy. (2019)\nSpanBERT: Improving Pre-training by Representing and Predicting Spans\nTACL\nPaper Link\nInfluential Citation Count (175), SS-ID (81f5810fbbab9b7203b9556f4ce3c741875407bc)\nABSTRACT\nWe present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1\n   Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism  M. Shoeybi, M. Patwary, Raul Puri, P. LeGresley, J. Casper, Bryan Catanzaro. (2019)\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nArXiv\nPaper Link\nInfluential Citation Count (70), SS-ID (8323c591e119eb09b28b29fd6c7bc76bd889df7a)\nABSTRACT\nRecent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).\n   GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding  Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman. (2018)\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\nBlackboxNLP@EMNLP\nPaper Link\nInfluential Citation Count (622), SS-ID (93b8da28d006415866bf48f9a6e06b5242129195)\nABSTRACT\nHuman ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.\n   Language Models are Unsupervised Multitask Learners  Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, Ilya Sutskever. (2019)\nLanguage Models are Unsupervised Multitask Learners\nPaper Link\nInfluential Citation Count (1306), SS-ID (9405cc0d6169988371b2755e573cc28650d14dfe)\nABSTRACT\nNatural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\n   Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems  Geoffrey E. Hinton. (1991)\nTensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems\nPaper Link\nInfluential Citation Count (52), SS-ID (9438172bfbb74a6a4ea4242b180d4335bb1f18b7)\nABSTRACT\nThis chapter contains sections titled: 1. Introduction, 2. Connectionist Representation and Tensor Product Binding: Definition and Examples, 3. Tensor Product Representation: Properties, 4. Conclusion\n   Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences  Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, D. Roth. (2018)\nLooking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences\nNAACL\nPaper Link\nInfluential Citation Count (46), SS-ID (99ad0533f84c110da2d0713d5798e6e14080b159)\nABSTRACT\nWe present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1%. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.\n   SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation  Daniel Matthew Cer, Mona T. Diab, Eneko Agirre, I. Lopez-Gazpio, Lucia Specia. (2017)\nSemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation\nSemEval@ACL\nPaper Link\nInfluential Citation Count (157), SS-ID (a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096)\nABSTRACT\nSemantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).\n   ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension  Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme. (2018)\nReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension\nArXiv\nPaper Link\nInfluential Citation Count (28), SS-ID (a5b66ee341cb990f7f70a124b5fab3316d3b7e27)\nABSTRACT\nWe present a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning. Experiments on this dataset demonstrate that the performance of state-of-the-art MRC systems fall far behind human performance. ReCoRD represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. ReCoRD is available at this http URL\n   Adam: A Method for Stochastic Optimization  Diederik P. Kingma, Jimmy Ba. (2014)\nAdam: A Method for Stochastic Optimization\nICLR\nPaper Link\nInfluential Citation Count (14594), SS-ID (a6cb366736791bcccc5c8639de5a8f9636bf87e8)\nABSTRACT\nWe introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.\n   WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations  Mohammad Taher Pilehvar, José Camacho-Collados. (2018)\nWiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations\nNAACL\nPaper Link\nInfluential Citation Count (7), SS-ID (a925f818f787e142c5f6bcb7bbd7ede2deb34860)\nABSTRACT\nBy design, word embeddings are unable to model the dynamic nature of words’ semantics, i.e., the property of words to correspond to potentially different meanings. To address this limitation, dozens of specialized meaning representation techniques such as sense or contextualized embeddings have been proposed. However, despite the popularity of research on this topic, very few evaluation benchmarks exist that specifically focus on the dynamic semantics of words. In this paper we show that existing models have surpassed the performance ceiling of the standard evaluation dataset for the purpose, i.e., Stanford Contextual Word Similarity, and highlight its shortcomings. To address the lack of a suitable benchmark, we put forward a large-scale Word in Context dataset, called WiC, based on annotations curated by experts, for generic evaluation of context-sensitive representations. WiC is released in https://pilehvar.github.io/wic/.\n   SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization  Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, T. Zhao. (2019)\nSMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\nACL\nPaper Link\nInfluential Citation Count (24), SS-ID (ab70853cd5912c470f6ff95e95481980f0a2a41b)\nABSTRACT\nTransfer learning has fundamentally changed the landscape of natural language processing (NLP). Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance. The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating. Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI. Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.\n   Deep Learning Based Text Classification: A Comprehensive Review  Shervin Minaee, E. Cambria, Jianfeng Gao. (2021)\nDeep Learning Based Text Classification: A Comprehensive Review\nPaper Link\nInfluential Citation Count (9), SS-ID (adc61e21eafecfbf6ebecc570f9f913659a2bfb2)\nABSTRACT\nDeep learning basedmodels have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions. Additional\n   SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference  Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi. (2018)\nSWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\nEMNLP\nPaper Link\nInfluential Citation Count (78), SS-ID (af5c4b80fbf847f69a202ba5a780a3dd18c1a027)\nABSTRACT\nGiven a partial description like “she opened the hood of the car,” humans can reason about the situation and anticipate what might come next (”then, she examined the engine”). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.\n   The Third PASCAL Recognizing Textual Entailment Challenge  Danilo Giampiccolo, B. Magnini, Ido Dagan, W. Dolan. (2007)\nThe Third PASCAL Recognizing Textual Entailment Challenge\nACL-PASCAL@ACL\nPaper Link\nInfluential Citation Count (62), SS-ID (b2815bc4c9e4260227cd7ca0c9d68d41c4c2f58b)\nABSTRACT\n   Transformer-XL: Attentive Language Models beyond a Fixed-Length Context  Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov. (2019)\nTransformer-XL: Attentive Language Models beyond a Fixed-Length Context\nACL\nPaper Link\nInfluential Citation Count (238), SS-ID (c4744a7c2bb298e4a52289a1e085c71cc3d37bc6)\nABSTRACT\nTransformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n   Self-Attention with Relative Position Representations  Peter Shaw, Jakob Uszkoreit, Ashish Vaswani. (2018)\nSelf-Attention with Relative Position Representations\nNAACL\nPaper Link\nInfluential Citation Count (134), SS-ID (c8efcc854d97dfc2a42b83316a2109f9d166e43f)\nABSTRACT\nRelying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.\n   Neural Network Acceptability Judgments  Alex Warstadt, Amanpreet Singh, Samuel R. Bowman. (2018)\nNeural Network Acceptability Judgments\nTransactions of the Association for Computational Linguistics\nPaper Link\nInfluential Citation Count (89), SS-ID (cb0f3ee1e98faf92429d601cdcd76c69c1e484eb)\nABSTRACT\nAbstract This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.’s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.\n   StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding  Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, Luo Si. (2019)\nStructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding\nICLR\nPaper Link\nInfluential Citation Count (21), SS-ID (d56c1fc337fb07ec004dc846f80582c327af717c)\nABSTRACT\nRecently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.\n   A Simple Method for Commonsense Reasoning  Trieu H. Trinh, Quoc V. Le. (2018)\nA Simple Method for Commonsense Reasoning\nArXiv\nPaper Link\nInfluential Citation Count (16), SS-ID (d7b6753a2d4a2b286c396854063bde3a91b75535)\nABSTRACT\nCommonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset (Levesque et al., 2011). In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.\n   Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving  Imanol Schlag, P. Smolensky, Roland Fernandez, N. Jojic, J. Schmidhuber, Jianfeng Gao. (2019)\nEnhancing the Transformer with Explicit Relational Encoding for Math Problem Solving\nArXiv\nPaper Link\nInfluential Citation Count (1), SS-ID (d88f31a0091eee02c5a2aa2013914818cdef114e)\nABSTRACT\nWe incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure. Our Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the recently-introduced Mathematics Dataset containing 56 categories of free-form math word-problems. The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of standard attention. The TP-Transformer\u0026rsquo;s attention maps give better insights into how it is capable of solving the Mathematics Dataset\u0026rsquo;s challenging problems. Pretrained models and code will be made available after publication.\n   SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems  Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman. (2019)\nSuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\nNeurIPS\nPaper Link\nInfluential Citation Count (137), SS-ID (d9f6ada77448664b71128bb19df15765336974a6)\nABSTRACT\nIn the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at this http URL.\n   The Sixth PASCAL Recognizing Textual Entailment Challenge  L. Bentivogli, Peter Clark, Ido Dagan, Danilo Giampiccolo. (2009)\nThe Sixth PASCAL Recognizing Textual Entailment Challenge\nTAC\nPaper Link\nInfluential Citation Count (80), SS-ID (db8885a0037fe47d973ade79d696586453710233)\nABSTRACT\nThis paper presents the Sixth Recognizing Textual Entailment (RTE-6) challenge. This year a major innovation was introduced, as the traditional Main Task was replaced by a new task, similar to the RTE-5 Search Pilot, in which Textual Entailment is performed on a real corpus in the Update Summarization scenario. A subtask was also proposed, aimed at detecting novel information. To continue the effort of testing RTE in NLP applications, a KBP Validation Pilot Task was set up, in which RTE systems had to validate the output of systems participating in the KBP Slot Filling Task. Eighteen teams participated in the Main Task (48 submitted runs) and 9 in the Novelty Detection Subtask (22 submitted runs). As for the Pilot, 10 runs were submitted by 3 participants. Finally, the exploratory effort started in RTE-5 to perform resource evaluation through ablation tests was not only reiterated in RTE-6, but also extended to tools.\n   The PASCAL Recognising Textual Entailment Challenge  Ido Dagan, Oren Glickman, B. Magnini. (2007)\nThe PASCAL Recognising Textual Entailment Challenge\nMLCW\nPaper Link\nInfluential Citation Count (230), SS-ID (de794d50713ea5f91a7c9da3d72041e2f5ef8452)\nABSTRACT\nThis paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year\u0026rsquo;s dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges.\n   BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. (2019)\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nNAACL\nPaper Link\nInfluential Citation Count (9858), SS-ID (df2b0e26d0599ce3e70df8a9da02e51594e0e992)\nABSTRACT\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n   XLNet: Generalized Autoregressive Pretraining for Language Understanding  Zhilin Yang, Zihang Dai, Yiming Yang, J. Carbonell, R. Salakhutdinov, Quoc V. Le. (2019)\nXLNet: Generalized Autoregressive Pretraining for Language Understanding\nNeurIPS\nPaper Link\nInfluential Citation Count (631), SS-ID (e0c6abdbdecf04ffac65c440da77fb9d66bb474c)\nABSTRACT\nWith the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.\n   Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing  Kamal Raj Kanakarajan, Bhuvana Kundumani, Malaikannan Sankarasubbu. (2021)\nSmall-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing\nArXiv\nPaper Link\nInfluential Citation Count (0), SS-ID (e762d09783b4cf1fd9907b48b3477eb355dd8690)\nABSTRACT\nRecent progress in the Natural Language Processing domain has given us several State-ofthe-Art (SOTA) pretrained models which can be finetuned for specific tasks. These large models with billions of parameters trained on numerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In this paper, we discuss the need for a benchmark for cost and time effective smaller models trained on a single GPU. This will enable researchers with resource constraints experiment with novel and innovative ideas on tokenization, pretraining tasks, architecture, fine tuning methods etc. We set up Small-Bench NLP, a benchmark for small efficient neural language models trained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks on the publicly available GLUE datasets and a leaderboard to track the progress of the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture achieves an average score of 81.53 which is comparable to that of BERT-Base’s 82.20 (110M parameters). Our models, code and leaderboard are available at https://github.com/small\n   Pointer Sentinel Mixture Models  Stephen Merity, Caiming Xiong, James Bradbury, R. Socher. (2016)\nPointer Sentinel Mixture Models\nICLR\nPaper Link\nInfluential Citation Count (218), SS-ID (efbd381493bb9636f489b965a2034d529cd56bcd)\nABSTRACT\nRecent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.\n   A Hybrid Neural Network Model for Commonsense Reasoning  Pengcheng He, Xiaodong Liu, Weizhu Chen, Jianfeng Gao. (2019)\nA Hybrid Neural Network Model for Commonsense Reasoning\nEMNLP\nPaper Link\nInfluential Citation Count (3), SS-ID (f5e3990ab9a67f824ef2c28114e2b158d653edca)\nABSTRACT\nThis paper proposes a hybrid neural network(HNN) model for commonsense reasoning. An HNN consists of two component models, a masked language model and a semantic similarity model, which share a BERTbased contextual encoder but use different model-specific input and output layers. HNN obtains new state-of-the-art results on three classic commonsense reasoning tasks, pushing the WNLI benchmark to 89%, the Winograd Schema Challenge (WSC) benchmark to 75.1%, and the PDP60 benchmark to 90.0%. An ablation study shows that language models and semantic similarity models are complementary approaches to commonsense reasoning, and HNN effectively combines the strengths of both. The code and pre-trained models will be publicly available at https: //github.com/namisan/mt-dnn.\n   SemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning  A. Gordon, Zornitsa Kozareva, Melissa Roemmele. (2011)\nSemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning\n*SEMEVAL\nPaper Link\nInfluential Citation Count (56), SS-ID (fb0b11046474b8f1c810f947f313c7c7229a988f)\nABSTRACT\nSemEval-2012 Task 7 presented a deceptively simple challenge: given an English sentence as a premise, select the sentence amongst two alternatives that more plausibly has a causal relation to the premise. In this paper, we describe the development of this task and its motivation. We describe the two systems that competed in this task as part of SemEval-2012, and compare their results to those achieved in previously published research. We discuss the characteristics that make this task so difficult, and offer our thoughts on how progress can be made in the future.\n   Music Transformer: Generating Music with Long-Term Structure  Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam M. Shazeer, Andrew M. Dai, M. Hoffman, Monica Dinculescu, D. Eck. (2019)\nMusic Transformer: Generating Music with Long-Term Structure\nICLR\nPaper Link\nInfluential Citation Count (35), SS-ID (fb507ada871d1e8c29e376dbf7b7879689aa89f9)\nABSTRACT\nMusic relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minutelong compositions (thousands of steps, four times the length modeled in Oore et al. (2018)) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies1. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.\n  ","date":"May 3, 2022","hero":"/blog-akitenkrad/posts/papers/20220503010000/hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/posts/papers/20220503010000/","summary":"Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments  Citation  He, P., Liu, X., Gao, J., \u0026amp; Chen, W. (2020).\nDeBERTa: Decoding-enhanced BERT with Disentangled Attention.\nPaper Link\n Abstract  Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques.","tags":["Round-1","BERT","Attention","Disentangled Attention","Virtual Adversarial Training","GLUE","SuperGLUE","SQuAD","RACE","ReCoRD","SWAG","MultiNLI","CoNLL"],"title":"DeBERTa: Decoding-Enhanced BERT with Disentangled Attention"},{"categories":null,"contents":"This is a sample post intended to test the followings:\n A different post author. Table of contents. Markdown content rendering. Math rendering. Emoji rendering.   Markdown Syntax Rendering Headings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n | Name | Age | | ----- | --- | | Bob | 27 | | Alice | 23 |    Name Age     Bob 27   Alice 23    Inline Markdown within tables | Inline\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp; | Markdown\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp; | In\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp; | Table | | ------------------------ | -------------------------- | ----------------------------------- | ------ | | *italics* | **bold** | ~~strikethrough~~\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp; | `code` |    Inline  Markdown  In  Table     italics bold strikethrough  code    Code Blocks Code block with backticks html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt;  \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt;  \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt;  \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Fruit  Apple Orange Banana   Dairy  Milk Cheese    Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n Math Rendering Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n Emoji Rendering 🙈 🙈 🙉 🙉 🙊 🙊\nExtended List {{ \u0026lt; fa-arrow-right-list \u0026gt; }} First --- Second --- Third {{ \u0026lt; /fa-arrow-right-list \u0026gt; }}  ul.fa-arrow-right-list { padding: 0.5em 1em 0.5em 2.3em; position: relative; } ul.fa-arrow-right-list li.fa-arrow-right-list { line-height: 1.5; padding: 0.5em 0; list-style-type: none !important; } li.fa-arrow-right-list:before { font: var(--fa-font-solid); content: \"\\f061\"; position: absolute; padding-top: 5pt; left: 1em; color: rgb(62, 64, 65); }  First Second Third    The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"May 3, 2022","hero":"/blog-akitenkrad/posts/introduction/hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/posts/introduction/","summary":"This is a sample post intended to test the followings:\n A different post author. Table of contents. Markdown content rendering. Math rendering. Emoji rendering.   Markdown Syntax Rendering Headings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur?","tags":["markdown"],"title":"Markdown Syntax Guide"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```\nSearching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category ``` \u0026hellip; \u0026ldquo;contents\u0026rdquo;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026ldquo;tags\u0026rdquo;:{{ .Params.tags | jsonify }}{{end}}, \u0026ldquo;categories\u0026rdquo; : {{ .Params.categories | jsonify }}, \u0026hellip; ```\nEdit fuse.js options to Search static/js/search.js ``` keys: [ \u0026ldquo;title\u0026rdquo;, \u0026ldquo;contents\u0026rdquo;, \u0026ldquo;tags\u0026rdquo;, \u0026ldquo;categories\u0026rdquo; ] ```\n","date":"January 1, 0001","hero":"/blog-akitenkrad/images/default-hero.jpg","permalink":"https://akitenkrad.github.io/blog-akitenkrad/search/","summary":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml ``` [outputs] home = [\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;JSON\u0026rdquo;] ```","tags":null,"title":"Search Results"}]