<!doctype html><html><head><title>DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/blog-akitenkrad/css/bootstrap.min.css><link rel=stylesheet href=/blog-akitenkrad/css/layouts/main.css><link rel=stylesheet href=/blog-akitenkrad/css/navigators/navbar.css><link rel=stylesheet href=/blog-akitenkrad/css/plyr.css><link rel=stylesheet href=/blog-akitenkrad/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/blog-akitenkrad/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/blog-akitenkrad/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_2.png><meta property="og:title" content="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention"><meta property="og:description" content="読んだ論文"><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/blog-akitenkrad/posts/20220503010000/"><meta property="article:published_time" content="2022-05-03T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-03T00:00:00+00:00"><meta name=description content="読んだ論文"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/blog-akitenkrad/css/layouts/single.css><link rel=stylesheet href=/blog-akitenkrad/css/navigators/sidebar.css><link rel=stylesheet href=/blog-akitenkrad/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/blog-akitenkrad><img src=/blog-akitenkrad/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_2.png alt=Logo>
Akitenkrad's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/blog-akitenkrad/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_2.png class=d-none id=main-logo alt=Logo>
<img src=/blog-akitenkrad/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_2.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/blog-akitenkrad/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/blog-akitenkrad/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-minus-circle"></i><a class=active href=/blog-akitenkrad/posts/papers/>Papers</a><ul class=active><li><a class=active href=/blog-akitenkrad/posts/20220503010000/ title=2022.05.03>2022.05.03</a></li><li><a href=/blog-akitenkrad/posts/20220505222900/ title=2022.05.05>2022.05.05</a></li></ul></li><li><a href=/blog-akitenkrad/posts/introduction/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/blog-akitenkrad/posts/20220503010000/hero.jpg)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/blog-akitenkrad/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_2.png alt="Author Image"><h5 class=author-name></h5><p>Tuesday, May 3, 2022</p></div><div class=title><h1>DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</h1></div><div class=post-content id=post-content><h2 id=citation>Citation</h2><blockquote><p>He, P., Liu, X., Gao, J., & Chen, W. (2020).<br><strong>DeBERTa: Decoding-enhanced BERT with Disentangled Attention.</strong><br><a href=https://doi.org/10.48550/arxiv.2006.03654>https://doi.org/10.48550/arxiv.2006.03654</a></p></blockquote><h2 id=abstract>Abstract</h2><blockquote><p>Recent progress in pre-trained neural language models has significantly
improved the performance of many natural language processing (NLP) tasks. In
this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT
with disentangled attention) that improves the BERT and RoBERTa models using
two novel techniques. The first is the disentangled attention mechanism, where
each word is represented using two vectors that encode its content and
position, respectively, and the attention weights among words are computed
using disentangled matrices on their contents and relative positions,
respectively. Second, an enhanced mask decoder is used to incorporate absolute
positions in the decoding layer to predict the masked tokens in model
pre-training. In addition, a new virtual adversarial training method is used
for fine-tuning to improve models&rsquo; generalization. We show that these
techniques significantly improve the efficiency of model pre-training and the
performance of both natural language understanding (NLU) and natural langauge
generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model
trained on half of the training data performs consistently better on a wide
range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),
on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).
Notably, we scale up DeBERTa by training a larger version that consists of 48
Transform layers with 1.5 billion parameters. The significant performance boost
makes the single DeBERTa model surpass the human performance on the SuperGLUE
benchmark (Wang et al., 2019a) for the first time in terms of macro-average
score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the
SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline
by a decent margin (90.3 versus 89.8).</p></blockquote><h2 id=whats-new>What&rsquo;s New</h2><h2 id=dataset>Dataset</h2><h2 id=model-description>Model Description</h2><h2 id=results>Results</h2></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-12 next-article"><a href=/blog-akitenkrad/posts/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Dense Passage Retrieval for Open-Domain Question Answering</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#citation>Citation</a></li><li><a href=#abstract>Abstract</a></li><li><a href=#whats-new>What&rsquo;s New</a></li><li><a href=#dataset>Dataset</a></li><li><a href=#model-description>Model Description</a></li><li><a href=#results>Results</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/blog-akitenkrad/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/blog-akitenkrad/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span><span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/blog-akitenkrad/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/blog-akitenkrad/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/blog-akitenkrad/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/blog-akitenkrad/js/popper.min.js></script><script type=text/javascript src=/blog-akitenkrad/js/bootstrap.min.js></script><script type=text/javascript src=/blog-akitenkrad/js/navbar.js></script><script type=text/javascript src=/blog-akitenkrad/js/plyr.js></script><script type=text/javascript src=/blog-akitenkrad/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/blog-akitenkrad/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=/blog-akitenkrad/katex/katex.min.css><script type=text/javascript defer src=/blog-akitenkrad/katex/katex.min.js></script><script type=text/javascript defer src=/blog-akitenkrad/katex/auto-render.min.js onload=renderMathInElement(document.body);>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"\\[",right:"\\]",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false}]});</script></body></html>