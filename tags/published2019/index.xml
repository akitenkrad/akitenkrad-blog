<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Published:2019 on Akitenkrad's Blog</title><link>https://akitenkrad.github.io/akitenkrad-blog/tags/published2019/</link><description>Recent content in Published:2019 on Akitenkrad's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 14 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://akitenkrad.github.io/akitenkrad-blog/tags/published2019/index.xml" rel="self" type="application/rss+xml"/><item><title>Language Models are Unsupervised Multitask Learners</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202305/20230514200241/</link><pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202305/20230514200241/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp;amp; Sutskever, I. (2019).
Language models are unsupervised multitask learners.
Persagen.Com. http://www.persagen.com/files/misc/radford2019language.pdf Abstract Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.</description></item><item><title>RoBERTa: A Robustly Optimized BERT Pretraining Approach</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220523223206/</link><pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220523223206/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &amp;amp; Stoyanov, V. (2019).
RoBERTa: A Robustly Optimized BERT Pretraining Approach.
https://doi.org/10.48550/arxiv.1907.11692 Abstract Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results.</description></item><item><title>A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220518224923/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220518224923/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Jeong, C., Jang, S., Shin, H., Park, E., &amp;amp; Choi, S. (2019).
A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks.
https://doi.org/10.48550/arxiv.1903.06464 Abstract With the tremendous growth in the number of scientific papers being published, searching for references while writing a scientific paper is a time-consuming process. A technique that could add a reference citation at the appropriate place in a sentence will be beneficial.</description></item><item><title>A Deep Cascade Model for Multi-Document Reading Comprehension</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220508162318/</link><pubDate>Sun, 08 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220508162318/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Yan, M., Xia, J., Wu, C., Bi, B., Zhao, Z., Zhang, J., Si, L., Wang, R., Wang, W., &amp;amp; Chen, H. (2019).
A Deep Cascade Model for Multi-Document Reading Comprehension.
Proceedings of the AAAI Conference on Artificial Intelligence, 33, 7354â€“7361.
https://doi.org/10.1609/aaai.v33i01.33017354 Abstract A fundamental trade-off between effectiveness and efficiency needs to be balanced when designing an online question answering system. Effectiveness comes from sophisticated functions such as extractive machine reading comprehension (MRC), while efficiency is obtained from improvements in preliminary retrieval components such as candidate document selection and paragraph ranking.</description></item></channel></rss>