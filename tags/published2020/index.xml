<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Published:2020 on Akitenkrad's Blog</title><link>https://akitenkrad.github.io/akitenkrad-blog/tags/published2020/</link><description>Recent content in Published:2020 on Akitenkrad's Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Tue, 20 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://akitenkrad.github.io/akitenkrad-blog/tags/published2020/index.xml" rel="self" type="application/rss+xml"/><item><title>A Learning-based Data Augmentation for network anomaly Detection</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202209/20220920101324/</link><pubDate>Tue, 20 Sep 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202209/20220920101324/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation al Olaimat, M., Lee, D., Kim, Y., Kim, J., &amp;amp; Kim, J. (2020).
A Learning-based Data Augmentation for Network Anomaly Detection.
International Conference on Computer Communications and Networks, ICCCN, 2020-August.
https://doi.org/10.1109/ICCCN49398.2020.9209598 Abstract While machine learning technologies have been remarkably advanced over the past several years, one of the fundamental requirements for the success of learning-based approaches would be the availability of high-quality data that thoroughly represent individual classes in a problem space.</description></item><item><title>Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202207/20220727145036/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202207/20220727145036/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Yang, L., Xiao, Z., Jiang, W., Wei, Y., Hu, Y., &amp;amp; Wang, H. (2020).
Dynamic heterogeneous graph embedding using hierarchical attentions.
Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 12036 LNCS.
https://doi.org/10.1007/978-3-030-45442-5_53 Abstract Graph embedding has attracted many research interests. Existing works mainly focus on static homogeneous/heterogeneous networks or dynamic homogeneous networks.</description></item><item><title>CodeBERT: A Pre-Trained Model for Programming and Natural Languages</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202206/20220608085622/</link><pubDate>Wed, 08 Jun 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202206/20220608085622/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., &amp;amp; Zhou, M. (2020).
CodeBERT: A Pre-Trained Model for Programming and Natural Languages.
Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020, 1536–1547.
https://doi.org/10.18653/V1/2020.FINDINGS-EMNLP.139 Abstract We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL).</description></item><item><title>Multi-Style Generative Reading Comprehension</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220511010217/</link><pubDate>Wed, 11 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220511010217/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Nishida, K., Saito, I., Nishida, K., Shinoda, K., Otsuka, A., Asano, H., &amp;amp; Tomita, J. (2020).
Multi-style Generative Reading Comprehension.
ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 2273–2284.
https://doi.org/10.18653/v1/p19-1220 Abstract This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We proposea multi-style abstractive summarization model for question answering, called Masque.</description></item><item><title>A Primer in BERTology: What We Know About How BERT Works</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220506021208/</link><pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220506021208/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Rogers, A., Kovaleva, O., &amp;amp; Rumshisky, A. (2020).
A Primer in BERTology: What We Know About How BERT Works.
Transactions of the Association for Computational Linguistics, 8, 842–866.
Paper Link Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model.</description></item><item><title>Dense Passage Retrieval for Open-Domain Question Answering</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220505222900/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220505222900/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., &amp;amp; Yih, W. (2020).
Dense Passage Retrieval for Open-Domain Question Answering.
Paper Link Abstract Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework.</description></item><item><title>DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220503010000/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220503010000/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation He, P., Liu, X., Gao, J., &amp;amp; Chen, W. (2020).
DeBERTa: Decoding-enhanced BERT with Disentangled Attention
Paper Link Abstract Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques.</description></item></channel></rss>