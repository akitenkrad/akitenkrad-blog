<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformer on Akitenkrad's Blog</title><link>https://akitenkrad.github.io/blog-akitenkrad/tags/transformer/</link><description>Recent content in Transformer on Akitenkrad's Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Sun, 29 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://akitenkrad.github.io/blog-akitenkrad/tags/transformer/index.xml" rel="self" type="application/rss+xml"/><item><title>Attention Is All You Need</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220529131339/</link><pubDate>Sun, 29 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220529131339/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp;amp; Polosukhin, I. (2017).
Attention is all you need.
Advances in Neural Information Processing Systems, 2017-Decem, 5999–6009.
http://arxiv.org/abs/1706.03762 Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism.</description></item></channel></rss>