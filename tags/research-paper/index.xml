<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>research paper on Akitenkrad's Blog</title><link>https://akitenkrad.github.io/blog-akitenkrad/tags/research-paper/</link><description>Recent content in research paper on Akitenkrad's Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Tue, 03 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://akitenkrad.github.io/blog-akitenkrad/tags/research-paper/index.xml" rel="self" type="application/rss+xml"/><item><title>DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/20220503/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/20220503/</guid><description>Citation He, P., Liu, X., Gao, J., &amp;amp; Chen, W. (2020).
DeBERTa: Decoding-enhanced BERT with Disentangled Attention.
https://doi.org/10.48550/arxiv.2006.03654
Abstract Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively.</description></item></channel></rss>