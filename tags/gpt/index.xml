<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GPT on Akitenkrad's Blog</title><link>https://akitenkrad.github.io/akitenkrad-blog/tags/gpt/</link><description>Recent content in GPT on Akitenkrad's Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Sun, 14 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://akitenkrad.github.io/akitenkrad-blog/tags/gpt/index.xml" rel="self" type="application/rss+xml"/><item><title>Language Models are Unsupervised Multitask Learners</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202305/20230514200241/</link><pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202305/20230514200241/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp;amp; Sutskever, I. (2019).
Language models are unsupervised multitask learners.
Persagen.Com. http://www.persagen.com/files/misc/radford2019language.pdf Abstract Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.</description></item><item><title>Improving Language Understanding by Generative Pre-Training</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202305/20230513094804/</link><pubDate>Sat, 13 May 2023 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202305/20230513094804/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Alec, R., &amp;amp; Karthik, N. (2018). Improving Language Understanding by Generative Pre-Training. OpenAI. https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035 Abstract We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus.</description></item></channel></rss>