<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Survey on Akitenkrad's Blog</title><link>https://akitenkrad.github.io/blog-akitenkrad/tags/survey/</link><description>Recent content in Survey on Akitenkrad's Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Fri, 06 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://akitenkrad.github.io/blog-akitenkrad/tags/survey/index.xml" rel="self" type="application/rss+xml"/><item><title>A Primer in BERTology: What We Know About How BERT Works</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/20220506021208/</link><pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/20220506021208/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Rogers, A., Kovaleva, O., &amp;amp; Rumshisky, A. (2020).
A Primer in BERTology: What We Know About How BERT Works.
Transactions of the Association for Computational Linguistics, 8, 842â€“866.
Paper Link
Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model.</description></item></channel></rss>