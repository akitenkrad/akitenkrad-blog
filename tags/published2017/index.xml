<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Published:2017 on Akitenkrad's Blog</title><link>https://akitenkrad.github.io/akitenkrad-blog/tags/published2017/</link><description>Recent content in Published:2017 on Akitenkrad's Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Sun, 12 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://akitenkrad.github.io/akitenkrad-blog/tags/published2017/index.xml" rel="self" type="application/rss+xml"/><item><title>Attributed Network Embedding for Learning in a Dynamic Environment</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202206/20220612105422/</link><pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202206/20220612105422/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, and Huan Liu. 2017.
Attributed Network Embedding for Learning in a Dynamic Environment.
In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM &amp;lsquo;17). Association for Computing Machinery, New York, NY, USA, 387–396.
https://doi.org/10.1145/3132847.3132919 Abstract Network embedding leverages the node proximity manifested to learn a low-dimensional node vector representation for each node in the network.</description></item><item><title>Attention Is All You Need</title><link>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220529131339/</link><pubDate>Sun, 29 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/akitenkrad-blog/posts/papers/202205/20220529131339/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp;amp; Polosukhin, I. (2017).
Attention is all you need.
Advances in Neural Information Processing Systems, 2017-Decem, 5999–6009.
http://arxiv.org/abs/1706.03762 Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism.</description></item></channel></rss>