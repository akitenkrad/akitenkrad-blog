<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>At:Round-2 on Akitenkrad's Blog</title><link>https://akitenkrad.github.io/blog-akitenkrad/tags/atround-2/</link><description>Recent content in At:Round-2 on Akitenkrad's Blog</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><lastBuildDate>Tue, 02 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://akitenkrad.github.io/blog-akitenkrad/tags/atround-2/index.xml" rel="self" type="application/rss+xml"/><item><title>Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202208/20220802103319/</link><pubDate>Tue, 02 Aug 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202208/20220802103319/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Xue, H., Yang, L., Jiang, W., Wei, Y., Hu, Y., &amp;amp; Lin, Y. (2021). Modeling Dynamic Heterogeneous Network for Link Prediction Using Hierarchical Attention with Temporal RNN. Springer, Cham. pp.282–298. https://doi.org/10.1007/978-3-030-67658-2_17 Abstract Network embedding aims to learn low-dimensional representations of nodes while capturing structure information of networks. It has achieved great success on many tasks of network analysis such as link prediction and node classification.</description></item><item><title>Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202207/20220727145036/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202207/20220727145036/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Yang, L., Xiao, Z., Jiang, W., Wei, Y., Hu, Y., &amp;amp; Wang, H. (2020).
Dynamic heterogeneous graph embedding using hierarchical attentions.
Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 12036 LNCS.
https://doi.org/10.1007/978-3-030-45442-5_53 Abstract Graph embedding has attracted many research interests. Existing works mainly focus on static homogeneous/heterogeneous networks or dynamic homogeneous networks.</description></item><item><title>High-order Proximity Preserved Embedding for Dynamic Networks</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220618223844/</link><pubDate>Sat, 18 Jun 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220618223844/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Zhu, D., Cui, P., Zhang, Z., Pei, J., &amp;amp; Zhu, W. (2018).
High-Order Proximity Preserved Embedding for Dynamic Networks.
IEEE Transactions on Knowledge and Data Engineering, 30(11), 2134–2144.
https://doi.org/10.1109/TKDE.2018.2822283 Abstract Network embedding, aiming to embed a network into a low dimensional vector space while preserving the inherent structural properties of the network, has attracted considerable attention. However, most existing embedding methods focus on the static network while neglecting the evolving characteristic of real-world networks.</description></item><item><title>Attributed Network Embedding for Learning in a Dynamic Environment</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220612105422/</link><pubDate>Sun, 12 Jun 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220612105422/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, and Huan Liu. 2017.
Attributed Network Embedding for Learning in a Dynamic Environment.
In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM &amp;lsquo;17). Association for Computing Machinery, New York, NY, USA, 387–396.
https://doi.org/10.1145/3132847.3132919 Abstract Network embedding leverages the node proximity manifested to learn a low-dimensional node vector representation for each node in the network.</description></item><item><title>CodeBERT: A Pre-Trained Model for Programming and Natural Languages</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220608085622/</link><pubDate>Wed, 08 Jun 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220608085622/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., &amp;amp; Zhou, M. (2020).
CodeBERT: A Pre-Trained Model for Programming and Natural Languages.
Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020, 1536–1547.
https://doi.org/10.18653/V1/2020.FINDINGS-EMNLP.139 Abstract We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL).</description></item><item><title>S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220602171700/</link><pubDate>Thu, 02 Jun 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202206/20220602171700/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du,Weifeng Lv, and Ming Zhou. 2018.
S-Net: Fromanswer extraction to answer synthesis for machinereading comprehension.
InAssociation for the Ad-vancement of Artificial Intelligence (AAAI), pages5940–5947. Abstract In this paper, we present a novel approach to machine reading comprehension forthe MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a ques-tion with exact text spans in a passage, the MS-MARCO dataset defines the taskas answering a question from multiple passages and the words in the answer arenot necessary in the passages.</description></item><item><title>Semi-Supervised Classification with Graph Convolutional Networks</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220520124748/</link><pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220520124748/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Kipf, T. N., &amp;amp; Welling, M. (2016).
Semi-Supervised Classification with Graph Convolutional Networks.
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2019)
https://doi.org/10.48550/arxiv.1609.02907 Abstract We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions.</description></item><item><title>Multi-Style Generative Reading Comprehension</title><link>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220511010217/</link><pubDate>Wed, 11 May 2022 00:00:00 +0000</pubDate><guid>https://akitenkrad.github.io/blog-akitenkrad/posts/papers/202205/20220511010217/</guid><description>Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Nishida, K., Saito, I., Nishida, K., Shinoda, K., Otsuka, A., Asano, H., &amp;amp; Tomita, J. (2020).
Multi-style Generative Reading Comprehension.
ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 2273–2284.
https://doi.org/10.18653/v1/p19-1220 Abstract This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We proposea multi-style abstractive summarization model for question answering, called Masque.</description></item></channel></rss>