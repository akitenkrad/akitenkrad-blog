<!doctype html><html><head><title>Post</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/blog-akitenkrad/css/bootstrap.min.css><link rel=stylesheet href=/blog-akitenkrad/css/layouts/main.css><link rel=stylesheet href=/blog-akitenkrad/css/navigators/navbar.css><link rel=stylesheet href=/blog-akitenkrad/css/plyr.css><link rel=stylesheet href=/blog-akitenkrad/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=/blog-akitenkrad/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/blog-akitenkrad/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="Post"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://akitenkrad.github.io/blog-akitenkrad/posts/"><link rel=stylesheet href=/blog-akitenkrad/css/layouts/list.css><link rel=stylesheet href=/blog-akitenkrad/css/navigators/sidebar.css><link rel=stylesheet href=/blog-akitenkrad/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/blog-akitenkrad><img src=/blog-akitenkrad/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div><img src=/blog-akitenkrad/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/blog-akitenkrad/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/blog-akitenkrad/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/blog-akitenkrad/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/blog-akitenkrad/posts/papers/>Papers</a><ul><li><a href=/blog-akitenkrad/posts/papers/20220503010000/ title=2022.05.03>2022.05.03</a></li><li><a href=/blog-akitenkrad/posts/papers/20220505222900/ title=2022.05.05>2022.05.05</a></li><li><a href=/blog-akitenkrad/posts/papers/20220506021208/ title=2022.05.06>2022.05.06</a></li></ul></li><li><a href=/blog-akitenkrad/posts/introduction/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid post-card-holder" id=post-card-holder><div class=post-card><a href=https://akitenkrad.github.io/blog-akitenkrad/posts/papers/20220506021208/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/blog-akitenkrad/posts/papers/20220506021208/hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>A Primer in BERTology: What We Know About How BERT Works</h5><p class="card-text post-summary">Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Rogers, A., Kovaleva, O., & Rumshisky, A. (2020).
A Primer in BERTology: What We Know About How BERT Works.
Transactions of the Association for Computational Linguistics, 8, 842–866.
Paper Link
Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model.</p></div><div class=card-footer><span class=float-left>May 6, 2022</span>
<a href=https://akitenkrad.github.io/blog-akitenkrad/posts/papers/20220506021208/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=https://akitenkrad.github.io/blog-akitenkrad/posts/papers/20220505222900/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/blog-akitenkrad/posts/papers/20220505222900/hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>Dense Passage Retrieval for Open-Domain Question Answering</h5><p class="card-text post-summary">Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation Karpukhin, V., Oğuz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., & Yih, W. (2020).
Dense Passage Retrieval for Open-Domain Question Answering.
Paper Link
Abstract Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework.</p></div><div class=card-footer><span class=float-left>May 5, 2022</span>
<a href=https://akitenkrad.github.io/blog-akitenkrad/posts/papers/20220505222900/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=https://akitenkrad.github.io/blog-akitenkrad/posts/papers/20220503010000/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/blog-akitenkrad/posts/papers/20220503010000/hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</h5><p class="card-text post-summary">Round-1: Overview Round-2: Model Implementation Details Round-3: Experiments Citation He, P., Liu, X., Gao, J., & Chen, W. (2020).
DeBERTa: Decoding-enhanced BERT with Disentangled Attention.
Paper Link
Abstract Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques.</p></div><div class=card-footer><span class=float-left>May 3, 2022</span>
<a href=https://akitenkrad.github.io/blog-akitenkrad/posts/papers/20220503010000/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div><div class=post-card><a href=https://akitenkrad.github.io/blog-akitenkrad/posts/introduction/ class=post-card-link><div class=card><div class=card-head><img class=card-img-top src=/blog-akitenkrad/posts/introduction/hero.jpg alt="Hero Image"></div><div class=card-body><h5 class=card-title>Markdown Syntax Guide</h5><p class="card-text post-summary">This is a sample post intended to test the followings:
A different post author. Table of contents. Markdown content rendering. Math rendering. Emoji rendering. Markdown Syntax Rendering Headings The following HTML &lt;h1>—&lt;h6> elements represent six levels of section headings. &lt;h1> is the highest section level while &lt;h6> is the lowest.
H1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur?</p></div><div class=card-footer><span class=float-left>May 3, 2022</span>
<a href=https://akitenkrad.github.io/blog-akitenkrad/posts/introduction/ class="float-right btn btn-outline-info btn-sm">Read</a></div></div></a></div></div><div class=paginator></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/blog-akitenkrad/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/blog-akitenkrad/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/blog-akitenkrad/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/blog-akitenkrad/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/blog-akitenkrad/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/blog-akitenkrad/js/popper.min.js></script>
<script type=text/javascript src=/blog-akitenkrad/js/bootstrap.min.js></script>
<script type=text/javascript src=/blog-akitenkrad/js/navbar.js></script>
<script type=text/javascript src=/blog-akitenkrad/js/plyr.js></script>
<script type=text/javascript src=/blog-akitenkrad/js/main.js></script>
<script src=/blog-akitenkrad/js/list.js></script></body></html>