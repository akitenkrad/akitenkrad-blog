<!doctype html><html><head><title>arXiv @ 2024.02.19</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.19"><meta property="og:description" content="Primary Categories cs.AI (2) cs.CE (1) cs.CL (37) cs.CR (3) cs.CV (12) cs.CY (1) cs.DL (1) cs.HC (4) cs.IR (4) cs.IT (2) cs.LG (26) cs.MA (1) cs.NI (2) cs.RO (3) econ.TH (1) eess.AS (4) eess.IV (3) math.CO (1) q-bio.NC (1) q-bio.QM (1) stat.ML (3) Keywords keyword cs.CL cs.CV cs.LG AI-generated Text Detection 1 Active Learning 1 Adversarial Attack 1 Autoencoder 1 2 Automatic Evaluation 1 BLEU 1 Benchmarking 10 3 4 Black Box 1 1 Chain-of-thought Prompt 1 ChatGPT 1 1 Clustering 1 1 Continual Learning 1 Convolution 1 1 1 Convolutional Neural Network 1 1 Counter-factual 2 Data Augmentation 2 Diffusion Model 1 Distribution Shift 2 Event Detection 1 Fairness 2 Federated Learning 1 Few-shot 5 Few-shot Learning 1 Fine-tuning 8 3 Foundation Model 1 1 GPT 5 1 GPT-3 1 GPT-3."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240219000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-19T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-19T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.19"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240219000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Monday, Feb 19, 2024</p></div><div class=title><h1>arXiv @ 2024.02.19</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#csai-2>cs.AI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#cscl-37>cs.CL (37)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#cscr-3>cs.CR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#cscv-12>cs.CV (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#csir-4>cs.IR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#cslg-26>cs.LG (26)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#csro-3>cs.RO (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#econth-1>econ.TH (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#eessas-4>eess.AS (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#eessiv-3>eess.IV (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>AI-generated Text Detection</td><td>1</td><td></td><td></td></tr><tr><td>Active Learning</td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td></tr><tr><td>Autoencoder</td><td></td><td>1</td><td>2</td></tr><tr><td>Automatic Evaluation</td><td>1</td><td></td><td></td></tr><tr><td>BLEU</td><td>1</td><td></td><td></td></tr><tr><td>Benchmarking</td><td>10</td><td>3</td><td>4</td></tr><tr><td>Black Box</td><td>1</td><td></td><td>1</td></tr><tr><td>Chain-of-thought Prompt</td><td>1</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>1</td><td>1</td><td></td></tr><tr><td>Clustering</td><td>1</td><td></td><td>1</td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td></tr><tr><td>Convolution</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td></td><td>1</td></tr><tr><td>Counter-factual</td><td>2</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>2</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>1</td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>2</td></tr><tr><td>Event Detection</td><td></td><td></td><td>1</td></tr><tr><td>Fairness</td><td></td><td></td><td>2</td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td></tr><tr><td>Few-shot</td><td>5</td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>8</td><td></td><td>3</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td>1</td></tr><tr><td>GPT</td><td>5</td><td></td><td>1</td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td></tr><tr><td>GPT-4</td><td>4</td><td></td><td>1</td></tr><tr><td>GPT-4 turbo</td><td>1</td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>1</td></tr><tr><td>Geometry</td><td></td><td>1</td><td></td></tr><tr><td>Graph</td><td>4</td><td>1</td><td>7</td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>2</td></tr><tr><td>Grounding</td><td></td><td>1</td><td></td></tr><tr><td>Image2text</td><td></td><td>1</td><td></td></tr><tr><td>In-context Learning</td><td>4</td><td></td><td>2</td></tr><tr><td>Information Retrieval</td><td>2</td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td></td><td>1</td></tr><tr><td>Instruction Tuning</td><td>2</td><td>1</td><td></td></tr><tr><td>Knowledge Distillation</td><td>2</td><td>4</td><td>4</td></tr><tr><td>Knowledge Graph</td><td>5</td><td></td><td></td></tr><tr><td>LLaMA</td><td>3</td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td>3</td></tr><tr><td>Language Generation</td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>55</td><td>2</td><td>4</td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>1</td></tr><tr><td>Mistral</td><td>2</td><td></td><td></td></tr><tr><td>Model Pruning</td><td>1</td><td></td><td></td></tr><tr><td>Model Quantization</td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>7</td><td>2</td><td></td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td></tr><tr><td>Named Entity Recognition</td><td>2</td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>1</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>2</td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td></tr><tr><td>Perplexity</td><td>1</td><td></td><td></td></tr><tr><td>Prompt</td><td>9</td><td>4</td><td>2</td></tr><tr><td>Prompt Learning</td><td>1</td><td></td><td></td></tr><tr><td>Pruning</td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td>3</td><td></td><td></td></tr><tr><td>Question Answering</td><td>10</td><td></td><td></td></tr><tr><td>Ransomware</td><td></td><td></td><td>1</td></tr><tr><td>Reasoning</td><td>10</td><td></td><td></td></tr><tr><td>Recommendation</td><td></td><td></td><td>1</td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>1</td></tr><tr><td>Reinforcement Learning</td><td></td><td></td><td>3</td></tr><tr><td>Relation Extraction</td><td>2</td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td>2</td></tr><tr><td>Retrieval-Augmented Generation</td><td>1</td><td></td><td></td></tr><tr><td>RoBERTa</td><td>1</td><td></td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>2</td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td></tr><tr><td>Simulation</td><td>1</td><td></td><td></td></tr><tr><td>Simulator</td><td>1</td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>1</td></tr><tr><td>Summarization</td><td></td><td></td><td>1</td></tr><tr><td>Supervised Learning</td><td></td><td>3</td><td>6</td></tr><tr><td>Text Analysis</td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td>4</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Transformer</td><td>2</td><td>4</td><td></td></tr><tr><td>Unsupervised Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td>8</td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>2</td><td></td></tr><tr><td>Zero-shot</td><td>3</td><td>1</td><td>1</td></tr><tr><td>Zero-shot Learning</td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-37>cs.CL (37)</h2><h3 id=137--1113-c-icl-contrastive-in-context-learning-for-information-extraction-ying-mo-et-al-2024>(1/37 | 1/113) C-ICL: Contrastive In-context Learning for Information Extraction (Ying Mo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Mo, Jian Yang, Jiahao Liu, Shun Zhang, Jingang Wang, Zhoujun Li. (2024)<br><strong>C-ICL: Contrastive In-context Learning for Information Extraction</strong><br><button class=copy-to-clipboard title="C-ICL: Contrastive In-context Learning for Information Extraction" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Few-shot, Information Retrieval, Named Entity Recognition, Named Entity Recognition, Reasoning, Relation Extraction, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11254v1.pdf filename=2402.11254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been increasing interest in exploring the capabilities of advanced <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in the field of <b>information</b> <b>extraction</b> (IE), specifically focusing on tasks related to <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER)</b> and <b>relation</b> <b>extraction</b> (RE). Although researchers are exploring the use of <b>few-shot</b> <b>information</b> <b>extraction</b> through <b>in-context</b> <b>learning</b> with <b>LLMs,</b> they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process. In this paper, we present c-ICL, a novel <b>few-shot</b> technique that leverages both correct and incorrect sample constructions to create <b>in-context</b> <b>learning</b> demonstrations. This approach enhances the ability of <b>LLMs</b> to extract entities and <b>relations</b> <b>by</b> utilizing <b>prompts</b> that incorporate not only the positive samples but also the <b>reasoning</b> behind them. This method allows for the identification and correction of potential interface errors. Specifically, our proposed method taps into the inherent contextual <b>information</b> <b>and</b> valuable <b>information</b> <b>in</b> hard negative samples and the nearest positive neighbors to the test and then applies the <b>in-context</b> <b>learning</b> demonstrations based on <b>LLMs.</b> Our experiments on various datasets indicate that c-ICL outperforms previous <b>few-shot</b> <b>in-context</b> <b>learning</b> methods, delivering substantial enhancements in performance across a broad spectrum of related tasks. These improvements are noteworthy, showcasing the versatility of our approach in miscellaneous scenarios.</p></p class="citation"></blockquote><h3 id=237--2113-gendec-a-robust-generative-question-decomposition-method-for-multi-hop-reasoning-jian-wu-et-al-2024>(2/37 | 2/113) GenDec: A robust generative Question-decomposition method for Multi-hop reasoning (Jian Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Wu, Linyi Yang, Yuliang Ji, Wenhao Huang, Börje F. Karlsson, Manabu Okumura. (2024)<br><strong>GenDec: A robust generative Question-decomposition method for Multi-hop reasoning</strong><br><button class=copy-to-clipboard title="GenDec: A robust generative Question-decomposition method for Multi-hop reasoning" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Retrieval-Augmented Generation, GPT, GPT-3, GPT-3.5, GPT-4, Question Answering, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11166v1.pdf filename=2402.11166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-hop <b>QA</b> (MHQA) involves step-by-step <b>reasoning</b> to answer complex <b>questions</b> <b>and</b> find multiple relevant supporting facts. However, Existing <b>large</b> <b>language</b> <b>models&rsquo;(LLMs)</b> <b>reasoning</b> ability in multi-hop <b>question</b> <b>answering</b> remains exploration, which is inadequate in answering multi-hop <b>questions.</b> <b>Moreover,</b> it is unclear whether <b>LLMs</b> follow a desired <b>reasoning</b> chain to reach the right final answer. In this paper, we propose a \textbf{gen}erative <b>question</b> <b>\textbf{dec}omposition</b> method (GenDec) from the perspective of explainable <b>QA</b> by generating independent and complete sub-questions based on incorporating additional extracted evidence for enhancing <b>LLMs&rsquo;</b> <b>reasoning</b> ability in <b>RAG.</b> To demonstrate the impact, generalization, and robustness of Gendec, we conduct two experiments, the first is combining GenDec with small <b>QA</b> systems on paragraph retrieval and <b>QA</b> tasks. We secondly examine the <b>reasoning</b> capabilities of various state-of-the-art <b>LLMs</b> including <b>GPT-4</b> and <b>GPT-3.5</b> combined with GenDec. We experiment on the HotpotQA, 2WikihopMultiHopQA, MuSiQue, and PokeMQA datasets.</p></p class="citation"></blockquote><h3 id=337--3113-reasoning-before-comparison-llm-enhanced-semantic-similarity-metrics-for-domain-specialized-text-analysis-shaochen-xu-et-al-2024>(3/37 | 3/113) Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis (Shaochen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaochen Xu, Zihao Wu, Huaqin Zhao, Peng Shu, Zhengliang Liu, Wenxiong Liao, Sheng Li, Andrea Sikora, Tianming Liu, Xiang Li. (2024)<br><strong>Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis</strong><br><button class=copy-to-clipboard title="Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Unsupervised Learning, Zero-shot, GPT, GPT-4, Reasoning, Text Analysis, BLEU, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11398v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11398v2.pdf filename=2402.11398v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we leverage <b>LLM</b> to enhance the semantic analysis and develop similarity metrics for <b>texts,</b> <b>addressing</b> the limitations of traditional <b>unsupervised</b> NLP metrics like <b>ROUGE</b> and <b>BLEU.</b> We develop a framework where <b>LLMs</b> such as <b>GPT-4</b> are employed for <b>zero-shot</b> <b>text</b> <b>identification</b> and label generation for radiology reports, where the labels are then used as measurements for <b>text</b> <b>similarity.</b> By testing the proposed framework on the MIMIC data, we find that <b>GPT-4</b> generated labels can significantly improve the semantic similarity assessment, with scores more closely aligned with clinical ground truth than traditional NLP metrics. Our work demonstrates the possibility of conducting semantic analysis of the <b>text</b> <b>data</b> using semi-quantitative <b>reasoning</b> results by the <b>LLMs</b> for highly specialized domains. While the framework is implemented for radiology report similarity analysis, its concept can be extended to other specialized domains as well.</p></p class="citation"></blockquote><h3 id=437--4113-laco-large-language-model-pruning-via-layer-collapse-yifei-yang-et-al-2024>(4/37 | 4/113) LaCo: Large Language Model Pruning via Layer Collapse (Yifei Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Yang, Zouying Cao, Hai Zhao. (2024)<br><strong>LaCo: Large Language Model Pruning via Layer Collapse</strong><br><button class=copy-to-clipboard title="LaCo: Large Language Model Pruning via Layer Collapse" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Knowledge Distillation, Knowledge Distillation, Model Pruning, Model Quantization, Pruning, Quantization, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11187v1.pdf filename=2402.11187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> based on <b>transformer</b> are witnessing a notable trend of size expansion, which brings considerable costs to both <b>model</b> <b>training</b> and inference. However, existing methods such as <b>model</b> <b>quantization,</b> <b>knowledge</b> <b>distillation,</b> and <b>model</b> <b>pruning</b> are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the internal structure of the <b>model.</b> <b>In</b> this paper, we propose a concise layer-wise <b>pruning</b> method called \textit{Layer Collapse (LaCo)}, in which rear <b>model</b> <b>layers</b> collapse into a prior layer, enabling a rapid reduction in <b>model</b> <b>size</b> while preserving the <b>model</b> <b>structure.</b> Comprehensive experiments show that our method maintains an average task performance of over 80% at <b>pruning</b> ratios of 25-30%, significantly outperforming existing state-of-the-art structured <b>pruning</b> methods. We also conduct post-training experiments to confirm that the proposed <b>pruning</b> method effectively inherits the parameters of the original <b>model.</b> <b>Finally,</b> we discuss our motivation from the perspective of layer-wise similarity and evaluate the performance of the pruned <b>LLMs</b> across various <b>pruning</b> ratios.</p></p class="citation"></blockquote><h3 id=537--5113-dissecting-human-and-llm-preferences-junlong-li-et-al-2024>(5/37 | 5/113) Dissecting Human and LLM Preferences (Junlong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junlong Li, Fan Zhou, Shichao Sun, Yikai Zhang, Hai Zhao, Pengfei Liu. (2024)<br><strong>Dissecting Human and LLM Preferences</strong><br><button class=copy-to-clipboard title="Dissecting Human and LLM Preferences" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, GPT, GPT-4, GPT-4 turbo, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11296v1.pdf filename=2402.11296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a relative quality comparison of model responses, human and <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> preferences serve as common alignment goals in model <b>fine-tuning</b> and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different <b>LLMs</b> to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced <b>LLMs</b> like <b>GPT-4-Turbo</b> <b>emphasize</b> correctness, clarity, and harmlessness more. Additionally, <b>LLMs</b> of similar sizes tend to exhibit similar preferences, regardless of their training methods, and <b>fine-tuning</b> for alignment does not significantly alter the preferences of pretrained-only <b>LLMs.</b> Finally, we show that preference-based evaluation can be intentionally manipulated. In both training-free and training-based settings, aligning a model with the preferences of judges boosts scores, while injecting the least preferred properties lowers them. This results in notable score shifts: up to 0.59 on <b>MT-Bench</b> (1-10 scale) and 31.94 on AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this strategic adaptation. Interactive Demo: <a href=https://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization>https://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization</a> Dataset: <a href=https://huggingface.co/datasets/GAIR/preference-dissection>https://huggingface.co/datasets/GAIR/preference-dissection</a> Code: <a href=https://github.com/GAIR-NLP/Preference-Dissection>https://github.com/GAIR-NLP/Preference-Dissection</a></p></p class="citation"></blockquote><h3 id=637--6113-assessing-llms-mathematical-reasoning-in-financial-document-question-answering-pragya-srivastava-et-al-2024>(6/37 | 6/113) Assessing LLMs&rsquo; Mathematical Reasoning in Financial Document Question Answering (Pragya Srivastava et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pragya Srivastava, Manuj Malik, Tanuja Ganu. (2024)<br><strong>Assessing LLMs&rsquo; Mathematical Reasoning in Financial Document Question Answering</strong><br><button class=copy-to-clipboard title="Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Mathematical Reasoning, Natural Language Understanding, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11194v1.pdf filename=2402.11194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> excel in <b>natural</b> <b>language</b> <b>understanding,</b> but their capability for complex <b>mathematical</b> <b>reasoning</b> with an amalgamation of structured tables and unstructured text is uncertain. This study explores <b>LLMs&rsquo;</b> <b>mathematical</b> <b>reasoning</b> on four financial tabular <b>question-answering</b> <b>datasets:</b> TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and <b>prompting</b> techniques, we assess how <b>LLMs</b> adapt to complex tables and <b>mathematical</b> <b>tasks.</b> We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic <b>reasoning</b> steps. The results provide insights into <b>LLMs&rsquo;</b> capabilities and limitations in handling complex <b>mathematical</b> <b>scenarios</b> for semi-structured tables. Ultimately, we introduce a novel <b>prompting</b> technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding of <b>LLMs</b> abilities for such a task.</p></p class="citation"></blockquote><h3 id=737--7113-grasping-the-essentials-tailoring-large-language-models-for-zero-shot-relation-extraction-sizhe-zhou-et-al-2024>(7/37 | 7/113) Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction (Sizhe Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sizhe Zhou, Yu Meng, Bowen Jin, Jiawei Han. (2024)<br><strong>Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction</strong><br><button class=copy-to-clipboard title="Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Zero-shot, Relation Extraction, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11142v1.pdf filename=2402.11142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Relation</b> <b>extraction</b> (RE), a crucial task in NLP, aims to identify semantic relationships between entities mentioned in texts. Despite significant advancements in this field, existing models typically rely on extensive annotated data for training, which can be both costly and time-consuming to acquire. Moreover, these models often struggle to adapt to new or unseen relationships. In contrast, <b>few-shot</b> <b>learning</b> settings, which aim to reduce annotation requirements, may offer incomplete and biased supervision for understanding target <b>relation</b> <b>semantics,</b> leading to degraded and unstable performance. To provide the model with accurate and explicit descriptions of the <b>relations</b> <b>types</b> and meanwhile minimize the annotation requirements, we study the definition only <b>zero-shot</b> RE setting where only <b>relation</b> <b>definitions</b> expressed in natural language are used to train a RE model. Motivated by the strong synthetic data generation power of <b>LLMs,</b> we propose a framework REPaL which consists of three stages: (1) We utilize <b>LLMs</b> to generate initial seed instances based on <b>relation</b> <b>definitions</b> and an unlabeled corpora. (2) We <b>fine-tune</b> a bidirectional Small Language Model (SLM) using these initial seeds to learn the <b>relations</b> <b>for</b> the target domain. (3) We enhance pattern coverage and mitigate bias resulting from the limited number of initial seeds by incorporating feedback acquired from SLM&rsquo;s predictions on unlabeled corpora. To accomplish this, we leverage the multi-turn conversation ability of <b>LLMs</b> to generate new instances in follow-up dialogues. Experiments on two datasets show REPaL achieves better <b>zero-shot</b> performance with <b>large</b> <b>margins</b> <b>over</b> baseline methods.</p></p class="citation"></blockquote><h3 id=837--8113-boosting-of-thoughts-trial-and-error-problem-solving-with-large-language-models-sijia-chen-et-al-2024>(8/37 | 8/113) Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models (Sijia Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sijia Chen, Baochun Li, Di Niu. (2024)<br><strong>Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models</strong><br><button class=copy-to-clipboard title="Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: GPT, GPT-4, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11140v1.pdf filename=2402.11140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>reasoning</b> performance of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> on a wide range of problems critically relies on <b>chain-of-thought</b> <b>prompting,</b> which involves providing a few chain of thought demonstrations as exemplars in <b>prompts.</b> Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in <b>reasoning</b> step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated <b>prompting</b> framework for problem solving with <b>LLMs</b> by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error <b>reasoning</b> experiences, which will serve as a new form of <b>prompting</b> to solve the complex problem. Starting from a simple <b>prompt</b> without requiring examples, BoT iteratively explores and evaluates a <b>large</b> <b>collection</b> <b>of</b> <b>reasoning</b> steps, and more importantly, uses error analysis obtained from the <b>LLM</b> on them to explicitly revise <b>prompting,</b> which in turn enhances <b>reasoning</b> step generation, until a final answer is attained. Our experiments with <b>GPT-4</b> and Llama2 across extensive complex mathematical problems demonstrate that BoT consistently achieves higher or comparable problem-solving rates than other advanced <b>prompting</b> approaches.</p></p class="citation"></blockquote><h3 id=937--9113-direct-evaluation-of-chain-of-thought-in-multi-hop-reasoning-with-knowledge-graphs-minh-vuong-nguyen-et-al-2024>(9/37 | 9/113) Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs (Minh-Vuong Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh-Vuong Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, Gholamreza Haffari. (2024)<br><strong>Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs</strong><br><button class=copy-to-clipboard title="Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11199v1.pdf filename=2402.11199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> demonstrate strong <b>reasoning</b> abilities when <b>prompted</b> to generate chain-of-thought (CoT) explanations alongside answers. However, previous research on evaluating <b>LLMs</b> has solely focused on answer accuracy, neglecting the correctness of the generated CoT. In this paper, we delve deeper into the CoT <b>reasoning</b> capabilities of <b>LLMs</b> in multi-hop <b>question</b> <b>answering</b> by utilizing <b>knowledge</b> <b>graphs</b> <b>(KGs).</b> We propose a novel discriminative and generative CoT evaluation paradigm to assess <b>LLMs&rsquo;</b> <b>knowledge</b> <b>of</b> <b>reasoning</b> and the accuracy of the generated CoT. Through experiments conducted on 5 different families of <b>LLMs</b> across 2 multi-hop <b>question-answering</b> <b>datasets,</b> we find that <b>LLMs</b> possess sufficient <b>knowledge</b> <b>to</b> perform <b>reasoning.</b> However, there exists a significant disparity between answer accuracy and faithfulness of the CoT <b>reasoning</b> generated by <b>LLMs,</b> indicating that they often arrive at correct answers through incorrect <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=1037--10113-kg-agent-an-efficient-autonomous-agent-framework-for-complex-reasoning-over-knowledge-graph-jinhao-jiang-et-al-2024>(10/37 | 10/113) KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph (Jinhao Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, Ji-Rong Wen. (2024)<br><strong>KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph</strong><br><button class=copy-to-clipboard title="KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Graph, Fine-tuning, Knowledge Graph, Knowledge Graph, LLaMA, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11163v1.pdf filename=2402.11163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we aim to improve the <b>reasoning</b> ability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> over <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> to answer complex questions. Inspired by existing methods that design the interaction strategy between <b>LLMs</b> and <b>KG,</b> we propose an autonomous <b>LLM-based</b> agent framework, called <b>KG-Agent,</b> which enables a small <b>LLM</b> to actively make decisions until finishing the <b>reasoning</b> process over <b>KGs.</b> In <b>KG-Agent,</b> we integrate the <b>LLM,</b> multifunctional toolbox, <b>KG-based</b> executor, and <b>knowledge</b> <b>memory,</b> and develop an iteration mechanism that autonomously selects the tool then updates the memory for <b>reasoning</b> over <b>KG.</b> To guarantee the effectiveness, we leverage program language to formulate the multi-hop <b>reasoning</b> process over the <b>KG,</b> and synthesize a code-based instruction dataset to <b>fine-tune</b> the base <b>LLM.</b> Extensive experiments demonstrate that only using 10K samples for tuning <b>LLaMA-7B</b> can outperform state-of-the-art methods using larger <b>LLMs</b> or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.</p></p class="citation"></blockquote><h3 id=1137--11113-can-large-language-models-perform-relation-based-argument-mining-deniz-gorur-et-al-2024>(11/37 | 11/113) Can Large Language Models perform Relation-based Argument Mining? (Deniz Gorur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deniz Gorur, Antonio Rago, Francesca Toni. (2024)<br><strong>Can Large Language Models perform Relation-based Argument Mining?</strong><br><button class=copy-to-clipboard title="Can Large Language Models perform Relation-based Argument Mining?" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: LLaMA, Mistral, RoBERTa, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11243v1.pdf filename=2402.11243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Argument mining (AM) is the process of automatically extracting arguments, their components and/or relations amongst arguments and components from text. As the number of platforms supporting online debate increases, the need for AM becomes ever more urgent, especially in support of downstream tasks. Relation-based AM (RbAM) is a form of AM focusing on identifying agreement (support) and disagreement (attack) relations amongst arguments. RbAM is a challenging classification task, with existing methods failing to perform satisfactorily. In this paper, we show that general-purpose <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> appropriately primed and <b>prompted,</b> can significantly outperform the best performing <b>(RoBERTa-based)</b> baseline. Specifically, we experiment with two open-source <b>LLMs</b> <b>(Llama-2</b> and <b>Mistral)</b> with ten datasets.</p></p class="citation"></blockquote><h3 id=1237--12113-a-question-answering-based-pipeline-for-comprehensive-chinese-ehr-information-extraction-huaiyuan-ying-et-al-2024>(12/37 | 12/113) A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction (Huaiyuan Ying et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huaiyuan Ying, Sheng Yu. (2024)<br><strong>A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction</strong><br><button class=copy-to-clipboard title="A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, Transfer Learning, Zero-shot, Information Retrieval, Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11177v1.pdf filename=2402.11177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electronic health records (EHRs) hold significant value for research and applications. As a new way of <b>information</b> <b>extraction,</b> <b>question</b> <b>answering</b> <b>(QA)</b> can extract more flexible <b>information</b> <b>than</b> conventional methods and is more accessible to clinical researchers, but its progress is impeded by the scarcity of annotated data. In this paper, we propose a novel approach that automatically generates training data for <b>transfer</b> <b>learning</b> of <b>QA</b> models. Our pipeline incorporates a preprocessing module to handle challenges posed by extraction types that are not readily compatible with extractive <b>QA</b> frameworks, including cases with discontinuous answers and many-to-one relationships. The obtained <b>QA</b> model exhibits excellent performance on subtasks of <b>information</b> <b>extraction</b> in EHRs, and it can effectively handle <b>few-shot</b> or <b>zero-shot</b> settings involving yes-no <b>questions.</b> <b>Case</b> studies and ablation studies demonstrate the necessity of each component in our design, and the resulting model is deemed suitable for practical use.</p></p class="citation"></blockquote><h3 id=1337--13113-panda-pedantic-answer-correctness-determination-and-adjudicationimproving-automatic-evaluation-for-question-answering-and-text-generation-zongxia-li-et-al-2024>(13/37 | 13/113) PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation (Zongxia Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, Jordan Lee Boyd-Graber. (2024)<br><strong>PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation</strong><br><button class=copy-to-clipboard title="PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Automatic Evaluation, Question Answering, Question Answering, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11161v1.pdf filename=2402.11161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Question</b> <b>answering</b> <b>(QA)</b> can only make progress if we know if an answer is correct, but for many of the most challenging and interesting <b>QA</b> examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from <b>large</b> <b>language</b> <b>models</b> <b>(LLM).</b> There are two challenges: a lack of data and that models are too big. <b>LLM</b> based scorers correlate better with humans, but this expensive task has only been tested on limited <b>QA</b> datasets. We rectify these issues by providing clear guidelines for evaluating machine <b>QA</b> adopted from human <b>QA</b> contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.</p></p class="citation"></blockquote><h3 id=1437--14113-tasks-that-language-models-dont-learn-bruce-w-lee-et-al-2024>(14/37 | 14/113) Tasks That Language Models Don&rsquo;t Learn (Bruce W. Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bruce W. Lee, JaeHyuk Lim. (2024)<br><strong>Tasks That Language Models Don&rsquo;t Learn</strong><br><button class=copy-to-clipboard title="Tasks That Language Models Don't Learn" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Few-shot, LLaMA, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11349v1.pdf filename=2402.11349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We argue that there are certain properties of language that our current <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> don&rsquo;t learn. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-TEST. This <b>benchmark</b> highlights a fundamental gap between human linguistic comprehension, which naturally integrates sensory experiences, and the sensory-deprived processing capabilities of <b>LLMs.</b> In support of our hypothesis, 1. deliberate <b>reasoning</b> (Chain-of-Thought), 2. <b>few-shot</b> examples, or 3. stronger <b>LLM</b> from the same model family <b>(LLaMA</b> 2 13B -> <b>LLaMA</b> 2 70B) do not trivially bring improvements in H-TEST performance. Therefore, we make a particular connection to the philosophical case of Mary, who learns about the world in a sensory-deprived environment (Jackson, 1986). Our experiments show that some of the strongest proprietary <b>LLMs</b> stay near random chance baseline accuracy of 50%, highlighting the limitations of knowledge acquired in the absence of sensory experience.</p></p class="citation"></blockquote><h3 id=1537--15113-phaseevo-towards-unified-in-context-prompt-optimization-for-large-language-models-wendi-cui-et-al-2024>(15/37 | 15/113) PhaseEvo: Towards Unified In-Context Prompt Optimization for Large Language Models (Wendi Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das, Bradley Malin, Sricharan Kumar. (2024)<br><strong>PhaseEvo: Towards Unified In-Context Prompt Optimization for Large Language Models</strong><br><button class=copy-to-clipboard title="PhaseEvo: Towards Unified In-Context Prompt Optimization for Large Language Models" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11347v1.pdf filename=2402.11347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crafting an ideal <b>prompt</b> for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is a challenging task that demands significant resources and expert human input. Existing work treats the optimization of <b>prompt</b> instruction and <b>in-context</b> <b>learning</b> examples as distinct problems, leading to sub-optimal <b>prompt</b> performance. This research addresses this limitation by establishing a unified <b>in-context</b> <b>prompt</b> optimization framework, which aims to achieve joint optimization of the <b>prompt</b> instruction and examples. However, formulating such optimization in the discrete and high-dimensional natural language space introduces challenges in terms of convergence and computational efficiency. To overcome these issues, we present PhaseEvo, an efficient automatic <b>prompt</b> optimization framework that combines the generative capability of <b>LLMs</b> with the global search proficiency of evolution algorithms. Our framework features a multi-phase design incorporating innovative <b>LLM-based</b> mutation operators to enhance search efficiency and accelerate convergence. We conduct an extensive evaluation of our approach across 35 <b>benchmark</b> tasks. The results demonstrate that PhaseEvo significantly outperforms the state-of-the-art baseline methods by a <b>large</b> <b>margin</b> <b>whilst</b> maintaining good efficiency.</p></p class="citation"></blockquote><h3 id=1637--16113-puzzle-solving-using-reasoning-of-large-language-models-a-survey-panagiotis-giadikiaroglou-et-al-2024>(16/37 | 16/113) Puzzle Solving using Reasoning of Large Language Models: A Survey (Panagiotis Giadikiaroglou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou. (2024)<br><strong>Puzzle Solving using Reasoning of Large Language Models: A Survey</strong><br><button class=copy-to-clipboard title="Puzzle Solving using Reasoning of Large Language Models: A Survey" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11291v1.pdf filename=2402.11291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploring the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in puzzle solving unveils critical insights into their potential and challenges in artificial intelligence, marking a significant step towards understanding their applicability in complex <b>reasoning</b> tasks. This survey leverages a unique taxonomy &ndash; dividing puzzles into rule-based and rule-less categories &ndash; to critically assess <b>LLMs</b> through various methodologies, including <b>prompting</b> techniques, neuro-symbolic approaches, and <b>fine-tuning.</b> Through a critical review of relevant datasets and <b>benchmarks,</b> we assess <b>LLMs&rsquo;</b> performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between <b>LLM</b> capabilities and human-like <b>reasoning,</b> particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance <b>LLMs&rsquo;</b> puzzle-solving proficiency and contribute to AI&rsquo;s logical <b>reasoning</b> and creative problem-solving advancements.</p></p class="citation"></blockquote><h3 id=1737--17113-llm-can-achieve-self-regulation-via-hyperparameter-aware-generation-siyin-wang-et-al-2024>(17/37 | 17/113) LLM can Achieve Self-Regulation via Hyperparameter Aware Generation (Siyin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyin Wang, Shimin Li, Tianxiang Sun, Jinlan Fu, Qinyuan Cheng, Jiasheng Ye, Junjie Ye, Xipeng Qiu, Xuanjing Huang. (2024)<br><strong>LLM can Achieve Self-Regulation via Hyperparameter Aware Generation</strong><br><button class=copy-to-clipboard title="LLM can Achieve Self-Regulation via Hyperparameter Aware Generation" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Reasoning, Text Generation, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11251v1.pdf filename=2402.11251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> users commonly employ diverse decoding strategies and adjust hyperparameters to control the generated <b>text.</b> <b>However,</b> a critical question emerges: Are <b>LLMs</b> conscious of the existence of these decoding strategies and capable of regulating themselves? The current decoding generation process often relies on empirical and heuristic manual adjustments to hyperparameters based on types of tasks and demands. However, this process is typically cumbersome, and the decoding hyperparameters may not always be optimal for each sample. To address the aforementioned challenges, we propose a novel <b>text</b> <b>generation</b> paradigm termed Hyperparameter Aware Generation (HAG). By leveraging hyperparameter-aware <b>instruction</b> <b>tuning,</b> the <b>LLM</b> autonomously determines the optimal decoding strategy and configs based on the input samples, enabling self-regulation. Our approach eliminates the need for extensive manual tuning, offering a more autonomous, self-regulate model behavior. Experimental results spanning six datasets across <b>reasoning,</b> creativity, translation, and mathematics tasks demonstrate that hyperparameter-aware <b>instruction</b> <b>tuning</b> empowers the <b>LLMs</b> to self-regulate the decoding strategy and hyperparameter. HAG extends the current paradigm in the <b>text</b> <b>generation</b> process, highlighting the feasibility of endowing the <b>LLMs</b> with self-regulate decoding strategies.</p></p class="citation"></blockquote><h3 id=1837--18113-knowtuning-knowledge-aware-fine-tuning-for-large-language-models-yougang-lyu-et-al-2024>(18/37 | 18/113) KnowTuning: Knowledge-aware Fine-tuning for Large Language Models (Yougang Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yougang Lyu, Lingyong Yan, Shuaiqiang Wang, Haibo Shi, Dawei Yin, Pengjie Ren, Zhumin Chen, Maarten de Rijke, Zhaochun Ren. (2024)<br><strong>KnowTuning: Knowledge-aware Fine-tuning for Large Language Models</strong><br><button class=copy-to-clipboard title="KnowTuning: Knowledge-aware Fine-tuning for Large Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11176v1.pdf filename=2402.11176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite their success at many natural language processing (NLP) tasks, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of <b>LLMs</b> during vanilla <b>fine-tuning.</b> To address these problems, we propose a knowledge-aware <b>fine-tuning</b> (KnowTuning) method to explicitly and implicitly improve the knowledge awareness of <b>LLMs.</b> We devise an explicit knowledge-aware generation stage to train <b>LLMs</b> to explicitly identify knowledge triples in answers. We also propose an implicit knowledge-aware comparison stage to train <b>LLMs</b> to implicitly distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical <b>question</b> <b>answering</b> <b>(QA)</b> datasets confirm the effectiveness of KnowTuning, through automatic and human evaluations, across various sizes of <b>LLMs.</b> Finally, we demonstrate that the improvements of KnowTuning generalize to unseen <b>QA</b> datasets.</p></p class="citation"></blockquote><h3 id=1937--19113-knowledge-graph-assisted-automatic-sports-news-writing-yang-cao-et-al-2024>(19/37 | 19/113) Knowledge Graph Assisted Automatic Sports News Writing (Yang Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Cao, Xinyi Chen, Xin Zhang, Siying Li. (2024)<br><strong>Knowledge Graph Assisted Automatic Sports News Writing</strong><br><button class=copy-to-clipboard title="Knowledge Graph Assisted Automatic Sports News Writing" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 48<br>Keywords: Graph, Convolution, Convolutional Neural Network, Few-shot, Knowledge Graph, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11191v1.pdf filename=2402.11191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a novel method for automatically generating sports news, which employs a unique algorithm that extracts pivotal moments from live text broadcasts and uses them to create an initial draft of the news. This draft is further refined by incorporating key details and background information from a specially designed sports <b>knowledge</b> <b>graph.</b> This <b>graph</b> contains 5,893 entities, which are classified into three distinct conceptual categories, interconnected through four relationship types, and characterized by 27 unique attributes. In addition, we create a multi-stage learning model by combining <b>convolutional</b> <b>neural</b> <b>networks</b> and a <b>transformer</b> encoder. This model expresses entity-task interactions using <b>convolutional</b> <b>neural</b> <b>networks</b> and enriches entity representations in the query set with the <b>transformer</b> encoder. It also includes a processor to compute matching scores for incomplete triples, addressing <b>few-shot</b> <b>knowledge</b> <b>graph</b> completion problem. The efficiency of this approach has been confirmed through both subjective and objective evaluations of 50 selected test cases, demonstrating its capability in revolutionizing the creation of sports news.</p></p class="citation"></blockquote><h3 id=2037--20113-evedit-event-based-knowledge-editing-with-deductive-editing-boundaries-jiateng-liu-et-al-2024>(20/37 | 20/113) EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries (Jiateng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiateng Liu, Pengfei Yu, Yuji Zhang, Sha Li, Zixuan Zhang, Heng Ji. (2024)<br><strong>EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries</strong><br><button class=copy-to-clipboard title="EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11324v1.pdf filename=2402.11324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The dynamic nature of real-world information necessitates efficient knowledge editing (KE) in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for knowledge updating. However, current KE approaches, which typically operate on (subject, relation, object) triples, ignore the contextual information and the relation among different knowledge. Such editing methods could thus encounter an uncertain editing boundary, leaving a lot of relevant knowledge in ambiguity: Queries that could be answered pre-edit cannot be reliably answered afterward. In this work, we analyze this issue by introducing a theoretical framework for KE that highlights an overlooked set of knowledge that remains unchanged and aids in knowledge deduction during editing, which we name as the deduction anchor. We further address this issue by proposing a novel task of event-based knowledge editing that pairs facts with event descriptions. This task manifests not only a closer <b>simulation</b> of real-world editing scenarios but also a more logically sound setting, implicitly defining the deduction anchor to address the issue of indeterminate editing boundaries. We empirically demonstrate the superiority of event-based editing over the existing setting on resolving uncertainty in edited models, and curate a new <b>benchmark</b> dataset EvEdit derived from the CounterFact dataset. Moreover, while we observe that the event-based setting is significantly challenging for existing approaches, we propose a novel approach Self-Edit that showcases stronger performance, achieving 55.6% consistency improvement while maintaining the naturalness of generation.</p></p class="citation"></blockquote><h3 id=2137--21113-moral-moe-augmented-lora-for-llms-lifelong-learning-shu-yang-et-al-2024>(21/37 | 21/113) MoRAL: MoE Augmented LoRA for LLMs&rsquo; Lifelong Learning (Shu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shu Yang, Muhammad Asif Ali, Cheng-Long Wang, Lijie Hu, Di Wang. (2024)<br><strong>MoRAL: MoE Augmented LoRA for LLMs&rsquo; Lifelong Learning</strong><br><button class=copy-to-clipboard title="MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11260v1.pdf filename=2402.11260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adapting <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the <b>fine-tuning</b> abilities of LoRA for effective life-long learning of <b>LLMs.</b> In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation <b>benchmark</b> namely: Life Long Learning of <b>LLM</b> (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) <b>LLMs</b> learn fast in open-book settings with up to 30.15% improvement in &ldquo;RA&rdquo; for Phi-2-2.7B compared to closed-book (for models <b>fine-tuned</b> with MoRAL); (ii) MoRAL shows higher performance improvement for models with a greater number of parameters; (iii) MoRAL is robust to catastrophic forgetting offering better knowledge retention compared to baselines.</p></p class="citation"></blockquote><h3 id=2237--22113-controlled-text-generation-for-large-language-model-with-dynamic-attribute-graphs-xun-liang-et-al-2024>(22/37 | 22/113) Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs (Xun Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xun Liang, Hanyu Wang, Shichao Song, Mengting Hu, Xunzhi Wang, Zhiyu Li, Feiyu Xiong, Bo Tang. (2024)<br><strong>Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs</strong><br><button class=copy-to-clipboard title="Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Graph, Text Generation, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11218v1.pdf filename=2402.11218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Controlled <b>Text</b> <b>Generation</b> (CTG) aims to produce <b>texts</b> <b>that</b> exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> named Dynamic Attribute <b>Graphs-based</b> controlled <b>text</b> <b>generation</b> (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by <b>LLMs</b> and constructs dynamic attribute <b>graphs.</b> DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five <b>LLMs</b> as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29% over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant decrease in <b>perplexity,</b> markedly improving <b>text</b> <b>fluency.</b></p></p class="citation"></blockquote><h3 id=2337--23113-renovi-a-benchmark-towards-remediating-norm-violations-in-socio-cultural-conversations-haolan-zhan-et-al-2024>(23/37 | 23/113) RENOVI: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations (Haolan Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haolan Zhan, Zhuang Li, Xiaoxi Kang, Tao Feng, Yuncheng Hua, Lizhen Qu, Yi Ying, Mei Rianto Chandra, Kelly Rosalin, Jureynolds Jureynolds, Suraj Sharma, Shilin Qu, Linhao Luo, Lay-Ki Soon, Zhaleh Semnani Azad, Ingrid Zukerman, Gholamreza Haffari. (2024)<br><strong>RENOVI: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations</strong><br><button class=copy-to-clipboard title="RENOVI: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, ChatGPT, Large Language Model, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11178v1.pdf filename=2402.11178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Norm violations occur when individuals fail to conform to culturally accepted behaviors, which may lead to potential conflicts. Remediating norm violations requires social awareness and cultural sensitivity of the nuances at play. To equip interactive AI systems with a remediation ability, we offer ReNoVi - a large-scale corpus of 9,258 multi-turn dialogues annotated with social norms, as well as define a sequence of tasks to help understand and remediate norm violations step by step. ReNoVi consists of two parts: 512 human-authored dialogues (real data), and 8,746 synthetic conversations generated by <b>ChatGPT</b> through <b>prompt</b> <b>learning.</b> While collecting sufficient human-authored data is costly, synthetic conversations provide suitable amounts of data to help mitigate the scarcity of training data, as well as the chance to assess the alignment between <b>LLMs</b> and humans in the awareness of social norms. We thus harness the power of <b>ChatGPT</b> to generate synthetic training data for our task. To ensure the quality of both human-authored and synthetic data, we follow a quality control protocol during data collection. Our experimental results demonstrate the importance of remediating norm violations in socio-cultural conversations, as well as the improvement in performance obtained from synthetic data.</p></p class="citation"></blockquote><h3 id=2437--24113-onebit-towards-extremely-low-bit-large-language-models-yuzhuang-xu-et-al-2024>(24/37 | 24/113) OneBit: Towards Extremely Low-bit Large Language Models (Yuzhuang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, Wanxiang Che. (2024)<br><strong>OneBit: Towards Extremely Low-bit Large Language Models</strong><br><button class=copy-to-clipboard title="OneBit: Towards Extremely Low-bit Large Language Models" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Quantization, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11295v1.pdf filename=2402.11295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model quantification uses low bit-width values to represent the weight matrices of models, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated <b>LLMs.</b> However, existing <b>quantization</b> methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to <b>quantize</b> models. This paper boldly <b>quantizes</b> the weight matrices of <b>LLMs</b> to 1-bit, paving the way for the extremely low bit-width deployment of <b>LLMs.</b> For this target, we introduce a 1-bit <b>quantization-aware</b> training (QAT) framework named OneBit, including a novel 1-bit parameter representation method to better <b>quantize</b> <b>LLMs</b> as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the QAT framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 83% of the non-quantized performance) with robust training processes when only using 1-bit weight matrices.</p></p class="citation"></blockquote><h3 id=2537--25113-token-ensemble-text-generation-on-attacking-the-automatic-ai-generated-text-detection-fan-huang-et-al-2024>(25/37 | 25/113) Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection (Fan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Huang, Haewoon Kwak, Jisun An. (2024)<br><strong>Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection</strong><br><button class=copy-to-clipboard title="Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: AI-generated Text Detection, Text Generation, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11167v1.pdf filename=2402.11167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The robustness of AI-content detection models against cultivated attacks (e.g., paraphrasing or word switching) remains a significant concern. This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches. We explore the ensemble attack strategy by completing the <b>prompt</b> with the next token generated from random candidate <b>LLMs.</b> We find the token-ensemble approach significantly drops the performance of AI-content detection models (The code and test sets will be released). Our findings reveal that token-ensemble generation poses a vital challenge to current detection models and underlines the need for advancing detection technologies to counter sophisticated adversarial strategies.</p></p class="citation"></blockquote><h3 id=2637--26113-contrastive-instruction-tuning-tianyi-yan-et-al-2024>(26/37 | 26/113) Contrastive Instruction Tuning (Tianyi Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyi Yan, Fei Wang, James Y. Huang, Wenxuan Zhou, Fan Yin, Aram Galstyan, Wenpeng Yin, Muhao Chen. (2024)<br><strong>Contrastive Instruction Tuning</strong><br><button class=copy-to-clipboard title="Contrastive Instruction Tuning" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11138v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11138v1.pdf filename=2402.11138v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> has been used as a promising approach to improve the performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on unseen tasks. However, current <b>LLMs</b> exhibit limited robustness to unseen <b>instructions,</b> <b>generating</b> inconsistent outputs when the same <b>instruction</b> <b>is</b> phrased with slightly varied forms or language styles. This behavior indicates <b>LLMs&rsquo;</b> lack of robustness to textual variations and generalizability to unseen <b>instructions,</b> <b>potentially</b> leading to trustworthiness issues. Accordingly, we propose Contrastive <b>Instruction</b> <b>Tuning,</b> which maximizes the similarity between the hidden representations of semantically equivalent <b>instruction-instance</b> <b>pairs</b> while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task <b>instructions.</b> <b>Experiments</b> on the PromptBench <b>benchmark</b> show that CoIN consistently improves <b>LLMs&rsquo;</b> robustness to unseen <b>instructions</b> <b>with</b> variations across character, word, sentence, and semantic levels by an average of +2.5% in accuracy.</p></p class="citation"></blockquote><h3 id=2737--27113-i-learn-better-if-you-speak-my-language-enhancing-large-language-model-fine-tuning-with-style-aligned-response-adjustments-xuan-ren-et-al-2024>(27/37 | 27/113) I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments (Xuan Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuan Ren, Biao Wu, Lingqiao Liu. (2024)<br><strong>I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments</strong><br><button class=copy-to-clipboard title="I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11192v1.pdf filename=2402.11192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model&rsquo;s ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the <b>fine-tuning</b> process. We found that matching the ground-truth response style with the <b>LLM&rsquo;s</b> inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the <b>LLM&rsquo;s</b> pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model&rsquo;s native response style, safeguarding the model&rsquo;s core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the <b>LLM&rsquo;s</b> task-specific accuracy but also crucially maintains its original competencies and effectiveness.</p></p class="citation"></blockquote><h3 id=2837--28113-m4gt-bench-evaluation-benchmark-for-black-box-machine-generated-text-detection-yuxia-wang-et-al-2024>(28/37 | 28/113) M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection (Yuxia Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Osama Mohanned Afzal, Tarek Mahmoud, Giovanni Puccetti, Thomas Arnold, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, Preslav Nakov. (2024)<br><strong>M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection</strong><br><button class=copy-to-clipboard title="M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 28<br>Keywords: Benchmarking, Black Box, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11175v1.pdf filename=2402.11175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new <b>benchmark</b> involving multilingual, multi-domain and multi-generator for MGT detection &ndash; M4GT-Bench. It is collected for three task formulations: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection identifies which particular model generates the text; and (3) human-machine mixed text detection, where a word boundary delimiting MGT from human-written content should be determined. Human evaluation for Task 2 shows less than random guess performance, demonstrating the challenges to distinguish unique <b>LLMs.</b> Promising results always occur when training and test data distribute within the same domain or generators.</p></p class="citation"></blockquote><h3 id=2937--29113-mmmmodal----multi-images-multi-audio-multi-turn-multi-modal-husein-zolkepli-et-al-2024>(29/37 | 29/113) MMMModal &ndash; Multi-Images Multi-Audio Multi-turn Multi-Modal (Husein Zolkepli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan. (2024)<br><strong>MMMModal &ndash; Multi-Images Multi-Audio Multi-turn Multi-Modal</strong><br><button class=copy-to-clipboard title="MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Mistral, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11297v1.pdf filename=2402.11297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our contribution introduces a groundbreaking <b>multimodal</b> <b>large</b> <b>language</b> <b>model</b> designed to comprehend multi-images, multi-audio, and multi-images-multi-audio within a single multiturn session. Leveraging state-of-the-art models, we utilize the SigLIP encoder for visual inputs and the Whisper Encoder for audio inputs. Notably, this <b>multimodal</b> <b>large</b> <b>language</b> <b>model</b> is bilingual, proficient in understanding both English and Malay simultaneously. We proudly unveil two versions of this model: TinyLlama with 1.1B parameters, and <b>Mistral</b> with 7B parameters. With its ability to navigate diverse modalities and languages, our model represents a significant advancement for the Malaysian context and beyond. All models released at <a href=https://huggingface.co/collections/mesolitica/multimodal-malaysian-llm-65c6f893e03f78fa9e5c8859>https://huggingface.co/collections/mesolitica/multimodal-malaysian-llm-65c6f893e03f78fa9e5c8859</a></p></p class="citation"></blockquote><h3 id=3037--30113-multi-perspective-consistency-enhances-confidence-estimation-in-large-language-models-pei-wang-et-al-2024>(30/37 | 30/113) Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models (Pei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pei Wang, Yejie Wang, Muxi Diao, Keqing He, Guanting Dong, Weiran Xu. (2024)<br><strong>Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models</strong><br><button class=copy-to-clipboard title="Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11279v1.pdf filename=2402.11279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the deployment of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> accurate confidence estimation is critical for assessing the credibility of model predictions. However, existing methods often fail to overcome the issue of overconfidence on incorrect answers. In this work, we focus on improving the confidence estimation of <b>large</b> <b>language</b> <b>models.</b> Considering the fragility of self-awareness in language models, we introduce a Multi-Perspective Consistency (MPC) method. We leverage complementary insights from different perspectives within models (MPC-Internal) and across different models (MPC-Across) to mitigate the issue of overconfidence arising from a singular viewpoint. The experimental results on eight publicly available datasets show that our MPC achieves state-of-the-art performance. Further analyses indicate that MPC can mitigate the problem of overconfidence and is effectively scalable to other models.</p></p class="citation"></blockquote><h3 id=3137--31113-disclosure-and-mitigation-of-gender-bias-in-llms-xiangjue-dong-et-al-2024>(31/37 | 31/113) Disclosure and Mitigation of Gender Bias in LLMs (Xiangjue Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangjue Dong, Yibo Wang, Philip S. Yu, James Caverlee. (2024)<br><strong>Disclosure and Mitigation of Gender Bias in LLMs</strong><br><button class=copy-to-clipboard title="Disclosure and Mitigation of Gender Bias in LLMs" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11190v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11190v1.pdf filename=2402.11190v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can generate biased responses. Yet previous direct probing techniques contain either gender mentions or predefined gender stereotypes, which are challenging to comprehensively collect. Hence, we propose an indirect probing framework based on conditional generation. This approach aims to induce <b>LLMs</b> to disclose their gender bias even without explicit gender or stereotype mentions. We explore three distinct strategies to disclose explicit and implicit gender bias in <b>LLMs.</b> Our experiments demonstrate that all tested <b>LLMs</b> exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs. In addition, an increased model size or model alignment amplifies bias in most cases. Furthermore, we investigate three methods to mitigate bias in <b>LLMs</b> via Hyperparameter Tuning, Instruction Guiding, and Debias Tuning. Remarkably, these methods prove effective even in the absence of explicit genders or stereotypes.</p></p class="citation"></blockquote><h3 id=3237--32113-understanding-news-thumbnail-representativeness-by-counterfactual-text-guided-contrastive-language-image-pretraining-yejun-yoon-et-al-2024>(32/37 | 32/113) Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining (Yejun Yoon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yejun Yoon, Seunghyun Yoon, Kunwoo Park. (2024)<br><strong>Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining</strong><br><button class=copy-to-clipboard title="Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 20<br>Keywords: Counter-factual, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11159v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11159v2.pdf filename=2402.11159v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper delves into the critical challenge of understanding the representativeness of news thumbnail images, which often serve as the first visual engagement for readers when an article is disseminated on social media. We focus on whether a news image represents the main subject discussed in the news text. To serve the challenge, we introduce NewsTT, a manually annotated dataset of news thumbnail image and text pairs. We found that pretrained vision and language models, such as CLIP and BLIP-2, struggle with this task. Since news subjects frequently involve named entities or proper nouns, a pretrained model could not have the ability to match its visual and textual appearances. To fill the gap, we propose CFT-CLIP, a <b>counterfactual</b> text-guided contrastive language-image pretraining framework. We hypothesize that learning to contrast news text with its <b>counterfactual,</b> of which named entities are replaced, can enhance the cross-modal matching ability in the target task. Evaluation experiments using NewsTT show that CFT-CLIP outperforms the pretrained models, such as CLIP and BLIP-2. Our code and data will be made accessible to the public after the paper is accepted.</p></p class="citation"></blockquote><h3 id=3337--33113-can-large-multimodal-models-uncover-deep-semantics-behind-images-yixin-yang-et-al-2024>(33/37 | 33/113) Can Large Multimodal Models Uncover Deep Semantics Behind Images? (Yixin Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixin Yang, Zheng Li, Qingxiu Dong, Heming Xia, Zhifang Sui. (2024)<br><strong>Can Large Multimodal Models Uncover Deep Semantics Behind Images?</strong><br><button class=copy-to-clipboard title="Can Large Multimodal Models Uncover Deep Semantics Behind Images?" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 19<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11281v1.pdf filename=2402.11281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive <b>benchmark</b> to assess Large <b>Multimodal</b> Models&rsquo; (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and <b>GPT-4V(ision).Our</b> evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, <b>GPT-4V</b> is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis indicates that the integration of description texts during the inference process notably enhances LMMs&rsquo; ability to perceive deep semantics. Furthermore, our dataset is divided into multiple categories, and we conducted a more detailed analysis within these categories.</p></p class="citation"></blockquote><h3 id=3437--34113-asclepius-a-spectrum-evaluation-benchmark-for-medical-multi-modal-large-language-models-wenxuan-wang-et-al-2024>(34/37 | 34/113) Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models (Wenxuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenxuan Wang, Yihang Su, Jingyuan Huan, Jie Liu, Wenting Chen, Yudi Zhang, Cheng-Yi Li, Kao-Jung Chang, Xiaohan Xin, Linlin Shen, Michael R. Lyu. (2024)<br><strong>Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models</strong><br><button class=copy-to-clipboard title="Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 16<br>Keywords: Benchmarking, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11217v1.pdf filename=2402.11217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The significant breakthroughs of Medical <b>Multi-Modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (Med-MLLMs) renovate modern healthcare with robust information synthesis and medical decision support. However, these models are often evaluated on <b>benchmarks</b> that are unsuitable for the Med-MLLMs due to the intricate nature of the real-world diagnostic frameworks, which encompass diverse medical specialties and involve complex clinical decisions. Moreover, these <b>benchmarks</b> are susceptible to data leakage, since Med-MLLMs are trained on <b>large</b> <b>assemblies</b> <b>of</b> publicly available data. Thus, an isolated and clinically representative <b>benchmark</b> is highly desirable for credible Med-MLLMs evaluation. To this end, we introduce Asclepius, a novel Med-MLLM <b>benchmark</b> that rigorously and comprehensively assesses model capability in terms of: distinct medical specialties (cardiovascular, gastroenterology, etc.) and different diagnostic capacities (perception, disease analysis, etc.). Grounded in 3 proposed core principles, Asclepius ensures a comprehensive evaluation by encompassing 15 medical specialties, stratifying into 3 main categories and 8 sub-categories of clinical tasks, and exempting from train-validate contamination. We further provide an in-depth analysis of 6 Med-MLLMs and compare them with 5 human specialists, providing insights into their competencies and limitations in various medical contexts. Our work not only advances the understanding of Med-MLLMs&rsquo; capabilities but also sets a precedent for future evaluations and the safe deployment of these models in clinical environments. We launch and maintain a leaderboard for community assessment of Med-MLLM capabilities (<a href=https://asclepius-med.github.io/)>https://asclepius-med.github.io/)</a>.</p></p class="citation"></blockquote><h3 id=3537--35113-k-semstamp-a-clustering-based-semantic-watermark-for-detection-of-machine-generated-text-abe-bohan-hou-et-al-2024>(35/37 | 35/113) k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text (Abe Bohan Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abe Bohan Hou, Jingyu Zhang, Yichen Wang, Daniel Khashabi, Tianxing He. (2024)<br><strong>k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text</strong><br><button class=copy-to-clipboard title="k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs-CY, cs-LG, cs.CL<br>Keyword Score: 13<br>Keywords: Clustering, Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11399v1.pdf filename=2402.11399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent watermarked generation algorithms inject detectable signatures during <b>language</b> <b>generation</b> to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means <b>clustering</b> as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.</p></p class="citation"></blockquote><h3 id=3637--36113-what-changed-converting-representational-interventions-to-natural-language-matan-avitan-et-al-2024>(36/37 | 36/113) What Changed? Converting Representational Interventions to Natural Language (Matan Avitan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matan Avitan, Ryan Cotterell, Yoav Goldberg, Shauli Ravfogel. (2024)<br><strong>What Changed? Converting Representational Interventions to Natural Language</strong><br><button class=copy-to-clipboard title="What Changed? Converting Representational Interventions to Natural Language" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11355v1.pdf filename=2402.11355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model&rsquo;s representations, creating a <b>counterfactual</b> representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space <b>counterfactuals</b> can be converted into natural language <b>counterfactuals.</b> We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting <b>counterfactuals</b> can be used to mitigate bias in classification.</p></p class="citation"></blockquote><h3 id=3737--37113-human-ai-interactions-in-the-communication-era-autophagy-makes-large-models-achieving-local-optima-shu-yang-et-al-2024>(37/37 | 37/113) Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima (Shu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shu Yang, Lijie Hu, Lu Yu, Muhammad Asif Ali, Di Wang. (2024)<br><strong>Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima</strong><br><button class=copy-to-clipboard title="Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-HC, cs.CL<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11271v1.pdf filename=2402.11271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing significance of large language and <b>multimodal</b> models in societal information processing has ignited debates on social safety and ethics. However, few studies have approached the analysis of these limitations from the comprehensive perspective of human and artificial intelligence system interactions. This study investigates biases and preferences when humans and large models are used as key links in communication. To achieve this, we design a <b>multimodal</b> dataset and three different experiments to evaluate generative models in their roles as producers and disseminators of information. Our main findings highlight that synthesized information is more likely to be incorporated into model training datasets and messaging than human-generated information. Additionally, large models, when acting as transmitters of information, tend to modify and lose specific content selectively. Conceptually, we present two realistic models of autophagic (&ldquo;self-consumption&rdquo;) loops to account for the suppression of human-generated information in the exchange of information between humans and AI systems. We generalize the declining diversity of social information and the bottleneck in model performance caused by the above trends to the local optima of large models.</p></p class="citation"></blockquote><h2 id=cslg-26>cs.LG (26)</h2><h3 id=126--38113-zerog-investigating-cross-dataset-zero-shot-transferability-in-graphs-yuhan-li-et-al-2024>(1/26 | 38/113) ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs (Yuhan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li. (2024)<br><strong>ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs</strong><br><button class=copy-to-clipboard title="ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 103<br>Keywords: Graph, Fine-tuning, Foundation Model, Semi-Supervised Learning, Transfer Learning, Zero-shot, GPT, GPT-4, Large Language Model, Prompt, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11235v1.pdf filename=2402.11235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of <b>foundation</b> <b>models</b> such as <b>large</b> <b>language</b> <b>models,</b> <b>zero-shot</b> <b>transfer</b> <b>learning</b> has become increasingly significant. This is highlighted by the generative capabilities of NLP models like <b>GPT-4,</b> and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of <b>graph</b> learning, the continuous emergence of new <b>graphs</b> and the challenges of human labeling also amplify the necessity for <b>zero-shot</b> <b>transfer</b> <b>learning,</b> driving the exploration of approaches that can generalize across diverse <b>graph</b> data without necessitating dataset-specific and label-specific <b>fine-tuning.</b> In this study, we extend such paradigms to <b>zero-shot</b> <b>transferability</b> in <b>graphs</b> by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative <b>transfer,</b> <b>we</b> leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a <b>prompt-based</b> subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using <b>prompting</b> nodes and neighborhood aggregation, respectively. We further adopt a lightweight <b>fine-tuning</b> strategy that reduces the risk of overfitting and maintains the <b>zero-shot</b> <b>learning</b> efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset <b>zero-shot</b> <b>transferability,</b> opening pathways for the development of <b>graph</b> <b>foundation</b> <b>models.</b> Especially, ZeroG, as a <b>zero-shot</b> <b>method,</b> can even achieve results comparable to those of <b>semi-supervised</b> <b>learning</b> on Pubmed.</p></p class="citation"></blockquote><h3 id=226--39113-ransomware-detection-using-stacked-autoencoder-for-feature-selection-mike-nkongolo-et-al-2024>(2/26 | 39/113) Ransomware detection using stacked autoencoder for feature selection (Mike Nkongolo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mike Nkongolo, Mahmut Tokmak. (2024)<br><strong>Ransomware detection using stacked autoencoder for feature selection</strong><br><button class=copy-to-clipboard title="Ransomware detection using stacked autoencoder for feature selection" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 90<br>Keywords: Autoencoder, Fine-tuning, Supervised Learning, Supervised Learning, Unsupervised Learning, LSTM, LSTM, LSTM, Ransomware<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11342v1.pdf filename=2402.11342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The aim of this study is to propose and evaluate an advanced <b>ransomware</b> detection and classification method that combines a Stacked <b>Autoencoder</b> (SAE) for precise feature selection with a <b>Long</b> <b>Short</b> <b>Term</b> <b>Memory</b> <b>(LSTM)</b> classifier to enhance <b>ransomware</b> stratification accuracy. The proposed approach involves thorough pre processing of the UGRansome dataset and training an <b>unsupervised</b> SAE for optimal feature selection or fine tuning via <b>supervised</b> <b>learning</b> to elevate the <b>LSTM</b> model&rsquo;s classification capabilities. The study meticulously analyzes the <b>autoencoder&rsquo;s</b> learned weights and activations to identify essential features for distinguishing <b>ransomware</b> families from other malware and creates a streamlined feature set for precise classification. Extensive experiments, including up to 400 epochs and varying learning rates, are conducted to optimize the model&rsquo;s performance. The results demonstrate the outstanding performance of the SAE-LSTM model across all <b>ransomware</b> families, boasting high precision, recall, and F1 score values that underscore its robust classification capabilities. Furthermore, balanced average scores affirm the proposed model&rsquo;s ability to generalize effectively across various malware types. The proposed model achieves an exceptional 99% accuracy in <b>ransomware</b> classification, surpassing the Extreme Gradient Boosting (XGBoost) algorithm primarily due to its effective SAE feature selection mechanism. The model also demonstrates outstanding performance in identifying signature attacks, achieving a 98% accuracy rate.</p></p class="citation"></blockquote><h3 id=326--40113-tunetables-context-optimization-for-scalable-prior-data-fitted-networks-benjamin-feuer-et-al-2024>(3/26 | 40/113) TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks (Benjamin Feuer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Feuer, Robin Tibor Schirrmeister, Valeriia Cherepanova, Chinmay Hegde, Frank Hutter, Micah Goldblum, Niv Cohen, Colin White. (2024)<br><strong>TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks</strong><br><button class=copy-to-clipboard title="TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fairness, In-context Learning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11137v1.pdf filename=2402.11137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to <b>large</b> <b>language</b> <b>models,</b> PFNs make use of pretraining and <b>in-context</b> <b>learning</b> to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs by developing context optimization techniques for PFNs. Specifically, we propose TuneTables, a novel <b>prompt-tuning</b> strategy that compresses <b>large</b> <b>datasets</b> <b>into</b> a smaller learned context. TuneTables scales TabPFN to be competitive with state-of-the-art tabular classification methods on larger datasets, while having a substantially lower inference time than TabPFN. Furthermore, we show that TuneTables can be used as an interpretability tool and can even be used to mitigate biases by optimizing a <b>fairness</b> objective.</p></p class="citation"></blockquote><h3 id=426--41113-lignn-graph-neural-networks-at-linkedin-fedor-borisyuk-et-al-2024>(4/26 | 41/113) LiGNN: Graph Neural Networks at LinkedIn (Fedor Borisyuk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fedor Borisyuk, Shihai He, Yunbo Ouyang, Morteza Ramezani, Peng Du, Xiaochen Hou, Chengming Jiang, Nitin Pasumarthy, Priya Bannur, Birjodh Tiwana, Ping Liu, Siddharth Dangi, Daqi Sun, Zhoutao Pei, Xiao Shi, Sirou Zhu, Qianqi Shen, Kuang-Hsuan Lee, David Stein, Baolei Li, Haichao Wei, Amol Ghoting, Souvik Ghosh. (2024)<br><strong>LiGNN: Graph Neural Networks at LinkedIn</strong><br><button class=copy-to-clipboard title="LiGNN: Graph Neural Networks at LinkedIn" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 48<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Recommendation, Representation Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11139v1.pdf filename=2402.11139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present LiGNN, a deployed large-scale <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> Framework. We share our insight on developing and deployment of <b>GNNs</b> at large scale at LinkedIn. We present a set of algorithmic improvements to the quality of <b>GNN</b> <b>representation</b> <b>learning</b> including temporal <b>graph</b> <b>architectures</b> <b>with</b> long term losses, effective cold start solutions via <b>graph</b> <b>densification,</b> <b>ID</b> embeddings and multi-hop neighbor sampling. We explain how we built and sped up by 7x our large-scale training on LinkedIn <b>graphs</b> <b>with</b> <b>adaptive</b> sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. We <b>summarize</b> our deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5% of Feed engaged daily active users, 0.2% session lift and 0.1% weekly active user lift from people <b>recommendation.</b> We believe that this work can provide practical solutions and insights for engineers who are interested in applying <b>Graph</b> <b>neural</b> <b>networks</b> at large scale.</p></p class="citation"></blockquote><h3 id=526--42113-aligning-large-language-models-by-on-policy-self-judgment-sangkyu-lee-et-al-2024>(5/26 | 42/113) Aligning Large Language Models by On-Policy Self-Judgment (Sangkyu Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min Yoo, Youngjae Yu. (2024)<br><strong>Aligning Large Language Models by On-Policy Self-Judgment</strong><br><button class=copy-to-clipboard title="Aligning Large Language Models by On-Policy Self-Judgment" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Supervised Learning, Instruction Following, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11253v1.pdf filename=2402.11253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To align <b>large</b> <b>language</b> <b>models</b> with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented <b>Supervised</b> <b>Fine-Tuning</b> (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the <b>instruction-following</b> <b>task,</b> choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines in preference <b>benchmarks.</b> We also show that self-rejection with oversampling can improve further without an additional evaluator. Our code is available at <a href=https://github.com/oddqueue/self-judge>https://github.com/oddqueue/self-judge</a>.</p></p class="citation"></blockquote><h3 id=626--43113-uncertainty-quantification-of-graph-convolution-neural-network-models-of-evolving-processes-jeremiah-hauth-et-al-2024>(6/26 | 43/113) Uncertainty Quantification of Graph Convolution Neural Network Models of Evolving Processes (Jeremiah Hauth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeremiah Hauth, Cosmin Safta, Xun Huan, Ravi G. Patel, Reese E. Jones. (2024)<br><strong>Uncertainty Quantification of Graph Convolution Neural Network Models of Evolving Processes</strong><br><button class=copy-to-clipboard title="Uncertainty Quantification of Graph Convolution Neural Network Models of Evolving Processes" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-ST, physics-comp-ph, stat-TH<br>Keyword Score: 33<br>Keywords: Graph, Convolution, Convolutional Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11179v1.pdf filename=2402.11179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The application of neural network models to scientific machine learning tasks has proliferated in recent years. In particular, neural network models have proved to be adept at modeling processes with spatial-temporal complexity. Nevertheless, these highly parameterized models have garnered skepticism in their ability to produce outputs with quantified error bounds over the regimes of interest. Hence there is a need to find uncertainty quantification methods that are suitable for neural networks. In this work we present comparisons of the parametric uncertainty quantification of neural networks modeling complex spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational gradient descent and its projected variant. Specifically we apply these methods to <b>graph</b> <b>convolutional</b> <b>neural</b> <b>network</b> models of evolving systems modeled with <b>recurrent</b> <b>neural</b> <b>network</b> and neural ordinary differential equations architectures. We show that Stein variational inference is a viable alternative to Monte Carlo methods with some clear advantages for complex neural network models. For our exemplars, Stein variational interference gave similar uncertainty profiles through time compared to Hamiltonian Monte Carlo, albeit with generally more generous variance.Projected Stein variational gradient descent also produced similar uncertainty profiles to the non-projected counterpart, but large reductions in the active weight space were confounded by the stability of the neural network predictions and the convoluted likelihood landscape.</p></p class="citation"></blockquote><h3 id=726--44113-heal-brain-inspired-hyperdimensional-efficient-active-learning-yang-ni-et-al-2024>(7/26 | 44/113) HEAL: Brain-inspired Hyperdimensional Efficient Active Learning (Yang Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Ni, Zhuowen Zou, Wenjun Huang, Hanning Chen, William Youngwoo Chung, Samuel Cho, Ranganath Krishnan, Pietro Mercati, Mohsen Imani. (2024)<br><strong>HEAL: Brain-inspired Hyperdimensional Efficient Active Learning</strong><br><button class=copy-to-clipboard title="HEAL: Brain-inspired Hyperdimensional Efficient Active Learning" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Active Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11223v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11223v1.pdf filename=2402.11223v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Drawing inspiration from the outstanding learning capability of our human brains, Hyperdimensional Computing (HDC) emerges as a novel computing paradigm, and it leverages high-dimensional vector presentation and operations for brain-like lightweight Machine Learning (ML). Practical deployments of HDC have significantly enhanced the learning efficiency compared to current deep ML methods on a broad spectrum of applications. However, boosting the data efficiency of HDC classifiers in <b>supervised</b> <b>learning</b> remains an open question. In this paper, we introduce Hyperdimensional Efficient <b>Active</b> <b>Learning</b> (HEAL), a novel <b>Active</b> <b>Learning</b> (AL) framework tailored for HDC classification. HEAL proactively annotates unlabeled data points via uncertainty and diversity-guided acquisition, leading to a more efficient dataset annotation and lowering labor costs. Unlike conventional AL methods that only support classifiers built upon deep neural networks (DNN), HEAL operates without the need for gradient or probabilistic computations. This allows it to be effortlessly integrated with any existing HDC classifier architecture. The key design of HEAL is a novel approach for uncertainty estimation in HDC classifiers through a lightweight HDC ensemble with prior hypervectors. Additionally, by exploiting hypervectors as prototypes (i.e., compact representations), we develop an extra metric for HEAL to select diverse samples within each batch for annotation. Our evaluation shows that HEAL surpasses a diverse set of baselines in AL quality and achieves notably faster acquisition than many BNN-powered or diversity-guided AL methods, recording 11 times to 40,000 times speedup in acquisition runtime per batch.</p></p class="citation"></blockquote><h3 id=826--45113-knowledge-distillation-based-on-transformed-teacher-matching-kaixiang-zheng-et-al-2024>(8/26 | 45/113) Knowledge Distillation Based on Transformed Teacher Matching (Kaixiang Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaixiang Zheng, En-Hui Yang. (2024)<br><strong>Knowledge Distillation Based on Transformed Teacher Matching</strong><br><button class=copy-to-clipboard title="Knowledge Distillation Based on Transformed Teacher Matching" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11148v1.pdf filename=2402.11148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in <b>knowledge</b> <b>distillation</b> <b>(KD).</b> Conventionally, temperature scaling is applied to both teacher&rsquo;s logits and student&rsquo;s logits in <b>KD.</b> Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of <b>KD,</b> dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original <b>KD,</b> TTM has an inherent R'enyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original <b>KD.</b> To further enhance student&rsquo;s capability to match teacher&rsquo;s power transformed probability distribution, we introduce a sample-adaptive weighting coefficient into TTM, yielding a novel <b>distillation</b> approach dubbed weighted TTM (WTTM). It is shown, by comprehensive experiments, that although WTTM is simple, it is effective, improves upon TTM, and achieves state-of-the-art accuracy performance. Our source code is available at <a href=https://github.com/zkxufo/TTM>https://github.com/zkxufo/TTM</a>.</p></p class="citation"></blockquote><h3 id=926--46113-debiased-offline-representation-learning-for-fast-online-adaptation-in-non-stationary-dynamics-xinyu-zhang-et-al-2024>(9/26 | 46/113) Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics (Xinyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Zhang, Wenjie Qiu, Yi-Chen Li, Lei Yuan, Chengxing Jia, Zongzhang Zhang, Yang Yu. (2024)<br><strong>Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics</strong><br><button class=copy-to-clipboard title="Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 28<br>Keywords: Benchmarking, Mutual Information, Reinforcement Learning, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11317v1.pdf filename=2402.11317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing policies that can adjust to non-stationary environments is essential for real-world <b>reinforcement</b> <b>learning</b> applications. However, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called Debiased Offline <b>Representation</b> <b>for</b> fast online Adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes <b>mutual</b> <b>information</b> between the dynamics encoding and the environmental data, while minimizing <b>mutual</b> <b>information</b> between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leveraging tractable bounds of the information bottleneck principle. Our experimental evaluation across six <b>benchmark</b> MuJoCo tasks with variable parameters demonstrates that DORA not only achieves a more precise dynamics encoding but also significantly outperforms existing baselines in terms of performance.</p></p class="citation"></blockquote><h3 id=1026--47113-evaluating-the-stability-of-deep-learning-latent-feature-spaces-ademide-o-mabadeje-et-al-2024>(10/26 | 47/113) Evaluating the Stability of Deep Learning Latent Feature Spaces (Ademide O. Mabadeje et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ademide O. Mabadeje, Michael J. Pyrcz. (2024)<br><strong>Evaluating the Stability of Deep Learning Latent Feature Spaces</strong><br><button class=copy-to-clipboard title="Evaluating the Stability of Deep Learning Latent Feature Spaces" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Autoencoder, Clustering, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11404v1.pdf filename=2402.11404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-dimensional datasets present substantial challenges in statistical modeling across various disciplines, necessitating effective dimensionality reduction methods. Deep learning approaches, notable for their capacity to <b>distill</b> essential features from complex data, facilitate modeling, visualization, and compression through reduced dimensionality latent feature spaces, have wide applications from bioinformatics to earth sciences. This study introduces a novel workflow to evaluate the stability of these latent spaces, ensuring consistency and reliability in subsequent analyses. Stability, defined as the invariance of latent spaces to minor data, training realizations, and parameter perturbations, is crucial yet often overlooked. Our proposed methodology delineates three stability types, sample, structural, and inferential, within latent spaces, and introduces a suite of metrics for comprehensive evaluation. We implement this workflow across 500 <b>autoencoder</b> realizations and three datasets, encompassing both synthetic and real-world scenarios to explain latent space dynamics. Employing k-means <b>clustering</b> and the modified Jonker-Volgenant algorithm for class alignment, alongside anisotropy metrics and convex hull analysis, we introduce adjusted stress and Jaccard dissimilarity as novel stability indicators. Our findings highlight inherent instabilities in latent feature spaces and demonstrate the workflow&rsquo;s efficacy in quantifying and interpreting these instabilities. This work advances the understanding of latent feature spaces, promoting improved model interpretability and quality control for more informed decision-making for diverse analytical workflows that leverage deep learning.</p></p class="citation"></blockquote><h3 id=1126--48113-maintaining-adversarial-robustness-in-continuous-learning-xiaolei-ru-et-al-2024>(11/26 | 48/113) Maintaining Adversarial Robustness in Continuous Learning (Xiaolei Ru et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaolei Ru, Xiaowei Cao, Zijia Liu, Jack Murdoch Moore, Xin-Ya Zhang, Xia Zhu, Wenjia Wei, Gang Yan. (2024)<br><strong>Maintaining Adversarial Robustness in Continuous Learning</strong><br><button class=copy-to-clipboard title="Maintaining Adversarial Robustness in Continuous Learning" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Continual Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11196v1.pdf filename=2402.11196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>robustness</b> is essential for security and reliability of machine learning systems. However, the <b>adversarial</b> <b>robustness</b> gained by sophisticated defense algorithms is easily erased as the neural network evolves to learn new tasks. This vulnerability can be addressed by fostering a novel capability for neural networks, termed <b>continual</b> <b>robust</b> learning, which focuses on both the (classification) performance and <b>adversarial</b> <b>robustness</b> on previous tasks during continuous learning. To achieve continuous robust learning, we propose an approach called Double Gradient Projection that projects the gradients for weight updates orthogonally onto two crucial subspaces &ndash; one for stabilizing the smoothed sample gradients and another for stabilizing the final outputs of the neural network. The experimental results on four <b>benchmarks</b> demonstrate that the proposed approach effectively maintains continuous robustness against strong <b>adversarial</b> <b>attacks,</b> outperforming the baselines formed by combining the existing defense strategies and <b>continual</b> <b>learning</b> methods.</p></p class="citation"></blockquote><h3 id=1226--49113-beyond-generalization-a-survey-of-out-of-distribution-adaptation-on-graphs-shuhan-liu-et-al-2024>(12/26 | 49/113) Beyond Generalization: A Survey of Out-Of-Distribution Adaptation on Graphs (Shuhan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuhan Liu, Kaize Ding. (2024)<br><strong>Beyond Generalization: A Survey of Out-Of-Distribution Adaptation on Graphs</strong><br><button class=copy-to-clipboard title="Beyond Generalization: A Survey of Out-Of-Distribution Adaptation on Graphs" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Distribution Shift, Distribution Shift, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11153v1.pdf filename=2402.11153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Distribution</b> <b>shifts</b> on <b>graphs</b> &ndash; the data <b>distribution</b> <b>discrepancies</b> between training and testing a <b>graph</b> machine learning model, are often ubiquitous and unavoidable in real-world scenarios. Such shifts may severely deteriorate the performance of the model, posing significant challenges for reliable <b>graph</b> machine learning. Consequently, there has been a surge in research on <b>graph</b> <b>Out-Of-Distribution</b> (OOD) adaptation methods that aim to mitigate the <b>distribution</b> <b>shifts</b> and adapt the knowledge from one <b>distribution</b> <b>to</b> another. In our survey, we provide an up-to-date and forward-looking review of <b>graph</b> OOD adaptation methods, covering two main problem scenarios including training-time as well as test-time <b>graph</b> OOD adaptation. We start by formally formulating the two problems and then discuss different types of <b>distribution</b> <b>shifts</b> on <b>graphs.</b> Based on our proposed taxonomy for <b>graph</b> OOD adaptation, we systematically categorize the existing methods according to their learning paradigm and investigate the techniques behind them. Finally, we point out promising research directions and the corresponding challenges. We also provide a continuously updated reading list at <a href=https://github.com/kaize0409/Awesome-Graph-OOD-Adaptation.git>https://github.com/kaize0409/Awesome-Graph-OOD-Adaptation.git</a></p></p class="citation"></blockquote><h3 id=1326--50113-expressive-higher-order-link-prediction-through-hypergraph-symmetry-breaking-simon-zhang-et-al-2024>(13/26 | 50/113) Expressive Higher-Order Link Prediction through Hypergraph Symmetry Breaking (Simon Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Zhang, Cheng Xin, Tamal K. Dey. (2024)<br><strong>Expressive Higher-Order Link Prediction through Hypergraph Symmetry Breaking</strong><br><button class=copy-to-clipboard title="Expressive Higher-Order Link Prediction through Hypergraph Symmetry Breaking" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Message-Passing, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11339v1.pdf filename=2402.11339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A hypergraph consists of a set of nodes along with a collection of subsets of the nodes called hyperedges. Higher-order link prediction is the task of predicting the existence of a missing hyperedge in a hypergraph. A hyperedge representation learned for higher order link prediction is fully expressive when it does not lose distinguishing power up to an isomorphism. Many existing hypergraph representation learners, are bounded in expressive power by the Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the Weisfeiler Lehman-1 algorithm. However, GWL-1 has limited expressive power. In fact, induced subhypergraphs with identical GWL-1 valued nodes are indistinguishable. Furthermore, message passing on hypergraphs can already be computationally expensive, especially on GPU memory. To address these limitations, we devise a preprocessing algorithm that can identify certain regular subhypergraphs exhibiting symmetry. Our preprocessing algorithm runs once with complexity the size of the input hypergraph. During training, we randomly replace subhypergraphs identified by the algorithm with covering hyperedges to break symmetry. We show that our method improves the expressivity of GWL-1. Our extensive experiments also demonstrate the effectiveness of our approach for higher-order link prediction on both <b>graph</b> and hypergraph datasets with negligible change in computation.</p></p class="citation"></blockquote><h3 id=1426--51113-reinforcement-learning-to-maximise-wind-turbine-energy-generation-daniel-soler-et-al-2024>(14/26 | 51/113) Reinforcement learning to maximise wind turbine energy generation (Daniel Soler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Soler, Oscar Mariño, David Huergo, Martín de Frutos, Esteban Ferrer. (2024)<br><strong>Reinforcement learning to maximise wind turbine energy generation</strong><br><button class=copy-to-clipboard title="Reinforcement learning to maximise wind turbine energy generation" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-MP, math-OC, math-ph<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11384v1.pdf filename=2402.11384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a <b>reinforcement</b> <b>learning</b> strategy to control wind turbine energy generation by actively changing the rotor speed, the rotor yaw angle and the blade pitch angle. A double deep Q-learning with a prioritized experience replay agent is coupled with a blade element momentum model and is trained to allow control for changing winds. The agent is trained to decide the best control (speed, yaw, pitch) for simple steady winds and is subsequently challenged with real dynamic turbulent winds, showing good performance. The double deep Q- learning is compared with a classic value iteration <b>reinforcement</b> <b>learning</b> control and both strategies outperform a classic PID control in all environments. Furthermore, the <b>reinforcement</b> <b>learning</b> approach is well suited to changing environments including turbulent/gusty winds, showing great adaptability. Finally, we compare all control strategies with real winds and compute the annual energy production. In this case, the double deep Q-learning algorithm also outperforms classic methodologies.</p></p class="citation"></blockquote><h3 id=1526--52113-multi-task-inverse-reinforcement-learning-for-common-sense-reward-neta-glazer-et-al-2024>(15/26 | 52/113) Multi Task Inverse Reinforcement Learning for Common Sense Reward (Neta Glazer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neta Glazer, Aviv Navon, Aviv Shamsian, Ethan Fetaya. (2024)<br><strong>Multi Task Inverse Reinforcement Learning for Common Sense Reward</strong><br><button class=copy-to-clipboard title="Multi Task Inverse Reinforcement Learning for Common Sense Reward" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11367v1.pdf filename=2402.11367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the challenges in applying <b>reinforcement</b> <b>learning</b> in a complex real-world environment lies in providing the agent with a sufficiently detailed reward function. Any misalignment between the reward and the desired behavior can result in unwanted outcomes. This may lead to issues like &ldquo;reward hacking&rdquo; where the agent maximizes rewards by unintended behavior. In this work, we propose to disentangle the reward into two distinct parts. A simple task-specific reward, outlining the particulars of the task at hand, and an unknown common-sense reward, indicating the expected behavior of the agent within the environment. We then explore how this common-sense reward can be learned from expert demonstrations. We first show that inverse <b>reinforcement</b> <b>learning,</b> even when it succeeds in training an agent, does not learn a useful reward function. That is, training a new agent with the learned reward does not impair the desired behaviors. We then demonstrate that this problem can be solved by training simultaneously on multiple tasks. That is, multi-task inverse <b>reinforcement</b> <b>learning</b> can be applied to learn a useful reward function.</p></p class="citation"></blockquote><h3 id=1626--53113-data-driven-stochastic-ac-opf-using-gaussian-processes-mile-mitrovic-2024>(16/26 | 53/113) Data-Driven Stochastic AC-OPF using Gaussian Processes (Mile Mitrovic, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mile Mitrovic. (2024)<br><strong>Data-Driven Stochastic AC-OPF using Gaussian Processes</strong><br><button class=copy-to-clipboard title="Data-Driven Stochastic AC-OPF using Gaussian Processes" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11365v1.pdf filename=2402.11365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The thesis focuses on developing a data-driven algorithm, based on machine learning, to solve the stochastic alternating current (AC) chance-constrained (CC) Optimal Power Flow (OPF) problem. Although the AC CC-OPF problem has been successful in academic circles, it is highly nonlinear and computationally demanding, which limits its practical impact. The proposed approach aims to address this limitation and demonstrate its empirical efficiency through applications to multiple IEEE test cases. To solve the non-convex and computationally challenging CC AC-OPF problem, the proposed approach relies on a machine learning <b>Gaussian</b> <b>process</b> regression (GPR) model. The full <b>Gaussian</b> <b>process</b> (GP) approach is capable of learning a simple yet non-convex data-driven approximation to the AC power flow equations that can incorporate uncertain inputs. The proposed approach uses various approximations for GP-uncertainty propagation. The full GP CC-OPF approach exhibits highly competitive and promising results, outperforming the state-of-the-art sample-based chance constraint approaches. To further improve the robustness and complexity/accuracy trade-off of the full GP CC-OPF, a fast data-driven setup is proposed. This setup relies on the sparse and hybrid <b>Gaussian</b> <b>processes</b> (GP) framework to model the power flow equations with input uncertainty.</p></p class="citation"></blockquote><h3 id=1726--54113-exploiting-t-norms-for-deep-learning-in-autonomous-driving-mihaela-cătălina-stoian-et-al-2024>(17/26 | 54/113) Exploiting T-norms for Deep Learning in Autonomous Driving (Mihaela Cătălina Stoian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mihaela Cătălina Stoian, Eleonora Giunchiglia, Thomas Lukasiewicz. (2024)<br><strong>Exploiting T-norms for Deep Learning in Autonomous Driving</strong><br><button class=copy-to-clipboard title="Exploiting T-norms for Deep Learning in Autonomous Driving" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-LO, cs.LG<br>Keyword Score: 10<br>Keywords: Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11362v1.pdf filename=2402.11362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning has been at the core of the autonomous driving field development, due to the neural networks&rsquo; success in finding patterns in raw data and turning them into accurate predictions. Moreover, recent neuro-symbolic works have shown that incorporating the available background knowledge about the problem at hand in the loss function via t-norms can further improve the deep learning models&rsquo; performance. However, t-norm-based losses may have very high memory requirements and, thus, they may be impossible to apply in complex application domains like autonomous driving. In this paper, we show how it is possible to define memory-efficient t-norm-based losses, allowing for exploiting t-norms for the task of <b>event</b> <b>detection</b> in autonomous driving. We conduct an extensive experimental analysis on the ROAD-R dataset and show (i) that our proposal can be implemented and run on GPUs with less than 25 GiB of available memory, while standard t-norm-based losses are estimated to require more than 100 GiB, far exceeding the amount of memory normally available, (ii) that t-norm-based losses improve performance, especially when limited labelled data are available, and (iii) that t-norm-based losses can further improve performance when exploited on both labelled and unlabelled data.</p></p class="citation"></blockquote><h3 id=1826--55113-fair-classification-with-partial-feedback-an-exploration-based-data-collection-approach-vijay-keswani-et-al-2024>(18/26 | 55/113) Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach (Vijay Keswani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vijay Keswani, Anay Mehrotra, L. Elisa Celis. (2024)<br><strong>Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach</strong><br><button class=copy-to-clipboard title="Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11338v1.pdf filename=2402.11338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a &ldquo;desired&rdquo; classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees and encode context-specific group <b>fairness</b> properties. Evaluation on real-world datasets shows that this approach consistently boosts the quality of collected outcome data and improves the fraction of true positives for all groups, with only a small reduction in predictive utility.</p></p class="citation"></blockquote><h3 id=1926--56113-adadagrad-adaptive-batch-size-schemes-for-adaptive-gradient-methods-tim-tsz-kit-lau-et-al-2024>(19/26 | 56/113) AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods (Tim Tsz-Kit Lau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Tsz-Kit Lau, Han Liu, Mladen Kolar. (2024)<br><strong>AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods</strong><br><button class=copy-to-clipboard title="AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11215v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11215v1.pdf filename=2402.11215v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The choice of batch sizes in <b>stochastic</b> <b>gradient</b> <b>optimizers</b> is critical for model training. However, the practice of varying batch sizes throughout the training process is less explored compared to other hyperparameters. We investigate adaptive batch size strategies derived from adaptive sampling methods, traditionally applied only in <b>stochastic</b> <b>gradient</b> <b>descent.</b> Given the significant interplay between learning rates and batch sizes, and considering the prevalence of adaptive gradient methods in deep learning, we emphasize the need for adaptive batch size strategies in these contexts. We introduce AdAdaGrad and its scalar variant AdAdaGradNorm, which incrementally increase batch sizes during training, while model updates are performed using AdaGrad and AdaGradNorm. We prove that AdaGradNorm converges with high probability at a rate of $\mathscr{O}(1/K)$ for finding a first-order stationary point of smooth nonconvex functions within $K$ iterations. AdaGrad also demonstrates similar convergence properties when integrated with a novel coordinate-wise variant of our adaptive batch size strategies. Our theoretical claims are supported by numerical experiments on various image classification tasks, highlighting the enhanced adaptability of progressive batching protocols in deep learning and the potential of such adaptive batch size strategies with adaptive gradient optimizers in large-scale model training.</p></p class="citation"></blockquote><h3 id=2026--57113-turn-waste-into-worth-rectifying-top-k-router-of-moe-zhiyuan-zeng-et-al-2024>(20/26 | 57/113) Turn Waste into Worth: Rectifying Top-$k$ Router of MoE (Zhiyuan Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Zeng, Qipeng Guo, Zhaoye Fei, Zhangyue Yin, Yunhua Zhou, Linyang Li, Tianxiang Sun, Hang Yan, Dahua Lin, Xipeng Qiu. (2024)<br><strong>Turn Waste into Worth: Rectifying Top-$k$ Router of MoE</strong><br><button class=copy-to-clipboard title="Turn Waste into Worth: Rectifying Top-$k$ Router of MoE" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12399v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12399v2.pdf filename=2402.12399v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparse Mixture of Experts (MoE) models are popular for training <b>large</b> <b>language</b> <b>models</b> due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectification effectively handle dropped tokens and padding, respectively. Furthermore, the combination of them achieves superior performance, surpassing the accuracy of the vanilla top-1 router by 4.7%.</p></p class="citation"></blockquote><h3 id=2126--58113-achieving-linear-speedup-in-asynchronous-federated-learning-with-heterogeneous-clients-xiaolu-wang-et-al-2024>(21/26 | 58/113) Achieving Linear Speedup in Asynchronous Federated Learning with Heterogeneous Clients (Xiaolu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaolu Wang, Zijian Li, Shi Jin, Jun Zhang. (2024)<br><strong>Achieving Linear Speedup in Asynchronous Federated Learning with Heterogeneous Clients</strong><br><button class=copy-to-clipboard title="Achieving Linear Speedup in Asynchronous Federated Learning with Heterogeneous Clients" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11198v1.pdf filename=2402.11198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is an emerging distributed training paradigm that aims to learn a common global model without exchanging or transferring the data that are stored locally at different clients. The <b>Federated</b> <b>Averaging</b> (FedAvg)-based algorithms have gained substantial popularity in FL to reduce the communication overhead, where each client conducts multiple localized iterations before communicating with a central server. In this paper, we focus on FL where the clients have diverse computation and/or communication capabilities. Under this circumstance, FedAvg can be less efficient since it requires all clients that participate in the global aggregation in a round to initiate iterations from the latest global model, and thus the synchronization among fast clients and straggler clients can severely slow down the overall training process. To address this issue, we propose an efficient asynchronous <b>federated</b> <b>learning</b> (AFL) framework called Delayed <b>Federated</b> <b>Averaging</b> (DeFedAvg). In DeFedAvg, the clients are allowed to perform local training with different stale global models at their own paces. Theoretical analyses demonstrate that DeFedAvg achieves asymptotic convergence rates that are on par with the results of FedAvg for solving nonconvex problems. More importantly, DeFedAvg is the first AFL algorithm that provably achieves the desirable linear speedup property, which indicates its high scalability. Additionally, we carry out extensive numerical experiments using real datasets to validate the efficiency and scalability of our approach when training deep neural networks.</p></p class="citation"></blockquote><h3 id=2226--59113-minimally-supervised-topological-projections-of-self-organizing-maps-for-phase-of-flight-identification-zimeng-lyu-et-al-2024>(22/26 | 59/113) Minimally Supervised Topological Projections of Self-Organizing Maps for Phase of Flight Identification (Zimeng Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zimeng Lyu, Pujan Thapa, Travis Desell. (2024)<br><strong>Minimally Supervised Topological Projections of Self-Organizing Maps for Phase of Flight Identification</strong><br><button class=copy-to-clipboard title="Minimally Supervised Topological Projections of Self-Organizing Maps for Phase of Flight Identification" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11185v1.pdf filename=2402.11185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying phases of flight is important in the field of general aviation, as knowing which phase of flight data is collected from aircraft flight data recorders can aid in the more effective detection of safety or hazardous events. General aviation flight data for phase of flight identification is usually per-second data, comes on a large scale, and is class imbalanced. It is expensive to manually label the data and training classification models usually faces class imbalance problems. This work investigates the use of a novel method for minimally <b>supervised</b> self-organizing maps (MS-SOMs) which utilize nearest neighbor majority votes in the SOM U-matrix for class estimation. Results show that the proposed method can reach or exceed a naive SOM approach which utilized a full data file of labeled data, with only 30 labeled datapoints per class. Additionally, the minimally <b>supervised</b> SOM is significantly more robust to the class imbalance of the phase of flight data. These results highlight how little data is required for effective phase of flight identification.</p></p class="citation"></blockquote><h3 id=2326--60113-trust-regions-for-explanations-via-black-box-probabilistic-certification-amit-dhurandhar-et-al-2024>(23/26 | 60/113) Trust Regions for Explanations via Black-Box Probabilistic Certification (Amit Dhurandhar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Dhurandhar, Swagatam Haldar, Dennis Wei, Karthikeyan Natesan Ramamurthy. (2024)<br><strong>Trust Regions for Explanations via Black-Box Probabilistic Certification</strong><br><button class=copy-to-clipboard title="Trust Regions for Explanations via Black-Box Probabilistic Certification" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11168v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11168v2.pdf filename=2402.11168v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the <b>black</b> <b>box</b> nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of <b>black</b> <b>box</b> (probabilistic) explanation certification. We ask the question: Given a <b>black</b> <b>box</b> model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a \emph{trust region} has multiple benefits: i) insight into model behavior in a \emph{region}, with a \emph{guarantee}; ii) ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse}, which can save time, energy and money by not having to find explanations for every example; and iv) a possible \emph{meta-metric} to compare explanation methods. Our contributions include formalizing this problem, proposing solutions, providing theoretical guarantees for these solutions that are computable, and experimentally showing their efficacy on synthetic and real data.</p></p class="citation"></blockquote><h3 id=2426--61113-random-projection-neural-networks-of-best-approximation-convergence-theory-and-practical-applications-gianluca-fabiani-2024>(24/26 | 61/113) Random Projection Neural Networks of Best Approximation: Convergence theory and practical applications (Gianluca Fabiani, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianluca Fabiani. (2024)<br><strong>Random Projection Neural Networks of Best Approximation: Convergence theory and practical applications</strong><br><button class=copy-to-clipboard title="Random Projection Neural Networks of Best Approximation: Convergence theory and practical applications" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 41A25, 41A30, 41A46, 41A50, 41A52, 65D15, 65Y20, 68W20, cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11397v1.pdf filename=2402.11397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the concept of Best Approximation for Feedforward Neural Networks (FNN) and explore their convergence properties through the lens of Random Projection (RPNNs). RPNNs have predetermined and fixed, once and for all, internal weights and biases, offering computational efficiency. We demonstrate that there exists a choice of external weights, for any family of such RPNNs, with non-polynomial infinitely differentiable activation functions, that exhibit an exponential convergence rate when approximating any infinitely differentiable function. For illustration purposes, we test the proposed RPNN-based function approximation, with parsimoniously chosen basis functions, across five <b>benchmark</b> function approximation problems. Results show that RPNNs achieve comparable performance to established methods such as Legendre Polynomials, highlighting their potential for efficient and accurate function approximation.</p></p class="citation"></blockquote><h3 id=2526--62113-probabilistic-routing-for-graph-based-approximate-nearest-neighbor-search-kejing-lu-et-al-2024>(25/26 | 62/113) Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search (Kejing Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kejing Lu, Chuan Xiao, Yoshiharu Ishikawa. (2024)<br><strong>Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search</strong><br><button class=copy-to-clipboard title="Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-DB, cs-DS, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11354v1.pdf filename=2402.11354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a pivotal challenge in the field of machine learning. In recent years, <b>graph-based</b> methods have emerged as the superior approach to ANNS, establishing a new state of the art. Although various optimizations for <b>graph-based</b> ANNS have been introduced, they predominantly rely on heuristic methods that lack formal theoretical backing. This paper aims to enhance routing within <b>graph-based</b> ANNS by introducing a method that offers a probabilistic guarantee when exploring a node&rsquo;s neighbors in the <b>graph.</b> We formulate the problem as probabilistic routing and develop two baseline strategies by incorporating locality-sensitive techniques. Subsequently, we introduce PEOs, a novel approach that efficiently identifies which neighbors in the <b>graph</b> should be considered for exact distance computation, thus significantly improving efficiency in practice. Our experiments demonstrate that equipping PEOs can increase throughput on a commonly utilized <b>graph</b> index (HNSW) by a factor of 1.6 to 2.5, and its efficiency consistently outperforms the leading-edge routing technique by 1.1 to 1.4 times.</p></p class="citation"></blockquote><h3 id=2626--63113-be-persistent-towards-a-unified-solution-for-mitigating-shortcuts-in-deep-learning-hadi-m-dolatabadi-et-al-2024>(26/26 | 63/113) Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning (Hadi M. Dolatabadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hadi M. Dolatabadi, Sarah M. Erfani, Christopher Leckie. (2024)<br><strong>Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning</strong><br><button class=copy-to-clipboard title="Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11237v1.pdf filename=2402.11237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than learning the intended task, they tend to draw inconclusive relationships between their inputs and outputs. Shortcut learning is ubiquitous among many failure cases of neural networks, and traces of this phenomenon can be seen in their generalizability issues, domain shift, adversarial vulnerability, and even bias towards majority groups. In this paper, we argue that this commonality in the cause of various DNN issues creates a significant opportunity that should be leveraged to find a unified solution for shortcut learning. To this end, we outline the recent advances in topological data analysis~(TDA), and persistent homology~(PH) in particular, to sketch a unified roadmap for detecting shortcuts in deep learning. We demonstrate our arguments by investigating the topological features of computational <b>graphs</b> in DNNs using two cases of unlearnable examples and bias in decision-making as our test studies. Our analysis of these two failure cases of DNNs reveals that finding a unified solution for shortcut learning in DNNs is not out of reach, and TDA can play a significant role in forming such a framework.</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=11--64113-towards-development-of-automated-knowledge-maps-and-databases-for-materials-engineering-using-large-language-models-deepak-prasad-et-al-2024>(1/1 | 64/113) Towards Development of Automated Knowledge Maps and Databases for Materials Engineering using Large Language Models (Deepak Prasad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deepak Prasad, Mayur Pimpude, Alankar Alankar. (2024)<br><strong>Towards Development of Automated Knowledge Maps and Databases for Materials Engineering using Large Language Models</strong><br><button class=copy-to-clipboard title="Towards Development of Automated Knowledge Maps and Databases for Materials Engineering using Large Language Models" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-DL, cs.DL<br>Keyword Score: 98<br>Keywords: Graph, Knowledge Graph, ChatGPT, GPT, GPT-3, GPT-3.5, Gemini, Large Language Model, Large Language Model, Rouge, Rouge-L<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11323v1.pdf filename=2402.11323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> based workflow is presented that utilizes OpenAI <b>ChatGPT</b> model <b>GPT-3.5-turbo-1106</b> and Google <b>Gemini</b> Pro model to create summary of text, data and images from research articles. It is demonstrated that by using a series of processing, the key information can be arranged in tabular form and <b>knowledge</b> <b>graphs</b> to capture underlying concepts. Our method offers efficiency and comprehension, enabling researchers to extract insights more effectively. Evaluation based on a diverse Scientific Paper Collection demonstrates our approach in facilitating discovery of <b>knowledge.</b> <b>This</b> work contributes to accelerated material design by smart literature review. The method has been tested based on various qualitative and quantitative measures of gathered information. The <b>ChatGPT</b> model achieved an F1 score of 0.40 for an exact match <b>(ROUGE-1,</b> <b>ROUGE-2)</b> but an impressive 0.479 for a relaxed match <b>(ROUGE-L,</b> <b>ROUGE-Lsum)</b> structural data format in performance evaluation. The Google <b>Gemini</b> Pro outperforms <b>ChatGPT</b> with an F1 score of 0.50 for an exact match and 0.63 for a relaxed match. This method facilitates high-throughput development of a database relevant to materials informatics. For demonstration, an example of data extraction and <b>knowledge</b> <b>graph</b> formation based on a manuscript about a titanium alloy is discussed.</p></p class="citation"></blockquote><h2 id=csir-4>cs.IR (4)</h2><h3 id=14--65113-exploring-chatgpt-for-next-generation-information-retrieval-opportunities-and-challenges-yizheng-huang-et-al-2024>(1/4 | 65/113) Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges (Yizheng Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizheng Huang, Jimmy Huang. (2024)<br><strong>Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges</strong><br><button class=copy-to-clipboard title="Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 80<br>Keywords: Generative AI, Supervised Learning, Supervised Learning, ChatGPT, GPT, GPT-3, GPT-4, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11203v1.pdf filename=2402.11203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of artificial intelligence (AI) has highlighted <b>ChatGPT</b> as a pivotal technology in the field of <b>information</b> <b>retrieval</b> (IR). Distinguished from its predecessors, <b>ChatGPT</b> offers significant benefits that have attracted the attention of both the industry and academic communities. While some view <b>ChatGPT</b> as a groundbreaking innovation, others attribute its success to the effective integration of product development and market strategies. The emergence of <b>ChatGPT,</b> alongside <b>GPT-4,</b> marks a new phase in <b>Generative</b> <b>AI,</b> generating content that is distinct from training examples and exceeding the capabilities of the prior <b>GPT-3</b> model by OpenAI. Unlike the traditional <b>supervised</b> <b>learning</b> approach in IR tasks, <b>ChatGPT</b> challenges existing paradigms, bringing forth new challenges and opportunities regarding text quality assurance, model bias, and efficiency. This paper seeks to examine the impact of <b>ChatGPT</b> on IR tasks and offer insights into its potential future developments.</p></p class="citation"></blockquote><h3 id=24--66113-foundation-models-for-recommender-systems-a-survey-and-new-perspectives-chengkai-huang-et-al-2024>(2/4 | 66/113) Foundation Models for Recommender Systems: A Survey and New Perspectives (Chengkai Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengkai Huang, Tong Yu, Kaige Xie, Shuai Zhang, Lina Yao, Julian McAuley. (2024)<br><strong>Foundation Models for Recommender Systems: A Survey and New Perspectives</strong><br><button class=copy-to-clipboard title="Foundation Models for Recommender Systems: A Survey and New Perspectives" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 35<br>Keywords: Foundation Model, Recommendation, Recommender System, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11143v1.pdf filename=2402.11143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>Foundation</b> <b>Models</b> (FMs), with their extensive knowledge bases and complex architectures, have offered unique opportunities within the realm of <b>recommender</b> <b>systems</b> (RSs). In this paper, we attempt to thoroughly examine FM-based <b>recommendation</b> systems (FM4RecSys). We start by reviewing the research background of FM4RecSys. Then, we provide a systematic taxonomy of existing FM4RecSys research works, which can be divided into four different parts including data characteristics, <b>representation</b> <b>learning,</b> model type, and downstream tasks. Within each part, we review the key recent research developments, outlining the representative models and discussing their characteristics. Moreover, we elaborate on the open problems and opportunities of FM4RecSys aiming to shed light on future research directions in this area. In conclusion, we recap our findings and discuss the emerging trends in this field.</p></p class="citation"></blockquote><h3 id=34--67113-mirror-gradient-towards-robust-multimodal-recommender-systems-via-exploring-flat-local-minima-shanshan-zhong-et-al-2024>(3/4 | 67/113) Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima (Shanshan Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanshan Zhong, Zhongzhan Huang, Daifeng Li, Wushao Wen, Jinghui Qin, Liang Lin. (2024)<br><strong>Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima</strong><br><button class=copy-to-clipboard title="Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11262v1.pdf filename=2402.11262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>recommender</b> <b>systems</b> utilize various types of information to model user preferences and item features, helping users discover items aligned with their interests. The integration of <b>multimodal</b> information mitigates the inherent challenges in <b>recommender</b> <b>systems,</b> e.g., the data sparsity problem and cold-start issues. However, it simultaneously magnifies certain risks from <b>multimodal</b> information inputs, such as information adjustment risk and inherent noise risk. These risks pose crucial challenges to the robustness of <b>recommendation</b> models. In this paper, we analyze <b>multimodal</b> <b>recommender</b> <b>systems</b> from the novel perspective of flat local minima and propose a concise yet effective gradient strategy called Mirror Gradient (MG). This strategy can implicitly enhance the model&rsquo;s robustness during the optimization process, mitigating instability risks arising from <b>multimodal</b> information inputs. We also provide strong theoretical evidence and conduct extensive empirical experiments to show the superiority of MG across various <b>multimodal</b> <b>recommendation</b> models and <b>benchmarks.</b> Furthermore, we find that the proposed MG can complement existing robust training methods and be easily extended to diverse advanced <b>recommendation</b> models, making it a promising new and fundamental paradigm for training <b>multimodal</b> <b>recommender</b> <b>systems.</b> The code is released at <a href=https://github.com/Qrange-group/Mirror-Gradient>https://github.com/Qrange-group/Mirror-Gradient</a>.</p></p class="citation"></blockquote><h3 id=44--68113-knowledge-graph-based-session-recommendation-with-adaptive-propagation-yu-wang-et-al-2024>(4/4 | 68/113) Knowledge Graph-based Session Recommendation with Adaptive Propagation (Yu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Wang, Amin Javari, Janani Balaji, Walid Shalaby, Tyler Derr, Xiquan Cui. (2024)<br><strong>Knowledge Graph-based Session Recommendation with Adaptive Propagation</strong><br><button class=copy-to-clipboard title="Knowledge Graph-based Session Recommendation with Adaptive Propagation" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 28<br>Keywords: Graph, Knowledge Graph, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11302v1.pdf filename=2402.11302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Session-based <b>recommender</b> <b>systems</b> (SBRSs) predict users&rsquo; next interacted items based on their historical activities. While most SBRSs capture purchasing intentions locally within each session, capturing items&rsquo; global information across different sessions is crucial in characterizing their general properties. Previous works capture this cross-session information by constructing <b>graphs</b> and incorporating neighbor information. However, this incorporation cannot vary adaptively according to the unique intention of each session, and the constructed <b>graphs</b> consist of only one type of user-item interaction. To address these limitations, we propose <b>knowledge</b> <b>graph-based</b> session <b>recommendation</b> with session-adaptive propagation. Specifically, we build a <b>knowledge</b> <b>graph</b> by connecting items with multi-typed edges to characterize various user-item interactions. Then, we adaptively aggregate items&rsquo; neighbor information considering user intention within the learned session. Experimental results demonstrate that equipping our constructed <b>knowledge</b> <b>graph</b> and session-adaptive propagation enhances session <b>recommendation</b> backbones by 10%-20%. Moreover, we provide an industrial case study showing our proposed framework achieves 2% performance boost over an existing well-deployed model at The Home Depot e-platform.</p></p class="citation"></blockquote><h2 id=cscv-12>cs.CV (12)</h2><h3 id=112--69113-collavo-crayon-large-language-and-vision-model-byung-kwan-lee-et-al-2024>(1/12 | 69/113) CoLLaVO: Crayon Large Language and Vision mOdel (Byung-Kwan Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro. (2024)<br><strong>CoLLaVO: Crayon Large Language and Vision mOdel</strong><br><button class=copy-to-clipboard title="CoLLaVO: Crayon Large Language and Vision mOdel" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Benchmarking, Zero-shot, Instruction Tuning, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11248v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11248v2.pdf filename=2402.11248v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The remarkable success of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>instruction</b> <b>tuning</b> drives the evolution of Vision Language Models (VLMs) towards a versatile general-purpose model. Yet, it remains unexplored whether current VLMs genuinely possess quality object-level image understanding capabilities determined from <code>what objects are in the image?' or </code>which object corresponds to a specified bounding box?&rsquo;. Our findings reveal that the image understanding capabilities of current VLMs are strongly correlated with their <b>zero-shot</b> performance on vision language (VL) tasks. This suggests that prioritizing basic image understanding is crucial for VLMs to excel at VL tasks. To enhance object-level image understanding, we propose Crayon <b>Large</b> <b>Language</b> <b>and</b> Vision mOdel(CoLLaVO), which incorporates <b>instruction</b> <b>tuning</b> with Crayon <b>Prompt</b> as a new visual <b>prompt</b> tuning scheme based on panoptic color maps. Furthermore, we present a learning strategy of Dual QLoRA to preserve object-level image understanding without forgetting it during visual <b>instruction</b> <b>tuning,</b> thereby achieving a significant leap in numerous VL <b>benchmarks</b> in a <b>zero-shot</b> setting.</p></p class="citation"></blockquote><h3 id=212--70113-fvit-a-focal-vision-transformer-with-gabor-filter-yulong-shi-et-al-2024>(2/12 | 70/113) FViT: A Focal Vision Transformer with Gabor Filter (Yulong Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulong Shi, Mingwei Sun, Yongshuai Wang, Rui Wang, Hui Sun, Zengqiang Chen. (2024)<br><strong>FViT: A Focal Vision Transformer with Gabor Filter</strong><br><button class=copy-to-clipboard title="FViT: A Focal Vision Transformer with Gabor Filter" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Transformer, Prompt, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11303v1.pdf filename=2402.11303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision</b> <b>transformers</b> have achieved encouraging progress in various computer <b>vision</b> <b>tasks.</b> A common belief is that this is attributed to the competence of <b>self-attention</b> in modeling the global dependencies among feature tokens. Unfortunately, <b>self-attention</b> still faces some challenges in dense prediction tasks, such as the high computational complexity and absence of desirable inductive bias. To address these above issues, we revisit the potential benefits of integrating <b>vision</b> <b>transformer</b> with Gabor filter, and propose a Learnable Gabor Filter (LGF) by using <b>convolution.</b> As an alternative to <b>self-attention,</b> we employ LGF to simulate the response of simple cells in the biological visual system to input images, <b>prompting</b> models to focus on discriminative feature representations of targets from various scales and orientations. Additionally, we designed a Bionic Focal <b>Vision</b> <b>(BFV)</b> block based on the LGF. This block draws inspiration from neuroscience and introduces a Multi-Path Feed Forward Network (MPFFN) to emulate the working way of biological visual cortex processing information in parallel. Furthermore, we develop a unified and efficient pyramid backbone network family called Focal <b>Vision</b> <b>Transformers</b> (FViTs) by stacking BFV blocks. Experimental results show that FViTs exhibit highly competitive performance in various <b>vision</b> <b>tasks.</b> Especially in terms of computational efficiency and scalability, FViTs show significantly advantages compared with other counterparts.</p></p class="citation"></blockquote><h3 id=312--71113-revit-enhancing-vision-transformers-with-attention-residual-connections-for-visual-recognition-anxhelo-diko-et-al-2024>(3/12 | 71/113) ReViT: Enhancing Vision Transformers with Attention Residual Connections for Visual Recognition (Anxhelo Diko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anxhelo Diko, Danilo Avola, Marco Cascio, Luigi Cinque. (2024)<br><strong>ReViT: Enhancing Vision Transformers with Attention Residual Connections for Visual Recognition</strong><br><button class=copy-to-clipboard title="ReViT: Enhancing Vision Transformers with Attention Residual Connections for Visual Recognition" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Vision Transformer, Object Detection, Benchmarking, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11301v1.pdf filename=2402.11301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision</b> <b>Transformer</b> (ViT) <b>self-attention</b> mechanism is characterized by feature collapse in deeper layers, resulting in the vanishing of low-level visual features. However, such features can be helpful to accurately represent and identify elements within an image and increase the accuracy and robustness of <b>vision-based</b> <b>recognition</b> systems. Following this rationale, we propose a novel residual attention learning method for improving ViT-based architectures, increasing their visual feature diversity and model robustness. In this way, the proposed network can capture and preserve significant low-level features, providing more details about the elements within the scene being analyzed. The effectiveness and robustness of the presented method are evaluated on five image classification <b>benchmarks,</b> including ImageNet1k, CIFAR10, CIFAR100, Oxford Flowers-102, and Oxford-IIIT Pet, achieving improved performances. Additionally, experiments on the COCO2017 dataset show that the devised approach discovers and incorporates semantic and spatial relationships for <b>object</b> <b>detection</b> and instance segmentation when implemented into spatial-aware <b>transformer</b> models.</p></p class="citation"></blockquote><h3 id=412--72113-chatearthnet-a-global-scale-high-quality-image-text-dataset-for-remote-sensing-zhenghang-yuan-et-al-2024>(4/12 | 72/113) ChatEarthNet: A Global-Scale, High-Quality Image-Text Dataset for Remote Sensing (Zhenghang Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenghang Yuan, Zhitong Xiong, Lichao Mou, Xiao Xiang Zhu. (2024)<br><strong>ChatEarthNet: A Global-Scale, High-Quality Image-Text Dataset for Remote Sensing</strong><br><button class=copy-to-clipboard title="ChatEarthNet: A Global-Scale, High-Quality Image-Text Dataset for Remote Sensing" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Foundation Model, ChatGPT, Image2text, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11325v1.pdf filename=2402.11325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An in-depth comprehension of global land cover is essential in Earth observation, forming the <b>foundation</b> <b>for</b> a multitude of applications. Although remote sensing technology has advanced rapidly, leading to a proliferation of satellite imagery, the inherent complexity of these images often makes them difficult for non-expert users to understand. Natural language, as a carrier of human knowledge, can be a bridge between common users and complicated satellite imagery. In this context, we introduce a global-scale, high-quality <b>image-text</b> dataset for remote sensing, providing natural language descriptions for Sentinel-2 data to facilitate the understanding of satellite imagery for common users. Specifically, we utilize Sentinel-2 data for its global coverage as the <b>foundational</b> <b>image</b> source, employing semantic segmentation labels from the European Space Agency&rsquo;s (ESA) WorldCover project to enrich the descriptions of land covers. By conducting in-depth semantic analysis, we formulate detailed <b>prompts</b> to elicit rich descriptions from <b>ChatGPT.</b> To enhance the dataset&rsquo;s quality, we introduce the manual verification process. This step involves manual inspection and correction to refine the dataset, thus significantly improving its accuracy and quality. Finally, we offer the community ChatEarthNet, a large-scale <b>image-text</b> dataset characterized by global coverage, high quality, wide-ranging diversity, and detailed descriptions. ChatEarthNet consists of 163,488 <b>image-text</b> pairs with captions generated by <b>ChatGPT-3.5</b> and an additional 10,000 <b>image-text</b> pairs with captions generated by <b>ChatGPT-4V(ision).</b> This dataset has significant potential for training <b>vision-language</b> <b>foundation</b> <b>models</b> and evaluating large <b>vision-language</b> models for remote sensing. The dataset will be made publicly available.</p></p class="citation"></blockquote><h3 id=512--73113-on-good-practices-for-task-specific-distillation-of-large-pretrained-models-juliette-marrie-et-al-2024>(5/12 | 73/113) On Good Practices for Task-Specific Distillation of Large Pretrained Models (Juliette Marrie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juliette Marrie, Michael Arbel, Julien Mairal, Diane Larlus. (2024)<br><strong>On Good Practices for Task-Specific Distillation of Large Pretrained Models</strong><br><button class=copy-to-clipboard title="On Good Practices for Task-Specific Distillation of Large Pretrained Models" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Data Augmentation, Knowledge Distillation, Knowledge Distillation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11305v1.pdf filename=2402.11305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large pretrained visual models exhibit remarkable generalization across diverse recognition tasks. Yet, real-world applications often demand compact models tailored to specific problems. Variants of <b>knowledge</b> <b>distillation</b> have been devised for such a purpose, enabling task-specific compact models (the students) to learn from a generic large pretrained one (the teacher). In this paper, we show that the excellent robustness and versatility of recent pretrained models challenge common practices established in the literature, calling for a new set of optimal guidelines for task-specific <b>distillation.</b> To address the lack of samples in downstream tasks, we also show that a variant of Mixup based on stable diffusion complements standard <b>data</b> <b>augmentation.</b> This strategy eliminates the need for engineered text <b>prompts</b> and improves <b>distillation</b> of generic models into streamlined specialized networks.</p></p class="citation"></blockquote><h3 id=612--74113-diffpoint-single-and-multi-view-point-cloud-reconstruction-with-vit-based-diffusion-model-yu-feng-et-al-2024>(6/12 | 74/113) DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model (Yu Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Feng, Xing Shi, Mengli Cheng, Yun Xiong. (2024)<br><strong>DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model</strong><br><button class=copy-to-clipboard title="DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11241v1.pdf filename=2402.11241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the task of 2D-to-3D reconstruction has gained significant attention in various real-world scenarios, it becomes crucial to be able to generate high-quality point clouds. Despite the recent success of deep learning models in generating point clouds, there are still challenges in producing high-fidelity results due to the disparities between images and point clouds. While <b>vision</b> <b>transformers</b> (ViT) and <b>diffusion</b> <b>models</b> have shown promise in various <b>vision</b> <b>tasks,</b> their benefits for reconstructing point clouds from images have not been demonstrated yet. In this paper, we first propose a neat and powerful architecture called DiffPoint that combines ViT and <b>diffusion</b> <b>models</b> for the task of point cloud reconstruction. At each <b>diffusion</b> <b>step,</b> we divide the noisy point clouds into irregular patches. Then, using a standard ViT backbone that treats all inputs as tokens (including time information, image embeddings, and noisy patches), we train our model to predict target points based on input images. We evaluate DiffPoint on both single-view and multi-view reconstruction tasks and achieve state-of-the-art results. Additionally, we introduce a unified and flexible feature fusion module for aggregating image features from single or multiple input images. Furthermore, our work demonstrates the feasibility of applying unified architectures across languages and images to improve 3D reconstruction tasks.</p></p class="citation"></blockquote><h3 id=712--75113-graphkd-exploring-knowledge-distillation-towards-document-object-detection-with-structured-graph-creation-ayan-banerjee-et-al-2024>(7/12 | 75/113) GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation (Ayan Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayan Banerjee, Sanket Biswas, Josep Lladós, Umapada Pal. (2024)<br><strong>GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation</strong><br><button class=copy-to-clipboard title="GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 36<br>Keywords: Object Detection, Graph, Benchmarking, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11401v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11401v2.pdf filename=2402.11401v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Object</b> <b>detection</b> in documents is a key step to automate the structural elements identification process in a digital or scanned document through understanding the hierarchical structure and relationships between different elements. Large and complex models, while achieving high accuracy, can be computationally expensive and memory-intensive, making them impractical for deployment on resource constrained devices. <b>Knowledge</b> <b>distillation</b> allows us to create small and more efficient models that retain much of the performance of their larger counterparts. Here we present a <b>graph-based</b> <b>knowledge</b> <b>distillation</b> framework to correctly identify and localize the document <b>objects</b> <b>in</b> a document image. Here, we design a structured <b>graph</b> with nodes containing proposal-level features and edges representing the relationship between the different proposal regions. Also, to reduce text bias an adaptive node sampling strategy is designed to prune the weight distribution and put more weightage on non-text nodes. We encode the complete <b>graph</b> as a <b>knowledge</b> <b>representation</b> and transfer it from the teacher to the student through the proposed <b>distillation</b> loss by effectively capturing both local and global information concurrently. Extensive experimentation on competitive <b>benchmarks</b> demonstrates that the proposed framework outperforms the current state-of-the-art approaches. The code will be available at: <a href=https://github.com/ayanban011/GraphKD>https://github.com/ayanban011/GraphKD</a>.</p></p class="citation"></blockquote><h3 id=812--76113-semi-supervised-medical-image-segmentation-method-based-on-cross-pseudo-labeling-leveraging-strong-and-weak-data-augmentation-strategies-yifei-chen-et-al-2024>(8/12 | 76/113) Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo Labeling Leveraging Strong and Weak Data Augmentation Strategies (Yifei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Chen, Chenyan Zhang, Yifan Ke, Yiyu Huang, Xuezhou Dai, Feiwei Qin, Yongquan Zhang, Xiaodong Zhang, Changmiao Wang. (2024)<br><strong>Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo Labeling Leveraging Strong and Weak Data Augmentation Strategies</strong><br><button class=copy-to-clipboard title="Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo Labeling Leveraging Strong and Weak Data Augmentation Strategies" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11273v1.pdf filename=2402.11273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional <b>supervised</b> <b>learning</b> methods have historically encountered certain constraints in medical image segmentation due to the challenging collection process, high labeling cost, low signal-to-noise ratio, and complex features characterizing biomedical images. This paper proposes a semi-supervised model, DFCPS, which innovatively incorporates the Fixmatch concept. This significantly enhances the model&rsquo;s performance and generalizability through <b>data</b> <b>augmentation</b> processing, employing varied strategies for unlabeled <b>data.</b> <b>Concurrently,</b> the model design gives appropriate emphasis to the generation, filtration, and refinement processes of pseudo-labels. The novel concept of cross-pseudo-supervision is introduced, integrating consistency learning with self-training. This enables the model to fully leverage pseudo-labels from multiple perspectives, thereby enhancing training diversity. The DFCPS model is compared with both baseline and advanced models using the publicly accessible Kvasir-SEG dataset. Across all four subdivisions containing different proportions of unlabeled <b>data,</b> <b>our</b> model consistently exhibits superior performance. Our source code is available at <a href=https://github.com/JustlfC03/DFCPS>https://github.com/JustlfC03/DFCPS</a>.</p></p class="citation"></blockquote><h3 id=912--77113-a-decoding-scheme-with-successive-aggregation-of-multi-level-features-for-light-weight-semantic-segmentation-jiwon-yoo-et-al-2024>(9/12 | 77/113) A Decoding Scheme with Successive Aggregation of Multi-Level Features for Light-Weight Semantic Segmentation (Jiwon Yoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiwon Yoo, Jangwon Lee, Gyeonghwan Kim. (2024)<br><strong>A Decoding Scheme with Successive Aggregation of Multi-Level Features for Light-Weight Semantic Segmentation</strong><br><button class=copy-to-clipboard title="A Decoding Scheme with Successive Aggregation of Multi-Level Features for Light-Weight Semantic Segmentation" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11201v1.pdf filename=2402.11201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-scale architecture, including hierarchical <b>vision</b> <b>transformer,</b> has been commonly applied to high-resolution semantic segmentation to deal with computational complexity with minimum performance loss. In this paper, we propose a novel decoding scheme for semantic segmentation in this regard, which takes multi-level features from the encoder with multi-scale architecture. The decoding scheme based on a multi-level <b>vision</b> <b>transformer</b> aims to achieve not only reduced computational expense but also higher segmentation accuracy, by introducing successive cross-attention in aggregation of the multi-level features. Furthermore, a way to enhance the multi-level features by the aggregated semantics is proposed. The effort is focused on maintaining the contextual consistency from the perspective of attention allocation and brings improved performance with significantly lower computational cost. Set of experiments on popular datasets demonstrates superiority of the proposed scheme to the state-of-the-art semantic segmentation models in terms of computational cost without loss of accuracy, and extensive ablation studies prove the effectiveness of ideas proposed.</p></p class="citation"></blockquote><h3 id=1012--78113-learning-by-reconstruction-produces-uninformative-features-for-perception-randall-balestriero-et-al-2024>(10/12 | 78/113) Learning by Reconstruction Produces Uninformative Features For Perception (Randall Balestriero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Randall Balestriero, Yann LeCun. (2024)<br><strong>Learning by Reconstruction Produces Uninformative Features For Perception</strong><br><button class=copy-to-clipboard title="Learning by Reconstruction Produces Uninformative Features For Perception" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, stat-ML<br>Keyword Score: 25<br>Keywords: Autoencoder, Representation Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11337v1.pdf filename=2402.11337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Input space reconstruction is an attractive <b>representation</b> <b>learning</b> paradigm. Despite interpretability of the reconstruction and generation, we identify a misalignment between learning by reconstruction, and learning for perception. We show that the former allocates a model&rsquo;s capacity towards a subspace of the data explaining the observed variance&ndash;a subspace with uninformative features for the latter. For example, the <b>supervised</b> TinyImagenet task with images projected onto the top subspace explaining 90% of the pixel variance can be solved with 45% test accuracy. Using the bottom subspace instead, accounting for only 20% of the pixel variance, reaches 55% test accuracy. The features for perception being learned last explains the need for long training time, e.g., with Masked <b>Autoencoders.</b> Learning by denoising is a popular strategy to alleviate that misalignment. We prove that while some noise strategies such as masking are indeed beneficial, others such as additive Gaussian noise are not. Yet, even in the case of masking, we find that the benefits vary as a function of the mask&rsquo;s shape, ratio, and the considered dataset. While tuning the noise strategy without knowledge of the perception task seems challenging, we provide first clues on how to detect if a noise strategy is never beneficial regardless of the perception task.</p></p class="citation"></blockquote><h3 id=1112--79113-hand-biometrics-in-digital-forensics-asish-bera-et-al-2024>(11/12 | 79/113) Hand Biometrics in Digital Forensics (Asish Bera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asish Bera, Debotosh Bhattacharjee, Mita Nasipuri. (2024)<br><strong>Hand Biometrics in Digital Forensics</strong><br><button class=copy-to-clipboard title="Hand Biometrics in Digital Forensics" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 11<br>Keywords: Geometry, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11206v1.pdf filename=2402.11206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digital forensic is now an unavoidable part for securing the digital world from identity theft. Higher order of crimes, dealing with a massive database is really very challenging problem for any intelligent system. Biometric is a better solution to win over the problems encountered by digital forensics. Many biometric characteristics are playing their significant roles in forensics over the decades. The potential benefits and scope of hand based modes in forensics have been investigated with an illustration of hand <b>geometry</b> verifi-cation method. It can be applied when effective biometric evidences are properly unavailable; gloves are damaged, and dirt or any kind of liquid can minimize the accessibility and reliability of the fingerprint or palmprint. Due to the crisis of pure uniqueness of hand features for a very large database, it may be relevant for verification only. Some unimodal and <b>multimodal</b> hand based biometrics (e.g. hand <b>geometry,</b> palmprint and hand vein) with several feature extractions, database and verification methods have been discussed with 2D, 3D and infrared images.</p></p class="citation"></blockquote><h3 id=1212--80113-beyond-literal-descriptions-understanding-and-locating-open-world-objects-aligned-with-human-intentions-wenxuan-wang-et-al-2024>(12/12 | 80/113) Beyond Literal Descriptions: Understanding and Locating Open-World Objects Aligned with Human Intentions (Wenxuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenxuan Wang, Yisi Zhang, Xingjian He, Yichen Yan, Zijia Zhao, Xinlong Wang, Jing Liu. (2024)<br><strong>Beyond Literal Descriptions: Understanding and Locating Open-World Objects Aligned with Human Intentions</strong><br><button class=copy-to-clipboard title="Beyond Literal Descriptions: Understanding and Locating Open-World Objects Aligned with Human Intentions" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11265v1.pdf filename=2402.11265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual <b>grounding</b> (VG) aims at locating the foreground entities that match the given natural language expression. Previous datasets and methods for classic VG task mainly rely on the prior assumption that the given expression must literally refer to the target object, which greatly impedes the practical deployment of agents in real-world scenarios. Since users usually prefer to provide the intention-based expressions for the desired object instead of covering all the details, it is necessary for the agents to interpret the intention-driven instructions. Thus, in this work, we take a step further to the intention-driven visual-language (V-L) understanding. To promote classic VG towards human intention interpretation, we propose a new intention-driven visual <b>grounding</b> (IVG) task and build a largest-scale IVG dataset named IntentionVG with free-form intention expressions. Considering that practical agents need to move and find specific targets among various scenarios to realize the <b>grounding</b> task, our IVG task and IntentionVG dataset have taken the crucial properties of both multi-scenario perception and egocentric view into consideration. Besides, various types of models are set up as the baselines to realize our IVG task. Extensive experiments on our IntentionVG dataset and baselines demonstrate the necessity and efficacy of our method for the V-L field. To foster future research in this direction, our newly built dataset and baselines will be publicly available.</p></p class="citation"></blockquote><h2 id=eessas-4>eess.AS (4)</h2><h3 id=14--81113-target-speech-extraction-with-pre-trained-self-supervised-learning-models-junyi-peng-et-al-2024>(1/4 | 81/113) Target Speech Extraction with Pre-trained Self-supervised Learning Models (Junyi Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyi Peng, Marc Delcroix, Tsubasa Ochiai, Oldrich Plchot, Shoko Araki, Jan Cernocky. (2024)<br><strong>Target Speech Extraction with Pre-trained Self-supervised Learning Models</strong><br><button class=copy-to-clipboard title="Target Speech Extraction with Pre-trained Self-supervised Learning Models" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 50<br>Keywords: Convolutional Neural Network, Fine-tuning, Self-supervised Learning, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13199v1.pdf filename=2402.13199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained <b>self-supervised</b> <b>learning</b> (SSL) models have achieved remarkable success in various speech tasks. However, their potential in target speech extraction (TSE) has not been fully exploited. TSE aims to extract the speech of a target speaker in a mixture guided by enrollment utterances. We exploit pre-trained SSL models for two purposes within a TSE framework, i.e., to process the input mixture and to derive speaker embeddings from the enrollment. In this paper, we focus on how to effectively use SSL models for TSE. We first introduce a novel TSE downstream task following the SUPERB principles. This simple experiment shows the potential of SSL models for TSE, but extraction performance remains far behind the state-of-the-art. We then extend a powerful TSE architecture by incorporating two SSL-based modules: an Adaptive Input Enhancer (AIE) and a speaker encoder. Specifically, the proposed AIE utilizes intermediate representations from the <b>CNN</b> encoder by adjusting the time resolution of <b>CNN</b> encoder and <b>transformer</b> blocks through progressive upsampling, capturing both fine-grained and hierarchical features. Our method outperforms current TSE systems achieving a SI-SDR improvement of 14.0 dB on LibriMix. Moreover, we can further improve performance by 0.7 dB by <b>fine-tuning</b> the whole model including the SSL model parameters.</p></p class="citation"></blockquote><h3 id=24--82113-when-llms-meets-acoustic-landmarks-an-efficient-approach-to-integrate-speech-into-large-language-models-for-depression-detection-xiangyu-zhang-et-al-2024>(2/4 | 82/113) When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection (Xiangyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyu Zhang, Hexin Liu, Kaishuai Xu, Qiquan Zhang, Daijiao Liu, Beena Ahmed, Julien Epps. (2024)<br><strong>When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection</strong><br><button class=copy-to-clipboard title="When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-SD, eess-AS, eess.AS<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13276v1.pdf filename=2402.13276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Depression is a critical concern in global mental health, <b>prompting</b> extensive research into AI-based detection methods. Among various AI technologies, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> stand out for their versatility in mental healthcare applications. However, their primary limitation arises from their exclusive dependence on textual input, which constrains their overall capabilities. Furthermore, the utilization of <b>LLMs</b> in identifying and analyzing depressive states is still relatively untapped. In this paper, we present an innovative approach to integrating acoustic speech information into the <b>LLMs</b> framework for <b>multimodal</b> depression detection. We investigate an efficient method for depression detection by integrating speech signals into <b>LLMs</b> utilizing Acoustic Landmarks. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration also provides insights into the unique speech patterns of individuals, revealing the potential mental states of individuals. Evaluations of the proposed approach on the DAIC-WOZ dataset reveal state-of-the-art results when compared with existing Audio-Text baselines. In addition, this approach is not only valuable for the detection of depression but also represents a new perspective in enhancing the ability of <b>LLMs</b> to comprehend and process speech signals.</p></p class="citation"></blockquote><h3 id=34--83113-probing-self-supervised-learning-models-with-target-speech-extraction-junyi-peng-et-al-2024>(3/4 | 83/113) Probing Self-supervised Learning Models with Target Speech Extraction (Junyi Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyi Peng, Marc Delcroix, Tsubasa Ochiai, Oldrich Plchot, Takanori Ashihara, Shoko Araki, Jan Cernocky. (2024)<br><strong>Probing Self-supervised Learning Models with Target Speech Extraction</strong><br><button class=copy-to-clipboard title="Probing Self-supervised Learning Models with Target Speech Extraction" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 23<br>Keywords: Benchmarking, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13200v1.pdf filename=2402.13200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale pre-trained <b>self-supervised</b> <b>learning</b> (SSL) models have shown remarkable advancements in speech-related tasks. However, the utilization of these models in complex multi-talker scenarios, such as extracting a target speaker in a mixture, is yet to be fully evaluated. In this paper, we introduce target speech extraction (TSE) as a novel downstream task to evaluate the feature extraction capabilities of pre-trained SSL models. TSE uniquely requires both speaker identification and speech separation, distinguishing it from other tasks in the Speech processing Universal PERformance <b>Benchmark</b> (SUPERB) evaluation. Specifically, we propose a TSE downstream model composed of two lightweight task-oriented modules based on the same frozen SSL model. One module functions as a speaker encoder to obtain target speaker information from an enrollment speech, while the other estimates the target speaker&rsquo;s mask to extract its speech from the mixture. Experimental results on the Libri2mix datasets reveal the relevance of the TSE downstream task to probe SSL models, as its performance cannot be simply deduced from other related tasks such as speaker verification and separation.</p></p class="citation"></blockquote><h3 id=44--84113-diffuse-sound-field-synthesis-franz-zotter-et-al-2024>(4/4 | 84/113) Diffuse Sound Field Synthesis (Franz Zotter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Franz Zotter, Stefan Riedel, Lukas Gölles, Matthias Frank. (2024)<br><strong>Diffuse Sound Field Synthesis</strong><br><button class=copy-to-clipboard title="Diffuse Sound Field Synthesis" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11330v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11330v2.pdf filename=2402.11330v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Can uncorrelated surrounding sound sources be used to generate extended diffuse sound fields? By definition, targets are a constant sound pressure level, a vanishing average sound intensity, uncorrelated sound waves arriving isotropically from all directions. Does this require specific sources and geometries for surrounding 2D and 3D source layouts? As methods, we employ numeric <b>simulations</b> and undertake a series of calculations with uncorrelated circular/spherical source layouts, or such with infinite excess dimensions, and we point out relations to potential theory. Using a radial decay 1/r^b modified by the exponent b, the representation of the resulting fields with hypergeometric functions, Gegenbauer polynomials, and circular as well as spherical harmonics yields fruitful insights. In circular layouts, waves decaying by the exponent b=1/2 synthesize ideally extended, diffuse sound fields; spherical layouts do so with b=1. None of the layouts synthesizes a perfectly constant expected sound pressure level but its flatness is acceptable. Spherical t-designs describe optimal source layouts with well-described area of high diffuseness, and non-spherical, convex layouts can be improved by restoring isotropy or by mode matching for a maximally diffuse synthesis. Theory and <b>simulation</b> offer a basis for loudspeaker-based synthesis of diffuse sound fields and contribute physical reasons to recent psychoacoustic findings in spatial audio.</p></p class="citation"></blockquote><h2 id=csro-3>cs.RO (3)</h2><h3 id=13--85113-hysteresis-compensation-of-flexible-continuum-manipulator-using-rgbd-sensing-and-temporal-convolutional-network-junhyun-park-et-al-2024>(1/3 | 85/113) Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network (Junhyun Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhyun Park, Seonghyeok Jang, Hyojae Park, Seongjun Bae, Minho Hwang. (2024)<br><strong>Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network</strong><br><button class=copy-to-clipboard title="Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11319v1.pdf filename=2402.11319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Flexible continuum manipulators are valued for minimally invasive surgery, offering access to confined spaces through nonlinear paths. However, cable-driven manipulators face control difficulties due to hysteresis from cabling effects such as friction, elongation, and coupling. These effects are difficult to model due to nonlinearity and the difficulties become even more evident when dealing with long and multi-segmented manipulator. This paper proposes a data-driven approach based on <b>recurrent</b> <b>neural</b> <b>networks</b> to capture these nonlinear and previous states-dependent characteristics of cable actuation. We design customized fiducial markers to collect physical joint configurations as a dataset. Result on a study comparing the learning performance of four Deep Neural Network (DNN) models show that the Temporal <b>Convolution</b> <b>Network</b> (TCN) demonstrates the highest predictive capability. Leveraging trained TCNs, we build a control algorithm to compensate for hysteresis. Tracking tests in task space using unseen trajectories show that the best controller reduces the mean position and orientation error by 61.39% (from 13.7 mm to 5.29 mm) and 64.04% (from 31.17{\deg} to 11.21{\deg}), respectively.</p></p class="citation"></blockquote><h3 id=23--86113-carla-autoware-bridge-facilitating-autonomous-driving-research-with-a-unified-framework-for-simulation-and-module-development-gemb-kaljavesi-et-al-2024>(2/3 | 86/113) CARLA-Autoware-Bridge: Facilitating Autonomous Driving Research with a Unified Framework for Simulation and Module Development (Gemb Kaljavesi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gemb Kaljavesi, Tobias Kerbl, Tobias Betz, Kirill Mitkovskii, Frank Diermeyer. (2024)<br><strong>CARLA-Autoware-Bridge: Facilitating Autonomous Driving Research with a Unified Framework for Simulation and Module Development</strong><br><button class=copy-to-clipboard title="CARLA-Autoware-Bridge: Facilitating Autonomous Driving Research with a Unified Framework for Simulation and Module Development" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11239v1.pdf filename=2402.11239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extensive testing is necessary to ensure the safety of autonomous driving modules. In addition to component tests, the safety assessment of individual modules also requires a holistic view at system level, which can be carried out efficiently with the help of <b>simulation.</b> Achieving seamless compatibility between a modular software stack and <b>simulation</b> is complex and poses a significant challenge for many researchers. To ensure testing at the system level with state-of-the-art AV software and <b>simulation</b> software, we have developed and analyzed a bridge connecting the CARLA simulator with the AV software Autoware Core/Universe. This publicly available bridge enables researchers to easily test their modules within the overall software. Our investigations show that an efficient and reliable communication system has been established. We provide the <b>simulation</b> bridge as open-source software at <a href=https://github.com/TUMFTM/Carla-Autoware-Bridge>https://github.com/TUMFTM/Carla-Autoware-Bridge</a></p></p class="citation"></blockquote><h3 id=33--87113-mob-net-limb-modularized-uncertainty-torque-learning-of-humanoids-for-sensorless-external-torque-estimation-daegyu-lim-et-al-2024>(3/3 | 87/113) MOB-Net: Limb-modularized Uncertainty Torque Learning of Humanoids for Sensorless External Torque Estimation (Daegyu Lim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daegyu Lim, Myeong-Ju Kim, Junhyeok Cha, Jaeheung Park. (2024)<br><strong>MOB-Net: Limb-modularized Uncertainty Torque Learning of Humanoids for Sensorless External Torque Estimation</strong><br><button class=copy-to-clipboard title="MOB-Net: Limb-modularized Uncertainty Torque Learning of Humanoids for Sensorless External Torque Estimation" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11221v1.pdf filename=2402.11221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Momentum observer (MOB) can estimate external joint torque without requiring additional sensors, such as force/torque or joint torque sensors. However, the estimation performance of MOB deteriorates due to the model uncertainty which encompasses the modeling errors and the joint friction. Moreover, the estimation error is significant when MOB is applied to high-dimensional floating-base humanoids, which prevents the estimated external joint torque from being used for force control or collision detection in the real humanoid robot. In this paper, the pure external joint torque estimation method named MOB-Net, is proposed for humanoids. MOB-Net learns the model uncertainty torque and calibrates the estimated signal of MOB. The external joint torque can be estimated in the generalized coordinate including whole-body and virtual joints of the floating-base robot with only internal sensors (an IMU on the pelvis and encoders in the joints). Our method substantially reduces the estimation errors of MOB, and the robust performance of MOB-Net for the unseen data is validated through extensive <b>simulations,</b> real robot experiments, and ablation studies. Finally, various collision handling scenarios are presented using the estimated external joint torque from MOB-Net: contact wrench feedback control for locomotion, collision detection, and collision reaction for safety.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--88113-multi-generative-agent-collective-decision-making-in-urban-planning-a-case-study-for-kendall-square-renovation-jin-gao-et-al-2024>(1/1 | 88/113) Multi-Generative Agent Collective Decision-Making in Urban Planning: A Case Study for Kendall Square Renovation (Jin Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin Gao, Hanyong Xu, Luc Dao. (2024)<br><strong>Multi-Generative Agent Collective Decision-Making in Urban Planning: A Case Study for Kendall Square Renovation</strong><br><button class=copy-to-clipboard title="Multi-Generative Agent Collective Decision-Making in Urban Planning: A Case Study for Kendall Square Renovation" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11314v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11314v1.pdf filename=2402.11314v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we develop a multiple-generative agent system to simulate community decision-making for the redevelopment of Kendall Square&rsquo;s Volpe building. Drawing on interviews with local stakeholders, our <b>simulations</b> incorporated varying degrees of communication, demographic data, and life values in the agent <b>prompts.</b> The results revealed that communication among agents improved collective <b>reasoning,</b> while the inclusion of demographic and life values led to more distinct opinions. These findings highlight the potential application of AI in understanding complex social interactions and decision-making processes, offering valuable insights for urban planning and community engagement in diverse settings like Kendall Square.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=14--89113-understanding-the-impact-of-long-term-memory-on-self-disclosure-with-large-language-model-driven-chatbots-for-public-health-intervention-eunkyung-jo-et-al-2024>(1/4 | 89/113) Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention (Eunkyung Jo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eunkyung Jo, Yuin Jeong, SoHyun Park, Daniel A. Epstein, Young-Ho Kim. (2024)<br><strong>Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention</strong><br><button class=copy-to-clipboard title="Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-2; I-2-7, cs-AI, cs-CL, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11353v1.pdf filename=2402.11353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> offer the potential to support public health monitoring by facilitating health disclosure through open-ended conversations but rarely preserve the knowledge gained about individuals across repeated interactions. Augmenting <b>LLMs</b> with long-term memory (LTM) presents an opportunity to improve engagement and self-disclosure, but we lack an understanding of how LTM impacts people&rsquo;s interaction with <b>LLM-driven</b> <b>chatbots</b> in public health interventions. We examine the case of CareCall &ndash; an <b>LLM-driven</b> voice <b>chatbot</b> with LTM &ndash; through the analysis of 1,252 call logs and interviews with nine users. We found that LTM enhanced health disclosure and fostered positive perceptions of the <b>chatbot</b> by offering familiarity. However, we also observed challenges in promoting self-disclosure through LTM, particularly around addressing chronic health conditions and privacy concerns. We discuss considerations for LTM integration in <b>LLM-driven</b> <b>chatbots</b> for public health monitoring, including carefully deciding what topics need to be remembered in light of public health goals.</p></p class="citation"></blockquote><h3 id=24--90113-from-text-to-map-a-system-dynamics-bot-for-constructing-causal-loop-diagrams-niyousha-hosseinichimeh-et-al-2024>(2/4 | 90/113) From Text to Map: A System Dynamics Bot for Constructing Causal Loop Diagrams (Niyousha Hosseinichimeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niyousha Hosseinichimeh, Aritra Majumdar, Ross Williams, Navid Ghaffarzadegan. (2024)<br><strong>From Text to Map: A System Dynamics Bot for Constructing Causal Loop Diagrams</strong><br><button class=copy-to-clipboard title="From Text to Map: A System Dynamics Bot for Constructing Causal Loop Diagrams" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11400v1.pdf filename=2402.11400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce and test the System Dynamics Bot, a computer program leveraging a <b>large</b> <b>language</b> <b>model</b> to automate the creation of causal loop diagrams from textual data. To evaluate its performance, we ensembled two distinct databases. The first dataset includes 20 causal loop diagrams and associated texts sourced from the system dynamics literature. The second dataset comprises responses from 30 participants to a vignette, along with causal loop diagrams coded by three system dynamics modelers. The bot uses textual data and successfully identifies approximately sixty percent of the links between variables and feedback loops in both datasets. This paper outlines our approach, provides examples, and presents evaluation results. We discuss encountered challenges and implemented solutions in developing the System Dynamics Bot. The bot can facilitate extracting mental models from textual data and improve model building processes. Moreover, the two datasets can serve as a testbed for similar programs.</p></p class="citation"></blockquote><h3 id=34--91113-ironies-of-generative-ai-understanding-and-mitigating-productivity-loss-in-human-ai-interactions-auste-simkute-et-al-2024>(3/4 | 91/113) Ironies of Generative AI: Understanding and mitigating productivity loss in human-AI interactions (Auste Simkute et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Auste Simkute, Lev Tankelevitch, Viktor Kewenig, Ava Elizabeth Scott, Abigail Sellen, Sean Rintel. (2024)<br><strong>Ironies of Generative AI: Understanding and mitigating productivity loss in human-AI interactions</strong><br><button class=copy-to-clipboard title="Ironies of Generative AI: Understanding and mitigating productivity loss in human-AI interactions" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11364v1.pdf filename=2402.11364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> (GenAI) systems offer opportunities to increase user productivity in many tasks, such as programming and writing. However, while they boost productivity in some studies, many others show that users are working ineffectively with GenAI systems and losing productivity. Despite the apparent novelty of these usability challenges, these &lsquo;ironies of automation&rsquo; have been observed for over three decades in Human Factors research on the introduction of automation in domains such as aviation, automated driving, and intelligence. We draw on this extensive research alongside recent GenAI user studies to outline four key reasons for productivity loss with GenAI systems: a shift in users&rsquo; roles from production to evaluation, unhelpful restructuring of workflows, interruptions, and a tendency for automation to make easy tasks easier and hard tasks harder. We then suggest how Human Factors research can also inform GenAI system design to mitigate productivity loss by using approaches such as continuous feedback, system personalization, ecological interface design, task stabilization, and clear task allocation. Thus, we ground developments in GenAI system usability in decades of Human Factors research, ensuring that the design of human-AI interactions in this rapidly moving field learns from history instead of repeating it.</p></p class="citation"></blockquote><h3 id=44--92113-supporting-experts-with-a-multimodal-machine-learning-based-tool-for-human-behavior-analysis-of-conversational-videos-riku-arakawa-et-al-2024>(4/4 | 92/113) Supporting Experts with a Multimodal Machine-Learning-Based Tool for Human Behavior Analysis of Conversational Videos (Riku Arakawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riku Arakawa, Kiyosu Maeda, Hiromu Yakura. (2024)<br><strong>Supporting Experts with a Multimodal Machine-Learning-Based Tool for Human Behavior Analysis of Conversational Videos</strong><br><button class=copy-to-clipboard title="Supporting Experts with a Multimodal Machine-Learning-Based Tool for Human Behavior Analysis of Conversational Videos" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CV, cs-HC, cs-LG, cs.HC<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11145v1.pdf filename=2402.11145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> scene search of conversations is essential for unlocking valuable insights into social dynamics and enhancing our communication. While experts in conversational analysis have their own knowledge and skills to find key scenes, a lack of comprehensive, user-friendly tools that streamline the processing of diverse <b>multimodal</b> queries impedes efficiency and objectivity. To solve it, we developed Providence, a visual-programming-based tool based on design considerations derived from a formative study with experts. It enables experts to combine various machine learning algorithms to capture human behavioral cues without writing code. Our study showed its preferable usability and satisfactory output with less cognitive load imposed in accomplishing scene search tasks of conversations, verifying the importance of its customizability and transparency. Furthermore, through the in-the-wild trial, we confirmed the objectivity and reusability of the tool transform experts&rsquo; workflow, suggesting the advantage of expert-AI teaming in a highly human-contextual domain.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--93113-fair-resource-allocation-in-virtualized-o-ran-platforms-fatih-aslan-et-al-2024>(1/2 | 93/113) Fair Resource Allocation in Virtualized O-RAN Platforms (Fatih Aslan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatih Aslan, George Iosifidis, Jose A. Ayala-Romero, Andres Garcia-Saavedra, Xavier Costa-Perez. (2024)<br><strong>Fair Resource Allocation in Virtualized O-RAN Platforms</strong><br><button class=copy-to-clipboard title="Fair Resource Allocation in Virtualized O-RAN Platforms" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-LG, cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Fairness, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11285v1.pdf filename=2402.11285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>O-RAN systems and their deployment in virtualized general-purpose computing platforms (O-Cloud) constitute a paradigm shift expected to bring unprecedented performance gains. However, these architectures raise new implementation challenges and threaten to worsen the already-high energy consumption of mobile networks. This paper presents first a series of experiments which assess the O-Cloud&rsquo;s energy costs and their dependency on the servers&rsquo; hardware, capacity and data traffic properties which, typically, change over time. Next, it proposes a compute policy for assigning the base station data loads to O-Cloud servers in an energy-efficient fashion; and a radio policy that determines at near-real-time the minimum transmission block size for each user so as to avoid unnecessary energy costs. The policies balance energy savings with performance, and ensure that both of them are dispersed fairly across the servers and users, respectively. To cater for the unknown and time-varying parameters affecting the policies, we develop a novel online learning framework with <b>fairness</b> guarantees that apply to the entire operation horizon of the system (long-term <b>fairness).</b> The policies are evaluated using trace-driven <b>simulations</b> and are fully implemented in an O-RAN compatible system where we measure the energy costs and throughput in realistic scenarios.</p></p class="citation"></blockquote><h3 id=22--94113-automated-optimization-of-parameterized-data-plane-programs-with-parasol-mary-hogan-et-al-2024>(2/2 | 94/113) Automated Optimization of Parameterized Data-Plane Programs with Parasol (Mary Hogan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mary Hogan, Devon Loehr, John Sonchack, Shir Landau Feibish, Jennifer Rexford, David Walker. (2024)<br><strong>Automated Optimization of Parameterized Data-Plane Programs with Parasol</strong><br><button class=copy-to-clipboard title="Automated Optimization of Parameterized Data-Plane Programs with Parasol" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11155v1.pdf filename=2402.11155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Programmable data planes allow for sophisticated applications that give operators the power to customize the functionality of their networks. Deploying these applications, however, often requires tedious and burdensome optimization of their layout and design, in which programmers must manually write, compile, and test an implementation, adjust the design, and repeat. In this paper we present Parasol, a framework that allows programmers to define general, parameterized network algorithms and automatically optimize their various parameters. The parameters of a Parasol program can represent a wide variety of implementation decisions, and may be optimized for arbitrary, high-level objectives defined by the programmer. Furthermore, optimization may be tailored to particular environments by providing a representative sample of traffic. We show how we implement the Parasol framework, which consists of a sketching language for writing parameterized programs, and a <b>simulation-based</b> optimizer for testing different parameter settings. We evaluate Parasol by implementing a suite of ten data-plane applications, and find that Parasol produces a solution with comparable performance to hand-optimized P4 code within a two-hour time budget.</p></p class="citation"></blockquote><h2 id=cscr-3>cs.CR (3)</h2><h3 id=13--95113-watch-out-for-your-agents-investigating-backdoor-threats-to-llm-based-agents-wenkai-yang-et-al-2024>(1/3 | 95/113) Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents (Wenkai Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, Xu Sun. (2024)<br><strong>Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents</strong><br><button class=copy-to-clipboard title="Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11208v1.pdf filename=2402.11208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leveraging the rapid development of <b>Large</b> <b>Language</b> <b>Models</b> <b>LLMs,</b> <b>LLM-based</b> agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of <b>LLM-based</b> agents during applications. However, the safety issues of <b>LLM-based</b> agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to <b>LLM-based</b> agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate <b>reasoning</b> process, while keeping the final output correct. Furthermore, the former category can be divided into two subcategories based on trigger locations: the backdoor trigger can be hidden either in the user query or in an intermediate observation returned by the external environment. We propose the corresponding data poisoning mechanisms to implement the above variations of agent backdoor attacks on two typical agent tasks, web shopping and tool utilization. Extensive experiments show that <b>LLM-based</b> agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on <b>LLM-based</b> agents. Warning: This paper may contain biased content.</p></p class="citation"></blockquote><h3 id=23--96113-secure-robust-and-energy-efficient-authenticated-data-sharing-in-uav-assisted-6g-networks-atefeh-mohseni-ejiyeh-2024>(2/3 | 96/113) Secure, Robust, and Energy-Efficient Authenticated Data Sharing in UAV-Assisted 6G Networks (Atefeh Mohseni Ejiyeh, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atefeh Mohseni Ejiyeh. (2024)<br><strong>Secure, Robust, and Energy-Efficient Authenticated Data Sharing in UAV-Assisted 6G Networks</strong><br><button class=copy-to-clipboard title="Secure, Robust, and Energy-Efficient Authenticated Data Sharing in UAV-Assisted 6G Networks" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11382v1.pdf filename=2402.11382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper confronts the pressing challenges of sixth-generation (6G) wireless communication networks by harnessing the unique capabilities of Unmanned Aerial Vehicles (UAVs). With the ambitious promises of 6G, including ultra-reliable 1 Tbps data delivery and ultra-low latency, the demand for innovative solutions becomes imperative. Traditional terrestrial base stations, though effective, exhibit limitations in scenarios requiring ubiquitous connectivity, <b>prompting</b> the integration of UAVs. In response to these challenges, we introduce a comprehensive solution. This involves UAVs collaboratively downloading desired content from service providers, and subsequently establishing secure connections with users for efficient content exchange. Accordingly, we introduce two new protocols: a collaborative group data downloading scheme among UAVs called SeGDS, and SeDDS for secure direct data sharing through out-of-band autonomous Device-to-Device (D2D) communication. Leveraging certificateless signcryption and certificateless multi-receiver encryption, these protocols offer lightweight, certificate-free solutions with features such as user revocation, non-repudiation, and mutual authentication. Prioritizing high availability, the proposed protocols effectively detect Denial of Service (DoS) and free riding attacks. A thorough evaluation underscores the superiority of the proposed protocols in both security and efficiency over existing models; SeDDS reduces overall computation by 3x, imposing a lighter communication load on UAVs, while SeGDS meets swarm UAV security requirements, reducing communication costs by 4x with low computation cost.</p></p class="citation"></blockquote><h3 id=33--97113-on-the-role-of-similarity-in-detecting-masquerading-files-jonathan-oliver-et-al-2024>(3/3 | 97/113) On the Role of Similarity in Detecting Masquerading Files (Jonathan Oliver et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Oliver, Jue Mo, Susmit Yenkar, Raghav Batta, Sekhar Josyoula. (2024)<br><strong>On the Role of Similarity in Detecting Masquerading Files</strong><br><button class=copy-to-clipboard title="On the Role of Similarity in Detecting Masquerading Files" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11227v1.pdf filename=2402.11227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Similarity has been applied to a wide range of security applications, typically used in machine learning models. We examine the problem posed by masquerading samples; that is samples crafted by bad actors to be similar or near identical to legitimate samples. We find that these samples potentially create significant problems for machine learning solutions. The primary problem being that bad actors can circumvent machine learning solutions by using masquerading samples. We then examine the interplay between digital signatures and machine learning solutions. In particular, we focus on executable files and code signing. We offer a taxonomy for masquerading files. We use a combination of similarity and <b>clustering</b> to find masquerading files. We use the insights gathered in this process to offer improvements to similarity based and machine learning security solutions.</p></p class="citation"></blockquote><h2 id=csai-2>cs.AI (2)</h2><h3 id=12--98113-an-empirical-evaluation-of-neural-and-neuro-symbolic-approaches-to-real-time-multimodal-complex-event-detection-liying-han-et-al-2024>(1/2 | 98/113) An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection (Liying Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liying Han, Mani B. Srivastava. (2024)<br><strong>An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection</strong><br><button class=copy-to-clipboard title="An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Event Detection, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11403v1.pdf filename=2402.11403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robots and autonomous systems require an understanding of complex <b>events</b> <b>(CEs)</b> from sensor data to interact with their environments and humans effectively. Traditional end-to-end neural architectures, despite processing sensor data efficiently, struggle with long-duration <b>events</b> <b>due</b> to limited context sizes and <b>reasoning</b> capabilities. Recent advances in neuro-symbolic methods, which integrate neural and symbolic models leveraging human knowledge, promise improved performance with less data. This study addresses the gap in understanding these approaches&rsquo; effectiveness in complex <b>event</b> <b>detection</b> (CED), especially in temporal <b>reasoning.</b> We investigate neural and neuro-symbolic architectures&rsquo; performance in a <b>multimodal</b> CED task, analyzing IMU and acoustic data streams to recognize CE patterns. Our methodology includes (i) end-to-end neural architectures for direct CE detection from sensor embeddings, (ii) two-stage concept-based neural models mapping sensor embeddings to atomic <b>events</b> <b>(AEs)</b> before CE detection, and (iii) a neuro-symbolic approach using a symbolic finite-state machine for CE detection from AEs. Empirically, the neuro-symbolic architecture significantly surpasses purely neural models, demonstrating superior performance in CE recognition, even with extensive training data and ample temporal context for neural approaches.</p></p class="citation"></blockquote><h3 id=22--99113-training-language-model-agents-without-modifying-language-models-shaokun-zhang-et-al-2024>(2/2 | 99/113) Training Language Model Agents without Modifying Language Models (Shaokun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, Qingyun Wu. (2024)<br><strong>Training Language Model Agents without Modifying Language Models</strong><br><button class=copy-to-clipboard title="Training Language Model Agents without Modifying Language Models" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11359v1.pdf filename=2402.11359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Researchers and practitioners have recently reframed powerful <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of <b>LLM</b> agents, we present a novel paradigm of training <b>LLM</b> agents without modifying the <b>LLM</b> weights, which is particularly useful when the <b>LLMs</b> are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent&rsquo;s functions to better solve the downstream tasks instead of modifying the <b>LLM</b> weights. By treating the functions as learnable `agent parameters&rsquo; and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the <b>LLM</b> to update agents&rsquo; functions and devise an agent training algorithm with two strategies, roll-back, and early-stop, to streamline the training process. With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative <b>LLM</b> agents in various downstream tasks. We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--100113-wireless-distributed-matrix-vector-multiplication-using-over-the-air-computation-and-analog-coding-jinho-choi-2024>(1/2 | 100/113) Wireless Distributed Matrix-Vector Multiplication using Over-the-Air Computation and Analog Coding (Jinho Choi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinho Choi. (2024)<br><strong>Wireless Distributed Matrix-Vector Multiplication using Over-the-Air Computation and Analog Coding</strong><br><button class=copy-to-clipboard title="Wireless Distributed Matrix-Vector Multiplication using Over-the-Air Computation and Analog Coding" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-DC, cs-IT, cs-SY, cs.IT, eess-SY, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11390v1.pdf filename=2402.11390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose an over-the-air (OTA)-based approach for distributed matrix-vector multiplications in the context of distributed machine learning (DML). Thanks to OTA computation, the column-wise partitioning of a large matrix enables efficient workload distribution among workers (i.e., local computing nodes) based on their computing capabilities. In addition, without requiring additional bandwidth, it allows the system to remain scalable even as the number of workers increases to mitigate the impact of slow workers, known as stragglers. However, despite the improvements, there are still instances where some workers experience deep fading and become stragglers, preventing them from transmitting their results. By analyzing the mean squared error (MSE), we demonstrate that incorporating more workers in the OTA-based approach leads to MSE reduction without the need for additional radio resources. Furthermore, we introduce an analog coding scheme to further enhance the performance and compare it with conventional coded multiplication (CM) schemes. Through <b>simulations,</b> it is shown that the OTA-based approach achieves comparable performance to CM schemes while potentially requiring fewer radio resources.</p></p class="citation"></blockquote><h3 id=22--101113-power-optimization-for-integrated-active-and-passive-sensing-in-dfrc-systems-xingliang-lou-et-al-2024>(2/2 | 101/113) Power Optimization for Integrated Active and Passive Sensing in DFRC Systems (Xingliang Lou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingliang Lou, Wenchao Xia, Kai-Kit Wong, Haitao Zhao, Tony Q. S. Quek, Hongbo Zhu. (2024)<br><strong>Power Optimization for Integrated Active and Passive Sensing in DFRC Systems</strong><br><button class=copy-to-clipboard title="Power Optimization for Integrated Active and Passive Sensing in DFRC Systems" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11294v1.pdf filename=2402.11294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most existing works on dual-function radar-communication (DFRC) systems mainly focus on active sensing, but ignore passive sensing. To leverage multi-static sensing capability, we explore integrated active and passive sensing (IAPS) in DFRC systems to remedy sensing performance. The multi-antenna base station (BS) is responsible for communication and active sensing by transmitting signals to user equipments while detecting a target according to echo signals. In contrast, passive sensing is performed at the receive access points (RAPs). We consider both the cases where the capacity of the backhaul links between the RAPs and BS is unlimited or limited and adopt different fusion strategies. Specifically, when the backhaul capacity is unlimited, the BS and RAPs transfer sensing signals they have received to the central controller (CC) for signal fusion. The CC processes the signals and leverages the generalized likelihood ratio test detector to determine the present of a target. However, when the backhaul capacity is limited, each RAP, as well as the BS, makes decisions independently and sends its binary inference results to the CC for result fusion via voting aggregation. Then, aiming at maximize the target detection probability under communication quality of service constraints, two power optimization algorithms are proposed. Finally, numerical <b>simulations</b> demonstrate that the sensing performance in case of unlimited backhaul capacity is much better than that in case of limited backhaul capacity. Moreover, it implied that the proposed IAPS scheme outperforms only-passive and only-active sensing schemes, especially in unlimited capacity case.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--102113-adaptive-split-balancing-for-optimal-random-forest-yuqian-zhang-et-al-2024>(1/3 | 102/113) Adaptive Split Balancing for Optimal Random Forest (Yuqian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqian Zhang, Weijie Ji, Jelena Bradic. (2024)<br><strong>Adaptive Split Balancing for Optimal Random Forest</strong><br><button class=copy-to-clipboard title="Adaptive Split Balancing for Optimal Random Forest" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-ST, stat-ME, stat-ML, stat-TH, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11228v1.pdf filename=2402.11228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While random forests are commonly used for regression problems, existing methods often lack adaptability in complex situations or lose optimality under simple, smooth scenarios. In this study, we introduce the adaptive split balancing forest (ASBF), capable of learning tree representations from data while simultaneously achieving minimax optimality under the Lipschitz class. To exploit higher-order smoothness levels, we further propose a localized version that attains the minimax rate under the H"older class $\mathcal{H}^{q,\beta}$ for any $q\in\mathbb{N}$ and $\beta\in(0,1]$. Rather than relying on the widely-used random feature selection, we consider a balanced modification to existing approaches. Our results indicate that an over-reliance on auxiliary randomness may compromise the approximation power of tree models, leading to suboptimal results. Conversely, a less random, more balanced approach demonstrates optimality. Additionally, we establish uniform upper bounds and explore the application of random forests in average treatment effect estimation problems. Through <b>simulation</b> studies and real-data applications, we demonstrate the superior empirical performance of the proposed methods over existing random forests.</p></p class="citation"></blockquote><h3 id=23--103113-efficient-low-rank-matrix-estimation-experimental-design-and-arm-set-dependent-low-rank-bandits-kyoungseok-jang-et-al-2024>(2/3 | 103/113) Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits (Kyoungseok Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyoungseok Jang, Chicheng Zhang, Kwang-Sung Jun. (2024)<br><strong>Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits</strong><br><button class=copy-to-clipboard title="Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11156v1.pdf filename=2402.11156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study low-rank matrix trace regression and the related problem of low-rank matrix <b>bandits.</b> <b>Assuming</b> access to the distribution of the covariates, we propose a novel low-rank matrix estimation method called LowPopArt and provide its recovery guarantee that depends on a novel quantity denoted by B(Q) that characterizes the hardness of the problem, where Q is the covariance matrix of the measurement distribution. We show that our method can provide tighter recovery guarantees than classical nuclear norm penalized least squares (Koltchinskii et al., 2011) in several problems. To perform efficient estimation with a limited number of measurements from an arbitrarily given measurement set A, we also propose a novel experimental design criterion that minimizes B(Q) with computational efficiency. We leverage our novel estimator and design of experiments to derive two low-rank linear <b>bandit</b> <b>algorithms</b> for general arm sets that enjoy improved regret upper bounds. This improves over previous works on low-rank <b>bandits,</b> <b>which</b> make somewhat restrictive assumptions that the arm set is the unit ball or that an efficient exploration distribution is given. To our knowledge, our experimental design criterion is the first one tailored to low-rank matrix estimation beyond the naive reduction to linear regression, which can be of independent interest.</p></p class="citation"></blockquote><h3 id=33--104113-variational-entropy-search-for-adjusting-expected-improvement-nuojin-cheng-et-al-2024>(3/3 | 104/113) Variational Entropy Search for Adjusting Expected Improvement (Nuojin Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nuojin Cheng, Stephen Becker. (2024)<br><strong>Variational Entropy Search for Adjusting Expected Improvement</strong><br><button class=copy-to-clipboard title="Variational Entropy Search for Adjusting Expected Improvement" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-OC, stat-ML, stat.ML<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11345v1.pdf filename=2402.11345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian optimization is a widely used technique for optimizing <b>black-box</b> <b>functions,</b> with Expected Improvement (EI) being the most commonly utilized acquisition function in this domain. While EI is often viewed as distinct from other information-theoretic acquisition functions, such as entropy search (ES) and max-value entropy search (MES), our work reveals that EI can be considered a special case of MES when approached through variational inference (VI). In this context, we have developed the Variational Entropy Search (VES) methodology and the VES-Gamma algorithm, which adapts EI by incorporating principles from information-theoretic concepts. The efficacy of VES-Gamma is demonstrated across a variety of test functions and read datasets, highlighting its theoretical and practical utilities in Bayesian optimization scenarios.</p></p class="citation"></blockquote><h2 id=eessiv-3>eess.IV (3)</h2><h3 id=13--105113-hierarchical-prior-based-super-resolution-for-point-cloud-geometry-compression-dingquan-li-et-al-2024>(1/3 | 105/113) Hierarchical Prior-based Super Resolution for Point Cloud Geometry Compression (Dingquan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingquan Li, Kede Ma, Jing Wang, Ge Li. (2024)<br><strong>Hierarchical Prior-based Super Resolution for Point Cloud Geometry Compression</strong><br><button class=copy-to-clipboard title="Hierarchical Prior-based Super Resolution for Point Cloud Geometry Compression" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-MM, eess-IV, eess.IV<br>Keyword Score: 15<br>Keywords: Geometry, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11250v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11250v1.pdf filename=2402.11250v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>Geometry-based</b> Point Cloud Compression (G-PCC) has been developed by the Moving Picture Experts Group to compress point clouds. In its lossy mode, the reconstructed point cloud by G-PCC often suffers from noticeable distortions due to the na"{i}ve <b>geometry</b> <b>quantization</b> (i.e., grid downsampling). This paper proposes a hierarchical prior-based super resolution method for point cloud <b>geometry</b> compression. The content-dependent hierarchical prior is constructed at the encoder side, which enables coarse-to-fine super resolution of the point cloud <b>geometry</b> at the decoder side. A more accurate prior generally yields improved reconstruction performance, at the cost of increased bits required to encode this side information. With a proper balance between prior accuracy and bit consumption, the proposed method demonstrates substantial Bjontegaard-delta bitrate savings on the MPEG Cat1A dataset, surpassing the octree-based and trisoup-based G-PCC v14. We provide our implementations for reproducible research at <a href=https://github.com/lidq92/mpeg-pcc-tmc13>https://github.com/lidq92/mpeg-pcc-tmc13</a>.</p></p class="citation"></blockquote><h3 id=23--106113-tc-diffrecon-texture-coordination-mri-reconstruction-method-based-on-diffusion-model-and-modified-mf-unet-method-chenyan-zhang-et-al-2024>(2/3 | 106/113) TC-DiffRecon: Texture coordination MRI reconstruction method based on diffusion model and modified MF-UNet method (Chenyan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyan Zhang, Yifei Chen, Zhenxiong Fan, Yiyu Huang, Wenchao Weng, Ruiquan Ge, Dong Zeng, Changmiao Wang. (2024)<br><strong>TC-DiffRecon: Texture coordination MRI reconstruction method based on diffusion model and modified MF-UNet method</strong><br><button class=copy-to-clipboard title="TC-DiffRecon: Texture coordination MRI reconstruction method based on diffusion model and modified MF-UNet method" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11274v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11274v1.pdf filename=2402.11274v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>diffusion</b> <b>models</b> have gained significant attention as a novel set of deep learning-based generative methods. These models attempt to sample data from a Gaussian distribution that adheres to a target distribution, and have been successfully adapted to the reconstruction of MRI data. However, as an unconditional generative model, the <b>diffusion</b> <b>model</b> typically disrupts image coordination because of the consistent projection of data introduced by conditional bootstrap. This often results in image fragmentation and incoherence. Furthermore, the inherent limitations of the <b>diffusion</b> <b>model</b> often lead to excessive smoothing of the generated images. In the same vein, some deep learning-based models often suffer from poor generalization performance, meaning their effectiveness is greatly affected by different acceleration factors. To address these challenges, we propose a novel <b>diffusion</b> <b>model-based</b> MRI reconstruction method, named TC-DiffRecon, which does not rely on a specific acceleration factor for training. We also suggest the incorporation of the MF-UNet module, designed to enhance the quality of MRI images generated by the model while mitigating the over-smoothing issue to a certain extent. During the image generation sampling process, we employ a novel TCKG module and a Coarse-to-Fine sampling scheme. These additions aim to harmonize image texture, expedite the sampling process, while achieving data consistency. Our source code is available at <a href=https://github.com/JustlfC03/TC-DiffRecon>https://github.com/JustlfC03/TC-DiffRecon</a>.</p></p class="citation"></blockquote><h3 id=33--107113-training-free-image-style-alignment-for-self-adapting-domain-shift-on-handheld-ultrasound-devices-hongye-zeng-et-al-2024>(3/3 | 107/113) Training-free image style alignment for self-adapting domain shift on handheld ultrasound devices (Hongye Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongye Zeng, Ke Zou, Zhihao Chen, Yuchong Gao, Hongbo Chen, Haibin Zhang, Kang Zhou, Meng Wang, Rick Siow Mong Goh, Yong Liu, Chang Jiang, Rui Zheng, Huazhu Fu. (2024)<br><strong>Training-free image style alignment for self-adapting domain shift on handheld ultrasound devices</strong><br><button class=copy-to-clipboard title="Training-free image style alignment for self-adapting domain shift on handheld ultrasound devices" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11211v1.pdf filename=2402.11211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Handheld ultrasound devices face usage limitations due to user inexperience and cannot benefit from <b>supervised</b> deep learning without extensive expert annotations. Moreover, the models trained on standard ultrasound device data are constrained by training data distribution and perform poorly when directly applied to handheld device data. In this study, we propose the Training-free Image Style Alignment (TISA) framework to align the style of handheld device data to those of standard devices. The proposed TISA can directly infer handheld device images without extra training and is suited for clinical applications. We show that TISA performs better and more stably in medical detection and segmentation tasks for handheld device data. We further validate TISA as the clinical model for automatic measurements of spinal curvature and carotid intima-media thickness. The automatic measurements agree well with manual measurements made by human experts and the measurement errors remain within clinically acceptable ranges. We demonstrate the potential for TISA to facilitate automatic diagnosis on handheld ultrasound devices and expedite their eventual widespread use.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--108113-transformer-based-de-novo-peptide-sequencing-for-data-independent-acquisition-mass-spectrometry-shiva-ebrahimi-et-al-2024>(1/1 | 108/113) Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry (Shiva Ebrahimi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiva Ebrahimi, Xuan Guo. (2024)<br><strong>Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry</strong><br><button class=copy-to-clipboard title="Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, q-bio-QM, q-bio.QM<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11363v1.pdf filename=2402.11363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput technique for comprehensively analyzing protein content within biological samples. This methodology is a cornerstone driving the advancement of proteomics. In recent years, substantial strides have been made in Data-Independent Acquisition (DIA) strategies, facilitating impartial and non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra present a formidable obstacle due to their inherent high multiplexing nature. Each spectrum encapsulates fragmented product ions originating from multiple precursor peptides. This intricacy poses a particularly acute challenge in de novo peptide/protein sequencing, where current methods are ill-equipped to address the multiplexing conundrum. In this paper, we introduce Casanovo-DIA, a deep-learning model based on <b>transformer</b> architecture. It deciphers peptide sequences from DIA mass spectrometry data. Our results show significant improvements over existing STOA methods, including DeepNovo-DIA and PepNet. Casanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94% at the amino acid level, and boosts precision by 59% to 81.36% at the peptide level. Integrating DIA data and our Casanovo-DIA model holds considerable promise to uncover novel peptides and more comprehensive profiling of biological samples. Casanovo-DIA is freely available under the GNU GPL license at <a href=https://github.com/Biocomputing-Research-Group/Casanovo-DIA>https://github.com/Biocomputing-Research-Group/Casanovo-DIA</a>.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--109113-implementation-of-a-model-of-the-cortex-basal-ganglia-loop-naoya-arakawa-2024>(1/1 | 109/113) Implementation of a Model of the Cortex Basal Ganglia Loop (Naoya Arakawa, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naoya Arakawa. (2024)<br><strong>Implementation of a Model of the Cortex Basal Ganglia Loop</strong><br><button class=copy-to-clipboard title="Implementation of a Model of the Cortex Basal Ganglia Loop" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-AI, q-bio-NC, q-bio.NC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13275v1.pdf filename=2402.13275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article presents a simple model of the cortex-basal ganglia-thalamus loop, which is thought to serve for action selection and executions, and reports the results of its implementation. The model is based on the hypothesis that the cerebral cortex predicts actions, while the basal ganglia use <b>reinforcement</b> <b>learning</b> to decide whether to perform the actions predicted by the cortex. The implementation is intended to be used as a component of models of the brain consisting of cortical regions or brain-inspired cognitive architectures.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--110113-materiality-and-risk-in-the-age-of-pervasive-ai-sensors-matthew-stewart-et-al-2024>(1/1 | 110/113) Materiality and Risk in the Age of Pervasive AI Sensors (Matthew Stewart et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Stewart, Emanuel Moss, Pete Warden, Brian Plancher, Susan Kennedy, Mona Sloane, Vijay Janapa Reddi. (2024)<br><strong>Materiality and Risk in the Age of Pervasive AI Sensors</strong><br><button class=copy-to-clipboard title="Materiality and Risk in the Age of Pervasive AI Sensors" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-HC, cs.CY<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11183v1.pdf filename=2402.11183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial intelligence systems connected to sensor-laden devices are becoming pervasive, which has significant implications for a range of AI risks, including to privacy, the environment, autonomy, and more. There is therefore a growing need for increased accountability around the responsible development and deployment of these technologies. In this paper, we provide a comprehensive analysis of the evolution of sensors, the risks they pose by virtue of their material existence in the world, and the impacts of ubiquitous sensing and on-device AI. We propose incorporating sensors into risk management frameworks and call for more responsible sensor and system design paradigms that address risks of such systems. To do so, we trace the evolution of sensors from analog devices to intelligent, networked systems capable of real-time data analysis and decision-making at the extreme edge of the network. We show that the proliferation of sensors is driven by calculative models that prioritize data collection and cost reduction and produce risks that emerge around privacy, surveillance, waste, and power dynamics. We then analyze these risks, highlighting issues of validity, safety, security, accountability, interpretability, and bias. We surface sensor-related risks not commonly captured in existing approaches to AI risk management, using a materiality lens that reveals how physical sensor properties shape data and algorithmic models. We conclude by advocating for increased attention to the materiality of algorithmic systems, and of on-device AI sensors in particular, and highlight the need for development of a responsible sensor design paradigm that empowers users and communities and leads to a future of increased <b>fairness,</b> accountability and transparency.</p></p class="citation"></blockquote><h2 id=econth-1>econ.TH (1)</h2><h3 id=11--111113-the-value-of-context-human-versus-black-box-evaluators-andrei-iakovlev-et-al-2024>(1/1 | 111/113) The Value of Context: Human versus Black Box Evaluators (Andrei Iakovlev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrei Iakovlev, Annie Liang. (2024)<br><strong>The Value of Context: Human versus Black Box Evaluators</strong><br><button class=copy-to-clipboard title="The Value of Context: Human versus Black Box Evaluators" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.TH<br>Categories: cs-GT, econ-TH, econ.TH<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11157v1.pdf filename=2402.11157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluations once solely within the domain of human experts (e.g., medical diagnosis by doctors) can now also be carried out by machine learning algorithms. This raises a new conceptual question: what is the difference between being evaluated by humans and algorithms, and when should an individual prefer one form of evaluation over the other? We propose a theoretical framework that formalizes one key distinction between the two forms of evaluation: Machine learning algorithms are standardized, fixing a common set of covariates by which to assess all individuals, while human evaluators customize which covariates are acquired to each individual. Our framework defines and analyzes the advantage of this customization &ndash; the value of context &ndash; in environments with very high-dimensional data. We show that unless the agent has precise knowledge about the joint distribution of covariates, the value of more covariates exceeds the value of context.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--112113-the-matrix-free-macro-element-hybridized-discontinuous-galerkin-method-for-steady-and-unsteady-compressible-flows-vahid-badrkhani-et-al-2024>(1/1 | 112/113) The matrix-free macro-element hybridized Discontinuous Galerkin method for steady and unsteady compressible flows (Vahid Badrkhani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vahid Badrkhani, Marco F. P. ten Eikelder, Rene R. Hiemstra, Dominik Schillinger. (2024)<br><strong>The matrix-free macro-element hybridized Discontinuous Galerkin method for steady and unsteady compressible flows</strong><br><button class=copy-to-clipboard title="The matrix-free macro-element hybridized Discontinuous Galerkin method for steady and unsteady compressible flows" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11361v1.pdf filename=2402.11361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The macro-element variant of the hybridized discontinuous Galerkin (HDG) method combines advantages of continuous and discontinuous finite element discretization. In this paper, we investigate the performance of the macro-element HDG method for the analysis of compressible flow problems at moderate Reynolds numbers. To efficiently handle the corresponding large systems of equations, we explore several strategies at the solver level. On the one hand, we devise a second-layer static condensation approach that reduces the size of the local system matrix in each macro-element and hence the factorization time of the local solver. On the other hand, we employ a multi-level preconditioner based on the FGMRES solver for the global system that integrates well within a matrix-free implementation. In addition, we integrate a standard diagonally implicit Runge-Kutta scheme for time integration. We test the matrix-free macro-element HDG method for compressible flow <b>benchmarks,</b> including Couette flow, flow past a sphere, and the Taylor-Green vortex. Our results show that unlike standard HDG, the macro-element HDG method can operate efficiently for moderate polynomial degrees, as the local computational load can be flexibly increased via mesh refinement within a macro-element. Our results also show that due to the balance of local and global operations, the reduction in degrees of freedom, and the reduction of the global problem size and the number of iterations for its solution, the macro-element HDG method can be a competitive option for the analysis of compressible flow problems.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--113113-treewidth-versus-clique-number-iv-tree-independence-number-of-graphs-excluding-an-induced-star-clément-dallard-et-al-2024>(1/1 | 113/113) Treewidth versus clique number. IV. Tree-independence number of graphs excluding an induced star (Clément Dallard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Clément Dallard, Matjaž Krnc, O-joung Kwon, Martin Milanič, Andrea Munaro, Kenny Štorgel, Sebastian Wiederrecht. (2024)<br><strong>Treewidth versus clique number. IV. Tree-independence number of graphs excluding an induced star</strong><br><button class=copy-to-clipboard title="Treewidth versus clique number. IV. Tree-independence number of graphs excluding an induced star" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C75 (Primary), 05C69, 05C76, 05C85 (Secondary), cs-DM, cs-DS, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11222v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11222v2.pdf filename=2402.11222v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many recent works address the question of characterizing induced obstructions to bounded treewidth. In 2022, Lozin and Razgon completely answered this question for <b>graph</b> classes defined by finitely many forbidden induced subgraphs. Their result also implies a characterization of <b>graph</b> classes defined by finitely many forbidden induced subgraphs that are $(tw,\omega)$-bounded, that is, treewidth can only be large due to the presence of a large clique. This condition is known to be satisfied for any <b>graph</b> class with bounded tree-independence number, a <b>graph</b> parameter introduced independently by Yolov in 2018 and by Dallard, Milani\v{c}, and \v{S}torgel in 2024. Dallard et al. conjectured that $(tw,\omega)$-boundedness is actually equivalent to bounded tree-independence number. We address this conjecture in the context of <b>graph</b> classes defined by finitely many forbidden induced subgraphs and prove it for the case of <b>graph</b> classes excluding an induced star. We also prove it for subclasses of the class of line <b>graphs,</b> determine the exact values of the tree-independence numbers of line <b>graphs</b> of complete <b>graphs</b> and line <b>graphs</b> of complete bipartite <b>graphs,</b> and characterize the tree-independence number of $P_4$-free <b>graphs,</b> which implies a linear-time algorithm for its computation. Applying the algorithmic framework provided in a previous paper of the series leads to polynomial-time algorithms for the Maximum Weight Independent Set problem in an infinite family of <b>graph</b> classes.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.18</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.20</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-37>cs.CL (37)</a><ul><li><a href=#137--1113-c-icl-contrastive-in-context-learning-for-information-extraction-ying-mo-et-al-2024>(1/37 | 1/113) C-ICL: Contrastive In-context Learning for Information Extraction (Ying Mo et al., 2024)</a></li><li><a href=#237--2113-gendec-a-robust-generative-question-decomposition-method-for-multi-hop-reasoning-jian-wu-et-al-2024>(2/37 | 2/113) GenDec: A robust generative Question-decomposition method for Multi-hop reasoning (Jian Wu et al., 2024)</a></li><li><a href=#337--3113-reasoning-before-comparison-llm-enhanced-semantic-similarity-metrics-for-domain-specialized-text-analysis-shaochen-xu-et-al-2024>(3/37 | 3/113) Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis (Shaochen Xu et al., 2024)</a></li><li><a href=#437--4113-laco-large-language-model-pruning-via-layer-collapse-yifei-yang-et-al-2024>(4/37 | 4/113) LaCo: Large Language Model Pruning via Layer Collapse (Yifei Yang et al., 2024)</a></li><li><a href=#537--5113-dissecting-human-and-llm-preferences-junlong-li-et-al-2024>(5/37 | 5/113) Dissecting Human and LLM Preferences (Junlong Li et al., 2024)</a></li><li><a href=#637--6113-assessing-llms-mathematical-reasoning-in-financial-document-question-answering-pragya-srivastava-et-al-2024>(6/37 | 6/113) Assessing LLMs&rsquo; Mathematical Reasoning in Financial Document Question Answering (Pragya Srivastava et al., 2024)</a></li><li><a href=#737--7113-grasping-the-essentials-tailoring-large-language-models-for-zero-shot-relation-extraction-sizhe-zhou-et-al-2024>(7/37 | 7/113) Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction (Sizhe Zhou et al., 2024)</a></li><li><a href=#837--8113-boosting-of-thoughts-trial-and-error-problem-solving-with-large-language-models-sijia-chen-et-al-2024>(8/37 | 8/113) Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models (Sijia Chen et al., 2024)</a></li><li><a href=#937--9113-direct-evaluation-of-chain-of-thought-in-multi-hop-reasoning-with-knowledge-graphs-minh-vuong-nguyen-et-al-2024>(9/37 | 9/113) Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs (Minh-Vuong Nguyen et al., 2024)</a></li><li><a href=#1037--10113-kg-agent-an-efficient-autonomous-agent-framework-for-complex-reasoning-over-knowledge-graph-jinhao-jiang-et-al-2024>(10/37 | 10/113) KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph (Jinhao Jiang et al., 2024)</a></li><li><a href=#1137--11113-can-large-language-models-perform-relation-based-argument-mining-deniz-gorur-et-al-2024>(11/37 | 11/113) Can Large Language Models perform Relation-based Argument Mining? (Deniz Gorur et al., 2024)</a></li><li><a href=#1237--12113-a-question-answering-based-pipeline-for-comprehensive-chinese-ehr-information-extraction-huaiyuan-ying-et-al-2024>(12/37 | 12/113) A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction (Huaiyuan Ying et al., 2024)</a></li><li><a href=#1337--13113-panda-pedantic-answer-correctness-determination-and-adjudicationimproving-automatic-evaluation-for-question-answering-and-text-generation-zongxia-li-et-al-2024>(13/37 | 13/113) PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation (Zongxia Li et al., 2024)</a></li><li><a href=#1437--14113-tasks-that-language-models-dont-learn-bruce-w-lee-et-al-2024>(14/37 | 14/113) Tasks That Language Models Don&rsquo;t Learn (Bruce W. Lee et al., 2024)</a></li><li><a href=#1537--15113-phaseevo-towards-unified-in-context-prompt-optimization-for-large-language-models-wendi-cui-et-al-2024>(15/37 | 15/113) PhaseEvo: Towards Unified In-Context Prompt Optimization for Large Language Models (Wendi Cui et al., 2024)</a></li><li><a href=#1637--16113-puzzle-solving-using-reasoning-of-large-language-models-a-survey-panagiotis-giadikiaroglou-et-al-2024>(16/37 | 16/113) Puzzle Solving using Reasoning of Large Language Models: A Survey (Panagiotis Giadikiaroglou et al., 2024)</a></li><li><a href=#1737--17113-llm-can-achieve-self-regulation-via-hyperparameter-aware-generation-siyin-wang-et-al-2024>(17/37 | 17/113) LLM can Achieve Self-Regulation via Hyperparameter Aware Generation (Siyin Wang et al., 2024)</a></li><li><a href=#1837--18113-knowtuning-knowledge-aware-fine-tuning-for-large-language-models-yougang-lyu-et-al-2024>(18/37 | 18/113) KnowTuning: Knowledge-aware Fine-tuning for Large Language Models (Yougang Lyu et al., 2024)</a></li><li><a href=#1937--19113-knowledge-graph-assisted-automatic-sports-news-writing-yang-cao-et-al-2024>(19/37 | 19/113) Knowledge Graph Assisted Automatic Sports News Writing (Yang Cao et al., 2024)</a></li><li><a href=#2037--20113-evedit-event-based-knowledge-editing-with-deductive-editing-boundaries-jiateng-liu-et-al-2024>(20/37 | 20/113) EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries (Jiateng Liu et al., 2024)</a></li><li><a href=#2137--21113-moral-moe-augmented-lora-for-llms-lifelong-learning-shu-yang-et-al-2024>(21/37 | 21/113) MoRAL: MoE Augmented LoRA for LLMs&rsquo; Lifelong Learning (Shu Yang et al., 2024)</a></li><li><a href=#2237--22113-controlled-text-generation-for-large-language-model-with-dynamic-attribute-graphs-xun-liang-et-al-2024>(22/37 | 22/113) Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs (Xun Liang et al., 2024)</a></li><li><a href=#2337--23113-renovi-a-benchmark-towards-remediating-norm-violations-in-socio-cultural-conversations-haolan-zhan-et-al-2024>(23/37 | 23/113) RENOVI: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations (Haolan Zhan et al., 2024)</a></li><li><a href=#2437--24113-onebit-towards-extremely-low-bit-large-language-models-yuzhuang-xu-et-al-2024>(24/37 | 24/113) OneBit: Towards Extremely Low-bit Large Language Models (Yuzhuang Xu et al., 2024)</a></li><li><a href=#2537--25113-token-ensemble-text-generation-on-attacking-the-automatic-ai-generated-text-detection-fan-huang-et-al-2024>(25/37 | 25/113) Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection (Fan Huang et al., 2024)</a></li><li><a href=#2637--26113-contrastive-instruction-tuning-tianyi-yan-et-al-2024>(26/37 | 26/113) Contrastive Instruction Tuning (Tianyi Yan et al., 2024)</a></li><li><a href=#2737--27113-i-learn-better-if-you-speak-my-language-enhancing-large-language-model-fine-tuning-with-style-aligned-response-adjustments-xuan-ren-et-al-2024>(27/37 | 27/113) I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments (Xuan Ren et al., 2024)</a></li><li><a href=#2837--28113-m4gt-bench-evaluation-benchmark-for-black-box-machine-generated-text-detection-yuxia-wang-et-al-2024>(28/37 | 28/113) M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection (Yuxia Wang et al., 2024)</a></li><li><a href=#2937--29113-mmmmodal----multi-images-multi-audio-multi-turn-multi-modal-husein-zolkepli-et-al-2024>(29/37 | 29/113) MMMModal &ndash; Multi-Images Multi-Audio Multi-turn Multi-Modal (Husein Zolkepli et al., 2024)</a></li><li><a href=#3037--30113-multi-perspective-consistency-enhances-confidence-estimation-in-large-language-models-pei-wang-et-al-2024>(30/37 | 30/113) Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models (Pei Wang et al., 2024)</a></li><li><a href=#3137--31113-disclosure-and-mitigation-of-gender-bias-in-llms-xiangjue-dong-et-al-2024>(31/37 | 31/113) Disclosure and Mitigation of Gender Bias in LLMs (Xiangjue Dong et al., 2024)</a></li><li><a href=#3237--32113-understanding-news-thumbnail-representativeness-by-counterfactual-text-guided-contrastive-language-image-pretraining-yejun-yoon-et-al-2024>(32/37 | 32/113) Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining (Yejun Yoon et al., 2024)</a></li><li><a href=#3337--33113-can-large-multimodal-models-uncover-deep-semantics-behind-images-yixin-yang-et-al-2024>(33/37 | 33/113) Can Large Multimodal Models Uncover Deep Semantics Behind Images? (Yixin Yang et al., 2024)</a></li><li><a href=#3437--34113-asclepius-a-spectrum-evaluation-benchmark-for-medical-multi-modal-large-language-models-wenxuan-wang-et-al-2024>(34/37 | 34/113) Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models (Wenxuan Wang et al., 2024)</a></li><li><a href=#3537--35113-k-semstamp-a-clustering-based-semantic-watermark-for-detection-of-machine-generated-text-abe-bohan-hou-et-al-2024>(35/37 | 35/113) k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text (Abe Bohan Hou et al., 2024)</a></li><li><a href=#3637--36113-what-changed-converting-representational-interventions-to-natural-language-matan-avitan-et-al-2024>(36/37 | 36/113) What Changed? Converting Representational Interventions to Natural Language (Matan Avitan et al., 2024)</a></li><li><a href=#3737--37113-human-ai-interactions-in-the-communication-era-autophagy-makes-large-models-achieving-local-optima-shu-yang-et-al-2024>(37/37 | 37/113) Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima (Shu Yang et al., 2024)</a></li></ul></li><li><a href=#cslg-26>cs.LG (26)</a><ul><li><a href=#126--38113-zerog-investigating-cross-dataset-zero-shot-transferability-in-graphs-yuhan-li-et-al-2024>(1/26 | 38/113) ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs (Yuhan Li et al., 2024)</a></li><li><a href=#226--39113-ransomware-detection-using-stacked-autoencoder-for-feature-selection-mike-nkongolo-et-al-2024>(2/26 | 39/113) Ransomware detection using stacked autoencoder for feature selection (Mike Nkongolo et al., 2024)</a></li><li><a href=#326--40113-tunetables-context-optimization-for-scalable-prior-data-fitted-networks-benjamin-feuer-et-al-2024>(3/26 | 40/113) TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks (Benjamin Feuer et al., 2024)</a></li><li><a href=#426--41113-lignn-graph-neural-networks-at-linkedin-fedor-borisyuk-et-al-2024>(4/26 | 41/113) LiGNN: Graph Neural Networks at LinkedIn (Fedor Borisyuk et al., 2024)</a></li><li><a href=#526--42113-aligning-large-language-models-by-on-policy-self-judgment-sangkyu-lee-et-al-2024>(5/26 | 42/113) Aligning Large Language Models by On-Policy Self-Judgment (Sangkyu Lee et al., 2024)</a></li><li><a href=#626--43113-uncertainty-quantification-of-graph-convolution-neural-network-models-of-evolving-processes-jeremiah-hauth-et-al-2024>(6/26 | 43/113) Uncertainty Quantification of Graph Convolution Neural Network Models of Evolving Processes (Jeremiah Hauth et al., 2024)</a></li><li><a href=#726--44113-heal-brain-inspired-hyperdimensional-efficient-active-learning-yang-ni-et-al-2024>(7/26 | 44/113) HEAL: Brain-inspired Hyperdimensional Efficient Active Learning (Yang Ni et al., 2024)</a></li><li><a href=#826--45113-knowledge-distillation-based-on-transformed-teacher-matching-kaixiang-zheng-et-al-2024>(8/26 | 45/113) Knowledge Distillation Based on Transformed Teacher Matching (Kaixiang Zheng et al., 2024)</a></li><li><a href=#926--46113-debiased-offline-representation-learning-for-fast-online-adaptation-in-non-stationary-dynamics-xinyu-zhang-et-al-2024>(9/26 | 46/113) Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics (Xinyu Zhang et al., 2024)</a></li><li><a href=#1026--47113-evaluating-the-stability-of-deep-learning-latent-feature-spaces-ademide-o-mabadeje-et-al-2024>(10/26 | 47/113) Evaluating the Stability of Deep Learning Latent Feature Spaces (Ademide O. Mabadeje et al., 2024)</a></li><li><a href=#1126--48113-maintaining-adversarial-robustness-in-continuous-learning-xiaolei-ru-et-al-2024>(11/26 | 48/113) Maintaining Adversarial Robustness in Continuous Learning (Xiaolei Ru et al., 2024)</a></li><li><a href=#1226--49113-beyond-generalization-a-survey-of-out-of-distribution-adaptation-on-graphs-shuhan-liu-et-al-2024>(12/26 | 49/113) Beyond Generalization: A Survey of Out-Of-Distribution Adaptation on Graphs (Shuhan Liu et al., 2024)</a></li><li><a href=#1326--50113-expressive-higher-order-link-prediction-through-hypergraph-symmetry-breaking-simon-zhang-et-al-2024>(13/26 | 50/113) Expressive Higher-Order Link Prediction through Hypergraph Symmetry Breaking (Simon Zhang et al., 2024)</a></li><li><a href=#1426--51113-reinforcement-learning-to-maximise-wind-turbine-energy-generation-daniel-soler-et-al-2024>(14/26 | 51/113) Reinforcement learning to maximise wind turbine energy generation (Daniel Soler et al., 2024)</a></li><li><a href=#1526--52113-multi-task-inverse-reinforcement-learning-for-common-sense-reward-neta-glazer-et-al-2024>(15/26 | 52/113) Multi Task Inverse Reinforcement Learning for Common Sense Reward (Neta Glazer et al., 2024)</a></li><li><a href=#1626--53113-data-driven-stochastic-ac-opf-using-gaussian-processes-mile-mitrovic-2024>(16/26 | 53/113) Data-Driven Stochastic AC-OPF using Gaussian Processes (Mile Mitrovic, 2024)</a></li><li><a href=#1726--54113-exploiting-t-norms-for-deep-learning-in-autonomous-driving-mihaela-cătălina-stoian-et-al-2024>(17/26 | 54/113) Exploiting T-norms for Deep Learning in Autonomous Driving (Mihaela Cătălina Stoian et al., 2024)</a></li><li><a href=#1826--55113-fair-classification-with-partial-feedback-an-exploration-based-data-collection-approach-vijay-keswani-et-al-2024>(18/26 | 55/113) Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach (Vijay Keswani et al., 2024)</a></li><li><a href=#1926--56113-adadagrad-adaptive-batch-size-schemes-for-adaptive-gradient-methods-tim-tsz-kit-lau-et-al-2024>(19/26 | 56/113) AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods (Tim Tsz-Kit Lau et al., 2024)</a></li><li><a href=#2026--57113-turn-waste-into-worth-rectifying-top-k-router-of-moe-zhiyuan-zeng-et-al-2024>(20/26 | 57/113) Turn Waste into Worth: Rectifying Top-$k$ Router of MoE (Zhiyuan Zeng et al., 2024)</a></li><li><a href=#2126--58113-achieving-linear-speedup-in-asynchronous-federated-learning-with-heterogeneous-clients-xiaolu-wang-et-al-2024>(21/26 | 58/113) Achieving Linear Speedup in Asynchronous Federated Learning with Heterogeneous Clients (Xiaolu Wang et al., 2024)</a></li><li><a href=#2226--59113-minimally-supervised-topological-projections-of-self-organizing-maps-for-phase-of-flight-identification-zimeng-lyu-et-al-2024>(22/26 | 59/113) Minimally Supervised Topological Projections of Self-Organizing Maps for Phase of Flight Identification (Zimeng Lyu et al., 2024)</a></li><li><a href=#2326--60113-trust-regions-for-explanations-via-black-box-probabilistic-certification-amit-dhurandhar-et-al-2024>(23/26 | 60/113) Trust Regions for Explanations via Black-Box Probabilistic Certification (Amit Dhurandhar et al., 2024)</a></li><li><a href=#2426--61113-random-projection-neural-networks-of-best-approximation-convergence-theory-and-practical-applications-gianluca-fabiani-2024>(24/26 | 61/113) Random Projection Neural Networks of Best Approximation: Convergence theory and practical applications (Gianluca Fabiani, 2024)</a></li><li><a href=#2526--62113-probabilistic-routing-for-graph-based-approximate-nearest-neighbor-search-kejing-lu-et-al-2024>(25/26 | 62/113) Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search (Kejing Lu et al., 2024)</a></li><li><a href=#2626--63113-be-persistent-towards-a-unified-solution-for-mitigating-shortcuts-in-deep-learning-hadi-m-dolatabadi-et-al-2024>(26/26 | 63/113) Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning (Hadi M. Dolatabadi et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#11--64113-towards-development-of-automated-knowledge-maps-and-databases-for-materials-engineering-using-large-language-models-deepak-prasad-et-al-2024>(1/1 | 64/113) Towards Development of Automated Knowledge Maps and Databases for Materials Engineering using Large Language Models (Deepak Prasad et al., 2024)</a></li></ul></li><li><a href=#csir-4>cs.IR (4)</a><ul><li><a href=#14--65113-exploring-chatgpt-for-next-generation-information-retrieval-opportunities-and-challenges-yizheng-huang-et-al-2024>(1/4 | 65/113) Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges (Yizheng Huang et al., 2024)</a></li><li><a href=#24--66113-foundation-models-for-recommender-systems-a-survey-and-new-perspectives-chengkai-huang-et-al-2024>(2/4 | 66/113) Foundation Models for Recommender Systems: A Survey and New Perspectives (Chengkai Huang et al., 2024)</a></li><li><a href=#34--67113-mirror-gradient-towards-robust-multimodal-recommender-systems-via-exploring-flat-local-minima-shanshan-zhong-et-al-2024>(3/4 | 67/113) Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima (Shanshan Zhong et al., 2024)</a></li><li><a href=#44--68113-knowledge-graph-based-session-recommendation-with-adaptive-propagation-yu-wang-et-al-2024>(4/4 | 68/113) Knowledge Graph-based Session Recommendation with Adaptive Propagation (Yu Wang et al., 2024)</a></li></ul></li><li><a href=#cscv-12>cs.CV (12)</a><ul><li><a href=#112--69113-collavo-crayon-large-language-and-vision-model-byung-kwan-lee-et-al-2024>(1/12 | 69/113) CoLLaVO: Crayon Large Language and Vision mOdel (Byung-Kwan Lee et al., 2024)</a></li><li><a href=#212--70113-fvit-a-focal-vision-transformer-with-gabor-filter-yulong-shi-et-al-2024>(2/12 | 70/113) FViT: A Focal Vision Transformer with Gabor Filter (Yulong Shi et al., 2024)</a></li><li><a href=#312--71113-revit-enhancing-vision-transformers-with-attention-residual-connections-for-visual-recognition-anxhelo-diko-et-al-2024>(3/12 | 71/113) ReViT: Enhancing Vision Transformers with Attention Residual Connections for Visual Recognition (Anxhelo Diko et al., 2024)</a></li><li><a href=#412--72113-chatearthnet-a-global-scale-high-quality-image-text-dataset-for-remote-sensing-zhenghang-yuan-et-al-2024>(4/12 | 72/113) ChatEarthNet: A Global-Scale, High-Quality Image-Text Dataset for Remote Sensing (Zhenghang Yuan et al., 2024)</a></li><li><a href=#512--73113-on-good-practices-for-task-specific-distillation-of-large-pretrained-models-juliette-marrie-et-al-2024>(5/12 | 73/113) On Good Practices for Task-Specific Distillation of Large Pretrained Models (Juliette Marrie et al., 2024)</a></li><li><a href=#612--74113-diffpoint-single-and-multi-view-point-cloud-reconstruction-with-vit-based-diffusion-model-yu-feng-et-al-2024>(6/12 | 74/113) DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model (Yu Feng et al., 2024)</a></li><li><a href=#712--75113-graphkd-exploring-knowledge-distillation-towards-document-object-detection-with-structured-graph-creation-ayan-banerjee-et-al-2024>(7/12 | 75/113) GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation (Ayan Banerjee et al., 2024)</a></li><li><a href=#812--76113-semi-supervised-medical-image-segmentation-method-based-on-cross-pseudo-labeling-leveraging-strong-and-weak-data-augmentation-strategies-yifei-chen-et-al-2024>(8/12 | 76/113) Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo Labeling Leveraging Strong and Weak Data Augmentation Strategies (Yifei Chen et al., 2024)</a></li><li><a href=#912--77113-a-decoding-scheme-with-successive-aggregation-of-multi-level-features-for-light-weight-semantic-segmentation-jiwon-yoo-et-al-2024>(9/12 | 77/113) A Decoding Scheme with Successive Aggregation of Multi-Level Features for Light-Weight Semantic Segmentation (Jiwon Yoo et al., 2024)</a></li><li><a href=#1012--78113-learning-by-reconstruction-produces-uninformative-features-for-perception-randall-balestriero-et-al-2024>(10/12 | 78/113) Learning by Reconstruction Produces Uninformative Features For Perception (Randall Balestriero et al., 2024)</a></li><li><a href=#1112--79113-hand-biometrics-in-digital-forensics-asish-bera-et-al-2024>(11/12 | 79/113) Hand Biometrics in Digital Forensics (Asish Bera et al., 2024)</a></li><li><a href=#1212--80113-beyond-literal-descriptions-understanding-and-locating-open-world-objects-aligned-with-human-intentions-wenxuan-wang-et-al-2024>(12/12 | 80/113) Beyond Literal Descriptions: Understanding and Locating Open-World Objects Aligned with Human Intentions (Wenxuan Wang et al., 2024)</a></li></ul></li><li><a href=#eessas-4>eess.AS (4)</a><ul><li><a href=#14--81113-target-speech-extraction-with-pre-trained-self-supervised-learning-models-junyi-peng-et-al-2024>(1/4 | 81/113) Target Speech Extraction with Pre-trained Self-supervised Learning Models (Junyi Peng et al., 2024)</a></li><li><a href=#24--82113-when-llms-meets-acoustic-landmarks-an-efficient-approach-to-integrate-speech-into-large-language-models-for-depression-detection-xiangyu-zhang-et-al-2024>(2/4 | 82/113) When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection (Xiangyu Zhang et al., 2024)</a></li><li><a href=#34--83113-probing-self-supervised-learning-models-with-target-speech-extraction-junyi-peng-et-al-2024>(3/4 | 83/113) Probing Self-supervised Learning Models with Target Speech Extraction (Junyi Peng et al., 2024)</a></li><li><a href=#44--84113-diffuse-sound-field-synthesis-franz-zotter-et-al-2024>(4/4 | 84/113) Diffuse Sound Field Synthesis (Franz Zotter et al., 2024)</a></li></ul></li><li><a href=#csro-3>cs.RO (3)</a><ul><li><a href=#13--85113-hysteresis-compensation-of-flexible-continuum-manipulator-using-rgbd-sensing-and-temporal-convolutional-network-junhyun-park-et-al-2024>(1/3 | 85/113) Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network (Junhyun Park et al., 2024)</a></li><li><a href=#23--86113-carla-autoware-bridge-facilitating-autonomous-driving-research-with-a-unified-framework-for-simulation-and-module-development-gemb-kaljavesi-et-al-2024>(2/3 | 86/113) CARLA-Autoware-Bridge: Facilitating Autonomous Driving Research with a Unified Framework for Simulation and Module Development (Gemb Kaljavesi et al., 2024)</a></li><li><a href=#33--87113-mob-net-limb-modularized-uncertainty-torque-learning-of-humanoids-for-sensorless-external-torque-estimation-daegyu-lim-et-al-2024>(3/3 | 87/113) MOB-Net: Limb-modularized Uncertainty Torque Learning of Humanoids for Sensorless External Torque Estimation (Daegyu Lim et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--88113-multi-generative-agent-collective-decision-making-in-urban-planning-a-case-study-for-kendall-square-renovation-jin-gao-et-al-2024>(1/1 | 88/113) Multi-Generative Agent Collective Decision-Making in Urban Planning: A Case Study for Kendall Square Renovation (Jin Gao et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#14--89113-understanding-the-impact-of-long-term-memory-on-self-disclosure-with-large-language-model-driven-chatbots-for-public-health-intervention-eunkyung-jo-et-al-2024>(1/4 | 89/113) Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention (Eunkyung Jo et al., 2024)</a></li><li><a href=#24--90113-from-text-to-map-a-system-dynamics-bot-for-constructing-causal-loop-diagrams-niyousha-hosseinichimeh-et-al-2024>(2/4 | 90/113) From Text to Map: A System Dynamics Bot for Constructing Causal Loop Diagrams (Niyousha Hosseinichimeh et al., 2024)</a></li><li><a href=#34--91113-ironies-of-generative-ai-understanding-and-mitigating-productivity-loss-in-human-ai-interactions-auste-simkute-et-al-2024>(3/4 | 91/113) Ironies of Generative AI: Understanding and mitigating productivity loss in human-AI interactions (Auste Simkute et al., 2024)</a></li><li><a href=#44--92113-supporting-experts-with-a-multimodal-machine-learning-based-tool-for-human-behavior-analysis-of-conversational-videos-riku-arakawa-et-al-2024>(4/4 | 92/113) Supporting Experts with a Multimodal Machine-Learning-Based Tool for Human Behavior Analysis of Conversational Videos (Riku Arakawa et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--93113-fair-resource-allocation-in-virtualized-o-ran-platforms-fatih-aslan-et-al-2024>(1/2 | 93/113) Fair Resource Allocation in Virtualized O-RAN Platforms (Fatih Aslan et al., 2024)</a></li><li><a href=#22--94113-automated-optimization-of-parameterized-data-plane-programs-with-parasol-mary-hogan-et-al-2024>(2/2 | 94/113) Automated Optimization of Parameterized Data-Plane Programs with Parasol (Mary Hogan et al., 2024)</a></li></ul></li><li><a href=#cscr-3>cs.CR (3)</a><ul><li><a href=#13--95113-watch-out-for-your-agents-investigating-backdoor-threats-to-llm-based-agents-wenkai-yang-et-al-2024>(1/3 | 95/113) Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents (Wenkai Yang et al., 2024)</a></li><li><a href=#23--96113-secure-robust-and-energy-efficient-authenticated-data-sharing-in-uav-assisted-6g-networks-atefeh-mohseni-ejiyeh-2024>(2/3 | 96/113) Secure, Robust, and Energy-Efficient Authenticated Data Sharing in UAV-Assisted 6G Networks (Atefeh Mohseni Ejiyeh, 2024)</a></li><li><a href=#33--97113-on-the-role-of-similarity-in-detecting-masquerading-files-jonathan-oliver-et-al-2024>(3/3 | 97/113) On the Role of Similarity in Detecting Masquerading Files (Jonathan Oliver et al., 2024)</a></li></ul></li><li><a href=#csai-2>cs.AI (2)</a><ul><li><a href=#12--98113-an-empirical-evaluation-of-neural-and-neuro-symbolic-approaches-to-real-time-multimodal-complex-event-detection-liying-han-et-al-2024>(1/2 | 98/113) An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection (Liying Han et al., 2024)</a></li><li><a href=#22--99113-training-language-model-agents-without-modifying-language-models-shaokun-zhang-et-al-2024>(2/2 | 99/113) Training Language Model Agents without Modifying Language Models (Shaokun Zhang et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--100113-wireless-distributed-matrix-vector-multiplication-using-over-the-air-computation-and-analog-coding-jinho-choi-2024>(1/2 | 100/113) Wireless Distributed Matrix-Vector Multiplication using Over-the-Air Computation and Analog Coding (Jinho Choi, 2024)</a></li><li><a href=#22--101113-power-optimization-for-integrated-active-and-passive-sensing-in-dfrc-systems-xingliang-lou-et-al-2024>(2/2 | 101/113) Power Optimization for Integrated Active and Passive Sensing in DFRC Systems (Xingliang Lou et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--102113-adaptive-split-balancing-for-optimal-random-forest-yuqian-zhang-et-al-2024>(1/3 | 102/113) Adaptive Split Balancing for Optimal Random Forest (Yuqian Zhang et al., 2024)</a></li><li><a href=#23--103113-efficient-low-rank-matrix-estimation-experimental-design-and-arm-set-dependent-low-rank-bandits-kyoungseok-jang-et-al-2024>(2/3 | 103/113) Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits (Kyoungseok Jang et al., 2024)</a></li><li><a href=#33--104113-variational-entropy-search-for-adjusting-expected-improvement-nuojin-cheng-et-al-2024>(3/3 | 104/113) Variational Entropy Search for Adjusting Expected Improvement (Nuojin Cheng et al., 2024)</a></li></ul></li><li><a href=#eessiv-3>eess.IV (3)</a><ul><li><a href=#13--105113-hierarchical-prior-based-super-resolution-for-point-cloud-geometry-compression-dingquan-li-et-al-2024>(1/3 | 105/113) Hierarchical Prior-based Super Resolution for Point Cloud Geometry Compression (Dingquan Li et al., 2024)</a></li><li><a href=#23--106113-tc-diffrecon-texture-coordination-mri-reconstruction-method-based-on-diffusion-model-and-modified-mf-unet-method-chenyan-zhang-et-al-2024>(2/3 | 106/113) TC-DiffRecon: Texture coordination MRI reconstruction method based on diffusion model and modified MF-UNet method (Chenyan Zhang et al., 2024)</a></li><li><a href=#33--107113-training-free-image-style-alignment-for-self-adapting-domain-shift-on-handheld-ultrasound-devices-hongye-zeng-et-al-2024>(3/3 | 107/113) Training-free image style alignment for self-adapting domain shift on handheld ultrasound devices (Hongye Zeng et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--108113-transformer-based-de-novo-peptide-sequencing-for-data-independent-acquisition-mass-spectrometry-shiva-ebrahimi-et-al-2024>(1/1 | 108/113) Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry (Shiva Ebrahimi et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--109113-implementation-of-a-model-of-the-cortex-basal-ganglia-loop-naoya-arakawa-2024>(1/1 | 109/113) Implementation of a Model of the Cortex Basal Ganglia Loop (Naoya Arakawa, 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--110113-materiality-and-risk-in-the-age-of-pervasive-ai-sensors-matthew-stewart-et-al-2024>(1/1 | 110/113) Materiality and Risk in the Age of Pervasive AI Sensors (Matthew Stewart et al., 2024)</a></li></ul></li><li><a href=#econth-1>econ.TH (1)</a><ul><li><a href=#11--111113-the-value-of-context-human-versus-black-box-evaluators-andrei-iakovlev-et-al-2024>(1/1 | 111/113) The Value of Context: Human versus Black Box Evaluators (Andrei Iakovlev et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--112113-the-matrix-free-macro-element-hybridized-discontinuous-galerkin-method-for-steady-and-unsteady-compressible-flows-vahid-badrkhani-et-al-2024>(1/1 | 112/113) The matrix-free macro-element hybridized Discontinuous Galerkin method for steady and unsteady compressible flows (Vahid Badrkhani et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--113113-treewidth-versus-clique-number-iv-tree-independence-number-of-graphs-excluding-an-induced-star-clément-dallard-et-al-2024>(1/1 | 113/113) Treewidth versus clique number. IV. Tree-independence number of graphs excluding an induced star (Clément Dallard et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>