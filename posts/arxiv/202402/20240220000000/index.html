<!doctype html><html><head><title>arXiv @ 2024.02.20</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.20"><meta property="og:description" content="Primary Categories cs.AI (3) cs.AR (2) cs.CC (2) cs.CL (61) cs.CR (2) cs.CV (23) cs.DC (1) cs.DM (1) cs.DS (2) cs.ET (1) cs.GT (2) cs.HC (1) cs.IR (4) cs.IT (4) cs.LG (26) cs.MA (2) cs.PL (1) cs.RO (7) cs.SE (3) econ.EM (1) eess.SP (1) eess.SY (8) hep-ph (1) math.NA (1) math.OC (1) q-bio.BM (2) q-bio.GN (1) stat.ML (1) Keywords keyword cs.CL cs.CV cs.LG Active Learning 1 Adversarial Attack 1 Adversarial Learning 2 Alpaca 1 Anomaly Detection 1 Autoencoder 1 1 Automatic Evaluation 2 Automatic Speech Recognition 1 BART 1 BERT 1 1 Benchmarking 14 8 10 Black Box 2 1 1 Causal Intervention 1 ChatGPT 1 Clustering 1 Code Generation 1 Common-sense Reasoning 2 Continual Learning 1 1 Contrastive Learning 1 Convolution 2 Convolutional Neural Network 3 Counter-factual 1 Data Augmentation 1 1 1 Domain Adaptation 1 Fairness 1 Federated Learning 1 Few-shot 4 Fine-tuning 13 6 GPT 16 1 GPT-2 1 1 GPT-3 1 GPT-3."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240220000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-20T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-20T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.20"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240220000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Tuesday, Feb 20, 2024</p></div><div class=title><h1>arXiv @ 2024.02.20</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#csai-3>cs.AI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#cscc-2>cs.CC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#cscl-61>cs.CL (61)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#cscr-2>cs.CR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#cscv-23>cs.CV (23)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#cset-1>cs.ET (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#csgt-2>cs.GT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#cshc-1>cs.HC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#csir-4>cs.IR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#csit-4>cs.IT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#cslg-26>cs.LG (26)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#csro-7>cs.RO (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#econem-1>econ.EM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#eesssy-8>eess.SY (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#hep-ph-1>hep-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#mathna-1>math.NA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#q-biobm-2>q-bio.BM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#q-biogn-1>q-bio.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/#statml-1>stat.ML (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>2</td></tr><tr><td>Alpaca</td><td>1</td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>1</td></tr><tr><td>Autoencoder</td><td></td><td>1</td><td>1</td></tr><tr><td>Automatic Evaluation</td><td>2</td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>1</td><td></td></tr><tr><td>BART</td><td></td><td></td><td>1</td></tr><tr><td>BERT</td><td>1</td><td></td><td>1</td></tr><tr><td>Benchmarking</td><td>14</td><td>8</td><td>10</td></tr><tr><td>Black Box</td><td>2</td><td>1</td><td>1</td></tr><tr><td>Causal Intervention</td><td></td><td></td><td>1</td></tr><tr><td>ChatGPT</td><td>1</td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td>1</td></tr><tr><td>Code Generation</td><td>1</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>2</td><td></td><td></td></tr><tr><td>Continual Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>1</td></tr><tr><td>Convolution</td><td></td><td>2</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>3</td><td></td></tr><tr><td>Counter-factual</td><td>1</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>1</td></tr><tr><td>Fairness</td><td>1</td><td></td><td></td></tr><tr><td>Federated Learning</td><td>1</td><td></td><td></td></tr><tr><td>Few-shot</td><td>4</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>13</td><td></td><td>6</td></tr><tr><td>GPT</td><td>16</td><td></td><td>1</td></tr><tr><td>GPT-2</td><td>1</td><td></td><td>1</td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td></tr><tr><td>GPT-4</td><td>12</td><td></td><td></td></tr><tr><td>Grammatical Error Correction</td><td>1</td><td></td><td></td></tr><tr><td>Graph</td><td>5</td><td></td><td>7</td></tr><tr><td>Graph Neural Network</td><td>2</td><td></td><td>3</td></tr><tr><td>Hallucination Detection</td><td></td><td>1</td><td></td></tr><tr><td>Hate Speech Detection</td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>6</td><td>3</td><td>3</td></tr><tr><td>Instruction Following</td><td>1</td><td></td><td>1</td></tr><tr><td>Instruction Tuning</td><td>3</td><td>1</td><td></td></tr><tr><td>Knowledge Distillation</td><td>2</td><td>3</td><td>3</td></tr><tr><td>Knowledge Graph</td><td>5</td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td>2</td><td></td><td></td></tr><tr><td>LLaMA</td><td>5</td><td></td><td>1</td></tr><tr><td>LSTM</td><td></td><td>2</td><td></td></tr><tr><td>Large Language Model</td><td>91</td><td>3</td><td>10</td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td>1</td><td>1</td></tr><tr><td>Machine Unlearning</td><td>1</td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td>1</td><td></td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td>2</td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>2</td><td></td><td></td></tr><tr><td>Mistral</td><td></td><td></td><td>1</td></tr><tr><td>Model Compression</td><td></td><td></td><td>1</td></tr><tr><td>Model Pruning</td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>7</td><td>3</td><td></td></tr><tr><td>Natural Language Explanation</td><td></td><td></td><td>1</td></tr><tr><td>Natural Language Generation</td><td>1</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>2</td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td></tr><tr><td>Object Detection</td><td></td><td>2</td><td></td></tr><tr><td>Opinion Summarization</td><td>1</td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td>1</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>2</td><td>2</td></tr><tr><td>Prompt</td><td>15</td><td></td><td>1</td></tr><tr><td>Pruning</td><td>1</td><td></td><td>1</td></tr><tr><td>Question Answering</td><td>7</td><td></td><td></td></tr><tr><td>Reasoning</td><td>11</td><td>3</td><td>2</td></tr><tr><td>Recommendation</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Recurrent Neural Network</td><td></td><td>1</td><td>1</td></tr><tr><td>Reinforcement Learning</td><td>6</td><td>1</td><td>4</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>1</td><td></td><td>1</td></tr><tr><td>Rerank</td><td></td><td>1</td><td></td></tr><tr><td>Retrieval Augmentation</td><td>1</td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>2</td><td></td><td></td></tr><tr><td>RoBERTa</td><td></td><td></td><td>2</td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td></tr><tr><td>Rouge-L</td><td>1</td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td>1</td></tr><tr><td>Scaling Law</td><td></td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td>1</td><td>4</td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Simulation</td><td>2</td><td></td><td></td></tr><tr><td>Simulator</td><td>2</td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>2</td></tr><tr><td>Style Transfer</td><td>1</td><td></td><td></td></tr><tr><td>Summarization</td><td>4</td><td>1</td><td>2</td></tr><tr><td>Supervised Learning</td><td>1</td><td>2</td><td>1</td></tr><tr><td>Text Classification</td><td>3</td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td></td></tr><tr><td>Transformer</td><td>1</td><td>1</td><td>2</td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td>1</td><td></td></tr><tr><td>Vision-and-Language</td><td>2</td><td>3</td><td></td></tr><tr><td>Weakly Supervised Learning</td><td>1</td><td></td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>2</td><td>2</td><td></td></tr><tr><td>Zero-shot Learning</td><td></td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-61>cs.CL (61)</h2><h3 id=161--1165-gnnavi-navigating-the-information-flow-in-large-language-models-by-graph-neural-network-shuzhou-yuan-et-al-2024>(1/61 | 1/165) GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network (Shuzhou Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuzhou Yuan, Ercong Nie, Michael Färber, Helmut Schmid, Hinrich Schütze. (2024)<br><strong>GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network</strong><br><button class=copy-to-clipboard title="GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 133<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Few-shot, Fine-tuning, GPT, GPT-2, Text Classification, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11709v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11709v1.pdf filename=2402.11709v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> exhibit strong <b>In-Context</b> <b>Learning</b> <b>(ICL)</b> capabilities when <b>prompts</b> with demonstrations are applied to them. However, <b>fine-tuning</b> still remains crucial to further enhance their adaptability. <b>Prompt-based</b> <b>fine-tuning</b> proves to be an effective <b>fine-tuning</b> method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a <b>prompt-based</b> parameter-efficient <b>fine-tuning</b> (PEFT) approach. GNNavi leverages insights into <b>ICL&rsquo;s</b> information flow dynamics, which indicates that label words act in <b>prompts</b> as anchors for information propagation. GNNavi employs a <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> layer to precisely guide the aggregation and distribution of information flow during the processing of <b>prompts</b> by hardwiring the desired information flow into the <b>GNN.</b> Our experiments on <b>text</b> <b>classification</b> tasks with <b>GPT-2</b> and Llama2 shows GNNavi surpasses standard <b>prompt-based</b> <b>fine-tuning</b> methods in <b>few-shot</b> settings by updating just 0.2% to 0.5% of parameters. We compare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and Adapter in terms of performance and efficiency. Our analysis reveals that GNNavi enhances information flow and ensures a clear aggregation process.</p></p class="citation"></blockquote><h3 id=261--2165-eventrl-enhancing-event-extraction-with-outcome-supervision-for-large-language-models-jun-gao-et-al-2024>(2/61 | 2/165) EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models (Jun Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Gao, Huan Zhao, Wei Wang, Changlong Yu, Ruifeng Xu. (2024)<br><strong>EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models</strong><br><button class=copy-to-clipboard title="EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Few-shot, Fine-tuning, Reinforcement Learning, Supervised Learning, GPT, GPT-4, LLaMA, Instruction Following, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11430v1.pdf filename=2402.11430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we present EventRL, a <b>reinforcement</b> <b>learning</b> approach developed to enhance event extraction for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in <b>LLMs,</b> such as <b>instruction</b> <b>following</b> and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like <b>Few-Shot</b> <b>Prompting</b> (FSP) (based on <b>GPT4)</b> and <b>Supervised</b> <b>Fine-Tuning</b> (SFT) across various <b>LLMs,</b> including <b>GPT-4,</b> <b>LLaMa,</b> and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing model size leads to higher accuracy, maintaining the ability to generalize is essential to avoid overfitting.</p></p class="citation"></blockquote><h3 id=361--3165-decoding-news-narratives-a-critical-analysis-of-large-language-models-in-framing-bias-detection-valeria-pastorino-et-al-2024>(3/61 | 3/165) Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection (Valeria Pastorino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valeria Pastorino, Jasivan A. Sivakumar, Nafise Sadat Moosavi. (2024)<br><strong>Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection</strong><br><button class=copy-to-clipboard title="Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Few-shot, Fine-tuning, Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11621v1.pdf filename=2402.11621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work contributes to the expanding research on the applicability of <b>LLMs</b> in social sciences by examining the performance of <b>GPT-3.5</b> Turbo, <b>GPT-4,</b> and Flan-T5 models in detecting framing bias in news headlines through <b>zero-shot,</b> <b>few-shot,</b> and explainable <b>prompting</b> methods. A key insight from our evaluation is the notable efficacy of explainable <b>prompting</b> in enhancing the reliability of these models, highlighting the importance of explainable settings for social science research on framing bias. <b>GPT-4,</b> in particular, demonstrated enhanced performance in <b>few-shot</b> scenarios when presented with a range of relevant, in-domain examples. FLAN-T5&rsquo;s poor performance indicates that smaller models may require additional task-specific <b>fine-tuning</b> for identifying framing bias detection. Our study also found that models, particularly <b>GPT-4,</b> often misinterpret emotional language as an indicator of framing bias, underscoring the challenge of distinguishing between reporting genuine emotional expression and intentionally use framing bias in news headlines. We further evaluated the models on two subsets of headlines where the presence or absence of framing bias was either clear-cut or more contested, with the results suggesting that these models&rsquo; can be useful in flagging potential annotation inaccuracies within existing or new datasets. Finally, the study evaluates the models in real-world conditions (&ldquo;in the wild&rdquo;), moving beyond the initial dataset focused on U.S. Gun Violence, assessing the models&rsquo; performance on framed headlines covering a broad range of topics.</p></p class="citation"></blockquote><h3 id=461--4165-learning-from-failure-integrating-negative-examples-when-fine-tuning-large-language-models-as-agents-renxi-wang-et-al-2024>(4/61 | 4/165) Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents (Renxi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, Timothy Baldwin. (2024)<br><strong>Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents</strong><br><button class=copy-to-clipboard title="Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-4, Mathematical Reasoning, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11651v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11651v1.pdf filename=2402.11651v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved success in acting as agents, which interact with environments through tools like search engines. However, <b>LLMs</b> are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between <b>GPT-4</b> and environments, and <b>fine-tuned</b> smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during <b>fine-tuning.</b> In this paper, we contend that <b>large</b> <b>language</b> <b>models</b> can learn from failures through appropriate data cleaning and <b>fine-tuning</b> strategies. We conduct experiments on <b>mathematical</b> <b>reasoning,</b> multi-hop <b>question</b> <b>answering,</b> and strategic <b>question</b> <b>answering</b> tasks. Experimental results demonstrate that compared to solely using positive examples, incorporating negative examples enhances model performance by a large margin.</p></p class="citation"></blockquote><h3 id=561--5165-why-lift-so-heavy-slimming-large-language-models-by-cutting-off-the-layers-shuzhou-yuan-et-al-2024>(5/61 | 5/165) Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers (Shuzhou Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuzhou Yuan, Ercong Nie, Bolei Ma, Michael Färber. (2024)<br><strong>Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers</strong><br><button class=copy-to-clipboard title="Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Knowledge Distillation, Model Pruning, Pruning, Text Classification, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11700v1.pdf filename=2402.11700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> possess outstanding capabilities in addressing various natural language processing (NLP) tasks. However, the sheer size of these <b>models</b> <b>poses</b> challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking. While traditional approaches such as <b>model</b> <b>pruning</b> or <b>distillation</b> offer ways for reducing <b>model</b> <b>size,</b> they often come at the expense of performance retention. In our investigation, we systematically explore the approach of reducing the number of layers in <b>LLMs.</b> Surprisingly, we observe that even with fewer layers, <b>LLMs</b> maintain similar or better performance levels, particularly in <b>prompt-based</b> <b>fine-tuning</b> for <b>text</b> <b>classification</b> tasks. Remarkably, in certain cases, <b>models</b> <b>with</b> a single layer outperform their fully layered counterparts. These findings offer valuable insights for future work aimed at mitigating the size constraints of <b>LLMs</b> while preserving their performance, thereby opening avenues for significantly more efficient use of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=661--6165-vision-flan-scaling-human-labeled-tasks-in-visual-instruction-tuning-zhiyang-xu-et-al-2024>(6/61 | 6/165) Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning (Zhiyang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, Lifu Huang. (2024)<br><strong>Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning</strong><br><button class=copy-to-clipboard title="Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 76<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, GPT, GPT-4, Instruction Tuning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11690v1.pdf filename=2402.11690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite <b>vision-language</b> models&rsquo; (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual <b>instruction</b> <b>tuning,</b> and (2) annotation error and bias in <b>GPT-4</b> synthesized <b>instruction</b> <b>tuning</b> data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual <b>instruction</b> <b>tuning</b> dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written <b>instruction.</b> <b>In</b> addition, we propose a two-stage <b>instruction</b> <b>tuning</b> framework, in which VLMs are firstly <b>finetuned</b> on Vision-Flan and further tuned on <b>GPT-4</b> synthesized data. We find this two-stage tuning framework significantly outperforms the traditional single-stage visual <b>instruction</b> <b>tuning</b> framework and achieves the state-of-the-art performance across a wide range of <b>multi-modal</b> evaluation <b>benchmarks.</b> Finally, we conduct in-depth analyses to understand visual <b>instruction</b> <b>tuning</b> and our findings reveal that: (1) <b>GPT-4</b> synthesized data does not substantially enhance VLMs&rsquo; capabilities but rather modulates the model&rsquo;s responses to human-preferred formats; (2) A minimal quantity (e.g., 1,000) of <b>GPT-4</b> synthesized data can effectively align VLM responses with human-preference; (3) Visual <b>instruction</b> <b>tuning</b> mainly helps <b>large-language</b> <b>models</b> <b>(LLMs)</b> to understand visual features.</p></p class="citation"></blockquote><h3 id=761--7165-longagent-scaling-language-models-to-128k-context-through-multi-agent-collaboration-jun-zhao-et-al-2024>(7/61 | 7/165) LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration (Jun Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi Zhang, Xuanjing Huang. (2024)<br><strong>LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration</strong><br><button class=copy-to-clipboard title="LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: GPT, GPT-4, LLaMA, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11550v1.pdf filename=2402.11550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated impressive performance in understanding language and executing complex <b>reasoning</b> tasks. However, <b>LLMs</b> with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as <b>GPT-4</b> and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the middle}. In this paper, we propose \textsc{LongAgent}, a method based on multi-agent collaboration, which scales <b>LLMs</b> (e.g., <b>LLaMA)</b> to a context of 128K and demonstrates potential superiority in long-text processing compared to <b>GPT-4.</b> In \textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members&rsquo; hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of members. To address this, we develop an \textit{inter-member communication} mechanism to resolve response conflicts caused by hallucinations through information sharing. Our experimental results indicate that \textsc{LongAgent} offers a promising alternative for long-text processing. The agent team instantiated with <b>LLaMA-7B</b> achieves significant improvements in tasks such as 128k-long text retrieval, multi-hop <b>question</b> <b>answering,</b> compared to <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=861--8165-dictllm-harnessing-key-value-data-structures-with-large-language-models-for-enhanced-medical-diagnostics-yiqiu-guo-et-al-2024>(8/61 | 8/165) DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics (YiQiu Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>YiQiu Guo, Yuchen Yang, Ya Zhang, Yu Wang, Yanfeng Wang. (2024)<br><strong>DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics</strong><br><button class=copy-to-clipboard title="DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, GPT, GPT-4, Large Language Model, Large Language Model, Rouge, Rouge-L<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11481v1.pdf filename=2402.11481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structured data offers a sophisticated mechanism for the organization of information. Existing methodologies for the text-serialization of structured data in the context of <b>large</b> <b>language</b> <b>models</b> fail to adequately address the heterogeneity inherent in key-value structured data. These methods are not ideal and frequently result in larger input sizes and poor adaptability to input changes. In this paper, we introduce DictLLM, an innovative framework designed to improve the modeling of key-value structured data, like medical laboratory reports, for generating medical diagnoses. DictLLM integrates three key components: (1) group positional encoding to maintain permutation invariance, (2) hierarchical attention bias to capture the inherent bias in structured data, and (3) an optimal transport alignment layer that aligns the embedding generated by the dictionary encoder with the <b>LLM,</b> thereby producing a sequence of fixed-length virtual tokens. We carry out experiments using various <b>LLM</b> models on a comprehensive real-world medical laboratory report dataset for automatic diagnosis generation, our findings illustrate that DictLLM significantly outperforms established baseline methods and <b>few-shot</b> <b>GPT-4</b> implementations in terms of both <b>Rouge-L</b> and Knowledge F1 scores. Furthermore, our evaluation of the framework&rsquo;s scalability and robustness, through a series of experiments, underscores its exceptional capability in accurately modeling the complex key-value data structure of medical dictionary data.</p></p class="citation"></blockquote><h3 id=961--9165-autoprm-automating-procedural-supervision-for-multi-step-reasoning-via-controllable-question-decomposition-zhaorun-chen-et-al-2024>(9/61 | 9/165) AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition (Zhaorun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, Huaxiu Yao. (2024)<br><strong>AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition</strong><br><button class=copy-to-clipboard title="AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Reinforcement Learning, Self-supervised Learning, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11452v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11452v1.pdf filename=2402.11452v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown promise in multi-step <b>reasoning</b> tasks, yet their reliance on extensive manual labeling to provide procedural feedback remains a significant impediment. To address this challenge, in this paper, we propose a novel <b>self-supervised</b> framework AutoPRM that efficiently enhances the <b>fine-tuning</b> of <b>LLMs</b> for intricate <b>reasoning</b> challenges. Specifically, AutoPRM first decomposes complex problems into more manageable subquestions with a controllable granularity switch, then sequentially apply <b>reinforcement</b> <b>learning</b> to iteratively improve the subquestion solver. Additionally, we propose context-guided-decoding to avoid reward tampering and guide the subquestion solver towards the solution of the holistic problem. Extensive experiments show that AutoPRM significantly improves performance on mathematical and <b>commonsense</b> <b>reasoning</b> tasks over SOTA. More encouragingly, AutoPRM can be easily integrated with other orthogonal <b>reasoning</b> pipelines.</p></p class="citation"></blockquote><h3 id=1061--10165-counter-intuitive-large-language-models-can-better-understand-knowledge-graphs-than-we-thought-xinbang-dai-et-al-2024>(10/61 | 10/165) Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought (Xinbang Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinbang Dai, Yuncheng Hua, Tongtong Wu, Yang Sheng, Guilin Qi. (2024)<br><strong>Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought</strong><br><button class=copy-to-clipboard title="Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-4; I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 68<br>Keywords: Graph, Black Box, Knowledge Graph, Knowledge Graph, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11541v1.pdf filename=2402.11541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although the method of enhancing <b>large</b> <b>language</b> <b>models&rsquo;</b> <b>(LLMs&rsquo;)</b> <b>reasoning</b> ability and reducing their hallucinations through the use of <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> has received widespread attention, the exploration of how to enable <b>LLMs</b> to integrate the structured <b>knowledge</b> <b>in</b> <b>KGs</b> on-the-fly remains inadequate. Researchers often co-train <b>KG</b> embeddings and <b>LLM</b> parameters to equip <b>LLMs</b> with the ability of comprehending <b>KG</b> <b>knowledge.</b> <b>However,</b> this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, <b>black-box</b> <b>LLMs.</b> In this paper, we employ complex <b>question</b> <b>answering</b> (CQA) as a task to assess the <b>LLM&rsquo;s</b> ability of comprehending <b>KG</b> <b>knowledge.</b> <b>We</b> conducted a comprehensive comparison of <b>KG</b> <b>knowledge</b> <b>injection</b> methods (from triples to natural language text), aiming to explore the optimal <b>prompting</b> method for supplying <b>KG</b> <b>knowledge</b> <b>to</b> <b>LLMs,</b> thereby enhancing their comprehension of <b>KG.</b> Contrary to our initial expectations, our analysis revealed that <b>LLMs</b> effectively handle messy, noisy, and linearized <b>KG</b> <b>knowledge,</b> <b>outperforming</b> methods that employ well-designed natural language (NL) textual <b>prompts.</b> This counter-intuitive finding provides substantial insights for future research on <b>LLMs&rsquo;</b> comprehension of structured knowledge.</p></p class="citation"></blockquote><h3 id=1161--11165-multi-task-inference-can-large-language-models-follow-multiple-instructions-at-once-guijin-son-et-al-2024>(11/61 | 11/165) Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once? (Guijin Son et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, Seungone Kim. (2024)<br><strong>Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?</strong><br><button class=copy-to-clipboard title="Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-4, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11597v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11597v1.pdf filename=2402.11597v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are typically <b>prompted</b> to follow a single instruction per inference call. In this work, we analyze whether <b>LLMs</b> also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task Inference <b>Benchmark),</b> a comprehensive evaluation <b>benchmark</b> encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that <b>LLMs</b> would perform better when tasks are divided, we find that state-of-the-art <b>LLMs,</b> such as <b>Llama-2-Chat-70B</b> and <b>GPT-4,</b> show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench dataset and our code at this link <a href=https://github.com/guijinSON/MTI-Bench>https://github.com/guijinSON/MTI-Bench</a>.</p></p class="citation"></blockquote><h3 id=1261--12165-factpico-factuality-evaluation-for-plain-language-summarization-of-medical-evidence-sebastian-antony-joseph-et-al-2024>(12/61 | 12/165) FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence (Sebastian Antony Joseph et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Antony Joseph, Lily Chen, Jan Trienes, Hannah Louisa Göke, Monika Coers, Wei Xu, Byron C Wallace, Junyi Jessy Li. (2024)<br><strong>FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence</strong><br><button class=copy-to-clipboard title="FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Alpaca, GPT, GPT-4, LLaMA, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11456v1.pdf filename=2402.11456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Plain language <b>summarization</b> with <b>LLMs</b> can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? This paper presents FactPICO, a factuality <b>benchmark</b> for plain language <b>summarization</b> of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. FactPICO consists of 345 plain language summaries of RCT abstracts generated from three <b>LLMs</b> (i.e., <b>GPT-4,</b> <b>Llama-2,</b> and <b>Alpaca),</b> with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. We also evaluate the correctness of the extra information (e.g., explanations) added by <b>LLMs.</b> Using FactPICO, we <b>benchmark</b> a range of existing factuality metrics, including the newly devised ones based on <b>LLMs.</b> We find that plain language <b>summarization</b> of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level.</p></p class="citation"></blockquote><h3 id=1361--13165-one-prompt-to-rule-them-all-llms-for-opinion-summary-evaluation-tejpalsingh-siledar-et-al-2024>(13/61 | 13/165) One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation (Tejpalsingh Siledar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tejpalsingh Siledar, Swaroop Nath, Sankara Sri Raghava Ravindra Muddu, Rupasai Rangaraju, Swaprava Nath, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Sudhanshu Shekhar Singh, Muthusamy Chelliah, Nikesh Garera. (2024)<br><strong>One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation</strong><br><button class=copy-to-clipboard title="One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Natural Language Generation, Opinion Summarization, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11683v1.pdf filename=2402.11683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluation of <b>opinion</b> <b>summaries</b> using conventional reference-based metrics rarely provides a holistic evaluation and has been shown to have a relatively low correlation with human judgments. Recent studies suggest using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as reference-free metrics for <b>NLG</b> evaluation, however, they remain unexplored for <b>opinion</b> <b>summary</b> evaluation. Moreover, limited <b>opinion</b> <b>summary</b> evaluation datasets inhibit progress. To address this, we release the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation of <b>opinion</b> <b>summaries:</b> fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a dimension-independent <b>prompt,</b> and Op-Prompts, a dimension-dependent set of <b>prompts</b> for <b>opinion</b> <b>summary</b> evaluation. Experiments indicate that Op-I-Prompt emerges as a good alternative for evaluating <b>opinion</b> <b>summaries</b> achieving an average Spearman correlation of 0.70 with humans, outperforming all previous approaches. To the best of our knowledge, we are the first to investigate <b>LLMs</b> as evaluators on both closed-source and open-source models in the <b>opinion</b> <b>summarization</b> domain.</p></p class="citation"></blockquote><h3 id=1461--14165-can-llms-reason-with-rules-logic-scaffolding-for-stress-testing-and-improving-llms-siyuan-wang-et-al-2024>(14/61 | 14/165) Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs (Siyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Wang, Zhongyu Wei, Yejin Choi, Xiang Ren. (2024)<br><strong>Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs</strong><br><button class=copy-to-clipboard title="Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, GPT, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11442v1.pdf filename=2402.11442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved impressive human-like performance across various <b>reasoning</b> tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of <b>GPT-series</b> models over a rule subset reveals significant gaps in <b>LLMs&rsquo;</b> logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further <b>distill</b> these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream <b>reasoning.</b> Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various <b>commonsense</b> <b>reasoning</b> tasks. Overall, our work sheds light on <b>LLMs&rsquo;</b> limitations in grasping inferential rule and suggests ways to enhance their logical <b>reasoning</b> abilities~\footnote{Code and data are available at \url{https://github.com/SiyuanWangw/ULogic}.}.</p></p class="citation"></blockquote><h3 id=1561--15165-multi-dimensional-evaluation-of-empathetic-dialog-responses-zhichao-xu-et-al-2024>(15/61 | 15/165) Multi-dimensional Evaluation of Empathetic Dialog Responses (Zhichao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhichao Xu, Jiepu Jiang. (2024)<br><strong>Multi-dimensional Evaluation of Empathetic Dialog Responses</strong><br><button class=copy-to-clipboard title="Multi-dimensional Evaluation of Empathetic Dialog Responses" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Recommendation, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11409v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11409v1.pdf filename=2402.11409v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Empathy is a critical element of effective and satisfactory conversational communication, yet previous studies in measuring conversational empathy mostly focus on expressed communicative intents &ndash; in which way empathy is expressed, ignoring the fact that conversation is also a collaborative practice involving both speakers and listeners. In contrast, we propose a multi-dimensional empathy evaluation framework that extends upon existing work to measure both expressed intents from the speaker&rsquo;s perspective and perceived empathy from the listener&rsquo;s perspective. Applying the proposed framework to analyzing our internal customer-service dialogue shows that the two dimensions (expressed intent types and perceived empathy) are inter-connected, while perceived empathy has high correlation with the satisfactory level of dialogue sessions. This proposed framework still requires subjective assessments from trained annotators, which can be non-trivial to collect. To scale up evaluation without excessive reliance on carefully annotated data, we explore different modeling options to automatically measure conversational empathy with (1) <b>prompting</b> frozen <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and (2) training language model-based classifiers. Extensive experiments on both internal and external dialogue datasets show that measuring conversational empathy remains a challenging task for <b>prompting</b> frozen <b>LLMs,</b> reflected by less satisfying performance of <b>GPT-4</b> and Flan family models. On the other hand, our proposed instruction-finetuned classifiers based on sequence-to-sequence (Seq2Seq) language models is able to achieve the best performance compared to prior works and competitive baselines. Finally, we perform comprehensive ablation studies on the performance of proposed instruction-finetuned classifiers and give <b>recommendations</b> on potentially adopting them as automatic conversational empathy evaluation metrics.</p></p class="citation"></blockquote><h3 id=1661--16165-matplotagent-method-and-evaluation-for-llm-based-agentic-scientific-data-visualization-zhiyu-yang-et-al-2024>(16/61 | 16/165) MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization (Zhiyu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, Zhiyuan Liu, Xiaodong Shi, Maosong Sun. (2024)<br><strong>MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization</strong><br><button class=copy-to-clipboard title="MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Automatic Evaluation, Benchmarking, Multi-modal, GPT, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11453v1.pdf filename=2402.11453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic <b>LLM</b> agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both <b>code</b> <b>LLMs</b> and <b>multi-modal</b> <b>LLMs,</b> MatPlotAgent consists of three core modules: query understanding, <b>code</b> <b>generation</b> with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of <b>benchmarks</b> in this field, we present MatPlotBench, a high-quality <b>benchmark</b> consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes <b>GPT-4V</b> for <b>automatic</b> <b>evaluation.</b> Experimental results demonstrate that MatPlotAgent can improve the performance of various <b>LLMs,</b> including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores.</p></p class="citation"></blockquote><h3 id=1761--17165-kmmlu-measuring-massive-multitask-language-understanding-in-korean-guijin-son-et-al-2024>(17/61 | 17/165) KMMLU: Measuring Massive Multitask Language Understanding in Korean (Guijin Son et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, Stella Biderman. (2024)<br><strong>KMMLU: Measuring Massive Multitask Language Understanding in Korean</strong><br><button class=copy-to-clipboard title="KMMLU: Measuring Massive Multitask Language Understanding in Korean" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT, GPT-4, Massive Multitask Language Understanding (MMLU), Massive Multitask Language Understanding (MMLU), Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11548v1.pdf filename=2402.11548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose KMMLU, a new Korean <b>benchmark</b> with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. Unlike previous Korean <b>benchmarks</b> that are translated from existing English <b>benchmarks,</b> KMMLU is collected from original Korean exams, capturing linguistic and cultural aspects of the Korean language. We test 26 publically available and proprietary <b>LLMs,</b> identifying significant room for improvement. The best publicly available model achieves 50.54% on KMMLU, far below the average human performance of 62.6%. This model was primarily trained for English and Chinese, not Korean. Current <b>LLMs</b> tailored to Korean, such as Polyglot-Ko, perform far worse. Surprisingly, even the most capable proprietary <b>LLMs,</b> e.g., <b>GPT-4</b> and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean <b>LLMs,</b> and KMMLU offers the right tool to track this progress. We make our dataset publicly available on the Hugging Face Hub and integrate the <b>benchmark</b> into EleutherAI&rsquo;s Language Model Evaluation Harness.</p></p class="citation"></blockquote><h3 id=1861--18165-modelling-political-coalition-negotiations-using-llm-based-agents-farhad-moghimifar-et-al-2024>(18/61 | 18/165) Modelling Political Coalition Negotiations Using LLM-based Agents (Farhad Moghimifar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Farhad Moghimifar, Yuan-Fang Li, Robert Thomson, Gholamreza Haffari. (2024)<br><strong>Modelling Political Coalition Negotiations Using LLM-based Agents</strong><br><button class=copy-to-clipboard title="Modelling Political Coalition Negotiations Using LLM-based Agents" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Markov Decision Process, Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11712v1.pdf filename=2402.11712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Coalition negotiations are a cornerstone of parliamentary democracies, characterised by complex interactions and strategic communications among political parties. Despite its significance, the modelling of these negotiations has remained unexplored with the domain of Natural Language Processing (NLP), mostly due to lack of proper data. In this paper, we introduce coalition negotiations as a novel NLP task, and model it as a negotiation between <b>large</b> <b>language</b> <b>model-based</b> agents. We introduce a multilingual dataset, POLCA, comprising manifestos of European political parties and coalition agreements over a number of elections in these countries. This dataset addresses the challenge of the current scope limitations in political negotiation modelling by providing a diverse, real-world basis for <b>simulation.</b> Additionally, we propose a hierarchical <b>Markov</b> <b>decision</b> <b>process</b> designed to simulate the process of coalition negotiation between political parties and predict the outcomes. We evaluate the performance of state-of-the-art <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as agents in handling coalition negotiations, offering insights into their capabilities and paving the way for future advancements in political modelling.</p></p class="citation"></blockquote><h3 id=1961--19165-chain-of-instructions-compositional-instruction-tuning-on-large-language-models-shirley-anugrah-hayati-et-al-2024>(19/61 | 19/165) Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models (Shirley Anugrah Hayati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shirley Anugrah Hayati, Taehee Jung, Tristan Bodding-Long, Sudipta Kar, Abhinav Sethy, Joo-Kyung Kim, Dongyeop Kang. (2024)<br><strong>Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models</strong><br><button class=copy-to-clipboard title="Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Instruction Tuning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11532v1.pdf filename=2402.11532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with a collection of <b>large</b> <b>and</b> <b>diverse</b> <b>instructions</b> <b>has</b> improved the model&rsquo;s generalization to different tasks, even for unseen tasks. However, most existing <b>instruction</b> <b>datasets</b> include only single <b>instructions,</b> <b>and</b> they struggle to follow complex <b>instructions</b> <b>composed</b> of multiple subtasks (Wang et al., 2023a). In this work, we propose a novel concept of compositional <b>instructions</b> <b>called</b> chain-of-instructions (CoI), where the output of one <b>instruction</b> <b>becomes</b> an input for the next like a chain. Unlike the conventional practice of solving single <b>instruction</b> <b>tasks,</b> our proposed method encourages a model to solve each subtask step by step until the final answer is reached. CoI-tuning (i.e., <b>fine-tuning</b> with CoI <b>instructions)</b> <b>improves</b> the model&rsquo;s ability to handle <b>instructions</b> <b>composed</b> of multiple subtasks. CoI-tuned models also outperformed baseline models on multilingual <b>summarization,</b> demonstrating the generalizability of CoI models on unseen composite downstream tasks.</p></p class="citation"></blockquote><h3 id=2061--20165-leia-facilitating-cross-lingual-knowledge-transfer-in-language-models-with-entity-based-data-augmentation-ikuya-yamada-et-al-2024>(20/61 | 20/165) LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation (Ikuya Yamada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ikuya Yamada, Ryokan Ri. (2024)<br><strong>LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation</strong><br><button class=copy-to-clipboard title="LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, Knowledge Transfer, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11485v1.pdf filename=2402.11485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adapting English-based <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse <b>question</b> <b>answering</b> datasets using 7B-parameter <b>LLMs,</b> demonstrating significant performance gains across various non-English languages. The source code is available at <a href=https://github.com/studio-ousia/leia>https://github.com/studio-ousia/leia</a>.</p></p class="citation"></blockquote><h3 id=2161--21165-in-context-example-ordering-guided-by-label-distributions-zhichao-xu-et-al-2024>(21/61 | 21/165) In-Context Example Ordering Guided by Label Distributions (Zhichao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhichao Xu, Daniel Cohen, Bei Wang, Vivek Srikumar. (2024)<br><strong>In-Context Example Ordering Guided by Label Distributions</strong><br><button class=copy-to-clipboard title="In-Context Example Ordering Guided by Label Distributions" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Text Classification, In-context Learning, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11447v1.pdf filename=2402.11447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>By allowing models to predict without task-specific training, <b>in-context</b> <b>learning</b> <b>(ICL)</b> with pretrained <b>LLMs</b> has enormous potential in NLP. However, a number of problems persist in <b>ICL.</b> In particular, its performance is sensitive to the choice and order of <b>in-context</b> <b>examples.</b> Given the same set of <b>in-context</b> <b>examples</b> with different orderings, model performance may vary between near random to near state-of-the-art. In this work, we formulate <b>in-context</b> <b>example</b> ordering as an optimization problem. We examine three problem settings that differ in the assumptions they make about what is known about the task. Inspired by the idea of learning from label proportions, we propose two principles for <b>in-context</b> <b>example</b> ordering guided by model&rsquo;s probability predictions. We apply our proposed principles to thirteen <b>text</b> <b>classification</b> datasets and nine different autoregressive <b>LLMs</b> with 700M to 13B parameters. We demonstrate that our approach outperforms the baselines by improving the classification accuracy, reducing model miscalibration, and also by selecting better <b>in-context</b> <b>examples.</b></p></p class="citation"></blockquote><h3 id=2261--22165-perils-of-self-feedback-self-bias-amplifies-in-large-language-models-wenda-xu-et-al-2024>(22/61 | 22/165) Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models (Wenda Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, William Yang Wang. (2024)<br><strong>Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models</strong><br><button class=copy-to-clipboard title="Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Mathematical Reasoning, Reasoning, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11436v1.pdf filename=2402.11436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies show that self-feedback improves <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on certain tasks while worsens other tasks. We discovered that such a contrary is due to <b>LLM&rsquo;s</b> bias towards their own output. In this paper, we formally define <b>LLM&rsquo;s</b> self-bias &ndash; the tendency to favor its own generation &ndash; using two statistics. We analyze six <b>LLMs</b> on translation, constrained <b>text</b> <b>generation,</b> and <b>mathematical</b> <b>reasoning</b> tasks. We find that self-bias is prevalent in all examined <b>LLMs</b> across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.</p></p class="citation"></blockquote><h3 id=2361--23165-dont-go-to-extremes-revealing-the-excessive-sensitivity-and-calibration-limitations-of-llms-in-implicit-hate-speech-detection-min-zhang-et-al-2024>(23/61 | 23/165) Don&rsquo;t Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection (Min Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Min Zhang, Jianfeng He, Taoran Ji, Chang-Tien Lu. (2024)<br><strong>Don&rsquo;t Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection</strong><br><button class=copy-to-clipboard title="Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fairness, Hate Speech Detection, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11406v1.pdf filename=2402.11406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>fairness</b> and trustworthiness of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are receiving increasing attention. Implicit <b>hate</b> <b>speech,</b> <b>which</b> employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which <b>LLMs</b> effectively address this issue remains insufficiently examined. This paper delves into the capability of <b>LLMs</b> to detect implicit <b>hate</b> <b>speech</b> <b>(Classification</b> Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various <b>prompt</b> patterns and mainstream uncertainty estimation methods. Our findings highlight that <b>LLMs</b> exhibit two extremes: (1) <b>LLMs</b> display excessive sensitivity towards groups or topics that may cause <b>fairness</b> issues, resulting in misclassifying benign statements as <b>hate</b> <b>speech.</b> <b>(2)</b> <b>LLMs&rsquo;</b> confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset&rsquo;s complexity. Consequently, the calibration performance is heavily reliant on primary classification accuracy. These discoveries unveil new limitations of <b>LLMs,</b> underscoring the need for caution when optimizing models to ensure they do not veer towards extremes. This serves as a reminder to carefully consider sensitivity and confidence in the pursuit of model <b>fairness.</b></p></p class="citation"></blockquote><h3 id=2461--24165-infuserki-enhancing-large-language-models-with-knowledge-graphs-via-infuser-guided-knowledge-integration-fali-wang-et-al-2024>(24/61 | 24/165) InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration (Fali Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, Haifeng Chen. (2024)<br><strong>InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration</strong><br><button class=copy-to-clipboard title="InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 48<br>Keywords: Graph, Fine-tuning, Knowledge Graph, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11441v1.pdf filename=2402.11441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Though <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown remarkable open-generation capabilities across diverse domains, they struggle with <b>knowledge-intensive</b> <b>tasks.</b> To alleviate this issue, <b>knowledge</b> <b>integration</b> methods have been proposed to enhance <b>LLMs</b> with domain-specific <b>knowledge</b> <b>graphs</b> using external modules. However, they suffer from data inefficiency as they require both known and unknown <b>knowledge</b> <b>for</b> <b>fine-tuning.</b> Thus, we study a novel problem of integrating unknown <b>knowledge</b> <b>into</b> <b>LLMs</b> efficiently without unnecessary overlap of known <b>knowledge.</b> <b>Injecting</b> new <b>knowledge</b> <b>poses</b> the risk of forgetting previously acquired <b>knowledge.</b> <b>To</b> tackle this, we propose a novel Infuser-Guided <b>Knowledge</b> <b>Integration</b> (InfuserKI) framework that utilizes <b>transformer</b> internal states to determine whether to enhance the original <b>LLM</b> output with additional information, thereby effectively mitigating <b>knowledge</b> <b>forgetting.</b> Evaluations on the UMLS-2.5k and MetaQA domain <b>knowledge</b> <b>graphs</b> demonstrate that InfuserKI can effectively acquire new <b>knowledge</b> <b>and</b> outperform state-of-the-art baselines by 9% and 6%, respectively, in reducing <b>knowledge</b> <b>forgetting.</b></p></p class="citation"></blockquote><h3 id=2561--25165-sciagent-tool-augmented-language-models-for-scientific-reasoning-yubo-ma-et-al-2024>(25/61 | 25/165) SciAgent: Tool-augmented Language Models for Scientific Reasoning (Yubo Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, Aixin Sun. (2024)<br><strong>SciAgent: Tool-augmented Language Models for Scientific Reasoning</strong><br><button class=copy-to-clipboard title="SciAgent: Tool-augmented Language Models for Scientific Reasoning" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, ChatGPT, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11451v1.pdf filename=2402.11451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientific <b>reasoning</b> poses an excessive challenge for even the most advanced <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> To make this task more practical and solvable for <b>LLMs,</b> we introduce a new task setting named tool-augmented scientific <b>reasoning.</b> This setting supplements <b>LLMs</b> with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a <b>benchmark,</b> SciToolBench, spanning five scientific domains to evaluate <b>LLMs&rsquo;</b> abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other <b>LLMs</b> with the same size by more than 13% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than <b>ChatGPT.</b></p></p class="citation"></blockquote><h3 id=2661--26165-can-deception-detection-go-deeper-dataset-evaluation-and-benchmark-for-deception-reasoning-kang-chen-et-al-2024>(26/61 | 26/165) Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning (Kang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kang Chen, Zheng Lian, Haiyang Sun, Bin Liu, Jianhua Tao. (2024)<br><strong>Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning</strong><br><button class=copy-to-clipboard title="Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, GPT, GPT-4, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11432v1.pdf filename=2402.11432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deception detection has attracted increasing attention due to its importance in many practical scenarios. Currently, data scarcity harms the development of this field. On the one hand, it is costly to hire participants to simulate deception scenarios. On the other hand, it is difficult to collect videos containing deceptive behaviors on the Internet. To address data scarcity, this paper proposes a new data collection pipeline. Specifically, we use <b>GPT-4</b> to simulate a role-play between a suspect and a police officer. During interrogation, the suspect lies to the police officer to evade responsibility for the crime, while the police officer uncovers the truth and gathers evidence. Compared with previous datasets, this strategy reduces data collection costs, providing a promising way to increase the dataset size. Meanwhile, we extend the traditional deception detection task to deception <b>reasoning,</b> further providing evidence for deceptive parts. This dataset can also be used to evaluate the complex <b>reasoning</b> capability of current <b>large</b> <b>language</b> <b>models</b> and serve as a <b>reasoning</b> <b>benchmark</b> for further research.</p></p class="citation"></blockquote><h3 id=2761--27165-morl-prompt-an-empirical-analysis-of-multi-objective-reinforcement-learning-for-discrete-prompt-optimization-yasaman-jafari-et-al-2024>(27/61 | 27/165) MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization (Yasaman Jafari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasaman Jafari, Dheeraj Mekala, Rose Yu, Taylor Berg-Kirkpatrick. (2024)<br><strong>MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization</strong><br><button class=copy-to-clipboard title="MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Neural Machine Translation, Style Transfer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11711v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11711v1.pdf filename=2402.11711v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>RL-based techniques can be used to search for <b>prompts</b> that when fed into a target language model maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another &ndash; for example, content preservation vs. <b>style</b> <b>matching</b> in <b>style</b> <b>transfer</b> tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to <b>prompts</b> that achieve balance across rewards &ndash; an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete <b>prompt</b> optimization &ndash; two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously. We conduct an empirical analysis of these methods on two NLP tasks: <b>style</b> <b>transfer</b> and <b>machine</b> <b>translation,</b> each using three competing reward functions. Our experiments demonstrate that multi-objective methods that directly optimize volume perform better and achieve a better balance of all rewards than those that attempt to find monotonic update directions.</p></p class="citation"></blockquote><h3 id=2861--28165-autocorrect-for-estonian-texts-final-report-from-project-ektb25-agnes-luhtaru-et-al-2024>(28/61 | 28/165) Autocorrect for Estonian texts: final report from project EKTB25 (Agnes Luhtaru et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Agnes Luhtaru, Martin Vainikko, Krista Liin, Kais Allkivi-Metsoja, Jaagup Kippar, Pille Eslon, Mark Fishel. (2024)<br><strong>Autocorrect for Estonian texts: final report from project EKTB25</strong><br><button class=copy-to-clipboard title="Autocorrect for Estonian texts: final report from project EKTB25" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Automatic Evaluation, Transfer Learning, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11671v1.pdf filename=2402.11671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The project was funded in 2021-2023 by the National Programme of Estonian Language Technology. Its main aim was to develop spelling and grammar correction tools for the Estonian language. The main challenge was the very small amount of available error correction data needed for such development. To mitigate this, (1) we annotated more correction data for model training and testing, (2) we tested <b>transfer-learning,</b> <b>i.e.</b> retraining machine learning models created for other tasks, so as not to depend solely on correction data, (3) we compared the developed method and model with alternatives, including <b>large</b> <b>language</b> <b>models.</b> We also developed <b>automatic</b> <b>evaluation,</b> which can calculate the accuracy and yield of corrections by error category, so that the effectiveness of different methods can be compared in detail. There has been a breakthrough in <b>large</b> <b>language</b> <b>models</b> during the project: <b>GPT4,</b> a commercial language model with Estonian-language support, has been created. We took into account the existence of the model when adjusting plans and in the report we present a comparison with the ability of <b>GPT4</b> to improve the Estonian language text. The final results show that the approach we have developed provides better scores than <b>GPT4</b> and the result is usable but not entirely reliable yet. The report also contains ideas on how <b>GPT4</b> and other major language models can be implemented in the future, focusing on open-source solutions. All results of this project are open-data/open-source, with licenses that allow them to be used for purposes including commercial ones.</p></p class="citation"></blockquote><h3 id=2961--29165-self-seeding-and-multi-intent-self-instructing-llms-for-generating-intent-aware-information-seeking-dialogs-arian-askari-et-al-2024>(29/61 | 29/165) Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs (Arian Askari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arian Askari, Roxana Petcu, Chuan Meng, Mohammad Aliannejadi, Amin Abolghasemi, Evangelos Kanoulas, Suzan Verberne. (2024)<br><strong>Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs</strong><br><button class=copy-to-clipboard title="Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11633v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11633v1.pdf filename=2402.11633v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying user intents in information-seeking dialogs is crucial for a system to meet user&rsquo;s information needs. Intent prediction (IP) is challenging and demands sufficient dialogs with human-labeled intents for training. However, manually annotating intents is resource-intensive. While <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been shown to be effective in generating synthetic data, there is no study on using <b>LLMs</b> to generate intent-aware information-seeking dialogs. In this paper, we focus on leveraging <b>LLMs</b> for <b>zero-shot</b> generation of <b>large-scale,</b> <b>open-domain,</b> <b>and</b> intent-aware information-seeking dialogs. We propose SOLID, which has novel self-seeding and multi-intent self-instructing schemes. The former improves the generation quality by using the <b>LLM&rsquo;s</b> own knowledge scope to initiate dialog generation; the latter <b>prompts</b> the <b>LLM</b> to generate utterances sequentially, and mitigates the need for manual <b>prompt</b> design by asking the <b>LLM</b> to autonomously adapt its <b>prompt</b> instruction when generating complex multi-intent utterances. Furthermore, we propose SOLID-RL, which is further trained to generate a dialog in one step on the data generated by SOLID. We propose a length-based quality estimation mechanism to assign varying weights to SOLID-generated dialogs based on their quality during the training process of SOLID-RL. We use SOLID and SOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size of existing datasets. Experiments show that IP methods trained on dialogs generated by SOLID and SOLID-RL achieve better IP quality than ones trained on human-generated dialogs.</p></p class="citation"></blockquote><h3 id=3061--30165-metacognitive-retrieval-augmented-large-language-models-yujia-zhou-et-al-2024>(30/61 | 30/165) Metacognitive Retrieval-Augmented Large Language Models (Yujia Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, Zhicheng Dou. (2024)<br><strong>Metacognitive Retrieval-Augmented Large Language Models</strong><br><button class=copy-to-clipboard title="Metacognitive Retrieval-Augmented Large Language Models" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 40<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11626v1.pdf filename=2402.11626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-augmented</b> <b>generation</b> <b>have</b> become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time <b>retrieval,</b> <b>more</b> <b>recent</b> approaches have shifted towards multi-time <b>retrieval</b> <b>for</b> <b>multi-hop</b> <b>reasoning</b> tasks. However, these strategies are bound by predefined <b>reasoning</b> steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the <b>retrieval-augmented</b> <b>generation</b> <b>process</b> with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective <b>reasoning</b> abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes them. Empirical evaluations show that MetaRAG significantly outperforms existing methods.</p></p class="citation"></blockquote><h3 id=3161--31165-preact-predicting-future-in-react-enhances-agents-planning-ability-dayuan-fu-et-al-2024>(31/61 | 31/165) PreAct: Predicting Future in ReAct Enhances Agent&rsquo;s Planning Ability (Dayuan Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dayuan Fu, Jianzhao Huang, Siyuan Lu, Guanting Dong, Yejie Wang, Keqing He, Weiran Xu. (2024)<br><strong>PreAct: Predicting Future in ReAct Enhances Agent&rsquo;s Planning Ability</strong><br><button class=copy-to-clipboard title="PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11534v1.pdf filename=2402.11534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating <b>reasoning</b> in the correct direction. In this paper, we introduce $\textbf{PreAct}$, an agent framework that integrates $\textbf{pre}$diction with $\textbf{rea}$soning and $\textbf{act}$ion. Leveraging the information provided by predictions, a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> based agent can offer more diversified and strategically oriented <b>reasoning,</b> which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We <b>prompt</b> the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on <b>LLM</b> planning. The differences in single-step <b>reasoning</b> between PreAct and ReAct show that PreAct indeed offers advantages in terms of diversity and strategic directivity over ReAct.</p></p class="citation"></blockquote><h3 id=3261--32165-advancing-translation-preference-modeling-with-rlhf-a-step-towards-cost-effective-solution-nuo-xu-et-al-2024>(32/61 | 32/165) Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution (Nuo Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nuo Xu, Jun Zhao, Can Zu, Tao Gui, Qi Zhang, Xuanjing Huang. (2024)<br><strong>Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution</strong><br><button class=copy-to-clipboard title="Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Low-Resource, Reinforcement Learning, Reinforcement Learning from Human Feedback, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11525v1.pdf filename=2402.11525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Faithfulness, expressiveness, and elegance is the constant pursuit in <b>machine</b> <b>translation.</b> However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging <b>reinforcement</b> <b>learning</b> with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for <b>low-resource</b> languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and <b>machine</b> <b>translations.</b> In this manner, the reward model learns the deficiencies of <b>machine</b> <b>translation</b> compared to human and guides subsequent improvements in <b>machine</b> <b>translation.</b> Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions not trained with \textit{RLHF}. Further analysis indicates that the model&rsquo;s language capabilities play a crucial role in preference learning. A reward model with strong language capabilities can more sensitively learn the subtle differences in translation quality and align better with real human translation preferences.</p></p class="citation"></blockquote><h3 id=3361--33165-knowledge-to-sql-enhancing-sql-generation-with-data-expert-llm-zijin-hong-et-al-2024>(33/61 | 33/165) Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM (Zijin Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijin Hong, Zheng Yuan, Hao Chen, Qinggang Zhang, Feiran Huang, Xiao Huang. (2024)<br><strong>Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM</strong><br><button class=copy-to-clipboard title="Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Reinforcement Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11517v1.pdf filename=2402.11517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating accurate SQL for user queries (text-to-SQL) is a long-standing problem since the generation of the SQL requires comprehending the query and database and retrivale the accurate data from the database accordingly. Existing models rely on the comprehensive ability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to generate the SQL according to the database schema. However, there is some necessary knowledge that is not explicitly included in the database schema or has been learned by <b>LLMs.</b> Thus, the generated SQL of the knowledge-insufficient queries may be inaccurate, which negatively impacts the robustness of the text-to-SQL models. To deal with this situation, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert <b>LLM</b> (DELLM) to provide helpful knowledge for all types of text-to-SQL models. Specifically, we provide the detailed design of DELLM, in terms of table reading, and the basic <b>fine-tuning</b> process. We further provide a <b>Reinforcement</b> <b>Learning</b> via Database Feedback (RLDBF) training strategy to guide the DELLM to generate more helpful knowledge for <b>LLMs.</b> Extensive experiments verify DELLM can enhance the state-of-the-art <b>LLMs</b> on text-to-SQL tasks. The model structure and the parameter weight of DELLM are released for further research.</p></p class="citation"></blockquote><h3 id=3461--34165-federated-fine-tuning-of-large-language-models-under-heterogeneous-language-tasks-and-client-resources-jiamu-bai-et-al-2024>(34/61 | 34/165) Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources (Jiamu Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, Yaliang Li. (2024)<br><strong>Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources</strong><br><button class=copy-to-clipboard title="Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Federated Learning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11505v1.pdf filename=2402.11505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) has recently been applied to the parameter-efficient <b>fine-tuning</b> of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for <b>LLM</b> <b>fine-tuning,</b> which mitigates the &ldquo;buckets effect&rdquo; in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, our experiments validate the efficacy of FlexLoRA, with the <b>federated</b> <b>global</b> model achieving up to a 3.1% average improvement in downstream NLP task performance. FlexLoRA&rsquo;s practicality is further underscored by its seamless integration with existing LoRA-based FL methods and theoretical analysis, offering a path toward scalable, privacy-preserving <b>federated</b> <b>tuning</b> for <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=3561--35165-loretta-low-rank-economic-tensor-train-adaptation-for-ultra-low-parameter-fine-tuning-of-large-language-models-yifan-yang-et-al-2024>(35/61 | 35/165) LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models (Yifan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Yang, Jiajun Zhou, Ngai Wong, Zheng Zhang. (2024)<br><strong>LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models</strong><br><button class=copy-to-clipboard title="LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11417v1.pdf filename=2402.11417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Various parameter-efficient <b>fine-tuning</b> (PEFT) techniques have been proposed to enable computationally efficient <b>fine-tuning</b> while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named {LoRETTA}$<em>{adp}$ and {LoRETTA}$</em>{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the <b>fine-tuning</b> of <b>LLMs.</b> The latter emphasizes <b>fine-tuning</b> via weight parameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\times$ fewer parameters on the <b>LLaMA-2-7B</b> models. Furthermore, empirical results demonstrate that the proposed method effectively improves training efficiency, enjoys better multi-task learning performance, and enhances the anti-overfitting capability. Plug-and-play codes built upon the Huggingface framework and PEFT library will be released.</p></p class="citation"></blockquote><h3 id=3661--36165-allava-harnessing-gpt4v-synthesized-data-for-a-lite-vision-language-model-guiming-hardy-chen-et-al-2024>(36/61 | 36/165) ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model (Guiming Hardy Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, Benyou Wang. (2024)<br><strong>ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model</strong><br><button class=copy-to-clipboard title="ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11684v1.pdf filename=2402.11684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in Large <b>Vision-Language</b> Models (LVLMs) have enabled processing of <b>multimodal</b> inputs in language models but require significant computational resources for deployment, especially in edge devices. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To do this, a synthetic dataset is created by leveraging <b>GPT-4V&rsquo;s</b> ability to generate detailed captions, complex <b>reasoning</b> instructions and detailed answers from images. The resulted model trained with our data, ALLaVA, achieves competitive performance on 12 <b>benchmarks</b> up to 3B LVLMs. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. Our online demo is available at \url{https://allava.freedomai.cn}.</p></p class="citation"></blockquote><h3 id=3761--37165-benchmarking-knowledge-boundary-for-large-language-model-a-different-perspective-on-model-evaluation-xunjian-yin-et-al-2024>(37/61 | 37/165) Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation (Xunjian Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xunjian Yin, Xu Zhang, Jie Ruan, Xiaojun Wan. (2024)<br><strong>Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation</strong><br><button class=copy-to-clipboard title="Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Question Answering, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11493v1.pdf filename=2402.11493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, substantial advancements have been made in the development of <b>large</b> <b>language</b> <b>models,</b> achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of <b>benchmarks</b> based on <b>question-answering</b> <b>pairs.</b> We argue that it is not reliable and comprehensive to evaluate language models with a fixed <b>question</b> <b>or</b> limited paraphrases as the query, since language models are sensitive to <b>prompt.</b> Therefore, we introduce a novel concept named knowledge boundary to encompass both <b>prompt-agnostic</b> and <b>prompt-sensitive</b> knowledge within language models. Knowledge boundary avoids <b>prompt</b> sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal <b>prompt</b> for each piece of knowledge. Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods. Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.</p></p class="citation"></blockquote><h3 id=3861--38165-question-answering-over-spatio-temporal-knowledge-graph-xinbang-dai-et-al-2024>(38/61 | 38/165) Question Answering Over Spatio-Temporal Knowledge Graph (Xinbang Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinbang Dai, Huiying Li, Guilin Qi. (2024)<br><strong>Question Answering Over Spatio-Temporal Knowledge Graph</strong><br><button class=copy-to-clipboard title="Question Answering Over Spatio-Temporal Knowledge Graph" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-4; I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11542v1.pdf filename=2402.11542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatio-temporal <b>knowledge</b> <b>graphs</b> (STKGs) extend the concept of <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> by incorporating time and location information. While the research community&rsquo;s focus on <b>Knowledge</b> <b>Graph</b> <b>Question</b> <b>Answering</b> (KGQA), the field of answering <b>questions</b> <b>incorporating</b> both spatio-temporal information based on STKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets also has hindered progress in this area. To address this issue, we present STQAD, a dataset comprising 10,000 natural language <b>questions</b> <b>for</b> spatio-temporal <b>knowledge</b> <b>graph</b> <b>question</b> <b>answering</b> (STKGQA). Unfortunately, various state-of-the-art KGQA approaches fall far short of achieving satisfactory performance on our dataset. In response, we propose STCQA, a new spatio-temporal KGQA approach that utilizes a novel STKG embedding method named STComplEx. By extracting temporal and spatial information from a <b>question,</b> <b>our</b> <b>QA</b> model can better comprehend the <b>question</b> <b>and</b> retrieve accurate answers from the STKG. Through extensive experiments, we demonstrate the quality of our dataset and the effectiveness of our STKGQA method.</p></p class="citation"></blockquote><h3 id=3961--39165-deciphering-the-lmpact-of-pretraining-data-on-large-language-models-through-machine-unlearning-yang-zhao-et-al-2024>(39/61 | 39/165) Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning (Yang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Zhouhao Sun, Jun Shi, Ting Liu, Bing Qin. (2024)<br><strong>Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning</strong><br><button class=copy-to-clipboard title="Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Machine Unlearning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11537v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11537v1.pdf filename=2402.11537v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Through pretraining on a corpus with various sources, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of <b>LLMs</b> and measure their impacts on <b>LLMs</b> using <b>benchmarks</b> about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of <b>LLMs,</b> along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data&rsquo;&rsquo; such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to support more efficient pretraining of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=4061--40165-how-susceptible-are-large-language-models-to-ideological-manipulation-kai-chen-et-al-2024>(40/61 | 40/165) How Susceptible are Large Language Models to Ideological Manipulation? (Kai Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Chen, Zihao He, Jun Yan, Taiwei Shi, Kristina Lerman. (2024)<br><strong>How Susceptible are Large Language Models to Ideological Manipulation?</strong><br><button class=copy-to-clipboard title="How Susceptible are Large Language Models to Ideological Manipulation?" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs-CY, cs.CL<br>Keyword Score: 30<br>Keywords: Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11725v1.pdf filename=2402.11725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively <b>LLMs</b> can learn and generalize ideological biases from their <b>instruction-tuning</b> <b>data.</b> Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of <b>LLMs.</b> Notably, <b>LLMs</b> demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which <b>LLMs&rsquo;</b> ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=4161--41165-a-multi-aspect-framework-for-counter-narrative-evaluation-using-large-language-models-jaylen-jones-et-al-2024>(41/61 | 41/165) A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models (Jaylen Jones et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaylen Jones, Lingbo Mo, Eric Fosler-Lussier, Huan Sun. (2024)<br><strong>A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models</strong><br><button class=copy-to-clipboard title="A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11676v1.pdf filename=2402.11676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy. While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped. Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria. To address prior evaluation limitations, we propose a novel evaluation framework <b>prompting</b> <b>LLMs</b> to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs. We found that <b>LLM</b> evaluators achieve strong alignment to human-annotated scores and feedback and outperform alternative metrics, indicating their potential as multi-aspect, reference-free and interpretable evaluators for counter narrative evaluation.</p></p class="citation"></blockquote><h3 id=4261--42165-competition-of-mechanisms-tracing-how-language-models-handle-facts-and-counterfactuals-francesco-ortu-et-al-2024>(42/61 | 42/165) Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals (Francesco Ortu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Ortu, Zhijing Jin, Diego Doimo, Mrinmaya Sachan, Alberto Cazzaniga, Bernhard Schölkopf. (2024)<br><strong>Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals</strong><br><button class=copy-to-clipboard title="Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Counter-factual, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11655v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11655v1.pdf filename=2402.11655v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interpretability research aims to bridge the gap between the empirical success and our scientific understanding of the inner workings of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, most existing research in this area focused on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose the formulation of competition of mechanisms, which instead of individual mechanisms focuses on the interplay of multiple mechanisms, and traces how one of them becomes dominant in the final prediction. We uncover how and where the competition of mechanisms happens within <b>LLMs</b> using two interpretability methods, logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components, and reveal attention positions that effectively control the strength of certain mechanisms. Our code and data are at <a href=https://github.com/francescortu/Competition_of_Mechanisms>https://github.com/francescortu/Competition_of_Mechanisms</a>.</p></p class="citation"></blockquote><h3 id=4361--43165-stumbling-blocks-stress-testing-the-robustness-of-machine-generated-text-detectors-under-attacks-yichen-wang-et-al-2024>(43/61 | 43/165) Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks (Yichen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichen Wang, Shangbin Feng, Abe Bohan Hou, Xiao Pu, Chao Shen, Xiaoming Liu, Yulia Tsvetkov, Tianxing He. (2024)<br><strong>Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks</strong><br><button class=copy-to-clipboard title="Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11638v1.pdf filename=2402.11638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread use of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors&rsquo; robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, <b>prompting,</b> and co-generating. Our attacks assume limited access to the generator <b>LLMs,</b> and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches to improve robustness.</p></p class="citation"></blockquote><h3 id=4461--44165-unveiling-the-secrets-of-engaging-conversations-factors-that-keep-users-hooked-on-role-playing-dialog-agents-shuai-zhang-et-al-2024>(44/61 | 44/165) Unveiling the Secrets of Engaging Conversations: Factors that Keep Users Hooked on Role-Playing Dialog Agents (Shuai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Zhang, Yu Lu, Junwen Liu, Jia Yu, Huachuan Qiu, Yuming Yan, Zhenzhong Lan. (2024)<br><strong>Unveiling the Secrets of Engaging Conversations: Factors that Keep Users Hooked on Role-Playing Dialog Agents</strong><br><button class=copy-to-clipboard title="Unveiling the Secrets of Engaging Conversations: Factors that Keep Users Hooked on Role-Playing Dialog Agents" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11522v1.pdf filename=2402.11522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the growing humanlike nature of dialog agents, people are now engaging in extended conversations that can stretch from brief moments to substantial periods of time. Understanding the factors that contribute to sustaining these interactions is crucial, yet existing studies primarily focusing on short-term <b>simulations</b> that rarely explore such prolonged and real conversations. In this paper, we investigate the factors influencing retention rates in real interactions with roleplaying models. By analyzing a <b>large</b> <b>dataset</b> <b>of</b> interactions between real users and thousands of characters, we systematically examine multiple factors and assess their impact on user retention rate. Surprisingly, we find that the degree to which the bot embodies the roles it plays has limited influence on retention rates, while the length of each turn it speaks significantly affects retention rates. This study sheds light on the critical aspects of user engagement with role-playing models and provides valuable insights for future improvements in the development of <b>large</b> <b>language</b> <b>models</b> for role-playing purposes.</p></p class="citation"></blockquote><h3 id=4561--45165-when-do-llms-need-retrieval-augmentation-mitigating-llms-overconfidence-helps-retrieval-augmentation-shiyu-ni-et-al-2024>(45/61 | 45/165) When Do LLMs Need Retrieval Augmentation? Mitigating LLMs&rsquo; Overconfidence Helps Retrieval Augmentation (Shiyu Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyu Ni, Keping Bi, Jiafeng Guo, Xueqi Cheng. (2024)<br><strong>When Do LLMs Need Retrieval Augmentation? Mitigating LLMs&rsquo; Overconfidence Helps Retrieval Augmentation</strong><br><button class=copy-to-clipboard title="When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11457v1.pdf filename=2402.11457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. <b>Retrieval</b> <b>Augmentation</b> (RA) has been extensively studied to mitigate <b>LLMs&rsquo;</b> hallucinations. However, due to the extra overhead and unassured quality of <b>retrieval,</b> <b>it</b> may not be optimal to conduct RA all the time. A straightforward idea is to only conduct <b>retrieval</b> <b>when</b> <b>LLMs</b> are uncertain about a question. This motivates us to enhance the <b>LLMs&rsquo;</b> ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure <b>LLMs&rsquo;</b> such ability and confirm their overconfidence. Then, we study how <b>LLMs&rsquo;</b> certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance <b>LLMs&rsquo;</b> perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, <b>LLMs</b> can achieve comparable or even better performance of RA with much fewer <b>retrieval</b> <b>calls.</b></p></p class="citation"></blockquote><h3 id=4661--46165-rethinking-the-roles-of-large-language-models-in-chinese-grammatical-error-correction-yinghui-li-et-al-2024>(46/61 | 46/165) Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction (Yinghui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinghui Li, Shang Qin, Jingheng Ye, Shirong Ma, Yangning Li, Libo Qin, Xuming Hu, Wenhao Jiang, Hai-Tao Zheng, Philip S. Yu. (2024)<br><strong>Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction</strong><br><button class=copy-to-clipboard title="Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Grammatical Error Correction, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11420v1.pdf filename=2402.11420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have been widely studied by researchers for their roles in various downstream NLP tasks. As a fundamental task in the NLP field, Chinese <b>Grammatical</b> <b>Error</b> <b>Correction</b> (CGEC) aims to correct all potential <b>grammatical</b> <b>errors</b> <b>in</b> the input sentences. Previous studies have shown that <b>LLMs&rsquo;</b> performance as correctors on CGEC remains unsatisfactory due to its challenging task focus. To promote the CGEC field to better adapt to the era of <b>LLMs,</b> we rethink the roles of <b>LLMs</b> in the CGEC task so that they can be better utilized and explored in CGEC. Considering the rich <b>grammatical</b> <b>knowledge</b> <b>stored</b> in <b>LLMs</b> and their powerful semantic understanding capabilities, we utilize <b>LLMs</b> as explainers to provide explanation information for the CGEC small models during error correction to enhance performance. We also use <b>LLMs</b> as evaluators to bring more reasonable CGEC evaluations, thus alleviating the troubles caused by the subjectivity of the CGEC task. In particular, our work is also an active exploration of how <b>LLMs</b> and small models better collaborate in downstream tasks. Extensive experiments and detailed analyses on widely used datasets verify the effectiveness of our thinking intuition and the proposed methods.</p></p class="citation"></blockquote><h3 id=4761--47165-opening-the-black-box-of-language-acquisition-jérôme-michaud-et-al-2024>(47/61 | 47/165) Opening the black box of language acquisition (Jérôme Michaud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jérôme Michaud, Anna Jon-and. (2024)<br><strong>Opening the black box of language acquisition</strong><br><button class=copy-to-clipboard title="Opening the black box of language acquisition" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-NA, cs.CL, math-NA<br>Keyword Score: 25<br>Keywords: Black Box, Reinforcement Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11681v1.pdf filename=2402.11681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>large</b> <b>language</b> <b>models</b> using deep learning techniques have renewed interest on how languages can be learned from data. However, it is unclear whether or how these models represent grammatical information from the learned languages. In addition, the models must be pre-trained on <b>large</b> <b>corpora</b> <b>before</b> they can be used. In this work, we propose an alternative, more transparent and cognitively plausible architecture for learning language. Instead of using deep learning, our approach uses a minimal cognitive architecture based on sequence memory and chunking. The learning mechanism is based on the principles of <b>reinforcement</b> <b>learning.</b> We test our architecture on a number of natural-like toy languages. Results show that the model can learn these artificial languages from scratch and extract grammatical information that supports learning. Our study demonstrates the power of this simple architecture and stresses the importance of sequence memory as a key component of the language learning process. Since other animals do not seem to have a faithful sequence memory, this may explain why only humans have developed complex languages.</p></p class="citation"></blockquote><h3 id=4861--48165-benchmark-self-evolving-a-multi-agent-framework-for-dynamic-llm-evaluation-siyuan-wang-et-al-2024>(48/61 | 48/165) Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation (Siyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, Xuanjing Huang. (2024)<br><strong>Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation</strong><br><button class=copy-to-clipboard title="Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11443v1.pdf filename=2402.11443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a <b>benchmark</b> self-evolving framework to dynamically evaluate rapidly advancing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> aiming for a more accurate assessment of their capabilities and limitations. We utilize a multi-agent system to manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extend existing <b>benchmarks.</b> Towards a more scalable, robust and fine-grained evaluation, we implement six reframing operations to construct evolving instances testing <b>LLMs</b> against diverse queries, data noise and probing their problem-solving sub-abilities. With this framework, we extend <b>benchmark</b> datasets of four tasks. Experimental results show a general performance decline in most <b>LLMs</b> against their original results. This decline under our scalable and robust evaluations, alongside our fine-grained evaluation, more accurately reflect models&rsquo; capabilities. Besides, our framework widens performance discrepancies both between different models and within the same model across various tasks, facilitating more informed model selection for specific tasks (Code and data are available at <a href=https://github.com/NanshineLoong/Self-Evolving-Benchmark)>https://github.com/NanshineLoong/Self-Evolving-Benchmark)</a>.</p></p class="citation"></blockquote><h3 id=4961--49165-specrawler-generating-openapi-specifications-from-api-documentation-using-large-language-models-koren-lazar-et-al-2024>(49/61 | 49/165) SpeCrawler: Generating OpenAPI Specifications from API Documentation Using Large Language Models (Koren Lazar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Koren Lazar, Matan Vetzler, Guy Uziel, David Boaz, Esther Goldbraich, David Amid, Ateret Anaby-Tavor. (2024)<br><strong>SpeCrawler: Generating OpenAPI Specifications from API Documentation Using Large Language Models</strong><br><button class=copy-to-clipboard title="SpeCrawler: Generating OpenAPI Specifications from API Documentation Using Large Language Models" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11625v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11625v1.pdf filename=2402.11625v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the digital era, the widespread use of APIs is evident. However, scalable utilization of APIs poses a challenge due to structure divergence observed in online API documentation. This underscores the need for automatic tools to facilitate API consumption. A viable approach involves the conversion of documentation into an API Specification format. While previous attempts have been made using rule-based methods, these approaches encountered difficulties in generalizing across diverse documentation. In this paper we introduce SpeCrawler, a comprehensive system that utilizes <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to generate OpenAPI Specifications from diverse API documentation through a carefully crafted pipeline. By creating a standardized format for numerous APIs, SpeCrawler aids in streamlining integration processes within API orchestrating systems and facilitating the incorporation of tools into <b>LLMs.</b> The paper explores SpeCrawler&rsquo;s methodology, supported by empirical evidence and case studies, demonstrating its efficacy through <b>LLM</b> capabilities.</p></p class="citation"></blockquote><h3 id=5061--50165-extensible-embedding-a-flexible-multipler-for-llms-context-length-ninglu-shao-et-al-2024>(50/61 | 50/165) Extensible Embedding: A Flexible Multipler For LLM&rsquo;s Context Length (Ninglu Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ninglu Shao, Shitao Xiao, Zheng Liu, Peitian Zhang. (2024)<br><strong>Extensible Embedding: A Flexible Multipler For LLM&rsquo;s Context Length</strong><br><button class=copy-to-clipboard title="Extensible Embedding: A Flexible Multipler For LLM's Context Length" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11577v1.pdf filename=2402.11577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we propose Extensible Embedding, which realizes high-quality extension of <b>LLM&rsquo;s</b> context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the <b>LLM</b> can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way. 3) Superior compatibility with the existing <b>LLMs,</b> where the extensible embedding can be seamlessly introduced as a plug-in component. Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the <b>LLM&rsquo;s</b> context.</p></p class="citation"></blockquote><h3 id=5161--51165-bge-landmark-embedding-a-chunking-free-embedding-method-for-retrieval-augmented-long-context-large-language-models-kun-luo-et-al-2024>(51/61 | 51/165) BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models (Kun Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Luo, Zheng Liu, Shitao Xiao, Kang Liu. (2024)<br><strong>BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models</strong><br><button class=copy-to-clipboard title="BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11573v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11573v1.pdf filename=2402.11573v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we proposeExtensible Embedding, which realizes high-quality extension of <b>LLM&rsquo;s</b> context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the <b>LLM</b> can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way. 3) Superior compatibility with the existing <b>LLMs,</b> where the extensible embedding can be seamlessly introduced as a plug-in component. Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the <b>LLM&rsquo;s</b> context.</p></p class="citation"></blockquote><h3 id=5261--52165-from-prejudice-to-parity-a-new-approach-to-debiasing-large-language-model-word-embeddings-aishik-rakshit-et-al-2024>(52/61 | 52/165) From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings (Aishik Rakshit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, Aman Chadha. (2024)<br><strong>From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings</strong><br><button class=copy-to-clipboard title="From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11512v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11512v2.pdf filename=2402.11512v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Embeddings play a pivotal role in the efficacy of <b>Large</b> <b>Language</b> <b>Models.</b> They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform &lsquo;soft debiasing&rsquo;. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.</p></p class="citation"></blockquote><h3 id=5361--53165-whats-the-plan-evaluating-and-developing-planning-aware-techniques-for-llms-eran-hirsch-et-al-2024>(53/61 | 53/165) What&rsquo;s the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs (Eran Hirsch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eran Hirsch, Guy Uziel, Ateret Anaby-Tavor. (2024)<br><strong>What&rsquo;s the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs</strong><br><button class=copy-to-clipboard title="What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11489v1.pdf filename=2402.11489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are increasingly used for applications that require planning capabilities, such as web or embodied agents. In line with recent studies, we demonstrate through experimentation that <b>LLMs</b> lack necessary skills required for planning. Based on these observations, we advocate for the potential of a hybrid approach that combines <b>LLMs</b> with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing <b>LLM-based</b> planners.</p></p class="citation"></blockquote><h3 id=5461--54165-lora-flow-dynamic-lora-fusion-for-large-language-models-in-generative-tasks-hanqing-wang-et-al-2024>(54/61 | 54/165) LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks (Hanqing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanqing Wang, Bowen Ping, Shuo Wang, Xu Han, Yun Chen, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks</strong><br><button class=copy-to-clipboard title="LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11455v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11455v1.pdf filename=2402.11455v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>LoRA employs lightweight modules to customize <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for each downstream task or domain, where different learned additional modules represent diverse skills. Combining existing LoRAs to address new tasks can enhance the reusability of learned LoRAs, particularly beneficial for tasks with limited annotated data. Most prior works on LoRA combination primarily rely on task-level weights for each involved LoRA, making different examples and tokens share the same LoRA weights. However, in generative tasks, different tokens may necessitate diverse skills to manage. Taking the Chinese math task as an example, understanding the problem description may depend more on the Chinese LoRA, while the calculation part may rely more on the math LoRA. To this end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the impact of different LoRAs. The weights at each step are determined by a fusion gate with extremely few parameters, which can be learned with only 200 training examples. Experiments across six generative tasks demonstrate that our method consistently outperforms baselines with task-level fusion weights. This underscores the necessity of introducing dynamic fusion weights for LoRA combination.</p></p class="citation"></blockquote><h3 id=5561--55165-mitigating-catastrophic-forgetting-in-multi-domain-chinese-spelling-correction-by-multi-stage-knowledge-transfer-framework-peng-xing-et-al-2024>(55/61 | 55/165) Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling Correction by Multi-stage Knowledge Transfer Framework (Peng Xing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Xing, Yinghui Li, Shirong Ma, Xinnian Liang, Haojing Huang, Yangning Li, Hai-Tao Zheng, Wenhao Jiang, Ying Shen. (2024)<br><strong>Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling Correction by Multi-stage Knowledge Transfer Framework</strong><br><button class=copy-to-clipboard title="Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling Correction by Multi-stage Knowledge Transfer Framework" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Continual Learning, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11422v1.pdf filename=2402.11422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chinese Spelling Correction (CSC) aims to detect and correct spelling errors in given sentences. Recently, multi-domain CSC has gradually attracted the attention of researchers because it is more practicable. In this paper, we focus on the key flaw of the CSC model when adapting to multi-domain scenarios: the tendency to forget previously acquired <b>knowledge</b> <b>upon</b> learning new domain-specific <b>knowledge</b> <b>(i.e.,</b> catastrophic forgetting). To address this, we propose a novel model-agnostic Multi-stage <b>Knowledge</b> <b>Transfer</b> (MKT) framework, which utilizes a continuously evolving teacher model for <b>knowledge</b> <b>transfer</b> in each domain, rather than focusing solely on new domain <b>knowledge.</b> <b>It</b> deserves to be mentioned that we are the first to apply <b>continual</b> <b>learning</b> methods to the multi-domain CSC task. Experiments prove the effectiveness of our proposed method, and further analyses demonstrate the importance of overcoming catastrophic forgetting for improving the model performance.</p></p class="citation"></blockquote><h3 id=5661--56165-cobra-effect-in-reference-free-image-captioning-metrics-zheng-ma-et-al-2024>(56/61 | 56/165) Cobra Effect in Reference-Free Image Captioning Metrics (Zheng Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Ma, Changxin Wang, Yawen Ouyang, Fei Zhao, Jianbing Zhang, Shujian Huang, Jiajun Chen. (2024)<br><strong>Cobra Effect in Reference-Free Image Captioning Metrics</strong><br><button class=copy-to-clipboard title="Cobra Effect in Reference-Free Image Captioning Metrics" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 16<br>Keywords: Benchmarking, Multi-modal, GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11572v1.pdf filename=2402.11572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating the compatibility between textual descriptions and corresponding images represents a core endeavor within <b>multi-modal</b> research. In recent years, a proliferation of reference-free methods, leveraging visual-language pre-trained models (VLMs), has emerged. Empirical evidence has substantiated that these innovative approaches exhibit a higher correlation with human judgment, marking a significant advancement in the field. However, does a higher correlation with human evaluations alone sufficiently denote the complete of a metric? In response to this question, in this paper, we study if there are any deficiencies in reference-free metrics. Specifically, inspired by the Cobra Effect, we utilize metric scores as rewards to direct the captioning model toward generating descriptions that closely align with the metric&rsquo;s criteria. If a certain metric has flaws, it will be exploited by the model and reflected in the generated sentences. Our findings reveal that descriptions guided by these metrics contain significant flaws, e.g. incoherent statements and excessive repetition. Subsequently, we propose a novel method termed Self-Improving to rectify the identified shortcomings within these metrics. We employ <b>GPT-4V</b> as an evaluative tool to assess generated sentences and the result reveals that our approach achieves state-of-the-art (SOTA) performance. In addition, we also introduce a challenging evaluation <b>benchmark</b> called Flaws Caption to evaluate reference-free image captioning metrics comprehensively. Our code is available at <a href=https://github.com/aaronma2020/robust_captioning_metric>https://github.com/aaronma2020/robust_captioning_metric</a></p></p class="citation"></blockquote><h3 id=5761--57165-fine-grained-and-explainable-factuality-evaluation-for-multimodal-summarization-liqiang-jing-et-al-2024>(57/61 | 57/165) Fine-grained and Explainable Factuality Evaluation for Multimodal Summarization (Liqiang Jing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liqiang Jing, Jingxuan Zuo, Yue Zhang. (2024)<br><strong>Fine-grained and Explainable Factuality Evaluation for Multimodal Summarization</strong><br><button class=copy-to-clipboard title="Fine-grained and Explainable Factuality Evaluation for Multimodal Summarization" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11414v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11414v1.pdf filename=2402.11414v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>summarization</b> aims to generate a concise summary based on the input text and image. However, the existing methods potentially suffer from unfactual output. To evaluate the factuality of <b>multimodal</b> <b>summarization</b> models, we propose two fine-grained and explainable evaluation frameworks (FALLACIOUS) for different application scenarios, i.e. reference-based factuality evaluation framework and reference-free factuality evaluation framework. Notably, the reference-free factuality evaluation framework doesn&rsquo;t need ground truth and hence it has a wider application scenario. To evaluate the effectiveness of the proposed frameworks, we compute the correlation between our frameworks and the other metrics. The experimental results show the effectiveness of our proposed method. We will release our code and dataset via github.</p></p class="citation"></blockquote><h3 id=5861--58165-numerical-claim-detection-in-finance-a-new-financial-dataset-weak-supervision-model-and-market-analysis-agam-shah-et-al-2024>(58/61 | 58/165) Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis (Agam Shah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Agam Shah, Arnav Hiray, Pratvi Shah, Arkaprabha Banerjee, Anushka Singh, Dheeraj Eidnani, Bhaskar Chaudhury, Sudheer Chava. (2024)<br><strong>Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis</strong><br><button class=copy-to-clipboard title="Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL, q-fin-CP<br>Keyword Score: 13<br>Keywords: Benchmarking, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11728v1.pdf filename=2402.11728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the influence of claims in analyst reports and earnings calls on financial market returns, considering them as significant quarterly events for publicly traded companies. To facilitate a comprehensive analysis, we construct a new financial dataset for the claim detection task in the financial domain. We <b>benchmark</b> various language models on this dataset and propose a novel <b>weak-supervision</b> <b>model</b> that incorporates the knowledge of subject matter experts (SMEs) in the aggregation function, outperforming existing approaches. Furthermore, we demonstrate the practical utility of our proposed model by constructing a novel measure ``optimism". Furthermore, we observed the dependence of earnings surprise and return on our optimism measure. Our dataset, models, and code will be made publicly (under CC BY 4.0 license) available on GitHub and Hugging Face.</p></p class="citation"></blockquote><h3 id=5961--59165-syntactic-language-change-in-english-and-german-metrics-parsers-and-convergences-yanran-chen-et-al-2024>(59/61 | 59/165) Syntactic Language Change in English and German: Metrics, Parsers, and Convergences (Yanran Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanran Chen, Wei Zhao, Anne Breitbarth, Manuel Stoeckel, Alexander Mehler, Steffen Eger. (2024)<br><strong>Syntactic Language Change in English and German: Metrics, Parsers, and Convergences</strong><br><button class=copy-to-clipboard title="Syntactic Language Change in English and German: Metrics, Parsers, and Convergences" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Optical Character Recognition, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11549v1.pdf filename=2402.11549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many studies have shown that human languages tend to optimize for lower complexity and increased communication efficiency. Syntactic dependency distance, which measures the linear distance between dependent words, is often considered a key indicator of language processing difficulty and working memory load. The current paper looks at diachronic trends in syntactic language change in both English and German, using corpora of parliamentary debates from the last c. 160 years. We base our observations on five dependency parsers, including the widely used Stanford CoreNLP as well as 4 newer alternatives. Our analysis of syntactic language change goes beyond linear dependency distance and explores 15 metrics relevant to dependency distance minimization (DDM) and/or based on tree <b>graph</b> properties, such as the tree height and degree variance. Even though we have evidence that recent parsers trained on modern treebanks are not heavily affected by data &rsquo;noise&rsquo; such as spelling changes and <b>OCR</b> errors in our historic data, we find that results of syntactic language change are sensitive to the parsers involved, which is a caution against using a single parser for evaluating syntactic language change as done in previous work. We also show that syntactic language change over the time period investigated is largely similar between English and German across the different metrics explored: only 4% of cases we examine yield opposite conclusions regarding upwards and downtrends of syntactic metrics across German and English. We also show that changes in syntactic measures seem to be more frequent at the tails of sentence length distributions. To our best knowledge, ours is the most comprehensive analysis of syntactic language using modern NLP technology in recent corpora of English and German.</p></p class="citation"></blockquote><h3 id=6061--60165-a-note-on-bias-to-complete-jia-xu-et-al-2024>(60/61 | 60/165) A Note on Bias to Complete (Jia Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jia Xu, Mona Diab. (2024)<br><strong>A Note on Bias to Complete</strong><br><button class=copy-to-clipboard title="A Note on Bias to Complete" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11710v1.pdf filename=2402.11710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Minimizing social bias strengthens societal bonds, promoting shared understanding and better decision-making. We revisit the definition of bias by discovering new bias types (e.g., societal status) in dynamic environments and describe them relative to context, such as culture, region, time, and personal background. Our framework includes eight hypotheses about bias and a minimizing bias strategy for each assumption as well as five methods as proposed solutions in <b>LLM.</b> The realization of the framework is yet to be completed.</p></p class="citation"></blockquote><h3 id=6161--61165-metric-learning-encoding-models-identify-processing-profiles-of-linguistic-features-in-berts-representations-louis-jalouzot-et-al-2024>(61/61 | 61/165) Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT&rsquo;s Representations (Louis Jalouzot et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Louis Jalouzot, Robin Sobczyk, Bastien Lhopitallier, Jeanne Salle, Nur Lan, Emmanuel Chemla, Yair Lakretz. (2024)<br><strong>Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT&rsquo;s Representations</strong><br><button class=copy-to-clipboard title="Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11608v1.pdf filename=2402.11608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to understand how neural systems represent the theoretical features of the objects they process. As a proof-of-concept, we apply MLEMs to neural representations extracted from <b>BERT,</b> and track a wide variety of linguistic features (e.g., tense, subject person, clause type, clause embedding). We find that: (1) linguistic features are ordered: they separate representations of sentences to different degrees in different layers; (2) neural representations are organized hierarchically: in some layers, we find clusters of representations nested within larger clusters, following successively important linguistic features; (3) linguistic features are disentangled in middle layers: distinct, selective units are activated by distinct linguistic features. Methodologically, MLEMs are superior (4) to multivariate decoding methods, being more robust to type-I errors, and (5) to univariate encoding methods, in being able to predict both local and distributed representations. Together, this demonstrates the utility of Metric-Learning Encoding Methods for studying how linguistic features are neurally encoded in language models and the advantage of MLEMs over traditional methods. MLEMs can be extended to other domains (e.g. vision) and to other neural systems, such as the human brain.</p></p class="citation"></blockquote><h2 id=csit-4>cs.IT (4)</h2><h3 id=14--62165-integrating-pre-trained-language-model-with-physical-layer-communications-ju-hyung-lee-et-al-2024>(1/4 | 62/165) Integrating Pre-Trained Language Model with Physical Layer Communications (Ju-Hyung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ju-Hyung Lee, Dong-Ho Lee, Joohan Lee, Jay Pujara. (2024)<br><strong>Integrating Pre-Trained Language Model with Physical Layer Communications</strong><br><button class=copy-to-clipboard title="Integrating Pre-Trained Language Model with Physical Layer Communications" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-CL, cs-IT, cs-LG, cs.IT, eess-SP, math-IT<br>Keyword Score: 80<br>Keywords: Autoencoder, Foundation Model, Quantization, Simulation, Simulator, Variational Autoencoder, Transformer, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11656v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11656v1.pdf filename=2402.11656v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The burgeoning field of on-device AI communication, where devices exchange information directly through embedded <b>foundation</b> <b>models,</b> such as language models (LMs), requires robust, efficient, and generalizable communication frameworks. However, integrating these frameworks with existing wireless systems and effectively managing noise and bit errors pose significant challenges. In this work, we introduce a practical on-device AI communication framework, integrated with physical layer (PHY) communication functions, demonstrated through its performance on a link-level simulator. Our framework incorporates end-to-end training with channel noise to enhance resilience, incorporates vector <b>quantized</b> <b>variational</b> <b>autoencoders</b> (VQ-VAE) for efficient and robust communication, and utilizes <b>pre-trained</b> <b>encoder-decoder</b> <b>transformers</b> for improved generalization capabilities. <b>Simulations,</b> across various communication scenarios, reveal that our framework achieves a 50% reduction in transmission size while demonstrating substantial generalization ability and noise robustness under standardized 3GPP channel models.</p></p class="citation"></blockquote><h3 id=24--63165-hybrid-ris-with-sub-connected-active-partitions-performance-analysis-and-transmission-design-konstantinos-ntougias-et-al-2024>(2/4 | 63/165) Hybrid RIS With Sub-Connected Active Partitions: Performance Analysis and Transmission Design (Konstantinos Ntougias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konstantinos Ntougias, Symeon Chatzinotas, Ioannis Krikidis. (2024)<br><strong>Hybrid RIS With Sub-Connected Active Partitions: Performance Analysis and Transmission Design</strong><br><button class=copy-to-clipboard title="Hybrid RIS With Sub-Connected Active Partitions: Performance Analysis and Transmission Design" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11547v1.pdf filename=2402.11547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emerging reflecting intelligent surface (RIS) technology promises to enhance the capacity of wireless communication systems via passive reflect beamforming. However, the product path loss limits its performance gains. Fully-connected (FC) active RIS, which integrates reflect-type power amplifiers into the RIS elements, has been recently introduced in response to this issue. Also, sub-connected (SC) active RIS and hybrid FC-active/passive RIS variants, which employ a limited number of reflect-type power amplifiers, have been proposed to provide energy savings. Nevertheless, their flexibility in balancing diverse capacity requirements and power consumption constraints is limited. In this direction, this study introduces novel hybrid RIS structures, wherein at least one reflecting sub-surface (RS) adopts the SC-active RIS design. The asymptotic signal-to-noise-ratio of the FC-active/passive and the proposed hybrid RIS variants is analyzed in a single-user single-input single-output setup. Furthermore, the transmit and RIS beamforming weights are jointly optimized in each scenario to maximize the energy efficiency of a hybrid RIS-aided multi-user multiple-input single-output downlink system subject to the power consumption constraints of the base station and the active RSs. Numerical <b>simulation</b> and analytic results highlight the performance gains of the proposed RIS designs over <b>benchmarks,</b> unveil non-trivial trade-offs, and provide valuable insights.</p></p class="citation"></blockquote><h3 id=34--64165-on-efficient-normal-bases-over-binary-fields-mohamadou-sall-et-al-2024>(3/4 | 64/165) On efficient normal bases over binary fields (Mohamadou Sall et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamadou Sall, M. Anwar Hasan. (2024)<br><strong>On efficient normal bases over binary fields</strong><br><button class=copy-to-clipboard title="On efficient normal bases over binary fields" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-CR, cs-IT, cs.IT, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11544v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11544v1.pdf filename=2402.11544v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Binary field extensions are fundamental to many applications, such as multivariate public key cryptography, code-based cryptography, and error-correcting codes. Their implementation requires a foundation in number theory and algebraic <b>geometry</b> and necessitates the utilization of efficient bases. The continuous increase in the power of computation, and the design of new (quantum) computers increase the threat to the security of systems and impose increasingly demanding encryption standards with huge polynomial or extension degrees. For cryptographic purposes or other common implementations of finite fields arithmetic, it is essential to explore a wide range of implementations with diverse bases. Unlike some bases, polynomial and Gaussian normal bases are well-documented and widely employed. In this paper, we explore other forms of bases of $\mathbb{F}_{2^n}$ over $\mathbb{F}_2$ to demonstrate efficient implementation of operations within different ranges. To achieve this, we leverage results on fast computations and elliptic periods introduced by Couveignes and Lercier, and subsequently expanded upon by Ezome and Sall. This leads to the establishment of new tables for efficient computation over binary fields.</p></p class="citation"></blockquote><h3 id=44--65165-age-of-kn-threshold-signature-scheme-on-a-gossip-network-erkan-bayram-et-al-2024>(4/4 | 65/165) Age of $(k,n)$-Threshold Signature Scheme on a Gossip Network (Erkan Bayram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erkan Bayram, Melih Bastopcu, Mohamed-Ali Belabbas, Tamer Başar. (2024)<br><strong>Age of $(k,n)$-Threshold Signature Scheme on a Gossip Network</strong><br><button class=copy-to-clipboard title="Age of $(k,n)$-Threshold Signature Scheme on a Gossip Network" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-NI, cs-SY, cs.IT, eess-SY, math-IT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11462v1.pdf filename=2402.11462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider information update systems on a gossip network, which consists of a single source and $n$ receiver nodes. The source encrypts the information into $n$ distinct keys with version stamps, sending a unique key to each node. For decryption in a $(k, n)$-Threshold Signature Scheme, each receiver node requires at least $k+1$ different keys with the same version, shared over peer-to-peer connections. We consider two different schemes: a memory scheme (in which the nodes keep the source&rsquo;s current and previous encrypted messages) and a memoryless scheme (in which the nodes are allowed to only keep the source&rsquo;s current message). We measure the &lsquo;&rsquo;timeliness&rsquo;&rsquo; of information updates by using the version age of information. Our work focuses on determining closed-form expressions for the time average age of information in a heterogeneous random <b>graph.</b> Our work not only allows to verify the expected outcome that a memory scheme results in a lower average age compared to a memoryless scheme, but also provides the quantitative difference between the two. In our numerical results, we quantify the value of memory and demonstrate that the advantages of memory diminish with infrequent source updates, frequent gossipping between nodes, or a decrease in $k$ for a fixed number of nodes.</p></p class="citation"></blockquote><h2 id=csro-7>cs.RO (7)</h2><h3 id=17--66165-aint-misbehavin----using-llms-to-generate-expressive-robot-behavior-in-conversations-with-the-tabletop-robot-haru-zining-wang-et-al-2024>(1/7 | 66/165) Ain&rsquo;t Misbehavin&rsquo; &ndash; Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru (Zining Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zining Wang, Paul Reisert, Eric Nichols, Randy Gomez. (2024)<br><strong>Ain&rsquo;t Misbehavin&rsquo; &ndash; Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru</strong><br><button class=copy-to-clipboard title="Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CL, cs-RO, cs.RO<br>Keyword Score: 80<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Emotion Recognition, Text-to-speech, Text-to-speech, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11571v1.pdf filename=2402.11571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social robots aim to establish long-term bonds with humans through engaging conversation. However, traditional conversational approaches, reliant on scripted interactions, often fall short in maintaining engaging conversations. This paper addresses this limitation by integrating <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> into social robots to achieve more dynamic and expressive conversations. We introduce a fully-automated conversation system that leverages <b>LLMs</b> to generate robot responses with expressive behaviors, congruent with the robot&rsquo;s personality. We incorporate robot behavior with two modalities: 1) a <b>text-to-speech</b> <b>(TTS)</b> engine capable of various delivery styles, and 2) a library of physical actions for the robot. We develop a custom, state-of-the-art <b>emotion</b> <b>recognition</b> model to dynamically select the robot&rsquo;s tone of voice and utilize emojis from <b>LLM</b> output as cues for generating robot actions. A demo of our system is available here. To illuminate design and implementation issues, we conduct a pilot study where volunteers chat with a social robot using our proposed system, and we analyze their feedback, conducting a rigorous error analysis of chat transcripts. Feedback was overwhelmingly positive, with participants commenting on the robot&rsquo;s empathy, helpfulness, naturalness, and entertainment. Most negative feedback was due to <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> errors which had limited impact on conversations. However, we observed a small class of errors, such as the <b>LLM</b> repeating itself or hallucinating fictitious information and human responses, that have the potential to derail conversations, raising important issues for <b>LLM</b> application.</p></p class="citation"></blockquote><h3 id=27--67165-learning-to-learn-faster-from-human-feedback-with-language-model-predictive-control-jacky-liang-et-al-2024>(2/7 | 67/165) Learning to Learn Faster from Human Feedback with Language Model Predictive Control (Jacky Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacky Liang, Fei Xia, Wenhao Yu, Andy Zeng, Montserrat Gonzalez Arenas, Maria Attarian, Maria Bauza, Matthew Bennice, Alex Bewley, Adil Dostmohamed, Chuyuan Kelly Fu, Nimrod Gileadi, Marissa Giustina, Keerthana Gopalakrishnan, Leonard Hasenclever, Jan Humplik, Jasmine Hsu, Nikhil Joshi, Ben Jyenis, Chase Kew, Sean Kirmani, Tsang-Wei Edward Lee, Kuang-Huei Lee, Assaf Hurwitz Michaely, Joss Moore, Ken Oslund, Dushyant Rao, Allen Ren, Baruch Tabanpour, Quan Vuong, Ayzaan Wahid, Ted Xiao, Ying Xu, Vincent Zhuang, Peng Xu, Erik Frey, Ken Caluwaerts, Tingnan Zhang, Brian Ichter, Jonathan Tompson, Leila Takayama, Vincent Vanhoucke, Izhak Shafran, Maja Mataric, Dorsa Sadigh, Nicolas Heess, Kanishka Rao, Nik Stewart, Jie Tan, Carolina Parada. (2024)<br><strong>Learning to Learn Faster from Human Feedback with Language Model Predictive Control</strong><br><button class=copy-to-clipboard title="Learning to Learn Faster from Human Feedback with Language Model Predictive Control" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Markov Decision Process, PaLM, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11450v1.pdf filename=2402.11450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands &ndash; enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks. However, these capabilities (driven by <b>in-context</b> <b>learning)</b> are limited to short-term interactions, where users&rsquo; feedback remains relevant for only as long as it fits within the context size of the <b>LLM,</b> and can be forgotten over longer interactions. In this work, we investigate <b>fine-tuning</b> the robot code-writing <b>LLMs,</b> to remember their <b>in-context</b> <b>interactions</b> and improve their teachability i.e., how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful). Our key observation is that when human-robot interactions are formulated as a partially observable <b>Markov</b> <b>decision</b> <b>process</b> (in which human language inputs are observations, and robot code outputs are actions), then training an <b>LLM</b> to complete previous interactions can be viewed as training a transition dynamics model &ndash; that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), a framework that <b>fine-tunes</b> <b>PaLM</b> 2 to improve its teachability on 78 tasks across 5 robot embodiments &ndash; improving non-expert teaching success rates of unseen tasks by 26.9% while reducing the average number of human corrections from 2.4 to 1.9. Experiments show that LMPC also produces strong meta-learners, improving the success rate of <b>in-context</b> <b>learning</b> new tasks on unseen robot embodiments and APIs by 31.5%. See videos, code, and demos at: <a href=https://robot-teaching.github.io/>https://robot-teaching.github.io/</a>.</p></p class="citation"></blockquote><h3 id=37--68165-lirafusion-deep-adaptive-lidar-radar-fusion-for-3d-object-detection-jingyu-song-et-al-2024>(3/7 | 68/165) LiRaFusion: Deep Adaptive LiDAR-Radar Fusion for 3D Object Detection (Jingyu Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyu Song, Lingjun Zhao, Katherine A. Skinner. (2024)<br><strong>LiRaFusion: Deep Adaptive LiDAR-Radar Fusion for 3D Object Detection</strong><br><button class=copy-to-clipboard title="LiRaFusion: Deep Adaptive LiDAR-Radar Fusion for 3D Object Detection" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Object Detection, Graph Attention Networks<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11735v1.pdf filename=2402.11735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose LiRaFusion to tackle LiDAR-radar fusion for 3D <b>object</b> <b>detection</b> to fill the performance gap of existing LiDAR-radar detectors. To improve the feature extraction capabilities from these two modalities, we design an early fusion module for joint voxel feature encoding, and a middle fusion module to adaptively fuse feature maps via a <b>gated</b> network. We perform extensive evaluation on nuScenes to demonstrate that LiRaFusion leverages the complementary information of LiDAR and radar effectively and achieves notable improvement over existing methods.</p></p class="citation"></blockquote><h3 id=47--69165-smooth-path-planning-with-subharmonic-artificial-potential-field-bo-peng-et-al-2024>(4/7 | 69/165) Smooth Path Planning with Subharmonic Artificial Potential Field (Bo Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Peng, Lingke Zhang, Rong Xiong. (2024)<br><strong>Smooth Path Planning with Subharmonic Artificial Potential Field</strong><br><button class=copy-to-clipboard title="Smooth Path Planning with Subharmonic Artificial Potential Field" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11601v1.pdf filename=2402.11601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When a mobile robot plans its path in an environment with obstacles using Artificial Potential Field (APF) strategy, it may fall into the local minimum point and fail to reach the goal. Also, the derivatives of APF will explode close to obstacles causing poor planning performance. To solve the problems, exponential functions are used to modify potential fields&rsquo; formulas. The potential functions can be subharmonic when the distance between the robot and obstacles is above a predefined threshold. Subharmonic functions do not have local minimum and the derivatives of exponential functions increase mildly when the robot is close to obstacles, thus eliminate the problems in theory. Circular sampling technique is used to keep the robot outside a danger distance to obstacles and support the construction of subharmonic functions. Through <b>simulations,</b> it is proven that mobile robots can bypass local minimum points and construct a smooth path to reach the goal successfully by the proposed methods.</p></p class="citation"></blockquote><h3 id=57--70165-imitation-learning-based-online-time-optimal-control-with-multiple-waypoint-constraints-for-quadrotors-jin-zhou-et-al-2024>(5/7 | 70/165) Imitation Learning-Based Online Time-Optimal Control with Multiple-Waypoint Constraints for Quadrotors (Jin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin Zhou, Jiahao Mei, Fangguo Zhao, Jiming Chen, Shuo Li. (2024)<br><strong>Imitation Learning-Based Online Time-Optimal Control with Multiple-Waypoint Constraints for Quadrotors</strong><br><button class=copy-to-clipboard title="Imitation Learning-Based Online Time-Optimal Control with Multiple-Waypoint Constraints for Quadrotors" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11570v1.pdf filename=2402.11570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past decade, there has been a remarkable surge in utilizing quadrotors for various purposes due to their simple structure and aggressive maneuverability, such as search and rescue, delivery and autonomous drone racing, etc. One of the key challenges preventing quadrotors from being widely used in these scenarios is online waypoint-constrained time-optimal trajectory generation and control technique. This letter proposes an imitation learning-based online solution to efficiently navigate the quadrotor through multiple waypoints with time-optimal performance. The neural networks (WN&amp;CNets) are trained to learn the control law from the dataset generated by the time-consuming CPC algorithm and then deployed to generate the optimal control commands online to guide the quadrotors. To address the challenge of limited training data and the hover maneuver at the final waypoint, we propose a transition phase strategy that utilizes polynomials to help the quadrotor &lsquo;jump over&rsquo; the stop-and-go maneuver when switching waypoints. Our method is demonstrated in both <b>simulation</b> and real-world experiments, achieving a maximum speed of 7 m/s while navigating through 7 waypoints in a confined space of 6.0 m * 4.0 m * 2.0 m. The results show that with a slight loss in optimality, the WN&amp;CNets significantly reduce the processing time and enable online optimal control for multiple-waypoint-constrained flight tasks.</p></p class="citation"></blockquote><h3 id=67--71165-verifiably-following-complex-robot-instructions-with-foundation-models-benedict-quartey-et-al-2024>(6/7 | 71/165) Verifiably Following Complex Robot Instructions with Foundation Models (Benedict Quartey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benedict Quartey, Eric Rosen, Stefanie Tellex, George Konidaris. (2024)<br><strong>Verifiably Following Complex Robot Instructions with Foundation Models</strong><br><button class=copy-to-clipboard title="Verifiably Following Complex Robot Instructions with Foundation Models" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Foundation Model, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11498v1.pdf filename=2402.11498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enabling robots to follow complex natural language instructions is an important yet challenging problem. People want to flexibly express constraints, refer to arbitrary landmarks and verify behavior when instructing robots. Conversely, robots must disambiguate human instructions into specifications and ground instruction referents in the real world. We propose Language Instruction <b>grounding</b> for Motion Planning (LIMP), a system that leverages <b>foundation</b> <b>models</b> and temporal logics to generate instruction-conditioned semantic maps that enable robots to verifiably follow expressive and long-horizon instructions with open vocabulary referents and complex spatiotemporal constraints. In contrast to prior methods for using <b>foundation</b> <b>models</b> in robot task execution, LIMP constructs an explainable instruction representation that reveals the robot&rsquo;s alignment with an instructor&rsquo;s intended motives and affords the synthesis of robot behaviors that are correct-by-construction. We demonstrate LIMP in three real-world environments, across a set of 35 complex spatiotemporal instructions, showing the generality of our approach and the ease of deployment in novel unstructured domains. In our experiments, LIMP can spatially ground open-vocabulary referents and synthesize constraint-satisfying plans in 90% of object-goal navigation and 71% of mobile manipulation instructions. See supplementary videos at <a href=https://robotlimp.github.io>https://robotlimp.github.io</a></p></p class="citation"></blockquote><h3 id=77--72165-developing-autonomous-robot-mediated-behavior-coaching-sessions-with-haru-matouš-jelínek-et-al-2024>(7/7 | 72/165) Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru (Matouš Jelínek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matouš Jelínek, Eric Nichols, Randy Gomez. (2024)<br><strong>Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru</strong><br><button class=copy-to-clipboard title="Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CL, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Dialogue System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11569v1.pdf filename=2402.11569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents an empirical investigation into the design and impact of autonomous <b>dialogues</b> <b>in</b> human-robot interaction for behavior change coaching. We focus on the use of Haru, a tabletop social robot, and explore the implementation of the Tiny Habits method for fostering positive behavior change. The core of our study lies in developing a fully autonomous <b>dialogue</b> <b>system</b> that maximizes Haru&rsquo;s emotional expressiveness and unique personality. Our methodology involved iterative design and extensive testing of the <b>dialogue</b> <b>system,</b> ensuring it effectively embodied the principles of the Tiny Habits method while also incorporating strategies for trust-raising and trust-dampening. The effectiveness of the final version of the <b>dialogue</b> <b>was</b> evaluated in an experimental study with human participants (N=12). The results indicated a significant improvement in perceptions of Haru&rsquo;s liveliness, interactivity, and neutrality. Additionally, our study contributes to the broader understanding of <b>dialogue</b> <b>design</b> in social robotics, offering practical insights for future developments in the field.</p></p class="citation"></blockquote><h2 id=cslg-26>cs.LG (26)</h2><h3 id=126--73165-a-curious-case-of-searching-for-the-correlation-between-training-data-and-adversarial-robustness-of-transformer-textual-models-cuong-dang-et-al-2024>(1/26 | 73/165) A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models (Cuong Dang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cuong Dang, Dung D. Le, Thai Le. (2024)<br><strong>A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models</strong><br><button class=copy-to-clipboard title="A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 80<br>Keywords: Adversarial Learning, Fine-tuning, Fine-tuning, BART, BERT, GPT-2, RoBERTa, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11469v1.pdf filename=2402.11469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing works have shown that <b>fine-tuned</b> textual <b>transformer</b> models achieve state-of-the-art prediction performances but are also vulnerable to <b>adversarial</b> <b>text</b> perturbations. Traditional <b>adversarial</b> <b>evaluation</b> is often done \textit{only after} <b>fine-tuning</b> the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input <b>fine-tuning</b> corpora properties and use them to predict the <b>adversarial</b> <b>robustness</b> of the <b>fine-tuned</b> models. Focusing mostly on encoder-only <b>transformer</b> models <b>BERT</b> and <b>RoBERTa</b> with additional results for <b>BART,</b> ELECTRA and <b>GPT2,</b> we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate and (b) features with the most influence on the model robustness have a clear correlation with the robustness. Second, our framework can be used as a fast and effective additional tool for robustness evaluation since it (a) saves 30x-193x runtime compared to the traditional technique, (b) is transferable across models, (c) can be used under <b>adversarial</b> <b>training,</b> and (d) robust to statistical randomness. Our code will be publicly available.</p></p class="citation"></blockquote><h3 id=226--74165-revisiting-zeroth-order-optimization-for-memory-efficient-llm-fine-tuning-a-benchmark-yihua-zhang-et-al-2024>(2/26 | 74/165) Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark (Yihua Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen. (2024)<br><strong>Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark</strong><br><button class=copy-to-clipboard title="Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 76<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Stochastic Gradient Descent, LLaMA, Mistral, RoBERTa, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11592v1.pdf filename=2402.11592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving landscape of natural language processing (NLP), <b>fine-tuning</b> pre-trained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with first-order (FO) optimizers like <b>SGD</b> and Adam has become standard. Yet, as <b>LLMs</b> grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during <b>LLM</b> <b>fine-tuning,</b> building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind <b>benchmarking</b> study across five <b>LLM</b> families <b>(Roberta,</b> OPT, <b>LLaMA,</b> Vicuna, <b>Mistral),</b> three task complexities, and five <b>fine-tuning</b> schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and <b>fine-tuning</b> performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient <b>LLM</b> <b>fine-tuning.</b> Codes to reproduce all our experiments are at <a href=https://github.com/ZO-Bench/ZO-LLM>https://github.com/ZO-Bench/ZO-LLM</a> .</p></p class="citation"></blockquote><h3 id=326--75165-large-language-model-driven-meta-structure-discovery-in-heterogeneous-information-network-lin-chen-et-al-2024>(3/26 | 75/165) Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network (Lin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Chen, Fengli Xu, Nian Li, Zhenyu Han, Meng Wang, Yong Li, Pan Hui. (2024)<br><strong>Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network</strong><br><button class=copy-to-clipboard title="Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 73<br>Keywords: Node Classification, Graph, Graph Neural Network, Recommendation, Natural Language Explanation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11518v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11518v1.pdf filename=2402.11518v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Heterogeneous information networks (HIN) have gained increasing popularity for being able to capture complex relations between <b>nodes</b> <b>of</b> diverse types. Meta-structure was proposed to identify important patterns of relations on HIN, which has been proven effective for extracting rich semantic information and facilitating <b>graph</b> <b>neural</b> <b>networks</b> to learn expressive representations. However, hand-crafted meta-structures pose challenges for scaling up, which draws wide research attention for developing automatic meta-structure search algorithms. Previous efforts concentrate on searching for meta-structures with good empirical prediction performance, overlooking explainability. Thus, they often produce meta-structures prone to overfitting and incomprehensible to humans. To address this, we draw inspiration from the emergent <b>reasoning</b> abilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We propose a novel <b>REasoning</b> meta-STRUCTure search (ReStruct) framework that integrates <b>LLM</b> <b>reasoning</b> into the evolutionary procedure. ReStruct uses a grammar translator to encode meta-structures into <b>natural</b> <b>language</b> <b>sentences,</b> and leverages the <b>reasoning</b> power of <b>LLMs</b> to evaluate semantically feasible meta-structures. ReStruct also employs performance-oriented evolutionary operations. These two competing forces jointly optimize for semantic explainability and empirical performance of meta-structures. We also design a differential <b>LLM</b> explainer that can produce <b>natural</b> <b>language</b> <b>explanations</b> for the discovered meta-structures, and refine the explanation by <b>reasoning</b> through the search history. Experiments on five datasets demonstrate ReStruct achieve SOTA performance in <b>node</b> <b>classification</b> and link <b>recommendation</b> tasks. Additionally, a survey study involving 73 graduate students shows that the meta-structures and <b>natural</b> <b>language</b> <b>explanations</b> generated by ReStruct are substantially more comprehensible.</p></p class="citation"></blockquote><h3 id=426--76165-aligning-modalities-in-vision-large-language-models-via-preference-fine-tuning-yiyang-zhou-et-al-2024>(4/26 | 76/165) Aligning Modalities in Vision Large Language Models via Preference Fine-tuning (Yiyang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, Huaxiu Yao. (2024)<br><strong>Aligning Modalities in Vision Large Language Models via Preference Fine-tuning</strong><br><button class=copy-to-clipboard title="Aligning Modalities in Vision Large Language Models via Preference Fine-tuning" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Reinforcement Learning from Human Feedback, GPT, Instruction Following, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11411v1.pdf filename=2402.11411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction-following</b> <b>Vision</b> <b>Large</b> <b>Language</b> <b>Models</b> (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core <b>LLM</b> is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth <b>instructions</b> <b>as</b> the preferred response and a two-stage approach to generate dispreferred data. First, we <b>prompt</b> <b>GPT-4V</b> to inject plausible hallucinations into the correct answer. Second, we distort the image to trigger the inherent hallucination behavior of the VLLM. This is an automated approach, which does not rely on human data generation or require a perfect expert, which makes it easily scalable. Finally, both of these generation strategies are integrated into an <b>RLHF</b> pipeline via Direct Preference Optimization. In experiments across broad <b>benchmarks,</b> we show that we can not only reduce hallucinations, but improve model performance across standard <b>benchmarks,</b> outperforming prior approaches. Our data and code are available at <a href=https://github.com/YiyangZhou/POVID>https://github.com/YiyangZhou/POVID</a>.</p></p class="citation"></blockquote><h3 id=526--77165-graph-out-of-distribution-generalization-via-causal-intervention-qitian-wu-et-al-2024>(5/26 | 77/165) Graph Out-of-Distribution Generalization via Causal Intervention (Qitian Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qitian Wu, Fan Nie, Chenxiao Yang, Tianyi Bao, Junchi Yan. (2024)<br><strong>Graph Out-of-Distribution Generalization via Causal Intervention</strong><br><button class=copy-to-clipboard title="Graph Out-of-Distribution Generalization via Causal Intervention" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 46<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Causal Intervention, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11494v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11494v1.pdf filename=2402.11494v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> (OOD) generalization has gained increasing attentions for learning on <b>graphs,</b> <b>as</b> <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on <b>graphs</b> <b>involve</b> <b>intricate</b> interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through <b>causal</b> <b>analysis:</b> the crux of <b>GNNs&rsquo;</b> failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes&rsquo; labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust <b>GNNs</b> under node-level distribution shifts, without prior knowledge of environment labels. Our method resorts to a new learning objective derived from <b>causal</b> <b>inference</b> that coordinates an environment estimator and a mixture-of-expert <b>GNN</b> predictor. The new approach can counteract the confounding bias in training data and facilitate learning generalizable predictive relations. Extensive experiment demonstrates that our model can effectively enhance generalization with various types of distribution shifts and yield up to 27.4% accuracy improvement over state-of-the-arts on <b>graph</b> <b>OOD</b> <b>generalization</b> <b>benchmarks.</b> Source codes are available at <a href=https://github.com/fannie1208/CaNet>https://github.com/fannie1208/CaNet</a>.</p></p class="citation"></blockquote><h3 id=626--78165-in-context-learning-with-transformers-softmax-attention-adapts-to-function-lipschitzness-liam-collins-et-al-2024>(6/26 | 78/165) In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness (Liam Collins et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, Sanjay Shakkottai. (2024)<br><strong>In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness</strong><br><button class=copy-to-clipboard title="In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Transformer, In-context Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11639v1.pdf filename=2402.11639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A striking property of <b>transformers</b> is their ability to perform <b>in-context</b> <b>learning</b> <b>(ICL),</b> a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such that learner must adapt to the context without additional training. We explore the role of softmax attention in an <b>ICL</b> setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses.</p></p class="citation"></blockquote><h3 id=726--79165-towards-versatile-graph-learning-approach-from-the-perspective-of-large-language-models-lanning-wei-et-al-2024>(7/26 | 79/165) Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models (Lanning Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lanning Wei, Jun Gao, Huan Zhao. (2024)<br><strong>Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models</strong><br><button class=copy-to-clipboard title="Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11641v1.pdf filename=2402.11641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph-structured</b> data are the commonly used and have wide application scenarios in the real world. For these diverse applications, the vast variety of learning tasks, <b>graph</b> domains, and complex <b>graph</b> learning procedures present challenges for human experts when designing versatile <b>graph</b> learning approaches. Facing these challenges, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> offer a potential solution due to the extensive knowledge and the human-like intelligence. This paper proposes a novel conceptual prototype for designing versatile <b>graph</b> learning methods with <b>LLMs,</b> with a particular focus on the <code>where'' and </code>how&rsquo;&rsquo; perspectives. From the <code>where'' perspective, we &lt;b>summarize&lt;/b> four key &lt;b>graph&lt;/b> learning procedures, including task definition, &lt;b>graph&lt;/b> data feature engineering, model selection and optimization, deployment and serving. We then explore the application scenarios of &lt;b>LLMs&lt;/b> in these procedures across a wider spectrum. In the </code>how&rsquo;&rsquo; perspective, we align the abilities of <b>LLMs</b> with the requirements of each procedure. Finally, we point out the promising directions that could better leverage the strength of <b>LLMs</b> towards versatile <b>graph</b> learning methods.</p></p class="citation"></blockquote><h3 id=826--80165-self-evolving-autoencoder-embedded-q-network-j-senthilnath-et-al-2024>(8/26 | 80/165) Self-evolving Autoencoder Embedded Q-Network (J. Senthilnath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>J. Senthilnath, Bangjian Zhou, Zhen Wei Ng, Deeksha Aggarwal, Rajdeep Dutta, Ji Wei Yoon, Aye Phyu Phyu Aung, Keyu Wu, Min Wu, Xiaoli Li. (2024)<br><strong>Self-evolving Autoencoder Embedded Q-Network</strong><br><button class=copy-to-clipboard title="Self-evolving Autoencoder Embedded Q-Network" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Autoencoder, Benchmarking, Pruning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11604v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11604v1.pdf filename=2402.11604v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of sequential decision-making tasks, the exploration capability of a <b>reinforcement</b> <b>learning</b> (RL) agent is paramount for achieving high rewards through interactions with the environment. To enhance this crucial ability, we propose SAQN, a novel approach wherein a self-evolving <b>autoencoder</b> (SA) is embedded with a Q-Network (QN). In SAQN, the self-evolving <b>autoencoder</b> architecture adapts and evolves as the agent explores the environment. This evolution enables the <b>autoencoder</b> to capture a diverse range of raw observations and represent them effectively in its latent space. By leveraging the disentangled states extracted from the encoder generated latent space, the QN is trained to determine optimal actions that improve rewards. During the evolution of the <b>autoencoder</b> architecture, a bias-variance regulatory strategy is employed to elicit the optimal response from the RL agent. This strategy involves two key components: (i) fostering the growth of nodes to retain previously acquired knowledge, ensuring a rich representation of the environment, and (ii) <b>pruning</b> the least contributing nodes to maintain a more manageable and tractable latent space. Extensive experimental evaluations conducted on three distinct <b>benchmark</b> environments and a real-world molecular environment demonstrate that the proposed SAQN significantly outperforms state-of-the-art counterparts. The results highlight the effectiveness of the self-evolving <b>autoencoder</b> and its collaboration with the Q-Network in tackling sequential decision-making tasks.</p></p class="citation"></blockquote><h3 id=926--81165-modelgpt-unleashing-llms-capabilities-for-tailored-model-generation-zihao-tang-et-al-2024>(9/26 | 81/165) ModelGPT: Unleashing LLM&rsquo;s Capabilities for Tailored Model Generation (Zihao Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Tang, Zheqi Lv, Shengyu Zhang, Fei Wu, Kun Kuang. (2024)<br><strong>ModelGPT: Unleashing LLM&rsquo;s Capabilities for Tailored Model Generation</strong><br><button class=copy-to-clipboard title="ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12408v1.pdf filename=2402.12408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has revolutionized various sectors by automating routine tasks, marking a step toward the realization of Artificial General Intelligence (AGI). However, they still struggle to accommodate the diverse and specific needs of users and simplify the utilization of AI models for the average user. In response, we propose ModelGPT, a novel framework designed to determine and generate AI models specifically tailored to the data or task descriptions provided by the user, leveraging the capabilities of <b>LLMs.</b> Given user requirements, ModelGPT is able to provide tailored models at most 270x faster than the previous paradigms (e.g. all-parameter or LoRA <b>finetuning).</b> Comprehensive experiments on NLP, CV, and Tabular datasets attest to the effectiveness of our framework in making AI models more accessible and user-friendly. Our code is available at <a href=https://github.com/IshiKura-a/ModelGPT>https://github.com/IshiKura-a/ModelGPT</a>.</p></p class="citation"></blockquote><h3 id=1026--82165-teacher-as-a-lenient-expert-teacher-agnostic-data-free-knowledge-distillation-hyunjune-shin-et-al-2024>(10/26 | 82/165) Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation (Hyunjune Shin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyunjune Shin, Dong-Wan Choi. (2024)<br><strong>Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12406v1.pdf filename=2402.12406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data-free <b>knowledge</b> <b>distillation</b> (DFKD) aims to <b>distill</b> pretrained <b>knowledge</b> <b>to</b> a student model with the help of a generator without using original data. In such data-free scenarios, achieving stable performance of DFKD is essential due to the unavailability of validation data. Unfortunately, this paper has discovered that existing DFKD methods are quite sensitive to different teacher models, occasionally showing catastrophic failures of <b>distillation,</b> even when using well-trained teacher models. Our observation is that the generator in DFKD is not always guaranteed to produce precise yet diverse samples using the existing representative strategy of minimizing both class-prior and adversarial losses. Through our empirical study, we focus on the fact that class-prior not only decreases the diversity of generated samples, but also cannot completely address the problem of generating unexpectedly low-quality samples depending on teacher models. In this paper, we propose the teacher-agnostic data-free <b>knowledge</b> <b>distillation</b> (TA-DFKD) method, with the goal of more robust and stable performance regardless of teacher models. Our basic idea is to assign the teacher model a lenient expert role for evaluating samples, rather than a strict supervisor that enforces its class-prior on the generator. Specifically, we design a sample selection approach that takes only clean samples verified by the teacher model without imposing restrictions on the power of generating diverse samples. Through extensive experiments, we show that our method successfully achieves both robustness and training stability across various teacher models, while outperforming the existing DFKD methods.</p></p class="citation"></blockquote><h3 id=1126--83165-continual-learning-on-graphs-challenges-solutions-and-opportunities-xikun-zhang-et-al-2024>(11/26 | 83/165) Continual Learning on Graphs: Challenges, Solutions, and Opportunities (Xikun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xikun Zhang, Dongjin Song, Dacheng Tao. (2024)<br><strong>Continual Learning on Graphs: Challenges, Solutions, and Opportunities</strong><br><button class=copy-to-clipboard title="Continual Learning on Graphs: Challenges, Solutions, and Opportunities" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Graph, Benchmarking, Continual Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11565v1.pdf filename=2402.11565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> on <b>graph</b> data has recently attracted paramount attention for its aim to resolve the catastrophic forgetting problem on existing tasks while adapting the sequentially updated model to newly emerged <b>graph</b> tasks. While there have been efforts to <b>summarize</b> progress on <b>continual</b> <b>learning</b> research over Euclidean data, e.g., images and texts, a systematic review of progress in <b>continual</b> <b>learning</b> on <b>graphs,</b> a.k.a, <b>continual</b> <b>graph</b> learning (CGL) or lifelong <b>graph</b> learning, is still demanding. <b>Graph</b> data are far more complex in terms of data structures and application scenarios, making CGL task settings, model designs, and applications extremely challenging. To bridge the gap, we provide a comprehensive review of existing <b>continual</b> <b>graph</b> learning (CGL) algorithms by elucidating the different task settings and categorizing the existing methods based on their characteristics. We compare the CGL methods with traditional <b>continual</b> <b>learning</b> techniques and analyze the applicability of the traditional <b>continual</b> <b>learning</b> techniques to CGL tasks. Additionally, we review the <b>benchmark</b> works that are crucial to CGL research. Finally, we discuss the remaining challenges and propose several future directions. We will maintain an up-to-date GitHub repository featuring a comprehensive list of CGL algorithms, accessible at <a href=https://github.com/UConn-DSIS/Survey-of-Continual-Learning-on-Graphs>https://github.com/UConn-DSIS/Survey-of-Continual-Learning-on-Graphs</a>.</p></p class="citation"></blockquote><h3 id=1226--84165-the-effectiveness-of-random-forgetting-for-robust-generalization-vijaya-raghavan-t-ramkumar-et-al-2024>(12/26 | 84/165) The Effectiveness of Random Forgetting for Robust Generalization (Vijaya Raghavan T Ramkumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vijaya Raghavan T Ramkumar, Bahram Zonooz, Elahe Arani. (2024)<br><strong>The Effectiveness of Random Forgetting for Robust Generalization</strong><br><button class=copy-to-clipboard title="The Effectiveness of Random Forgetting for Robust Generalization" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Adversarial Learning, Benchmarking, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11733v1.pdf filename=2402.11733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks are susceptible to <b>adversarial</b> <b>attacks,</b> which can compromise their performance and accuracy. <b>Adversarial</b> <b>Training</b> (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network&rsquo;s robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called &ldquo;Forget to Mitigate Overfitting (FOMO)&rdquo;. FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model&rsquo;s information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on <b>benchmark</b> datasets and <b>adversarial</b> <b>attacks</b> show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline <b>adversarial</b> <b>methods.</b> Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.</p></p class="citation"></blockquote><h3 id=1326--85165-discrete-neural-algorithmic-reasoning-gleb-rodionov-et-al-2024>(13/26 | 85/165) Discrete Neural Algorithmic Reasoning (Gleb Rodionov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gleb Rodionov, Liudmila Prokhorenkova. (2024)<br><strong>Discrete Neural Algorithmic Reasoning</strong><br><button class=copy-to-clipboard title="Discrete Neural Algorithmic Reasoning" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Out-of-distribution, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11628v1.pdf filename=2402.11628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural algorithmic <b>reasoning</b> aims to capture computations with neural networks via learning the models to imitate the execution of classical algorithms. While common architectures are expressive enough to contain the correct model in the weights space, current neural reasoners are struggling to generalize well on <b>out-of-distribution</b> data. On the other hand, classical computations are not affected by distribution shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. Trained with supervision on the algorithm&rsquo;s state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on the SALSA-CLRS <b>benchmark,</b> where we get perfect test scores for all tasks. Moreover, the proposed architectural choice allows us to prove the correctness of the learned algorithms for any test data.</p></p class="citation"></blockquote><h3 id=1426--86165-temporal-disentangled-contrastive-diffusion-model-for-spatiotemporal-imputation-yakun-chen-et-al-2024>(14/26 | 86/165) Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal Imputation (Yakun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yakun Chen, Kaize Shi, Zhangkai Wu, Juan Chen, Xianzhi Wang, Julian McAuley, Guandong Xu, Shui Yu. (2024)<br><strong>Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal Imputation</strong><br><button class=copy-to-clipboard title="Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal Imputation" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Contrastive Learning, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11558v1.pdf filename=2402.11558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatiotemporal data analysis is pivotal across various domains, including transportation, meteorology, and healthcare. However, the data collected in real-world scenarios often suffers incompleteness due to sensor malfunctions and network transmission errors. Spatiotemporal imputation endeavours to predict missing values by exploiting the inherent spatial and temporal dependencies present in the observed data. Traditional approaches, which rely on classical statistical and machine learning techniques, are often inadequate, particularly when the data fails to meet strict distributional assumptions. In contrast, recent deep learning-based methods, leveraging <b>graph</b> and <b>recurrent</b> <b>neural</b> <b>networks,</b> have demonstrated enhanced efficacy. Nonetheless, these approaches are prone to error accumulation. Generative models have been increasingly adopted to circumvent the reliance on potentially inaccurate historical imputed values for future predictions. These models grapple with the challenge of producing unstable results, a particular issue in diffusion-based models. We aim to address these challenges by designing conditional features to guide the generative process and expedite training. Specifically, we introduce C$^2$TSD, a novel approach incorporating trend and seasonal information as conditional features and employing <b>contrastive</b> <b>learning</b> to improve model generalizability. The extensive experiments on three real-world datasets demonstrate the superior performance of C$^2$TSD over various state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=1526--87165-optex-expediting-first-order-optimization-with-approximately-parallelized-iterations-yao-shu-et-al-2024>(15/26 | 87/165) OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations (Yao Shu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Shu, Jiongfeng Fang, Ying Tiffany He, Fei Richard Yu. (2024)<br><strong>OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations</strong><br><button class=copy-to-clipboard title="OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11427v1.pdf filename=2402.11427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>First-order optimization (FOO) algorithms are pivotal in numerous computational domains such as machine learning and signal denoising. However, their application to complex tasks like neural network training often entails significant inefficiencies due to the need for many sequential iterations for convergence. In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first framework that enhances the efficiency of FOO by leveraging parallel computing to mitigate its iterative bottleneck. OptEx employs kernelized gradient estimation to make use of gradient history for future gradient prediction, enabling parallelization of iterations &ndash; a strategy once considered impractical because of the inherent iterative dependency in FOO. We provide theoretical guarantees for the reliability of our kernelized gradient estimation and the iteration complexity of <b>SGD-based</b> OptEx, confirming that estimation errors diminish to zero as historical gradients accumulate and that <b>SGD-based</b> OptEx enjoys an effective acceleration rate of $\Omega(\sqrt{N})$ over standard <b>SGD</b> given parallelism of N. We also use extensive empirical studies, including synthetic functions, <b>reinforcement</b> <b>learning</b> tasks, and neural network training across various datasets, to underscore the substantial efficiency improvements achieved by OptEx.</p></p class="citation"></blockquote><h3 id=1626--88165-invertible-fourier-neural-operators-for-tackling-both-forward-and-inverse-problems-da-long-et-al-2024>(16/26 | 88/165) Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems (Da Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Da Long, Shandian Zhe. (2024)<br><strong>Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems</strong><br><button class=copy-to-clipboard title="Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11722v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11722v1.pdf filename=2402.11722v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fourier Neural Operator (FNO) is a popular operator learning method, which has demonstrated state-of-the-art performance across many tasks. However, FNO is mainly used in forward prediction, yet a large family of applications rely on solving inverse problems. In this paper, we propose an invertible Fourier Neural Operator (iFNO) that tackles both the forward and inverse problems. We designed a series of invertible Fourier blocks in the latent channel space to share the model parameters, efficiently exchange the information, and mutually regularize the learning for the bi-directional tasks. We integrated a variational auto-encoder to capture the intrinsic structures within the input space and to enable posterior inference so as to overcome challenges of illposedness, data shortage, noises, etc. We developed a three-step process for pre-training and fine tuning for efficient training. The evaluations on five <b>benchmark</b> problems have demonstrated the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=1726--89165-simplifying-hyperparameter-tuning-in-online-machine-learning----the-spotrivergui-thomas-bartz-beielstein-2024>(17/26 | 89/165) Simplifying Hyperparameter Tuning in Online Machine Learning &ndash; The spotRiverGUI (Thomas Bartz-Beielstein, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Bartz-Beielstein. (2024)<br><strong>Simplifying Hyperparameter Tuning in Online Machine Learning &ndash; The spotRiverGUI</strong><br><button class=copy-to-clipboard title="Simplifying Hyperparameter Tuning in Online Machine Learning -- The spotRiverGUI" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 90C26, I-2-6; G-1-6, cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Anomaly Detection, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11594v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11594v1.pdf filename=2402.11594v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Batch Machine Learning (BML) reaches its limits when dealing with very large amounts of streaming data. This is especially true for available memory, handling drift in data streams, and processing new, unknown data. Online Machine Learning (OML) is an alternative to BML that overcomes the limitations of BML. OML is able to process data in a sequential manner, which is especially useful for data streams. The <code>river</code> package is a Python OML-library, which provides a variety of online learning algorithms for classification, regression, <b>clustering,</b> <b>anomaly</b> <b>detection,</b> and more. The <code>spotRiver</code> package provides a framework for hyperparameter tuning of OML models. The <code>spotRiverGUI</code> is a graphical user interface for the <code>spotRiver</code> package. The <code>spotRiverGUI</code> releases the user from the burden of manually searching for the optimal hyperparameter setting. After the data is provided, users can compare different OML algorithms from the powerful <code>river</code> package in a convenient way and tune the selected algorithms very efficiently.</p></p class="citation"></blockquote><h3 id=1826--90165-optimal-parallelization-strategies-for-active-flow-control-in-deep-reinforcement-learning-based-computational-fluid-dynamics-wang-jia-et-al-2024>(18/26 | 90/165) Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics (Wang Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wang Jia, Hang Xu. (2024)<br><strong>Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics</strong><br><button class=copy-to-clipboard title="Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11515v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11515v1.pdf filename=2402.11515v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>Reinforcement</b> <b>Learning</b> (DRL) has emerged as a promising approach for handling highly dynamic and nonlinear Active Flow Control (AFC) problems. However, the computational cost associated with training DRL models presents a significant performance bottleneck. To address this challenge and enable efficient scaling on high-performance computing architectures, this study focuses on optimizing DRL-based algorithms in parallel settings. We validate an existing state-of-the-art DRL framework used for AFC problems and discuss its efficiency bottlenecks. Subsequently, by deconstructing the overall framework and conducting extensive scalability <b>benchmarks</b> for individual components, we investigate various hybrid parallelization configurations and propose efficient parallelization strategies. Moreover, we refine input/output (I/O) operations in multi-environment DRL training to tackle critical overhead associated with data movement. Finally, we demonstrate the optimized framework for a typical AFC problem where near-linear scaling can be obtained for the overall framework. We achieve a significant boost in parallel efficiency from around 49% to approximately 78%, and the training process is accelerated by approximately 47 times using 60 CPU cores. These findings are expected to provide valuable insights for further advancements in DRL-based AFC studies.</p></p class="citation"></blockquote><h3 id=1926--91165-balanced-data-imbalanced-spectra-unveiling-class-disparities-with-spectral-imbalance-chiraag-kaushik-et-al-2024>(19/26 | 91/165) Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance (Chiraag Kaushik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chiraag Kaushik, Ran Liu, Chi-Heng Lin, Amrit Khera, Matthew Y Jin, Wenrui Ma, Vidya Muthukumar, Eva L Dyer. (2024)<br><strong>Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance</strong><br><button class=copy-to-clipboard title="Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11742v1.pdf filename=2402.11742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. We then study this phenomenon in 11 different state-of-the-art pretrained encoders and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine <b>data</b> <b>augmentation</b> strategies to mitigate the issue. Our work sheds light on the class-dependent effects of learning, and provides new insights into how state-of-the-art pretrained features may have unknown biases that can be diagnosed through their spectra.</p></p class="citation"></blockquote><h3 id=2026--92165-extraction-of-nonlinearity-in-neural-networks-and-model-compression-with-koopman-operator-naoki-sugishita-et-al-2024>(20/26 | 92/165) Extraction of nonlinearity in neural networks and model compression with Koopman operator (Naoki Sugishita et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naoki Sugishita, Kayo Kinjo, Jun Ohkubo. (2024)<br><strong>Extraction of nonlinearity in neural networks and model compression with Koopman operator</strong><br><button class=copy-to-clipboard title="Extraction of nonlinearity in neural networks and model compression with Koopman operator" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Model Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11740v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11740v1.pdf filename=2402.11740v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nonlinearity plays a crucial role in deep neural networks. In this paper, we first investigate the degree to which the nonlinearity of the neural network is essential. For this purpose, we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format. The results imply that restricted nonlinearity is enough for the classification of handwritten numbers. Then, we propose a <b>model</b> <b>compression</b> method for deep neural networks, which could be beneficial to handling large networks in resource-constrained environments. Leveraging the Koopman operator, the proposed method enables us to use linear algebra in the internal processing of neural networks. We numerically show that the proposed method performs comparably or better than conventional methods in highly compressed <b>model</b> <b>settings</b> for the handwritten number recognition task.</p></p class="citation"></blockquote><h3 id=2126--93165-compression-repair-for-feedforward-neural-networks-based-on-model-equivalence-evaluation-zihao-mo-et-al-2024>(21/26 | 93/165) Compression Repair for Feedforward Neural Networks Based on Model Equivalence Evaluation (Zihao Mo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Mo, Yejiang Yang, Shuaizheng Lu, Weiming Xiang. (2024)<br><strong>Compression Repair for Feedforward Neural Networks Based on Model Equivalence Evaluation</strong><br><button class=copy-to-clipboard title="Compression Repair for Feedforward Neural Networks Based on Model Equivalence Evaluation" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11737v1.pdf filename=2402.11737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a method of repairing compressed Feedforward Neural Networks (FNNs) based on equivalence evaluation of two neural networks. In the repairing framework, a novel neural network equivalence evaluation method is developed to compute the output discrepancy between two neural networks. The output discrepancy can quantitatively characterize the output difference produced by compression procedures. Based on the computed output discrepancy, the repairing method first initializes a new training set for the compressed networks to narrow down the discrepancy between the two neural networks and improve the performance of the compressed network. Then, we repair the compressed FNN by re-training based on the training set. We apply our developed method to the <b>MNIST</b> dataset to demonstrate the effectiveness and advantages of our proposed repair method.</p></p class="citation"></blockquote><h3 id=2226--94165-learning-conditional-invariances-through-non-commutativity-abhra-chaudhuri-et-al-2024>(22/26 | 94/165) Learning Conditional Invariances through Non-Commutativity (Abhra Chaudhuri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhra Chaudhuri, Serban Georgescu, Anjan Dutta. (2024)<br><strong>Learning Conditional Invariances through Non-Commutativity</strong><br><button class=copy-to-clipboard title="Learning Conditional Invariances through Non-Commutativity" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11682v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11682v1.pdf filename=2402.11682v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Invariance learning algorithms that conditionally filter out <b>domain-specific</b> <b>random</b> variables as distractors, do so based only on the data semantics, and not the target <b>domain</b> <b>under</b> evaluation. We show that a provably optimal and sample-efficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target <b>domain.</b> <b>Under</b> <b>domain</b> <b>asymmetry,</b> i.e., when the target <b>domain</b> <b>contains</b> semantically relevant information absent in the source, the risk of the encoder $\varphi^<em>$ that is optimal on average across <b>domains</b> <b>is</b> strictly lower-bounded by the risk of the target-specific optimal encoder $\Phi^</em><em>\tau$. We prove that non-commutativity steers the optimization towards $\Phi^*</em>\tau$ instead of $\varphi^<em>$, bringing the $\mathcal{H}$-divergence between <b>domains</b> <b>down</b> to zero, leading to a stricter bound on the target risk. Both our theory and experiments demonstrate that non-commutative invariance (NCI) can leverage source <b>domain</b> <b>samples</b> to meet the sample complexity needs of learning $\Phi^</em>_\tau$, surpassing SOTA invariance learning algorithms for <b>domain</b> <b>adaptation,</b> at times by over $2%$, approaching the performance of an oracle. Implementation is available at <a href=https://github.com/abhrac/nci>https://github.com/abhrac/nci</a>.</p></p class="citation"></blockquote><h3 id=2326--95165-theoretical-foundations-for-programmatic-reinforcement-learning-guruprerana-shabadi-et-al-2024>(23/26 | 95/165) Theoretical foundations for programmatic reinforcement learning (Guruprerana Shabadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guruprerana Shabadi, Nathanaël Fijalkow, Théo Matricon. (2024)<br><strong>Theoretical foundations for programmatic reinforcement learning</strong><br><button class=copy-to-clipboard title="Theoretical foundations for programmatic reinforcement learning" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-LO, cs-PL, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11650v1.pdf filename=2402.11650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of <b>Reinforcement</b> <b>Learning</b> (RL) is concerned with algorithms for learning optimal policies in unknown stochastic environments. Programmatic RL studies representations of policies as programs, meaning involving higher order constructs such as control loops. Despite attracting a lot of attention at the intersection of the machine learning and formal methods communities, very little is known on the theoretical front about programmatic RL: what are good classes of programmatic policies? How large are optimal programmatic policies? How can we learn them? The goal of this paper is to give first answers to these questions, initiating a theoretical study of programmatic RL.</p></p class="citation"></blockquote><h3 id=2426--96165-improved-indoor-localization-with-machine-learning-techniques-for-iot-applications-m-w-p-maduranga-2024>(24/26 | 96/165) Improved Indoor Localization with Machine Learning Techniques for IoT applications (M. W. P. Maduranga, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M. W. P. Maduranga. (2024)<br><strong>Improved Indoor Localization with Machine Learning Techniques for IoT applications</strong><br><button class=copy-to-clipboard title="Improved Indoor Localization with Machine Learning Techniques for IoT applications" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11433v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11433v1.pdf filename=2402.11433v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of the Internet of Things (IoT) and mobile internet applications has spurred interest in location-based services (LBS) for commercial, military, and social applications. While the global positioning system (GPS) dominates outdoor localization, its efficacy wanes indoors due to signal challenges. Indoor localization systems leverage wireless technologies like Wi-Fi, ZigBee, Bluetooth, UWB, selecting based on context. Received signal strength indicator (RSSI) technology, known for its accuracy and simplicity, is widely adopted. This study employs machine learning algorithms in three phases: <b>supervised</b> regressors, <b>supervised</b> classifiers, and ensemble methods for RSSI-based indoor localization. Additionally, it introduces a weighted least squares technique and pseudo-linear solution approach to address non-linear RSSI measurement equations by approximating them with linear equations. An experimental testbed, utilizing diverse wireless technologies and anchor nodes, is designed for data collection, employing IoT cloud architectures. Pre-processing involves investigating filters for data refinement before algorithm training. The study employs machine learning models like linear regression, polynomial regression, support vector regression, random forest regression, and decision tree regressor across various wireless technologies. These models estimate the geographical coordinates of a moving target node, and their performance is evaluated using metrics such as accuracy, root mean square errors, precision, recall, sensitivity, coefficient of determinant, and the f1-score. The experiment&rsquo;s outcomes provide insights into the effectiveness of different <b>supervised</b> machine learning techniques in terms of localization accuracy and robustness in indoor environments.</p></p class="citation"></blockquote><h3 id=2526--97165-learning-the-topology-and-behavior-of-discrete-dynamical-systems-zirou-qiu-et-al-2024>(25/26 | 97/165) Learning the Topology and Behavior of Discrete Dynamical Systems (Zirou Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zirou Qiu, Abhijin Adiga, Madhav V. Marathe, S. S. Ravi, Daniel J. Rosenkrantz, Richard E. Stearns, Anil Vullikanti. (2024)<br><strong>Learning the Topology and Behavior of Discrete Dynamical Systems</strong><br><button class=copy-to-clipboard title="Learning the Topology and Behavior of Discrete Dynamical Systems" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 8<br>Keywords: Graph, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11686v1.pdf filename=2402.11686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Discrete dynamical systems are commonly used to model the spread of contagions on real-world networks. Under the PAC framework, existing research has studied the problem of learning the behavior of a system, assuming that the underlying network is known. In this work, we focus on a more challenging setting: to learn both the behavior and the underlying topology of a <b>black-box</b> <b>system.</b> We show that, in general, this learning problem is computationally intractable. On the positive side, we present efficient learning methods under the PAC model when the underlying <b>graph</b> of the dynamical system belongs to some classes. Further, we examine a relaxed setting where the topology of an unknown system is partially observed. For this case, we develop an efficient PAC learner to infer the system and establish the sample complexity. Lastly, we present a formal analysis of the expressive power of the hypothesis class of dynamical systems where both the topology and behavior are unknown, using the well-known formalism of the Natarajan dimension. Our results provide a theoretical foundation for learning both the behavior and topology of discrete dynamical systems.</p></p class="citation"></blockquote><h3 id=2626--98165-prospector-heads-generalized-feature-attribution-for-large-models--data-gautam-machiraju-et-al-2024>(26/26 | 98/165) Prospector Heads: Generalized Feature Attribution for Large Models & Data (Gautam Machiraju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gautam Machiraju, Alexander Derry, Arjun Desai, Neel Guha, Amir-Hossein Karimi, James Zou, Russ Altman, Christopher Ré, Parag Mallick. (2024)<br><strong>Prospector Heads: Generalized Feature Attribution for Large Models & Data</strong><br><button class=copy-to-clipboard title="Prospector Heads: Generalized Feature Attribution for Large Models & Data" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, q-bio-QM<br>Keyword Score: 6<br>Keywords: Graph, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11729v1.pdf filename=2402.11729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for machine learning models in scientific and biomedical domains. Current methods for feature attribution, which rely on &ldquo;explaining&rdquo; the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small <b>sample</b> <b>sizes</b> and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based methods for feature attribution that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and <b>graphs</b> (protein structures), outperforming baseline attribution methods by up to 49 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpretation and discovery of class-specific patterns in the input data. Through their high performance, flexibility, and generalizability, prospectors provide a framework for improving trust and transparency for machine learning models in complex domains.</p></p class="citation"></blockquote><h2 id=cscv-23>cs.CV (23)</h2><h3 id=123--99165-visual-in-context-learning-for-large-vision-language-models-yucheng-zhou-et-al-2024>(1/23 | 99/165) Visual In-Context Learning for Large Vision-Language Models (Yucheng Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yucheng Zhou, Xiang Li, Qianning Wang, Jianbing Shen. (2024)<br><strong>Visual In-Context Learning for Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="Visual In-Context Learning for Large Vision-Language Models" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Rerank, Reasoning, In-context Learning, In-context Learning, In-context Learning, Summarization, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11574v1.pdf filename=2402.11574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Large Visual Language Models (LVLMs), the efficacy of <b>In-Context</b> <b>Learning</b> <b>(ICL)</b> remains limited by challenges in cross-modal interactions and representation disparities. To overcome these challenges, we introduce a novel Visual <b>In-Context</b> <b>Learning</b> (VICL) method comprising Visual Demonstration Retrieval, Intent-Oriented Image <b>Summarization,</b> and Intent-Oriented Demonstration Composition. Our approach retrieves images via &lsquo;&lsquo;Retrieval & <b>Rerank&rsquo;&rsquo;</b> paradigm, summarises images with task intent and task-specific visual parsing, and composes language-based demonstrations that reduce token count and alleviate cross-modal interaction problem. Experimental evaluations on five visual <b>reasoning</b> datasets demonstrate the effectiveness of our method. Moreover, our extensive experiments leverage information flow analysis to elucidate the effectiveness of our method, and investigate the impact of length and position of demonstrations for LVLM. The use of <b>in-context</b> <b>unlearning</b> further shows promise in resetting specific model knowledge without retraining.</p></p class="citation"></blockquote><h3 id=223--100165-data-distribution-distilled-generative-model-for-generalized-zero-shot-recognition-yijie-wang-et-al-2024>(2/23 | 100/165) Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition (Yijie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijie Wang, Mingjian Hong, Luwen Huangfu, Sheng Huang. (2024)<br><strong>Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition</strong><br><button class=copy-to-clipboard title="Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Knowledge Distillation, Knowledge Distillation, Out-of-distribution, Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11424v1.pdf filename=2402.11424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of <b>Zero-Shot</b> <b>Learning</b> (ZSL), we address biases in Generalized <b>Zero-Shot</b> <b>Learning</b> (GZSL) models, which favor seen data. To counter this, we introduce an end-to-end generative GZSL framework called D$^3$GZSL. This framework respects seen and synthesized unseen data as in-distribution and <b>out-of-distribution</b> data, respectively, for a more balanced model. D$^3$GZSL comprises two core modules: in-distribution dual space <b>distillation</b> (ID$^2$SD) and <b>out-of-distribution</b> batch <b>distillation</b> (O$^2$DBD). ID$^2$SD aligns teacher-student outcomes in embedding and label spaces, enhancing learning coherence. O$^2$DBD introduces low-dimensional <b>out-of-distribution</b> representations per batch sample, capturing shared structures between seen and unseen categories. Our approach demonstrates its effectiveness across established GZSL <b>benchmarks,</b> seamlessly integrating into mainstream generative frameworks. Extensive experiments consistently showcase that D$^3$GZSL elevates the performance of existing generative GZSL methods, underscoring its potential to refine <b>zero-shot</b> <b>learning</b> practices.The code is available at: <a href=https://github.com/PJBQ/D3GZSL.git>https://github.com/PJBQ/D3GZSL.git</a></p></p class="citation"></blockquote><h3 id=323--101165-boosting-semi-supervised-2d-human-pose-estimation-by-revisiting-data-augmentation-and-consistency-training-huayi-zhou-et-al-2024>(3/23 | 101/165) Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data Augmentation and Consistency Training (Huayi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huayi Zhou, Mukun Luo, Fei Jiang, Yue Ding, Hongtao Lu. (2024)<br><strong>Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data Augmentation and Consistency Training</strong><br><button class=copy-to-clipboard title="Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data Augmentation and Consistency Training" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Data Augmentation, Semi-Supervised Learning, Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11566v1.pdf filename=2402.11566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The 2D human pose estimation is a basic visual problem. However, <b>supervised</b> <b>learning</b> of a model requires massive labeled images, which is expensive and labor-intensive. In this paper, we aim at boosting the accuracy of a pose estimator by excavating extra unlabeled images in a <b>semi-supervised</b> <b>learning</b> (SSL) way. Most previous consistency-based SSL methods strive to constraint the model to predict consistent results for differently augmented images. Following this consensus, we revisit two core aspects including advanced <b>data</b> <b>augmentation</b> methods and concise consistency training frameworks. Specifically, we heuristically dig various collaborative combinations of existing <b>data</b> <b>augmentations,</b> and discover novel superior <b>data</b> <b>augmentation</b> schemes to more effectively add noise on unlabeled samples. They can compose easy-hard augmentation pairs with larger transformation difficulty gaps, which play a crucial role in consistency-based SSL. Moreover, we propose to strongly augment unlabeled images repeatedly with diverse augmentations, generate multi-path predictions sequentially, and optimize corresponding <b>unsupervised</b> consistency losses using one single network. This simple and compact design is on a par with previous methods consisting of dual or triple networks. Furthermore, it can also be integrated with multiple networks to produce better performance. Comparing to state-of-the-art SSL approaches, our method brings substantial improvements on public datasets. Code is released for academic use in \url{https://github.com/hnuzhy/MultiAugs}.</p></p class="citation"></blockquote><h3 id=423--102165-efficient-multimodal-learning-from-data-centric-perspective-muyang-he-et-al-2024>(4/23 | 102/165) Efficient Multimodal Learning from Data-centric Perspective (Muyang He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, Bo Zhao. (2024)<br><strong>Efficient Multimodal Learning from Data-centric Perspective</strong><br><button class=copy-to-clipboard title="Efficient Multimodal Learning from Data-centric Perspective" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Reasoning, Large Language Model, Scaling Law, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11530v1.pdf filename=2402.11530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have demonstrated notable capabilities in general visual understanding and <b>reasoning</b> tasks. However, their deployment is hindered by substantial computational costs in both training and inference, limiting accessibility to the broader research and user communities. A straightforward solution is to leverage smaller pre-trained vision and language models, which inevitably causes significant performance drop. In this paper, we demonstrate the possibility to beat the <b>scaling</b> <b>law</b> and train a smaller but better MLLM by exploring more informative training data. Specifically, we introduce Bunny, a family of lightweight MLLMs with flexible vision and language backbones for efficient <b>multimodal</b> learning from condensed training data. Remarkably, our Bunny-3B outperforms the state-of-the-art <b>large</b> <b>MLLMs,</b> <b>especially</b> LLaVA-v1.5-13B, on multiple <b>benchmarks.</b> The code, models and data can be found in <a href=https://github.com/BAAI-DCAI/Bunny>https://github.com/BAAI-DCAI/Bunny</a>.</p></p class="citation"></blockquote><h3 id=523--103165-momentor-advancing-video-large-language-model-with-fine-grained-temporal-reasoning-long-qian-et-al-2024>(5/23 | 103/165) Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning (Long Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, Siliang Tang. (2024)<br><strong>Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning</strong><br><button class=copy-to-clipboard title="Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Zero-shot, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11435v1.pdf filename=2402.11435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrate remarkable proficiency in comprehending and handling text-based tasks. Many efforts are being made to transfer these attributes to video modality, which are termed Video-LLMs. However, existing Video-LLMs can only capture the coarse-grained semantics and are unable to effectively handle tasks related to comprehension or localization of specific video segments. In light of these challenges, we propose Momentor, a Video-LLM capable of accomplishing fine-grained temporal understanding tasks. To support the training of Momentor, we design an automatic data generation engine to construct Moment-10M, a <b>large-scale</b> <b>video</b> <b>instruction</b> dataset with segment-level instruction data. We train Momentor on Moment-10M, enabling it to perform segment-level <b>reasoning</b> and localization. <b>Zero-shot</b> evaluations on several tasks demonstrate that Momentor excels in fine-grained temporally grounded comprehension and localization.</p></p class="citation"></blockquote><h3 id=623--104165-logical-closed-loop-uncovering-object-hallucinations-in-large-vision-language-models-junfei-wu-et-al-2024>(6/23 | 104/165) Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models (Junfei Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan. (2024)<br><strong>Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Hallucination Detection, Instruction Tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11622v1.pdf filename=2402.11622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Object <b>hallucination</b> <b>has</b> been an Achilles&rsquo; heel which hinders the broader applications of large <b>vision-language</b> models (LVLMs). Object <b>hallucination</b> <b>refers</b> to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object <b>hallucinations,</b> <b>instruction</b> <b>tuning</b> and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object <b>hallucinations.</b> <b>In</b> this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object <b>Hallucination</b> <b>Detection</b> and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object <b>hallucination.</b> <b>As</b> a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three <b>benchmarks</b> across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality.</p></p class="citation"></blockquote><h3 id=723--105165-polypnextlstm-a-lightweight-and-fast-polyp-video-segmentation-network-using-convnext-and-convlstm-debayan-bhattacharya-et-al-2024>(7/23 | 105/165) PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM (Debayan Bhattacharya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Debayan Bhattacharya, Konrad Reuter, Finn Behrendnt, Lennart Maack, Sarah Grube, Alexander Schlaefer. (2024)<br><strong>PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM</strong><br><button class=copy-to-clipboard title="PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11585v1.pdf filename=2402.11585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Commonly employed in polyp segmentation, single image UNet architectures lack the temporal insight clinicians gain from video data in diagnosing polyps. To mirror clinical practices more faithfully, our proposed solution, PolypNextLSTM, leverages video-based deep learning, harnessing temporal information for superior segmentation performance with the least parameter overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting the last two layers to reduce parameter overhead. Our temporal fusion module, a <b>Convolutional</b> <b>Long</b> <b>Short</b> <b>Term</b> <b>Memory</b> (ConvLSTM), effectively exploits temporal features. Our primary novelty lies in PolypNextLSTM, which stands out as the leanest in parameters and the fastest model, surpassing the performance of five state-of-the-art image and video-based deep learning models. The evaluation of the SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios, along with videos containing challenging artefacts like fast motion and occlusion.</p></p class="citation"></blockquote><h3 id=823--106165-a-novel-fourier-neural-operator-framework-for-classification-of-multi-sized-images-application-to-3d-digital-porous-media-ali-kashefi-et-al-2024>(8/23 | 106/165) A novel Fourier neural operator framework for classification of multi-sized images: Application to 3D digital porous media (Ali Kashefi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Kashefi, Tapan Mukerji. (2024)<br><strong>A novel Fourier neural operator framework for classification of multi-sized images: Application to 3D digital porous media</strong><br><button class=copy-to-clipboard title="A novel Fourier neural operator framework for classification of multi-sized images: Application to 3D digital porous media" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11568v1.pdf filename=2402.11568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fourier neural operators (FNOs) are invariant with respect to the size of input images, and thus images with any size can be fed into FNO-based frameworks without any modification of network architectures, in contrast to traditional <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs).</b> Leveraging the advantage of FNOs, we propose a novel deep-learning framework for classifying images with varying sizes. Particularly, we simultaneously train the proposed network on multi-sized images. As a practical application, we consider the problem of predicting the label (e.g., permeability) of three-dimensional digital porous media. To construct the framework, an intuitive approach is to connect FNO layers to a classifier using adaptive max pooling. First, we show that this approach is only effective for porous media with fixed sizes, whereas it fails for porous media of varying sizes. To overcome this limitation, we introduce our approach: instead of using adaptive max pooling, we use static max pooling with the size of channel width of FNO layers. Since the channel width of the FNO layers is independent of input image size, the introduced framework can handle multi-sized images during training. We show the effectiveness of the introduced framework and compare its performance with the intuitive approach through the example of the classification of three-dimensional digital porous media of varying sizes.</p></p class="citation"></blockquote><h3 id=923--107165-mal-motion-aware-loss-with-temporal-and-distillation-hints-for-self-supervised-depth-estimation-yup-jiang-dong-et-al-2024>(9/23 | 107/165) MAL: Motion-Aware Loss with Temporal and Distillation Hints for Self-Supervised Depth Estimation (Yup-Jiang Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yup-Jiang Dong, Fang-Lue Zhang, Song-Hai Zhang. (2024)<br><strong>MAL: Motion-Aware Loss with Temporal and Distillation Hints for Self-Supervised Depth Estimation</strong><br><button class=copy-to-clipboard title="MAL: Motion-Aware Loss with Temporal and Distillation Hints for Self-Supervised Depth Estimation" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Knowledge Distillation, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11507v1.pdf filename=2402.11507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Depth perception is crucial for a wide range of robotic applications. Multi-frame <b>self-supervised</b> depth estimation methods have gained research interest due to their ability to leverage large-scale, unlabeled real-world data. However, the <b>self-supervised</b> methods often rely on the assumption of a static scene and their performance tends to degrade in dynamic environments. To address this issue, we present Motion-Aware Loss, which leverages the temporal relation among consecutive input frames and a novel <b>distillation</b> scheme between the teacher and student networks in the multi-frame <b>self-supervised</b> depth estimation methods. Specifically, we associate the spatial locations of moving objects with the temporal order of input frames to eliminate errors induced by object motion. Meanwhile, we enhance the original <b>distillation</b> scheme in multi-frame methods to better exploit the knowledge from a teacher network. MAL is a novel, plug-and-play module designed for seamless integration into multi-frame <b>self-supervised</b> monocular depth estimation methods. Adding MAL into previous state-of-the-art methods leads to a reduction in depth estimation errors by up to 4.2% and 10.8% on KITTI and CityScapes <b>benchmarks,</b> respectively.</p></p class="citation"></blockquote><h3 id=1023--108165-genad-generative-end-to-end-autonomous-driving-wenzhao-zheng-et-al-2024>(10/23 | 108/165) GenAD: Generative End-to-End Autonomous Driving (Wenzhao Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenzhao Zheng, Ruiqi Song, Xianda Guo, Long Chen. (2024)<br><strong>GenAD: Generative End-to-End Autonomous Driving</strong><br><button class=copy-to-clipboard title="GenAD: Generative End-to-End Autonomous Driving" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Autoencoder, Benchmarking, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11502v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11502v2.pdf filename=2402.11502v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Directly producing planning results from raw sensors has been a long-desired solution for autonomous driving and has attracted increasing attention recently. Most existing end-to-end autonomous driving methods factorize this problem into perception, motion prediction, and planning. However, we argue that the conventional progressive pipeline still cannot comprehensively model the entire traffic evolution process, e.g., the future interaction between the ego car and other traffic participants and the structural trajectory prior. In this paper, we explore a new paradigm for end-to-end autonomous driving, where the key is to predict how the ego car and the surroundings evolve given past scenes. We propose GenAD, a generative framework that casts autonomous driving into a generative modeling problem. We propose an instance-centric scene tokenizer that first transforms the surrounding scenes into map-aware instance tokens. We then employ a <b>variational</b> <b>autoencoder</b> to learn the future trajectory distribution in a structural latent space for trajectory prior modeling. We further adopt a temporal model to capture the agent and ego movements in the latent space to generate more effective future trajectories. GenAD finally simultaneously performs motion prediction and planning by sampling distributions in the learned structural latent space conditioned on the instance tokens and using the learned temporal model to generate futures. Extensive experiments on the widely used nuScenes <b>benchmark</b> show that the proposed GenAD achieves state-of-the-art performance on vision-centric end-to-end autonomous driving with high efficiency. Code: <a href=https://github.com/wzzheng/GenAD>https://github.com/wzzheng/GenAD</a>.</p></p class="citation"></blockquote><h3 id=1123--109165-3d-point-cloud-compression-with-recurrent-neural-network-and-image-compression-methods-till-beemelmanns-et-al-2024>(11/23 | 109/165) 3D Point Cloud Compression with Recurrent Neural Network and Image Compression Methods (Till Beemelmanns et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Till Beemelmanns, Yuchen Tao, Bastian Lampe, Lennart Reiher, Raphael van Kempen, Timo Woopen, Lutz Eckstein. (2024)<br><strong>3D Point Cloud Compression with Recurrent Neural Network and Image Compression Methods</strong><br><button class=copy-to-clipboard title="3D Point Cloud Compression with Recurrent Neural Network and Image Compression Methods" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11680v1.pdf filename=2402.11680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Storing and transmitting LiDAR point cloud data is essential for many AV applications, such as training data collection, remote control, cloud services or SLAM. However, due to the sparsity and unordered structure of the data, it is difficult to compress point cloud data to a low volume. Transforming the raw point cloud data into a dense 2D matrix structure is a promising way for applying compression algorithms. We propose a new lossless and calibrated 3D-to-2D transformation which allows compression algorithms to efficiently exploit spatial correlations within the 2D representation. To compress the structured representation, we use common image compression methods and also a <b>self-supervised</b> deep compression approach using a <b>recurrent</b> <b>neural</b> <b>network.</b> We also rearrange the LiDAR&rsquo;s intensity measurements to a dense 2D representation and propose a new metric to evaluate the compression performance of the intensity. Compared to approaches that are based on generic octree point cloud compression or based on raw point cloud data compression, our approach achieves the best quantitative and visual performance. Source code and dataset are available at <a href=https://github.com/ika-rwth-aachen/Point-Cloud-Compression>https://github.com/ika-rwth-aachen/Point-Cloud-Compression</a>.</p></p class="citation"></blockquote><h3 id=1223--110165-interactive-garment-recommendation-with-user-in-the-loop-federico-becattini-et-al-2024>(12/23 | 110/165) Interactive Garment Recommendation with User in the Loop (Federico Becattini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Federico Becattini, Xiaolin Chen, Andrea Puccia, Haokun Wen, Xuemeng Song, Liqiang Nie, Alberto Del Bimbo. (2024)<br><strong>Interactive Garment Recommendation with User in the Loop</strong><br><button class=copy-to-clipboard title="Interactive Garment Recommendation with User in the Loop" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IR, cs.CV<br>Keyword Score: 20<br>Keywords: Recommendation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11627v1.pdf filename=2402.11627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recommending fashion items often leverages rich user profiles and makes targeted suggestions based on past history and previous purchases. In this paper, we work under the assumption that no prior knowledge is given about a user. We propose to build a user profile on the fly by integrating user reactions as we recommend complementary items to compose an outfit. We present a <b>reinforcement</b> <b>learning</b> agent capable of suggesting appropriate garments and ingesting user feedback so to improve its <b>recommendations</b> and maximize user satisfaction. To train such a model, we resort to a proxy model to be able to simulate having user feedback in the training loop. We experiment on the IQON3000 fashion dataset and we find that a <b>reinforcement</b> <b>learning-based</b> agent becomes capable of improving its <b>recommendations</b> by taking into account personal preferences. Furthermore, such task demonstrated to be hard for non-reinforcement models, that cannot exploit exploration during training.</p></p class="citation"></blockquote><h3 id=1323--111165-sdit-spiking-diffusion-model-with-transformer-shu-yang-et-al-2024>(13/23 | 111/165) SDiT: Spiking Diffusion Model with Transformer (Shu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shu Yang, Hanzhi Ma, Chengting Yu, Aili Wang, Er-Ping Li. (2024)<br><strong>SDiT: Spiking Diffusion Model with Transformer</strong><br><button class=copy-to-clipboard title="SDiT: Spiking Diffusion Model with Transformer" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: MNIST, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11588v1.pdf filename=2402.11588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spiking neural networks (SNNs) have low power consumption and bio-interpretable characteristics, and are considered to have tremendous potential for energy-efficient computing. However, the exploration of SNNs on image generation tasks remains very limited, and a unified and effective structure for SNN-based generative models has yet to be proposed. In this paper, we explore a novel diffusion model architecture within spiking neural networks. We utilize <b>transformer</b> to replace the commonly used U-net structure in mainstream diffusion models. It can generate higher quality images with relatively lower computational cost and shorter sampling time. It aims to provide an empirical baseline for research of generative models based on SNNs. Experiments on <b>MNIST,</b> Fashion-MNIST, and CIFAR-10 datasets demonstrate that our work is highly competitive compared to existing SNN generative models.</p></p class="citation"></blockquote><h3 id=1423--112165-thyroid-ultrasound-diagnosis-improvement-via-multi-view-self-supervised-learning-and-two-stage-pre-training-jian-wang-et-al-2024>(14/23 | 112/165) Thyroid ultrasound diagnosis improvement via multi-view self-supervised learning and two-stage pre-training (Jian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Wang, Xin Yang, Xiaohong Jia, Wufeng Xue, Rusi Chen, Yanlin Chen, Xiliang Zhu, Lian Liu, Yan Cao, Jianqiao Zhou, Dong Ni, Ning Gu. (2024)<br><strong>Thyroid ultrasound diagnosis improvement via multi-view self-supervised learning and two-stage pre-training</strong><br><button class=copy-to-clipboard title="Thyroid ultrasound diagnosis improvement via multi-view self-supervised learning and two-stage pre-training" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11497v1.pdf filename=2402.11497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Thyroid nodule classification and segmentation in ultrasound images are crucial for computer-aided diagnosis; however, they face limitations owing to insufficient labeled data. In this study, we proposed a multi-view contrastive <b>self-supervised</b> <b>method</b> to improve thyroid nodule classification and segmentation performance with limited manual labels. Our method aligns the transverse and longitudinal views of the same nodule, thereby enabling the model to focus more on the nodule area. We designed an adaptive loss function that eliminates the limitations of the paired data. Additionally, we adopted a two-stage pre-training to exploit the pre-training on ImageNet and thyroid ultrasound images. Extensive experiments were conducted on a large-scale dataset collected from multiple centers. The results showed that the proposed method significantly improves nodule classification and segmentation performance with limited manual labels and outperforms state-of-the-art <b>self-supervised</b> <b>methods.</b> The two-stage pre-training also significantly exceeded ImageNet pre-training.</p></p class="citation"></blockquote><h3 id=1523--113165-multicorrupt-a-multi-modal-robustness-dataset-and-benchmark-of-lidar-camera-fusion-for-3d-object-detection-till-beemelmanns-et-al-2024>(15/23 | 113/165) MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection (Till Beemelmanns et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Till Beemelmanns, Quan Zhang, Lutz Eckstein. (2024)<br><strong>MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection</strong><br><button class=copy-to-clipboard title="MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Object Detection, Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11677v1.pdf filename=2402.11677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> 3D <b>object</b> <b>detection</b> models for automated driving have demonstrated exceptional performance on computer vision <b>benchmarks</b> like nuScenes. However, their reliance on densely sampled LiDAR point clouds and meticulously calibrated sensor arrays poses challenges for real-world applications. Issues such as sensor misalignment, miscalibration, and disparate sampling frequencies lead to spatial and temporal misalignment in data from LiDAR and cameras. Additionally, the integrity of LiDAR and camera data is often compromised by adverse environmental conditions such as inclement weather, leading to occlusions and noise interference. To address this challenge, we introduce MultiCorrupt, a comprehensive <b>benchmark</b> designed to evaluate the robustness of <b>multi-modal</b> 3D <b>object</b> <b>detectors</b> against ten distinct types of corruptions. We evaluate five state-of-the-art <b>multi-modal</b> detectors on MultiCorrupt and analyze their performance in terms of their resistance ability. Our results show that existing methods exhibit varying degrees of robustness depending on the type of corruption and their fusion strategy. We provide insights into which <b>multi-modal</b> design choices make such models robust against certain perturbations. The dataset generation code and <b>benchmark</b> are open-sourced at <a href=https://github.com/ika-rwth-aachen/MultiCorrupt>https://github.com/ika-rwth-aachen/MultiCorrupt</a>.</p></p class="citation"></blockquote><h3 id=1623--114165-challenging-the-black-box-a-comprehensive-evaluation-of-attribution-maps-of-cnn-applications-in-agriculture-and-forestry-lars-nieradzik-et-al-2024>(16/23 | 114/165) Challenging the Black Box: A Comprehensive Evaluation of Attribution Maps of CNN Applications in Agriculture and Forestry (Lars Nieradzik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lars Nieradzik, Henrike Stephani, Jördis Sieburg-Rockel, Stephanie Helmling, Andrea Olbrich, Janis Keuper. (2024)<br><strong>Challenging the Black Box: A Comprehensive Evaluation of Attribution Maps of CNN Applications in Agriculture and Forestry</strong><br><button class=copy-to-clipboard title="Challenging the Black Box: A Comprehensive Evaluation of Attribution Maps of CNN Applications in Agriculture and Forestry" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 15<br>Keywords: Black Box, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11670v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11670v1.pdf filename=2402.11670v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we explore the explainability of neural networks in agriculture and forestry, specifically in fertilizer treatment classification and wood identification. The opaque nature of these models, often considered <b>&lsquo;black</b> <b>boxes&rsquo;,</b> is addressed through an extensive evaluation of state-of-the-art Attribution Maps (AMs), also known as class activation maps (CAMs) or saliency maps. Our comprehensive qualitative and quantitative analysis of these AMs uncovers critical practical limitations. Findings reveal that AMs frequently fail to consistently highlight crucial features and often misalign with the features considered important by domain experts. These discrepancies raise substantial questions about the utility of AMs in understanding the decision-making process of neural networks. Our study provides critical insights into the trustworthiness and practicality of AMs within the agriculture and forestry sectors, thus facilitating a better understanding of neural networks in these application areas.</p></p class="citation"></blockquote><h3 id=1723--115165-cross-attention-fusion-of-visual-and-geometric-features-for-large-vocabulary-arabic-lipreading-samar-daou-et-al-2024>(17/23 | 115/165) Cross-Attention Fusion of Visual and Geometric Features for Large Vocabulary Arabic Lipreading (Samar Daou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samar Daou, Ahmed Rekik, Achraf Ben-Hamadou, Abdelaziz Kallel. (2024)<br><strong>Cross-Attention Fusion of Visual and Geometric Features for Large Vocabulary Arabic Lipreading</strong><br><button class=copy-to-clipboard title="Cross-Attention Fusion of Visual and Geometric Features for Large Vocabulary Arabic Lipreading" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11520v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11520v1.pdf filename=2402.11520v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lipreading involves using visual data to recognize spoken words by analyzing the movements of the lips and surrounding area. It is a hot research topic with many potential applications, such as human-machine interaction and enhancing audio <b>speech</b> <b>recognition.</b> Recent deep-learning based works aim to integrate visual features extracted from the mouth region with landmark points on the lip contours. However, employing a simple combination method such as concatenation may not be the most effective approach to get the optimal feature vector. To address this challenge, firstly, we propose a cross-attention fusion-based approach for large lexicon Arabic vocabulary to predict spoken words in videos. Our method leverages the power of cross-attention networks to efficiently integrate visual and geometric features computed on the mouth region. Secondly, we introduce the first large-scale Lip Reading in the Wild for Arabic (LRW-AR) dataset containing 20,000 videos for 100-word classes, uttered by 36 speakers. The experimental results obtained on LRW-AR and ArabicVisual databases showed the effectiveness and robustness of the proposed approach in recognizing Arabic words. Our work provides insights into the feasibility and effectiveness of applying lipreading techniques to the Arabic language, opening doors for further research in this field. Link to the project page: <a href=https://crns-smartvision.github.io/lrwar>https://crns-smartvision.github.io/lrwar</a></p></p class="citation"></blockquote><h3 id=1823--116165-visual-concept-driven-image-generation-with-text-to-image-diffusion-model-tanzila-rahman-et-al-2024>(18/23 | 116/165) Visual Concept-driven Image Generation with Text-to-Image Diffusion Model (Tanzila Rahman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanzila Rahman, Shweta Mahajan, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Leonid Sigal. (2024)<br><strong>Visual Concept-driven Image Generation with Text-to-Image Diffusion Model</strong><br><button class=copy-to-clipboard title="Visual Concept-driven Image Generation with Text-to-Image Diffusion Model" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11487v1.pdf filename=2402.11487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> (TTI) diffusion models have demonstrated impressive results in generating high-resolution images of complex and imaginative scenes. Recent approaches have further extended these methods with personalization techniques that allow them to integrate user-illustrated concepts (e.g., the user him/herself) using a few sample image illustrations. However, the ability to generate images with multiple interacting concepts, such as human subjects, as well as concepts that may be entangled in one, or across multiple, image illustrations remains illusive. In this work, we propose a concept-driven TTI personalization framework that addresses these core challenges. We build on existing works that learn custom tokens for user-illustrated concepts, allowing those to interact with existing text tokens in the TTI model. However, importantly, to disentangle and better learn the concepts in question, we jointly learn (latent) segmentation masks that disentangle these concepts in user-provided image illustrations. We do so by introducing an Expectation Maximization (EM)-like optimization procedure where we alternate between learning the custom tokens and estimating masks encompassing corresponding concepts in user-supplied images. We obtain these masks based on cross-attention, from within the U-Net parameterized latent diffusion model and subsequent Dense CRF optimization. We illustrate that such joint alternating refinement leads to the learning of better tokens for concepts and, as a bi-product, latent masks. We illustrate the benefits of the proposed approach qualitatively and quantitatively (through user studies) with a number of examples and use cases that can combine up to three entangled concepts.</p></p class="citation"></blockquote><h3 id=1923--117165-endoood-uncertainty-aware-out-of-distribution-detection-in-capsule-endoscopy-diagnosis-qiaozhi-tan-et-al-2024>(19/23 | 117/165) EndoOOD: Uncertainty-aware Out-of-distribution Detection in Capsule Endoscopy Diagnosis (Qiaozhi Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiaozhi Tan, Long Bai, Guankun Wang, Mobarakol Islam, Hongliang Ren. (2024)<br><strong>EndoOOD: Uncertainty-aware Out-of-distribution Detection in Capsule Endoscopy Diagnosis</strong><br><button class=copy-to-clipboard title="EndoOOD: Uncertainty-aware Out-of-distribution Detection in Capsule Endoscopy Diagnosis" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11476v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11476v1.pdf filename=2402.11476v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wireless capsule endoscopy (WCE) is a non-invasive diagnostic procedure that enables visualization of the gastrointestinal (GI) tract. Deep learning-based methods have shown effectiveness in disease screening using WCE data, alleviating the burden on healthcare professionals. However, existing capsule endoscopy classification methods mostly rely on pre-defined categories, making it challenging to identify and classify <b>out-of-distribution</b> (OOD) data, such as undefined categories or anatomical landmarks. To address this issue, we propose the Endoscopy <b>Out-of-Distribution</b> (EndoOOD) framework, which aims to effectively handle the OOD detection challenge in WCE diagnosis. The proposed framework focuses on improving the robustness and reliability of WCE diagnostic capabilities by incorporating uncertainty-aware mixup training and long-tailed in-distribution (ID) data calibration techniques. Additionally, virtual-logit matching is employed to accurately distinguish between OOD and ID data while minimizing information loss. To assess the performance of our proposed solution, we conduct evaluations and comparisons with 12 state-of-the-art (SOTA) methods using two publicly available datasets. The results demonstrate the effectiveness of the proposed framework in enhancing diagnostic accuracy and supporting clinical decision-making.</p></p class="citation"></blockquote><h3 id=2023--118165-key-patch-proposer-key-patches-contain-rich-information-jing-xu-et-al-2024>(20/23 | 118/165) Key Patch Proposer: Key Patches Contain Rich Information (Jing Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Xu, Beiwen Tian, Hao Zhao. (2024)<br><strong>Key Patch Proposer: Key Patches Contain Rich Information</strong><br><button class=copy-to-clipboard title="Key Patch Proposer: Key Patches Contain Rich Information" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11458v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11458v1.pdf filename=2402.11458v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel algorithm named Key Patch Proposer (KPP) designed to select key patches in an image without additional training. Our experiments showcase KPP&rsquo;s robust capacity to capture semantic information by both reconstruction and classification tasks. The efficacy of KPP suggests its potential application in <b>active</b> <b>learning</b> for semantic segmentation. Our source code is publicly available at <a href=https://github.com/CA-TT-AC/key-patch-proposer>https://github.com/CA-TT-AC/key-patch-proposer</a>.</p></p class="citation"></blockquote><h3 id=2123--119165-a-multispectral-automated-transfer-technique-matt-for-machine-driven-image-labeling-utilizing-the-segment-anything-model-sam-james-e-gallagher-et-al-2024>(21/23 | 119/165) A Multispectral Automated Transfer Technique (MATT) for machine-driven image labeling utilizing the Segment Anything Model (SAM) (James E. Gallagher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James E. Gallagher, Aryav Gogia, Edward J. Oughton. (2024)<br><strong>A Multispectral Automated Transfer Technique (MATT) for machine-driven image labeling utilizing the Segment Anything Model (SAM)</strong><br><button class=copy-to-clipboard title="A Multispectral Automated Transfer Technique (MATT) for machine-driven image labeling utilizing the Segment Anything Model (SAM)" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11413v1.pdf filename=2402.11413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Segment Anything Model (SAM) is drastically accelerating the speed and accuracy of automatically segmenting and labeling large Red-Green-Blue (RGB) imagery datasets. However, SAM is unable to segment and label images outside of the visible light spectrum, for example, for multispectral or hyperspectral imagery. Therefore, this paper outlines a method we call the Multispectral Automated Transfer Technique (MATT). By transposing SAM segmentation masks from RGB images we can automatically segment and label multispectral imagery with high precision and efficiency. For example, the results demonstrate that segmenting and labeling a 2,400-image dataset utilizing MATT achieves a time reduction of 87.8% in developing a trained model, reducing roughly 20 hours of manual labeling, to only 2.4 hours. This efficiency gain is associated with only a 6.7% decrease in overall mean average precision (mAP) when training multispectral models via MATT, compared to a manually labeled dataset. We consider this an acceptable level of precision loss when considering the time saved during training, especially for rapidly prototyping experimental modeling methods. This research greatly contributes to the study of multispectral <b>object</b> <b>detection</b> by providing a novel and open-source method to rapidly segment, label, and train multispectral <b>object</b> <b>detection</b> models with minimal human interaction. Future research needs to focus on applying these methods to (i) space-based multispectral, and (ii) drone-based hyperspectral imagery.</p></p class="citation"></blockquote><h3 id=2223--120165-neuromorphic-face-analysis-a-survey-federico-becattini-et-al-2024>(22/23 | 120/165) Neuromorphic Face Analysis: a Survey (Federico Becattini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Federico Becattini, Lorenzo Berlincioni, Luca Cultrera, Alberto Del Bimbo. (2024)<br><strong>Neuromorphic Face Analysis: a Survey</strong><br><button class=copy-to-clipboard title="Neuromorphic Face Analysis: a Survey" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-ET, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11631v1.pdf filename=2402.11631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neuromorphic sensors, also known as event cameras, are a class of imaging devices mimicking the function of biological visual systems. Unlike traditional frame-based cameras, which capture fixed images at discrete intervals, neuromorphic sensors continuously generate events that represent changes in light intensity or motion in the visual field with high temporal resolution and low latency. These properties have proven to be interesting in modeling human faces, both from an effectiveness and a privacy-preserving point of view. Neuromorphic face analysis however is still a raw and unstructured field of research, with several attempts at addressing different tasks with no clear standard or <b>benchmark.</b> This survey paper presents a comprehensive overview of capabilities, challenges and emerging applications in the domain of neuromorphic face analysis, to outline promising directions and open issues. After discussing the fundamental working principles of neuromorphic vision and presenting an in-depth overview of the related research, we explore the current state of available data, standard data representations, emerging challenges, and limitations that require further investigation. This paper aims to highlight the recent process in this evolving field to provide to both experienced and newly come researchers an all-encompassing analysis of the state of the art along with its problems and shortcomings.</p></p class="citation"></blockquote><h3 id=2323--121165-cpn-complementary-proposal-network-for-unconstrained-text-detection-longhuang-wu-et-al-2024>(23/23 | 121/165) CPN: Complementary Proposal Network for Unconstrained Text Detection (Longhuang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Longhuang Wu, Shangxuan Tian, Youxin Wang, Pengfei Xiong. (2024)<br><strong>CPN: Complementary Proposal Network for Unconstrained Text Detection</strong><br><button class=copy-to-clipboard title="CPN: Complementary Proposal Network for Unconstrained Text Detection" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11540v1.pdf filename=2402.11540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing methods for scene text detection can be divided into two paradigms: segmentation-based and anchor-based. While Segmentation-based methods are well-suited for irregular shapes, they struggle with compact or overlapping layouts. Conversely, anchor-based approaches excel for complex layouts but suffer from irregular shapes. To strengthen their merits and overcome their respective demerits, we propose a Complementary Proposal Network (CPN) that seamlessly and parallelly integrates semantic and geometric information for superior performance. The CPN comprises two efficient networks for proposal generation: the Deformable Morphology Semantic Network, which generates semantic proposals employing an innovative deformable morphological operator, and the Balanced Region Proposal Network, which produces geometric proposals with pre-defined anchors. To further enhance the complementarity, we introduce an Interleaved Feature Attention module that enables semantic and geometric features to interact deeply before proposal generation. By leveraging both complementary proposals and features, CPN outperforms state-of-the-art approaches with significant margins under comparable computation cost. Specifically, our approach achieves improvements of 3.6%, 1.3% and 1.0% on challenging <b>benchmarks</b> ICDAR19-ArT, IC15, and MSRA-TD500, respectively. Code for our method will be released.</p></p class="citation"></blockquote><h2 id=cscr-2>cs.CR (2)</h2><h3 id=12--122165-urlberta-contrastive-and-adversarial-pre-trained-model-for-url-classification-yujie-li-et-al-2024>(1/2 | 122/165) URLBERT:A Contrastive and Adversarial Pre-trained Model for URL Classification (Yujie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujie Li, Yanbin Wang, Haitao Xu, Zhenhao Guo, Zheng Cao, Lun Zhang. (2024)<br><strong>URLBERT:A Contrastive and Adversarial Pre-trained Model for URL Classification</strong><br><button class=copy-to-clipboard title="URLBERT:A Contrastive and Adversarial Pre-trained Model for URL Classification" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 65<br>Keywords: Adversarial Learning, Contrastive Learning, Fine-tuning, Recommendation, Representation Learning, Self-supervised Learning, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11495v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11495v1.pdf filename=2402.11495v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>URLs play a crucial role in understanding and categorizing web content, particularly in tasks related to security control and online <b>recommendations.</b> While pre-trained models are currently dominating various fields, the domain of URL analysis still lacks specialized pre-trained models. To address this gap, this paper introduces URLBERT, the first pre-trained <b>representation</b> <b>learning</b> model applied to a variety of URL classification or detection tasks. We first train a URL tokenizer on a corpus of billions of URLs to address URL data <b>tokenization.</b> Additionally, we propose two novel pre-training tasks: (1) <b>self-supervised</b> <b>contrastive</b> <b>learning</b> tasks, which strengthen the model&rsquo;s understanding of URL structure and the capture of category differences by distinguishing different variants of the same URL; (2) virtual <b>adversarial</b> <b>training,</b> aimed at improving the model&rsquo;s robustness in extracting semantic features from URLs. Finally, our proposed methods are evaluated on tasks including phishing URL detection, web page classification, and ad filtering, achieving state-of-the-art performance. Importantly, we also explore multi-task learning with URLBERT, and experimental results demonstrate that multi-task learning model based on URLBERT exhibit equivalent effectiveness compared to independently <b>fine-tuned</b> models, showing the simplicity of URLBERT in handling complex task requirements. The code for our work is available at <a href=https://github.com/Davidup1/URLBERT>https://github.com/Davidup1/URLBERT</a>.</p></p class="citation"></blockquote><h3 id=22--123165-poisoning-federated-recommender-systems-with-fake-users-ming-yin-et-al-2024>(2/2 | 123/165) Poisoning Federated Recommender Systems with Fake Users (Ming Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Yin, Yichang Xu, Minghong Fang, Neil Zhenqiang Gong. (2024)<br><strong>Poisoning Federated Recommender Systems with Fake Users</strong><br><button class=copy-to-clipboard title="Poisoning Federated Recommender Systems with Fake Users" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-IR, cs-LG, cs.CR<br>Keyword Score: 33<br>Keywords: Benchmarking, Federated Learning, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11637v1.pdf filename=2402.11637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>recommendation</b> is a prominent use case within <b>federated</b> <b>learning,</b> yet it remains susceptible to various attacks, from user to server-side vulnerabilities. Poisoning attacks are particularly notable among user-side attacks, as participants upload malicious model updates to deceive the global model, often intending to promote or demote specific targeted items. This study investigates strategies for executing promotion attacks in <b>federated</b> <b>recommender</b> <b>systems.</b> Current poisoning attacks on <b>federated</b> <b>recommender</b> <b>systems</b> often rely on additional information, such as the local training data of genuine users or item popularity. However, such information is challenging for the potential attacker to obtain. Thus, there is a need to develop an attack that requires no extra information apart from item embeddings obtained from the server. In this paper, we introduce a novel fake user based poisoning attack named PoisonFRS to promote the attacker-chosen targeted item in <b>federated</b> <b>recommender</b> <b>systems</b> without requiring knowledge about user-item rating data, user attributes, or the aggregation rule used by the server. Extensive experiments on multiple real-world datasets demonstrate that PoisonFRS can effectively promote the attacker-chosen targeted item to a large portion of genuine users and outperform current <b>benchmarks</b> that rely on additional information about the system. We further observe that the model updates from both genuine and fake users are indistinguishable within the latent space.</p></p class="citation"></blockquote><h2 id=csir-4>cs.IR (4)</h2><h3 id=14--124165-neighborhood-enhanced-supervised-contrastive-learning-for-collaborative-filtering-peijie-sun-et-al-2024>(1/4 | 124/165) Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering (Peijie Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peijie Sun, Le Wu, Kun Zhang, Xiangzhi Chen, Meng Wang. (2024)<br><strong>Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering</strong><br><button class=copy-to-clipboard title="Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 53<br>Keywords: Graph, Contrastive Learning, Data Augmentation, Recommendation, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11523v1.pdf filename=2402.11523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While effective in <b>recommendation</b> tasks, collaborative filtering (CF) techniques face the challenge of <b>data</b> <b>sparsity.</b> Researchers have begun leveraging <b>contrastive</b> <b>learning</b> to introduce additional <b>self-supervised</b> signals to address this. However, this approach often unintentionally distances the target user/item from their collaborative neighbors, limiting its efficacy. In response, we propose a solution that treats the collaborative neighbors of the anchor node as positive samples within the final objective loss function. This paper focuses on developing two unique <b>supervised</b> <b>contrastive</b> <b>loss</b> functions that effectively combine supervision signals with <b>contrastive</b> <b>loss.</b> We analyze our proposed loss functions through the gradient lens, demonstrating that different positive samples simultaneously influence updating the anchor node&rsquo;s embeddings. These samples&rsquo; impact depends on their similarities to the anchor node and the negative samples. Using the <b>graph-based</b> collaborative filtering model as our backbone and following the same <b>data</b> <b>augmentation</b> methods as the existing <b>contrastive</b> <b>learning</b> model SGL, we effectively enhance the performance of the <b>recommendation</b> model. Our proposed Neighborhood-Enhanced <b>Supervised</b> <b>Contrastive</b> <b>Loss</b> (NESCL) model substitutes the <b>contrastive</b> <b>loss</b> function in SGL with our novel loss function, showing marked performance improvement. On three real-world datasets, Yelp2018, Gowalla, and Amazon-Book, our model surpasses the original SGL by 10.09%, 7.09%, and 35.36% on NDCG@20, respectively.</p></p class="citation"></blockquote><h3 id=24--125165-large-language-models-as-data-augmenters-for-cold-start-item-recommendation-jianling-wang-et-al-2024>(2/4 | 125/165) Large Language Models as Data Augmenters for Cold-Start Item Recommendation (Jianling Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianling Wang, Haokai Lu, James Caverlee, Ed Chi, Minmin Chen. (2024)<br><strong>Large Language Models as Data Augmenters for Cold-Start Item Recommendation</strong><br><button class=copy-to-clipboard title="Large Language Models as Data Augmenters for Cold-Start Item Recommendation" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Recommendation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11724v1.pdf filename=2402.11724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>reasoning</b> and generalization capabilities of <b>LLMs</b> can help us better understand user preferences and item characteristics, offering exciting prospects to enhance <b>recommendation</b> systems. Though effective while user-item interactions are abundant, conventional <b>recommendation</b> systems struggle to recommend cold-start items without historical interactions. To address this, we propose utilizing <b>LLMs</b> as data augmenters to bridge the knowledge gap on cold-start items during training. We employ <b>LLMs</b> to infer user preferences for cold-start items based on textual description of user historical behaviors and new item descriptions. The augmented training signals are then incorporated into learning the downstream <b>recommendation</b> models through an auxiliary pairwise loss. Through experiments on public Amazon datasets, we demonstrate that <b>LLMs</b> can effectively augment the training signals for cold-start items, leading to significant improvements in cold-start item <b>recommendation</b> for various <b>recommendation</b> models.</p></p class="citation"></blockquote><h3 id=34--126165-pattern-wise-transparent-sequential-recommendation-kun-ma-et-al-2024>(3/4 | 126/165) Pattern-wise Transparent Sequential Recommendation (Kun Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Ma, Cong Xu, Zeyuan Chen, Wei Zhang. (2024)<br><strong>Pattern-wise Transparent Sequential Recommendation</strong><br><button class=copy-to-clipboard title="Pattern-wise Transparent Sequential Recommendation" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11480v1.pdf filename=2402.11480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A transparent decision-making process is essential for developing reliable and trustworthy <b>recommender</b> <b>systems.</b> For sequential <b>recommendation,</b> it means that the model can identify critical items asthe justifications for its <b>recommendation</b> results. However, achieving both model transparency and <b>recommendation</b> performance simultaneously is challenging, especially for models that take the entire sequence of items as input without screening. In this paper,we propose an interpretable framework (named PTSR) that enables a pattern-wise transparent decision-making process. It breaks the sequence of items into multi-level patterns that serve as atomic units for the entire <b>recommendation</b> process. The contribution of each pattern to the outcome is quantified in the probability space. With a carefully designed pattern weighting correction, the pattern contribution can be learned in the absence of ground-truth critical patterns. The final recommended items are those items that most critical patterns strongly endorse. Extensive experiments on four public datasets demonstrate remarkable <b>recommendation</b> performance, while case studies validate the model transparency. Our code is available at <a href=https://anonymous.4open.science/r/PTSR-2237>https://anonymous.4open.science/r/PTSR-2237</a>.</p></p class="citation"></blockquote><h3 id=44--127165-search-engines-post-chatgpt-how-generative-artificial-intelligence-could-make-search-less-reliable-shahan-ali-memon-et-al-2024>(4/4 | 127/165) Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable (Shahan Ali Memon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shahan Ali Memon, Jevin D. West. (2024)<br><strong>Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable</strong><br><button class=copy-to-clipboard title="Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CY, cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11707v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11707v1.pdf filename=2402.11707v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this commentary, we discuss the evolving nature of search engines, as they begin to generate, index, and distribute content created by generative artificial intelligence (GenAI). Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases. We discuss how output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability. Furthermore, search engines are already answering queries with error-laden, generated content, further blurring the provenance of information and impacting the integrity of the information ecosystem. We argue how all these factors could reduce the reliability of search engines. Finally, we <b>summarize</b> some of the active research directions and open questions.</p></p class="citation"></blockquote><h2 id=q-biobm-2>q-bio.BM (2)</h2><h3 id=12--128165-ddiprompt-drug-drug-interaction-event-prediction-based-on-graph-prompt-learning-yingying-wang-et-al-2024>(1/2 | 128/165) DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning (Yingying Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingying Wang, Yun Xiong, Xixi Wu, Xiangguo Sun, Jiawei Zhang. (2024)<br><strong>DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning</strong><br><button class=copy-to-clipboard title="DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-AI, cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 46<br>Keywords: Graph, Graph Neural Network, Benchmarking, Few-shot, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11472v1.pdf filename=2402.11472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>Graph</b> <b>Neural</b> <b>Networks</b> have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in <b>graph</b> <b>prompting.</b> <b>Our</b> framework aims to address these issues by leveraging the intrinsic knowledge from pre-trained models, which can be efficiently deployed with minimal downstream data. Specifically, to solve the first challenge, DDIPrompt employs augmented links between drugs, considering both structural and interactive proximity. It features a hierarchical pre-training strategy that comprehends intra-molecular structures and inter-molecular interactions, fostering a comprehensive and unbiased understanding of drug properties. For the second challenge, we implement a prototype-enhanced <b>prompting</b> <b>mechanism</b> during inference. This mechanism, refined by <b>few-shot</b> examples from each category, effectively harnesses the rich pre-training knowledge to enhance prediction accuracy, particularly for these rare but crucial interactions. Comprehensive evaluations on two <b>benchmark</b> datasets demonstrate the superiority of DDIPrompt, particularly in predicting rare DDI events.</p></p class="citation"></blockquote><h3 id=22--129165-re-dock-towards-flexible-and-realistic-molecular-docking-with-diffusion-bridge-yufei-huang-et-al-2024>(2/2 | 129/165) Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge (Yufei Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufei Huang, Odin Zhang, Lirong Wu, Cheng Tan, Haitao Lin, Zhangyang Gao, Siyuan Li, Stan. Z. Li. (2024)<br><strong>Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge</strong><br><button class=copy-to-clipboard title="Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-AI, cs-LG, physics-chem-ph, q-bio-BM, q-bio.BM<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11459v1.pdf filename=2402.11459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed <b>benchmark</b> datasets including apo-dock and cross-dock demonstrate our model&rsquo;s superior effectiveness and efficiency over current methods.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--130165-can-chatgpt-support-developers-an-empirical-evaluation-of-large-language-models-for-code-generation-kailun-jin-et-al-2024>(1/3 | 130/165) Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation (Kailun Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kailun Jin, Chung-Yu Wang, Hung Viet Pham, Hadi Hemmati. (2024)<br><strong>Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation</strong><br><button class=copy-to-clipboard title="Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: I-2-2, cs-AI, cs-LG, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: ChatGPT, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11702v1.pdf filename=2402.11702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated notable proficiency in <b>code</b> <b>generation,</b> with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively <b>LLMs</b> can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers&rsquo; conversations with <b>ChatGPT</b> (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using <b>LLM-generated</b> <b>code</b> <b>is</b> typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready <b>code.</b> <b>These</b> findings indicate that there is much future work needed to improve <b>LLMs</b> in <b>code</b> <b>generation</b> before they can be integral parts of modern software development.</p></p class="citation"></blockquote><h3 id=23--131165-tool-augmented-llms-as-a-universal-interface-for-ides-yaroslav-zharov-et-al-2024>(2/3 | 131/165) Tool-Augmented LLMs as a Universal Interface for IDEs (Yaroslav Zharov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaroslav Zharov, Yury Khudyakov, Evgeniia Fedotova, Evgeny Grigorenko, Egor Bogomolov. (2024)<br><strong>Tool-Augmented LLMs as a Universal Interface for IDEs</strong><br><button class=copy-to-clipboard title="Tool-Augmented LLMs as a Universal Interface for IDEs" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11635v1.pdf filename=2402.11635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern-day Integrated Development Environments (IDEs) have come a long way from the early text editing utilities to the complex programs encompassing thousands of functions to help developers. However, with the increasing number of efficiency-enhancing tools incorporated, IDEs gradually became sophisticated software with a steep learning curve. The rise of the <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> capable of both natural language dialogue and <b>code</b> <b>generation</b> leads to a discourse on the obsolescence of the concept of IDE. In this work, we offer a view on the place of the <b>LLMs</b> in the IDEs as the universal interface wrapping the IDE facilities. We envision a model that is able to perform complex actions involving multiple IDE features upon user command, stripping the user experience of the tedious work involved in searching through options and actions. For the practical part of the work, we engage with the works exploring the ability of <b>LLMs</b> to call for external tools to expedite a given task execution. We showcase a proof-of-concept of such a tool.</p></p class="citation"></blockquote><h3 id=33--132165-using-rule-engine-in-self-healing-systems-and-mape-model-zahra-yazdanparast-2024>(3/3 | 132/165) Using rule engine in self-healing systems and MAPE model (Zahra Yazdanparast, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zahra Yazdanparast. (2024)<br><strong>Using rule engine in self-healing systems and MAPE model</strong><br><button class=copy-to-clipboard title="Using rule engine in self-healing systems and MAPE model" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11581v1.pdf filename=2402.11581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software malfunction presents a significant hurdle within the computing domain, carrying substantial risks for systems, enterprises, and users universally. To produce software with high reliability and quality, effective debugging is essential. Program debugging is an activity to reduce software maintenance costs. In this study, a failure repair method that uses a rule engine is presented. The <b>simulation</b> on mRUBIS showed that the proposed method could be efficient in the operational environment. Through a thorough grasp of software failure and the adoption of efficient mitigation strategies, stakeholders can bolster the dependability, security, and adaptability of software systems. This, in turn, reduces the repercussions of failures and cultivates increased confidence in digital technologies.</p></p class="citation"></blockquote><h2 id=hep-ph-1>hep-ph (1)</h2><h3 id=11--133165-pascl-supervised-contrastive-learning-with-perturbative-augmentation-for-particle-decay-reconstruction-junjian-lu-et-al-2024>(1/1 | 133/165) PASCL: Supervised Contrastive Learning with Perturbative Augmentation for Particle Decay Reconstruction (Junjian Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjian Lu, Siwei Liu, Dmitrii Kobylianski, Etienne Dreyer, Eilam Gross, Shangsong Liang. (2024)<br><strong>PASCL: Supervised Contrastive Learning with Perturbative Augmentation for Particle Decay Reconstruction</strong><br><button class=copy-to-clipboard title="PASCL: Supervised Contrastive Learning with Perturbative Augmentation for Particle Decay Reconstruction" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ph<br>Categories: cs-LG, hep-ph, hep-ph<br>Keyword Score: 33<br>Keywords: Graph, Graph Contrastive Learning, Contrastive Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11538v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11538v1.pdf filename=2402.11538v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In high-energy physics, particles produced in collision events decay in a format of a hierarchical tree structure, where only the final decay products can be observed using detectors. However, the large combinatorial space of possible tree structures makes it challenging to recover the actual decay process given a set of final particles. To better analyse the hierarchical tree structure, we propose a <b>graph-based</b> <b>deep</b> <b>learning</b> model to infer the tree structure to reconstruct collision events. In particular, we use a compact matrix representation termed as lowest common ancestor generations (LCAG) matrix, to encode the particle decay tree structure. Then, we introduce a perturbative augmentation technique applied to node features, aiming to mimic experimental uncertainties and increase data diversity. We further propose a <b>supervised</b> <b>graph</b> <b>contrastive</b> <b>learning</b> algorithm to utilize the information of inter-particle relations from multiple decay processes. Extensive experiments show that our proposed <b>supervised</b> <b>graph</b> <b>contrastive</b> <b>learning</b> with perturbative augmentation (PASCL) method outperforms state-of-the-art baseline models on an existing physics-based dataset, significantly improving the reconstruction accuracy. This method provides a more effective training strategy for models with the same parameters and makes way for more accurate and efficient high-energy particle physics data analysis.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--134165-solving-data-centric-tasks-using-large-language-models-shraddha-barke-et-al-2024>(1/1 | 134/165) Solving Data-centric Tasks using Large Language Models (Shraddha Barke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shraddha Barke, Christian Poelitz, Carina Suzana Negreanu, Benjamin Zorn, José Cambronero, Andrew D. Gordon, Vu Le, Elnaz Nouri, Nadia Polikarpova, Advait Sarkar, Brian Slininger, Neil Toronto, Jack Williams. (2024)<br><strong>Solving Data-centric Tasks using Large Language Models</strong><br><button class=copy-to-clipboard title="Solving Data-centric Tasks using Large Language Models" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-AI, cs-PL, cs-SE, cs.PL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11734v1.pdf filename=2402.11734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select <b>prompting</b> technique, which adds the most representative rows from the input data to the <b>LLM</b> <b>prompt.</b> Our experiments show that <b>LLM</b> performance is indeed sensitive to the amount of data passed in the <b>prompt,</b> and that for tasks with a lot of syntactic variation in the input table, our cluster-then-select technique outperforms a random selection baseline.</p></p class="citation"></blockquote><h2 id=cshc-1>cs.HC (1)</h2><h3 id=11--135165-shaping-human-ai-collaboration-varied-scaffolding-levels-in-co-writing-with-language-models-paramveer-s-dhillon-et-al-2024>(1/1 | 135/165) Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models (Paramveer S. Dhillon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paramveer S. Dhillon, Somayeh Molaei, Jiaqi Li, Maximilian Golub, Shaochun Zheng, Lionel P. Robert. (2024)<br><strong>Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models</strong><br><button class=copy-to-clipboard title="Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CL, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11723v1.pdf filename=2402.11723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in language modeling have paved the way for novel human-AI co-writing experiences. This paper explores how varying levels of scaffolding from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> shape the co-writing process. Employing a within-subjects field experiment with a Latin square design, we asked participants (N=131) to respond to argumentative writing <b>prompts</b> under three randomly sequenced conditions: no AI assistance (control), next-sentence suggestions (low scaffolding), and next-paragraph suggestions (high scaffolding). Our findings reveal a U-shaped impact of scaffolding on writing quality and productivity (words/time). While low scaffolding did not significantly improve writing quality or productivity, high scaffolding led to significant improvements, especially benefiting non-regular writers and less tech-savvy users. No significant cognitive burden was observed while using the scaffolded writing tools, but a moderate decrease in text ownership and satisfaction was noted. Our results have broad implications for the design of AI-powered writing tools, including the need for personalized scaffolding mechanisms.</p></p class="citation"></blockquote><h2 id=csgt-2>cs.GT (2)</h2><h3 id=12--136165-a-three-party-repeated-coalition-formation-game-for-pls-in-wireless-communications-with-irss-haipeng-zhou-et-al-2024>(1/2 | 136/165) A Three-Party Repeated Coalition Formation Game for PLS in Wireless Communications with IRSs (Haipeng Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haipeng Zhou, Ruoyang Chen, Changyan Yi, Juan Li, Jun Cai. (2024)<br><strong>A Three-Party Repeated Coalition Formation Game for PLS in Wireless Communications with IRSs</strong><br><button class=copy-to-clipboard title="A Three-Party Repeated Coalition Formation Game for PLS in Wireless Communications with IRSs" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-NI, cs.GT<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11500v1.pdf filename=2402.11500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, a repeated coalition formation game (RCFG) with dynamic decision-making for physical layer security (PLS) in wireless communications with intelligent reflecting surfaces (IRSs) has been investigated. In the considered system, one central legitimate transmitter (LT) aims to transmit secret signals to a group of legitimate receivers (LRs) under the threat of a proactive eavesdropper (EV), while there exist a number of third-party IRSs (TIRSs) which can choose to form a coalition with either legitimate pairs (LPs) or the EV to improve their respective performances in exchange for potential benefits (e.g., payments). Unlike existing works that commonly restricted to friendly IRSs or malicious IRSs only, we study the complicated dynamic ally-adversary relationships among LPs, EV and TIRSs, under unpredictable wireless channel conditions, and introduce a RCFG to model their long-term strategic interactions. Particularly, we first analyze the existence of Nash equilibrium (NE) in the formulated RCFG, and then propose a switch operations-based coalition selection along with a deep <b>reinforcement</b> <b>learning</b> (DRL)-based algorithm for obtaining such equilibrium. <b>Simulations</b> examine the feasibility of the proposed algorithm and show its superiority over counterparts.</p></p class="citation"></blockquote><h3 id=22--137165-the-assignment-game-new-mechanisms-for-equitable-core-imputations-vijay-v-vazirani-2024>(2/2 | 137/165) The Assignment Game: New Mechanisms for Equitable Core Imputations (Vijay V. Vazirani, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vijay V. Vazirani. (2024)<br><strong>The Assignment Game: New Mechanisms for Equitable Core Imputations</strong><br><button class=copy-to-clipboard title="The Assignment Game: New Mechanisms for Equitable Core Imputations" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT, econ-TH<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11437v1.pdf filename=2402.11437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The set of core imputations of the assignment game forms a (non-finite) distributive lattice. So far, efficient algorithms were known for computing only its two extreme imputations; however, each of them maximally favors one side and disfavors the other side of the bipartition, leading to inequitable profit sharing. Another issue is that a sub-coalition consisting of one player (or a set of players from the same side of the bipartition) can make zero profit, therefore a core imputation is not obliged to give them any profit. Hence core imputations make no <b>fairness</b> guarantee at the level of individual agents. This raises the question of computing {\em more equitable core imputations}. In this paper, we give combinatorial (i.e., the mechanism does not invoke an LP-solver) polynomial time mechanisms for computing the leximin and leximax core imputations for the assignment game. These imputations achieve <code>fairness'' in different ways: whereas leximin tries to make poor agents more rich, leximax tries to make rich agents less rich. In general, the two imputations are different. Our mechanisms were derived by a suitable adaptation of the classical primal-dual paradigm from combinatorial optimization. The </code>engine&rsquo;&rsquo; driving them involves recent insights, obtained via complementarity, into core imputations \cite{Va.New-characterizations} and the pristine combinatorial structure of matching. We have identified other natural games which could benefit from our approach.</p></p class="citation"></blockquote><h2 id=eesssy-8>eess.SY (8)</h2><h3 id=18--138165-a-fisher-information-based-receding-horizon-control-method-for-signal-strength-model-estimation-yancheng-zhu-et-al-2024>(1/8 | 138/165) A Fisher Information based Receding Horizon Control Method for Signal Strength Model Estimation (Yancheng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yancheng Zhu, Sean B. Andersson. (2024)<br><strong>A Fisher Information based Receding Horizon Control Method for Signal Strength Model Estimation</strong><br><button class=copy-to-clipboard title="A Fisher Information based Receding Horizon Control Method for Signal Strength Model Estimation" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Pruning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11483v1.pdf filename=2402.11483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers the problem of localizing a set of nodes in a wireless sensor network when both their positions and the parameters of the communication model are unknown. We assume that a single agent moves through the environment, taking measurements of the Received Signal Strength (RSS), and seek a controller that optimizes a performance metric based on the Fisher Information Matrix (FIM). We develop a receding horizon (RH) approach that alternates between estimating the parameter values (using a maximum likelihood estimator) and determining where to move so as to maximally inform the estimation problem. The receding horizon controller solves a multi-stage look ahead problem to determine the next control to be applied, executes the move, collects the next measurement, and then re-estimates the parameters before repeating the sequence. We consider both a Dynamic Programming (DP) approach to solving the optimal control problem at each step, and a simplified heuristic based on a <b>pruning</b> algorithm that significantly reduces the computational complexity. We also consider a modified cost function that seeks to balance the information acquired about each of the parameters to ensure the controller does not focus on a single value in its optimization. These approaches are compared against two baselines, one based on a purely random trajectory and one on a greedy control solution. The <b>simulations</b> indicate our RH schemes outperform the baselines, while the <b>pruning</b> algorithm produces significant reductions in computation time with little effect on overall performance.</p></p class="citation"></blockquote><h3 id=28--139165-balanced-truncation-of-linear-systems-with-quadratic-outputs-in-limited-time-and-frequency-intervals-qiu-yan-song-et-al-2024>(2/8 | 139/165) Balanced Truncation of Linear Systems with Quadratic Outputs in Limited Time and Frequency Intervals (Qiu-Yan Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiu-Yan Song, Umair Zulfiqar, Zhi-Hua Xiao, Mohammad Monir Uddin, Victor Sreeram. (2024)<br><strong>Balanced Truncation of Linear Systems with Quadratic Outputs in Limited Time and Frequency Intervals</strong><br><button class=copy-to-clipboard title="Balanced Truncation of Linear Systems with Quadratic Outputs in Limited Time and Frequency Intervals" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11445v1.pdf filename=2402.11445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model order reduction involves constructing a reduced-order approximation of a high-order model while retaining its essential characteristics. This reduced-order model serves as a substitute for the original one in various applications such as <b>simulation,</b> analysis, and design. Often, there&rsquo;s a need to maintain high accuracy within a specific time or frequency interval, while errors beyond this limit can be tolerated. This paper addresses time-limited and frequency-limited model order reduction scenarios for linear systems with quadratic outputs, by generalizing the recently introduced structure-preserving balanced truncation algorithm. To that end, limited interval system Gramians are defined, and the corresponding generalized Lyapunov equations governing their computation are derived. Additionally, low-rank solutions for these equations are investigated. Next, balanced truncation algorithms are proposed for time-limited and frequency-limited scenarios, each utilizing its corresponding limited-interval system Gramians. The proposed algorithms ensure accurate results within specified time and frequency intervals while preserving the quadratic-output structure. Two <b>benchmark</b> numerical examples are presented to demonstrate the effectiveness of the algorithms, showcasing their ability to achieve superior accuracy within the desired time or frequency interval.</p></p class="citation"></blockquote><h3 id=38--140165-iterative-linear-quadratic-regulator-with-variational-equation-based-discretization-katsuya-shigematsu-et-al-2024>(3/8 | 140/165) Iterative Linear Quadratic Regulator With Variational Equation-Based Discretization (Katsuya Shigematsu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Katsuya Shigematsu, Hikaru Hoshino, Eiko Furutani. (2024)<br><strong>Iterative Linear Quadratic Regulator With Variational Equation-Based Discretization</strong><br><button class=copy-to-clipboard title="Iterative Linear Quadratic Regulator With Variational Equation-Based Discretization" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11648v1.pdf filename=2402.11648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper discusses discretization methods for implementing nonlinear model predictive controllers using Iterative Linear Quadratic Regulator (ILQR). Finite-difference approximations are mostly used to derive a discrete-time state equation from the original continuous-time model. However, the timestep of the discretization is sometimes restricted to be small to suppress the approximation error. In this paper, we propose to use the variational equation for deriving linearizations of the discretized system required in ILQR algorithms, which allows accurate computation regardless of the timestep. Numerical <b>simulations</b> of the swing-up control of an inverted pendulum demonstrate the effectiveness of this method. By the relaxing stringent requirement for the size of the timestep, the use of the variational equation can improve control performance by increasing the number of ILQR iterations possible at each timestep in the realtime computation.</p></p class="citation"></blockquote><h3 id=48--141165-signed-perturbed-sums-estimation-of-arx-systems-exact-coverage-and-strong-consistency-extended-version-algo-carè-et-al-2024>(4/8 | 141/165) Signed-Perturbed Sums Estimation of ARX Systems: Exact Coverage and Strong Consistency (Extended Version) (Algo Carè et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Algo Carè, Erik Weyer, Balázs Cs. Csáji, Marco C. Campi. (2024)<br><strong>Signed-Perturbed Sums Estimation of ARX Systems: Exact Coverage and Strong Consistency (Extended Version)</strong><br><button class=copy-to-clipboard title="Signed-Perturbed Sums Estimation of ARX Systems: Exact Coverage and Strong Consistency (Extended Version)" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SP, eess-SY, eess.SY, math-ST, stat-TH<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11528v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11528v1.pdf filename=2402.11528v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sign-Perturbed Sums (SPS) is a system identification method that constructs confidence regions for the unknown system parameters. In this paper, we study SPS for ARX systems, and establish that the confidence regions are guaranteed to include the true model parameter with exact, user-chosen, probability under mild statistical assumptions, a property that holds true for any finite number of observed input-output data. Furthermore, we prove the strong consistency of the method, that is, as the number of data points increases, the confidence region gets smaller and smaller and will asymptotically almost surely exclude any parameter value different from the true one. In addition, we also show that, asymptotically, the SPS region is included in an ellipsoid which is marginally larger than the confidence ellipsoid obtained from the asymptotic theory of system identification. The results are theoretically proven and illustrated in a <b>simulation</b> example.</p></p class="citation"></blockquote><h3 id=58--142165-exponential-cluster-synchronization-in-fast-switching-network-topologies-a-pinning-control-approach-with-necessary-and-sufficient-conditions-ku-du-et-al-2024>(5/8 | 142/165) Exponential Cluster Synchronization in Fast Switching Network Topologies: A Pinning Control Approach with Necessary and Sufficient Conditions (Ku Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ku Du, Yu Kang. (2024)<br><strong>Exponential Cluster Synchronization in Fast Switching Network Topologies: A Pinning Control Approach with Necessary and Sufficient Conditions</strong><br><button class=copy-to-clipboard title="Exponential Cluster Synchronization in Fast Switching Network Topologies: A Pinning Control Approach with Necessary and Sufficient Conditions" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11492v1.pdf filename=2402.11492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research investigates the intricate domain of synchronization problem among multiple agents operating within a dynamic fast switching network topology. We concentrate on cluster synchronization within coupled linear system under pinning control, providing both necessary and sufficient conditions. As a pivotal aspect, this paper aim to president the weakest possible conditions to make the coupled linear system realize cluster synchronization exponentially. Within the context of fast switching framework, we initially examine the necessary conditions, commencing with the transformation of the consensus problem into a stability problem, introducing a new variable to make the coupled system achieve cluster synchronization if the system is controllable; communication topology switching fast enough and the coupling strength should be sufficiently robust. Then, by using the Lyapunov theorem, we also present that the state matrix controllable is necessary for cluster synchronization. Furthermore, this paper culminating in the incorporation of contraction theory and an invariant manifold, demonstrating that the switching topology has an average is imperative for achieving cluster synchronization. Finally, we introduce three <b>simulations</b> to validate the efficacy of the proposed approach.</p></p class="citation"></blockquote><h3 id=68--143165-specifying-and-analyzing-networked-and-layered-control-systems-operating-on-multiple-clocks-inigo-incer-et-al-2024>(6/8 | 143/165) Specifying and Analyzing Networked and Layered Control Systems Operating on Multiple Clocks (Inigo Incer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Inigo Incer, Noel Csomay-Shanklin, Aaron Ames, Richard M. Murray. (2024)<br><strong>Specifying and Analyzing Networked and Layered Control Systems Operating on Multiple Clocks</strong><br><button class=copy-to-clipboard title="Specifying and Analyzing Networked and Layered Control Systems Operating on Multiple Clocks" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11666v1.pdf filename=2402.11666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of <b>reasoning</b> about networked and layered control systems using assume-guarantee specifications. As these systems are formed by the interconnection of components that operate under various clocks, we introduce a new logic, Multiclock Logic (MCL), to be able to express the requirements of components form the point of view of their local clocks. Specifying components locally promotes independent design and component reuse. We carry out a contract-based analysis of a control system implemented via two control algorithms (model predictive control and feedback linearization) running on their own processors and clocks. Then we implement each of the contracts to build a system. The system performs as desired when the requirements derived from our system-level analysis are respected. Violating the constraints required by the contract-based analysis of the system leads to error.</p></p class="citation"></blockquote><h3 id=78--144165-federated-reinforcement-learning-for-uplink-centric-broadband-communication-optimization-over-unlicensed-spectrum-hui-zhou-et-al-2024>(7/8 | 144/165) Federated Reinforcement Learning for Uplink Centric Broadband Communication Optimization over Unlicensed Spectrum (Hui Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui Zhou, Yansha Deng. (2024)<br><strong>Federated Reinforcement Learning for Uplink Centric Broadband Communication Optimization over Unlicensed Spectrum</strong><br><button class=copy-to-clipboard title="Federated Reinforcement Learning for Uplink Centric Broadband Communication Optimization over Unlicensed Spectrum" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11478v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11478v1.pdf filename=2402.11478v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To provide Uplink Centric Broadband Communication (UCBC), New Radio Unlicensed (NR-U) network has been standardized to exploit the unlicensed spectrum using Listen Before Talk (LBT) scheme to fairly coexist with the incumbent Wireless Fidelity (WiFi) network. Existing access schemes over unlicensed spectrum are required to perform Clear Channel Assessment (CCA) before transmissions, where fixed Energy Detection (ED) thresholds are adopted to identify the channel as idle or busy. However, fixed ED thresholds setting prevents devices from accessing the channel effectively and efficiently, which leads to the hidden node (HN) and exposed node (EN) problems. In this paper, we first develop a centralized double Deep Q-Network (DDQN) algorithm to optimize the uplink system throughput, where the agent is deployed at the central server to dynamically adjust the ED thresholds for NR-U and WiFi networks. Considering that heterogeneous NR-U and WiFi networks, in practice, cannot share the raw data with the central server directly, we then develop a federated DDQN algorithm, where two agents are deployed in the NR-U and WiFi networks, respectively. Our results have shown that the uplink system throughput increases by over 100%, where cell throughput of NR-U network rises by 150%, and cell throughput of WiFi network decreases by 30%. To guarantee the cell throughput of WiFi network, we redesign the reward function to punish the agent when the cell throughput of WiFi network is below the threshold, and our revised design can still provide over 50% uplink system throughput gain.</p></p class="citation"></blockquote><h3 id=88--145165-a-transition-system-abstraction-framework-for-neural-network-dynamical-system-models-yejiang-yang-et-al-2024>(8/8 | 145/165) A Transition System Abstraction Framework for Neural Network Dynamical System Models (Yejiang Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yejiang Yang, Zihao Mo, Hoang-Dung Tran, Weiming Xiang. (2024)<br><strong>A Transition System Abstraction Framework for Neural Network Dynamical System Models</strong><br><button class=copy-to-clipboard title="A Transition System Abstraction Framework for Neural Network Dynamical System Models" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11739v1.pdf filename=2402.11739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a transition system abstraction framework for neural network dynamical system models to enhance the model interpretability, with applications to complex dynamical systems such as human behavior learning and verification. To begin with, the localized working zone will be segmented into multiple localized partitions under the data-driven Maximum Entropy (ME) partitioning method. Then, the transition matrix will be obtained based on the set-valued reachability analysis of neural networks. Finally, applications to human handwriting dynamics learning and verification are given to validate our proposed abstraction framework, which demonstrates the advantages of enhancing the interpretability of the <b>black-box</b> <b>model,</b> i.e., our proposed framework is able to abstract a data-driven neural network model into a transition system, making the neural network model interpretable through verifying specifications described in Computational Tree Logic (CTL) languages.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=12--146165-stochastic-nonlinear-dynamical-modelling-of-sram-bitcells-in-retention-mode-léopold-van-brandt-et-al-2024>(1/2 | 146/165) Stochastic Nonlinear Dynamical Modelling of SRAM Bitcells in Retention Mode (Léopold Van Brandt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Léopold Van Brandt, Denis Flandre, Jean-Charles Delvenne. (2024)<br><strong>Stochastic Nonlinear Dynamical Modelling of SRAM Bitcells in Retention Mode</strong><br><button class=copy-to-clipboard title="Stochastic Nonlinear Dynamical Modelling of SRAM Bitcells in Retention Mode" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-CE, cs.AR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11691v1.pdf filename=2402.11691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>SRAM bitcells in retention mode behave as autonomous stochastic nonlinear dynamical systems. From observation of variability-aware transient noise <b>simulations,</b> we provide an unidimensional model, fully characterizable by conventional deterministic SPICE <b>simulations,</b> insightfully explaining the mechanism of intrinsic noise-induced bit flips. The proposed model is exploited to, first, explain the reported inaccuracy of existing closed-form near-equilibrium formulas aimed at predicting the mean time to failure and, secondly, to propose a closer estimate attractive in terms of CPU time.</p></p class="citation"></blockquote><h3 id=22--147165-variability-aware-noise-induced-dynamic-instability-of-ultra-low-voltage-sram-bitcells-léopold-van-brandt-et-al-2024>(2/2 | 147/165) Variability-Aware Noise-Induced Dynamic Instability of Ultra-Low-Voltage SRAM Bitcells (Léopold Van Brandt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Léopold Van Brandt, Jean-Charles Delvenne, Denis Flandre. (2024)<br><strong>Variability-Aware Noise-Induced Dynamic Instability of Ultra-Low-Voltage SRAM Bitcells</strong><br><button class=copy-to-clipboard title="Variability-Aware Noise-Induced Dynamic Instability of Ultra-Low-Voltage SRAM Bitcells" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11685v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11685v1.pdf filename=2402.11685v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stability of ultra-low-voltage SRAM bitcells in retention mode is threatened by two types of uncertainty: process variability and intrinsic noise. While variability dominates the failure probability, noise-induced bit flips in weakened bitcells lead to dynamic instability. We study both effects jointly in a unified SPICE <b>simulation</b> framework. Starting from a synthetic representation of process variations introduced in a previous work, we identify the cases of poor noise immunity that require thorough noise analyses. Relying on a rigorous and systematic methodology, we simulate them in the time domain so as to emulate a true data retention operation. Short times to failure, unacceptable for a practical ultra-low-power memory system application, are recorded. The transient bit-flip mechanism is analysed and a dynamic failure criterion involving the unstable point is established. We conclude that, beyond static variability, the dynamic noise inflates defectiveness among SRAM bitcells. We also discuss the limits of existing analytical formulas from the literature, which rely on a linear near-equilibrium approximation of the SRAM dynamics to, inaccurately, predict the mean time to failure.</p></p class="citation"></blockquote><h2 id=cset-1>cs.ET (1)</h2><h3 id=11--148165-a-fast-algorithm-to-simulate-nonlinear-resistive-networks-benjamin-scellier-2024>(1/1 | 148/165) A Fast Algorithm to Simulate Nonlinear Resistive Networks (Benjamin Scellier, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Scellier. (2024)<br><strong>A Fast Algorithm to Simulate Nonlinear Resistive Networks</strong><br><button class=copy-to-clipboard title="A Fast Algorithm to Simulate Nonlinear Resistive Networks" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cond-mat-dis-nn, cs-ET, cs-LG, cs.ET<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11674v1.pdf filename=2402.11674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the quest for energy-efficient artificial intelligence systems, resistor networks are attracting interest as an alternative to conventional GPU-based neural networks. These networks leverage the physics of electrical circuits for inference and can be optimized with local training techniques such as equilibrium propagation. Despite their potential advantage in terms of power consumption, the challenge of efficiently simulating these resistor networks has been a significant bottleneck to assess their scalability, with current methods either being limited to linear networks or relying on realistic, yet slow circuit simulators like SPICE. Assuming ideal circuit elements, we introduce a novel approach for the <b>simulation</b> of nonlinear resistive networks, which we frame as a quadratic programming problem with linear inequality constraints, and which we solve using a fast, exact coordinate descent algorithm. Our <b>simulation</b> methodology significantly outperforms existing SPICE-based <b>simulations,</b> enabling the training of networks up to 325 times larger at speeds 150 times faster, resulting in a 50,000-fold improvement in the ratio of network size to epoch duration. Our approach, adaptable to other electrical components, can foster more rapid progress in the <b>simulations</b> of nonlinear electrical networks.</p></p class="citation"></blockquote><h2 id=csai-3>cs.AI (3)</h2><h3 id=13--149165-combinatorial-client-master-multiagent-deep-reinforcement-learning-for-task-offloading-in-mobile-edge-computing-tesfay-zemuy-gebrekidan-et-al-2024>(1/3 | 149/165) Combinatorial Client-Master Multiagent Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing (Tesfay Zemuy Gebrekidan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tesfay Zemuy Gebrekidan, Sebastian Stein, Timothy J. Norman. (2024)<br><strong>Combinatorial Client-Master Multiagent Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing</strong><br><button class=copy-to-clipboard title="Combinatorial Client-Master Multiagent Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-11, cs-AI, cs-DC, cs-NI, cs.AI<br>Keyword Score: 20<br>Keywords: Face Recognition, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11653v1.pdf filename=2402.11653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been an explosion of mobile applications that perform computationally intensive tasks such as video streaming, data mining, virtual reality, augmented reality, image processing, video processing, <b>face</b> <b>recognition,</b> and online gaming. However, user devices (UDs), such as tablets and smartphones, have a limited ability to perform the computation needs of the tasks. Mobile edge computing (MEC) has emerged as a promising technology to meet the increasing computing demands of UDs. Task offloading in MEC is a strategy that meets the demands of UDs by distributing tasks between UDs and MEC servers. Deep <b>reinforcement</b> <b>learning</b> (DRL) is gaining attention in task-offloading problems because it can adapt to dynamic changes and minimize online computational complexity. However, the various types of continuous and discrete resource constraints on UDs and MEC servers pose challenges to the design of an efficient DRL-based task-offloading strategy. Existing DRL-based task-offloading algorithms focus on the constraints of the UDs, assuming the availability of enough storage resources on the server. Moreover, existing multiagent DRL (MADRL)&ndash;based task-offloading algorithms are homogeneous agents and consider homogeneous constraints as a penalty in their reward function. We proposed a novel combinatorial client-master MADRL (CCM_MADRL) algorithm for task offloading in MEC (CCM_MADRL_MEC) that enables UDs to decide their resource requirements and the server to make a combinatorial decision based on the requirements of the UDs. CCM_MADRL_MEC is the first MADRL in task offloading to consider server storage capacity in addition to the constraints in the UDs. By taking advantage of the combinatorial action selection, CCM_MADRL_MEC has shown superior convergence over existing MADDPG and heuristic algorithms.</p></p class="citation"></blockquote><h3 id=23--150165-fgeo-hypergnet-geometry-problem-solving-integrating-formal-symbolic-system-and-hypergraph-neural-network-xiaokai-zhang-et-al-2024>(2/3 | 150/165) FGeo-HyperGNet: Geometry Problem Solving Integrating Formal Symbolic System and Hypergraph Neural Network (Xiaokai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaokai Zhang, Na Zhu, Yiming He, Jia Zou, Cheng Qin, Yang Li, Zhenbing Zeng, Tuo Leng. (2024)<br><strong>FGeo-HyperGNet: Geometry Problem Solving Integrating Formal Symbolic System and Hypergraph Neural Network</strong><br><button class=copy-to-clipboard title="FGeo-HyperGNet: Geometry Problem Solving Integrating Formal Symbolic System and Hypergraph Neural Network" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 15<br>Keywords: Geometry, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11461v1.pdf filename=2402.11461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Geometry</b> problem solving has always been a long-standing challenge in the fields of automated <b>reasoning</b> and artificial intelligence. This is the fifth article in a series of our works, we built a neural-symbolic system to automatically perform human-like geometric deductive <b>reasoning.</b> The symbolic part is a formal system built on FormalGeo, which can automatically perform geomertic relational <b>reasoning</b> and algebraic calculations and organize the solving process into a solution hypertree with conditions as hypernodes and theorems as hyperedges. The neural part, called HyperGNet, is a hypergraph neural network based on the attention mechanism, including a encoder to effectively encode the structural and semantic information of the hypertree, and a solver to provide problem-solving guidance. The neural part predicts theorems according to the hypertree, and the symbolic part applies theorems and updates the hypertree, thus forming a Predict-Apply Cycle to ultimately achieve readable and traceable automatic solving of geometric problems. Experiments demonstrate the correctness and effectiveness of this neural-symbolic architecture. We achieved a step-wised accuracy of 87.65% and an overall accuracy of 85.53% on the formalgeo7k datasets. The code and data is available at <a href=https://github.com/BitSecret/HyperGNet>https://github.com/BitSecret/HyperGNet</a>.</p></p class="citation"></blockquote><h3 id=33--151165-dynamic-planning-in-hierarchical-active-inference-matteo-priorelli-et-al-2024>(3/3 | 151/165) Dynamic planning in hierarchical active inference (Matteo Priorelli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Priorelli, Ivilin Peev Stoianov. (2024)<br><strong>Dynamic planning in hierarchical active inference</strong><br><button class=copy-to-clipboard title="Dynamic planning in hierarchical active inference" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-RO, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11658v1.pdf filename=2402.11658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process &ndash; either as discrete decision-making or continuous motor control &ndash; inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand and exploit affordances for object manipulation, and to learn the hierarchical interactions between the self and the environment, including other agents. We start from a simple unit and gradually describe more advanced structures, comparing recently proposed design choices and providing basic examples for each section. This study distances itself from traditional views centered on neural networks and <b>reinforcement</b> <b>learning,</b> and points toward a yet unexplored direction in active inference: hybrid representations in hierarchical models.</p></p class="citation"></blockquote><h2 id=econem-1>econ.EM (1)</h2><h3 id=11--152165-doubly-robust-inference-in-causal-latent-factor-models-alberto-abadie-et-al-2024>(1/1 | 152/165) Doubly Robust Inference in Causal Latent Factor Models (Alberto Abadie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Abadie, Anish Agarwal, Raaz Dwivedi, Abhin Shah. (2024)<br><strong>Doubly Robust Inference in Causal Latent Factor Models</strong><br><button class=copy-to-clipboard title="Doubly Robust Inference in Causal Latent Factor Models" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.EM<br>Categories: cs-LG, econ-EM, econ.EM, stat-ME, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11652v1.pdf filename=2402.11652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article introduces a new framework for estimating average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. <b>Simulation</b> results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article.</p></p class="citation"></blockquote><h2 id=q-biogn-1>q-bio.GN (1)</h2><h3 id=11--153165-scinterpreter-training-large-language-models-to-interpret-scrna-seq-data-for-cell-type-annotation-cong-li-et-al-2024>(1/1 | 153/165) scInterpreter: Training Large Language Models to Interpret scRNA-seq Data for Cell Type Annotation (Cong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cong Li, Meng Xiao, Pengfei Wang, Guihai Feng, Xin Li, Yuanchun Zhou. (2024)<br><strong>scInterpreter: Training Large Language Models to Interpret scRNA-seq Data for Cell Type Annotation</strong><br><button class=copy-to-clipboard title="scInterpreter: Training Large Language Models to Interpret scRNA-seq Data for Cell Type Annotation" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-AI, q-bio-GN, q-bio.GN<br>Keyword Score: 20<br>Keywords: Foundation Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12405v1.pdf filename=2402.12405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the inherent limitations of existing <b>Large</b> <b>Language</b> <b>Models</b> in directly reading and interpreting single-cell omics data, they demonstrate significant potential and flexibility as the <b>Foundation</b> <b>Model.</b> This research focuses on how to train and adapt the <b>Large</b> <b>Language</b> <b>Model</b> with the capability to interpret and distinguish cell types in single-cell RNA sequencing data. Our preliminary research results indicate that these <b>foundational</b> <b>models</b> excel in accurately categorizing known cell types, demonstrating the potential of the <b>Large</b> <b>Language</b> <b>Models</b> as effective tools for uncovering new biological insights.</p></p class="citation"></blockquote><h2 id=cscc-2>cs.CC (2)</h2><h3 id=12--154165-a-simple-proof-that-ricochet-robots-is-pspace-complete-jose-balanza-martinez-et-al-2024>(1/2 | 154/165) A Simple Proof that Ricochet Robots is PSPACE-Complete (Jose Balanza-Martinez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jose Balanza-Martinez, Angel A. Cantu, Robert Schweller, Tim Wylie. (2024)<br><strong>A Simple Proof that Ricochet Robots is PSPACE-Complete</strong><br><button class=copy-to-clipboard title="A Simple Proof that Ricochet Robots is PSPACE-Complete" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DS, cs.CC<br>Keyword Score: 15<br>Keywords: Geometry, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11440v1.pdf filename=2402.11440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we seek to provide a simpler proof that the relocation problem in Ricochet Robots (Lunar Lockout with fixed <b>geometry)</b> is PSPACE-complete via a reduction from Finite Function Generation (FFG). Although this result was originally proven in 2003, we give a simpler reduction by utilizing the FFG problem, and put the result in context with recent publications showing that relocation is also PSPACE-complete in related models.</p></p class="citation"></blockquote><h3 id=22--155165-computational-complexity-of-the-weisfeiler-leman-dimension-moritz-lichter-et-al-2024>(2/2 | 155/165) Computational complexity of the Weisfeiler-Leman dimension (Moritz Lichter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moritz Lichter, Simon Raßmann, Pascal Schweitzer. (2024)<br><strong>Computational complexity of the Weisfeiler-Leman dimension</strong><br><button class=copy-to-clipboard title="Computational complexity of the Weisfeiler-Leman dimension" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DM, cs.CC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11531v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11531v1.pdf filename=2402.11531v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Weisfeiler-Leman dimension of a <b>graph</b> $G$ is the least number $k$ such that the $k$-dimensional Weisfeiler-Leman algorithm distinguishes $G$ from every other non-isomorphic <b>graph.</b> The dimension is a standard measure of the descriptive complexity of a <b>graph</b> and recently finds various applications in particular in the context of machine learning. In this paper, we study the computational complexity of computing the Weisfeiler-Leman dimension. We observe that in general the problem of deciding whether the Weisfeiler-Leman dimension of $G$ is at most $k$ is NP-hard. This is also true for the more restricted problem with <b>graphs</b> of color multiplicity at most 4. Therefore, we study parameterized and approximate versions of the problem. We give, for each fixed $k\geq 2$, a polynomial-time algorithm that decides whether the Weisfeiler-Leman dimension of a given <b>graph</b> of color multiplicity at most $5$ is at most $k$. Moreover, we show that for these color multiplicities this is optimal in the sense that this problem is P-hard under logspace-uniform $\text{AC}_0$-reductions. Furthermore, for each larger bound $c$ on the color multiplicity and each fixed $k \geq 2$, we provide a polynomial-time approximation algorithm for the abelian case: given a relational structure with abelian color classes of size at most $c$, the algorithm outputs either that its Weisfeiler-Leman dimension is at most $(k+1)c$ or that it is larger than $k$.</p></p class="citation"></blockquote><h2 id=statml-1>stat.ML (1)</h2><h3 id=11--156165-empirical-density-estimation-based-on-spline-quasi-interpolation-with-applications-to-copulas-clustering-modeling-cristiano-tamborrino-et-al-2024>(1/1 | 156/165) Empirical Density Estimation based on Spline Quasi-Interpolation with applications to Copulas clustering modeling (Cristiano Tamborrino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristiano Tamborrino, Antonella Falini, Francesca Mazzia. (2024)<br><strong>Empirical Density Estimation based on Spline Quasi-Interpolation with applications to Copulas clustering modeling</strong><br><button class=copy-to-clipboard title="Empirical Density Estimation based on Spline Quasi-Interpolation with applications to Copulas clustering modeling" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, cs-NA, math-NA, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Anomaly Detection, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11552v1.pdf filename=2402.11552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Density estimation is a fundamental technique employed in various fields to model and to understand the underlying distribution of data. The primary objective of density estimation is to estimate the probability density function of a random variable. This process is particularly valuable when dealing with univariate or multivariate data and is essential for tasks such as <b>clustering,</b> <b>anomaly</b> <b>detection,</b> and generative modeling. In this paper we propose the mono-variate approximation of the density using spline quasi interpolation and we applied it in the context of <b>clustering</b> modeling. The <b>clustering</b> technique used is based on the construction of suitable multivariate distributions which rely on the estimation of the monovariate empirical densities (marginals). Such an approximation is achieved by using the proposed spline quasi-interpolation, while the joint distributions to model the sought <b>clustering</b> partition is constructed with the use of copulas functions. In particular, since copulas can capture the dependence between the features of the data independently from the marginal distributions, a finite mixture copula model is proposed. The presented algorithm is validated on artificial and real datasets.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--157165-adaptive-decision-making-for-autonomous-vehicles-a-learning-enhanced-game-theoretic-approach-in-interactive-environments-heye-huang-et-al-2024>(1/2 | 157/165) Adaptive Decision-Making for Autonomous Vehicles: A Learning-Enhanced Game-Theoretic Approach in Interactive Environments (Heye Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heye Huang, Jinxin Liu, Guanya Shi, Shiyue Zhao, Boqi Li, Jianqiang Wang. (2024)<br><strong>Adaptive Decision-Making for Autonomous Vehicles: A Learning-Enhanced Game-Theoretic Approach in Interactive Environments</strong><br><button class=copy-to-clipboard title="Adaptive Decision-Making for Autonomous Vehicles: A Learning-Enhanced Game-Theoretic Approach in Interactive Environments" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11467v1.pdf filename=2402.11467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes an adaptive behavioral decision-making method for autonomous vehicles (AVs) focusing on complex merging scenarios. Leveraging principles from non-cooperative game theory, we develop a vehicle interaction behavior model that defines key traffic elements and integrates a multifactorial reward function. Maximum entropy inverse <b>reinforcement</b> <b>learning</b> (IRL) is employed for behavior model parameter optimization. Optimal matching parameters can be obtained using the interaction behavior feature vector and the behavior probabilities output by the vehicle interaction model. Further, a behavioral decision-making method adapted to dynamic environments is proposed. By establishing a mapping model between multiple environmental variables and model parameters, it enables parameters online learning and recognition, and achieves to output interactive behavior probabilities of AVs. Quantitative analysis employing naturalistic driving datasets (highD and exiD) and real-vehicle test data validates the model&rsquo;s high consistency with human decision-making. In 188 tested interaction scenarios, the average human-like similarity rate is 81.73%, with a notable 83.12% in the highD dataset. Furthermore, in 145 dynamic interactions, the method matches human decisions at 77.12%, with 6913 consistence instances. Moreover, in real-vehicle tests, a 72.73% similarity with 0% safety violations are obtained. Results demonstrate the effectiveness of our proposed method in enabling AVs to make informed adaptive behavior decisions in interactive environments.</p></p class="citation"></blockquote><h3 id=22--158165-on-the-limits-of-information-spread-by-memory-less-agents-niccolò-darchivio-et-al-2024>(2/2 | 158/165) On the Limits of Information Spread by Memory-less Agents (Niccolò D&rsquo;Archivio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niccolò D&rsquo;Archivio, Robin Vacus. (2024)<br><strong>On the Limits of Information Spread by Memory-less Agents</strong><br><button class=copy-to-clipboard title="On the Limits of Information Spread by Memory-less Agents" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-DC, cs-MA, cs.MA<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11553v1.pdf filename=2402.11553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the self-stabilizing bit-dissemination problem, designed to capture the challenges of spreading information and reaching consensus among entities with minimal cognitive and communication capacities. Specifically, a group of $n$ agents is required to adopt the correct opinion, initially held by a single informed individual, choosing from two possible opinions. In order to make decisions, agents are restricted to observing the opinions of a few randomly <b>sampled</b> <b>agents,</b> and lack the ability to communicate further and to identify the informed individual. Additionally, agents cannot retain any information from one round to the next. According to a recent publication in SODA (2024), a logarithmic convergence time without memory is achievable in the parallel setting (where agents are updated simultaneously), as long as the number of <b>samples</b> <b>is</b> at least $\Omega(\sqrt{n \log n})$. However, determining the minimal <b>sample</b> <b>size</b> for an efficient protocol to exist remains a challenging open question. As a preliminary step towards an answer, we establish the first lower bound for this problem in the parallel setting. Specifically, we demonstrate that any protocol with constant <b>sample</b> <b>size</b> requires asymptotically an almost-linear number of rounds to converge, with high probability. This lower bound holds even when agents are aware of both the exact value of $n$ and their own opinion, and encompasses various simple existing dynamics designed to achieve consensus. Beyond the bit-dissemination problem, our result sheds light on the convergence time of the &ldquo;minority&rdquo; dynamics, the counterpart of the well-known majority rule, whose chaotic behavior is yet to be fully understood despite the apparent simplicity of the algorithm.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--159165-to-store-or-not-to-store-a-graph-theoretical-approach-for-dataset-versioning-anxin-guo-et-al-2024>(1/2 | 159/165) To Store or Not to Store: a graph theoretical approach for Dataset Versioning (Anxin Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anxin Guo, Jingwei Li, Pattara Sukprasert, Samir Khuller, Amol Deshpande, Koyel Mukherjee. (2024)<br><strong>To Store or Not to Store: a graph theoretical approach for Dataset Versioning</strong><br><button class=copy-to-clipboard title="To Store or Not to Store: a graph theoretical approach for Dataset Versioning" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CC, cs-DB, cs-DC, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11741v1.pdf filename=2402.11741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we study the cost efficient data versioning problem, where the goal is to optimize the storage and reconstruction (retrieval) costs of data versions, given a <b>graph</b> of datasets as nodes and edges capturing edit/delta information. One central variant we study is MinSum Retrieval (MSR) where the goal is to minimize the total retrieval costs, while keeping the storage costs bounded. This problem (along with its variants) was introduced by Bhattacherjee et al. [VLDB'15]. While such problems are frequently encountered in collaborative tools (e.g., version control systems and data analysis pipelines), to the best of our knowledge, no existing research studies the theoretical aspects of these problems. We establish that the currently best-known heuristic, LMG, can perform arbitrarily badly in a simple worst case. Moreover, we show that it is hard to get $o(n)$-approximation for MSR on general <b>graphs</b> even if we relax the storage constraints by an $O(\log n)$ factor. Similar hardness results are shown for other variants. Meanwhile, we propose poly-time approximation schemes for tree-like <b>graphs,</b> motivated by the fact that the <b>graphs</b> arising in practice from typical edit operations are often not arbitrary. As version <b>graphs</b> typically have low treewidth, we further develop new algorithms for bounded treewidth <b>graphs.</b> Furthermore, we propose two new heuristics and evaluate them empirically. First, we extend LMG by considering more potential ``moves&rsquo;&rsquo;, to propose a new heuristic LMG-All. LMG-All consistently outperforms LMG while having comparable run time on a wide variety of datasets, i.e., version <b>graphs.</b> Secondly, we apply our tree algorithms on the minimum-storage arborescence of an instance, yielding algorithms that are qualitatively better than all previous heuristics for MSR, as well as for another variant BoundedMin Retrieval (BMR).</p></p class="citation"></blockquote><h3 id=22--160165-odd-cycle-transversal-on-p_5-free-graphs-in-polynomial-time-akanksha-agrawal-et-al-2024>(2/2 | 160/165) Odd Cycle Transversal on $P_5$-free Graphs in Polynomial Time (Akanksha Agrawal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akanksha Agrawal, Paloma T. Lima, Daniel Lokshtanov, Pawel Rzążewski, Saket Saurabh, Roohani Sharma. (2024)<br><strong>Odd Cycle Transversal on $P_5$-free Graphs in Polynomial Time</strong><br><button class=copy-to-clipboard title="Odd Cycle Transversal on $P_5$-free Graphs in Polynomial Time" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: 68Q25, 05C85, F-2, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11465v1.pdf filename=2402.11465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An independent set in a <b>graph</b> G is a set of pairwise non-adjacent vertices. A <b>graph</b> $G$ is bipartite if its vertex set can be partitioned into two independent sets. In the Odd Cycle Transversal problem, the input is a <b>graph</b> $G$ along with a weight function $w$ associating a rational weight with each vertex, and the task is to find a smallest weight vertex subset $S$ in $G$ such that $G - S$ is bipartite; the weight of $S$, $w(S) = \sum_{v\in S} w(v)$. We show that Odd Cycle Transversal is polynomial-time solvable on <b>graphs</b> excluding $P_5$ (a path on five vertices) as an induced subgraph. The problem was previously known to be polynomial-time solvable on $P_4$-free <b>graphs</b> and NP-hard on $P_6$-free <b>graphs</b> [Dabrowski, Feghali, Johnson, Paesani, Paulusma and Rz\k{a}.zewski, Algorithmica 2020]. Bonamy, Dabrowski, Feghali, Johnson and Paulusma [Algorithmica 2019] posed the existence of a polynomial-time algorithm on $P_5$-free <b>graphs</b> as an open problem, this was later re-stated by Rz\k{a}.zewski [Dagstuhl Reports, 9(6): 2019] and by Chudnovsky, King, Pilipczuk, Rz\k{a}.zewski, and Spirkl [SIDMA 2021], who gave an algorithm with running time $n^{O(\sqrt{n})}$.</p></p class="citation"></blockquote><h2 id=mathna-1>math.NA (1)</h2><h3 id=11--161165-mixed-material-point-method-formulation-stabilization-and-validation-for-a-unified-analysis-of-free-surface-and-seepage-flow-bodhinanda-chandra-et-al-2024>(1/1 | 161/165) Mixed material point method formulation, stabilization, and validation for a unified analysis of free-surface and seepage flow (Bodhinanda Chandra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bodhinanda Chandra, Ryota Hashimoto, Ken Kamrin, Kenichi Soga. (2024)<br><strong>Mixed material point method formulation, stabilization, and validation for a unified analysis of free-surface and seepage flow</strong><br><button class=copy-to-clipboard title="Mixed material point method formulation, stabilization, and validation for a unified analysis of free-surface and seepage flow" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11719v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11719v1.pdf filename=2402.11719v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel stabilized mixed material point method (MPM) designed for the unified modeling of free-surface and seepage flow. The unified formulation integrates the Navier-Stokes equation with the Darcy-Brinkman-Forchheimer equation, effectively capturing flows in both non-porous and porous domains. In contrast to the conventional Eulerian computational fluid dynamics (CFD) solver, which solves the velocity and pressure fields as unknown variables, the proposed method employs a monolithic displacement-pressure formulation adopted from the mixed-form updated-Lagrangian finite element method (FEM). To satisfy the discrete inf-sup stability condition, a stabilization strategy based on the variational multiscale method (VMS) is derived and integrated into the proposed formulation. Another distinctive feature is the implementation of blurred interfaces, which facilitate a seamless and stable transition of flows between free and porous domains, as well as across two distinct porous media. The efficacy of the proposed formulation is verified and validated through several <b>benchmark</b> cases in 1D, 2D, and 3D scenarios. Conducted numerical examples demonstrate enhanced accuracy and stability compared to analytical, experimental, and other numerical solutions.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--162165-model-free-μ-synthesis-a-nonsmooth-optimization-perspective-darioush-keivan-et-al-2024>(1/1 | 162/165) Model-Free $μ$-Synthesis: A Nonsmooth Optimization Perspective (Darioush Keivan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Darioush Keivan, Xingang Guo, Peter Seiler, Geir Dullerud, Bin Hu. (2024)<br><strong>Model-Free $μ$-Synthesis: A Nonsmooth Optimization Perspective</strong><br><button class=copy-to-clipboard title="Model-Free $μ$-Synthesis: A Nonsmooth Optimization Perspective" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11654v1.pdf filename=2402.11654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we revisit model-free policy search on an important robust control <b>benchmark,</b> namely $\mu$-synthesis. In the general output-feedback setting, there do not exist convex formulations for this problem, and hence global optimality guarantees are not expected. Apkarian (2011) presented a nonconvex nonsmooth policy optimization approach for this problem, and achieved state-of-the-art design results via using subgradient-based policy search algorithms which generate update directions in a model-based manner. Despite the lack of convexity and global optimality guarantees, these subgradient-based policy search methods have led to impressive numerical results in practice. Built upon such a policy optimization persepctive, our paper extends these subgradient-based search methods to a model-free setting. Specifically, we examine the effectiveness of two model-free policy optimization strategies: the model-free non-derivative sampling method and the zeroth-order policy search with uniform smoothing. We performed an extensive numerical study to demonstrate that both methods consistently replicate the design outcomes achieved by their model-based counterparts. Additionally, we provide some theoretical justifications showing that convergence guarantees to stationary points can be established for our model-free $\mu$-synthesis under some assumptions related to the coerciveness of the cost function. Overall, our results demonstrate that derivative-free policy optimization offers a competitive and viable approach for solving general output-feedback $\mu$-synthesis problems in the model-free setting.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--163165-spectral-independence-beyond-uniqueness-with-the-topological-method----an-extended-view-charilaos-efthymiou-2024>(1/1 | 163/165) Spectral Independence Beyond Uniqueness with. the topological method &ndash; An extended view (Charilaos Efthymiou, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charilaos Efthymiou. (2024)<br><strong>Spectral Independence Beyond Uniqueness with. the topological method &ndash; An extended view</strong><br><button class=copy-to-clipboard title="Spectral Independence Beyond Uniqueness with. the topological method -- An extended view" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11647v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11647v1.pdf filename=2402.11647v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present novel results for fast mixing of Glauber dynamics using the newly introduced and powerful Spectral Independence method from [Anari, Liu, Oveis-Gharan: FOCS 2020]. We mainly focus on the Hard-core model and the Ising model. We obtain bounds for fast mixing with the parameters expressed in terms of the spectral radius of the adjacency matrix, improving on the seminal work in [Hayes: FOCS 2006]. Furthermore, we go beyond the adjacency matrix and establish &ndash; for the first time &ndash; rapid mixing results for Glauber dynamics expressed in terms of the spectral radius of the Hashimoto non-backtracking matrix of the underlying <b>graph</b> $G$. Working with the non-backtracking spectrum is extremely challenging, but also more desirable. Its eigenvalues are less correlated with the high-degree vertices than those of the adjacency matrix and express more accurately invariants of the <b>graph</b> such as the growth rate. Our results require ``weak normality" from the Hashimoto matrix. This condition is mild and allows us to obtain very interesting bound. We study the pairwise influence matrix ${I}^{\Lambda,\tau}_{G}$ by exploiting the connection between the matrix and the trees of self-avoiding walks, however, we go beyond the standard treatment of the distributional recursions. The common framework that underlies our techniques we call the topological method. Our approach is novel and gives new insights into how to establish Spectral Independence for Gibbs distributions. More importantly, it allows us to derive new &ndash; improved &ndash; rapid mixing bounds for Glauber dynamics on distributions such as the Hard-core model and the Ising model for <b>graphs</b> that the spectral radius is smaller than the maximum degree.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--164165-towards-distributed-and-intelligent-integrated-sensing-and-communications-for-6g-networks-emilio-calvanese-strinati-et-al-2024>(1/1 | 164/165) Towards Distributed and Intelligent Integrated Sensing and Communications for 6G Networks (Emilio Calvanese Strinati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emilio Calvanese Strinati, George C. Alexandropoulos, Navid Amani, Maurizio Crozzoli, Giyyarpuram Madhusudan, Sami Mekki, Francois Rivet, Vincenzo Sciancalepore, Philippe Sehier, Maximilian Stark, Henk Wymeersch. (2024)<br><strong>Towards Distributed and Intelligent Integrated Sensing and Communications for 6G Networks</strong><br><button class=copy-to-clipboard title="Towards Distributed and Intelligent Integrated Sensing and Communications for 6G Networks" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11630v1.pdf filename=2402.11630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the distributed and intelligent integrated sensing and communications (DISAC) concept, a transformative approach for 6G wireless networks that extends the emerging concept of integrated sensing and communications (ISAC). DISAC addresses the limitations of the existing ISAC models and, to overcome them, it introduces two novel foundational functionalities for both sensing and communications: a distributed architecture and a semantic and goal-oriented framework. The distributed architecture enables large-scale and energy-efficient tracking of connected users and objects, leveraging the fusion of heterogeneous sensors. The semantic and goal-oriented intelligent and parsimonious framework, enables the transition from classical data fusion to the composition of semantically selected information, offering new paradigms for the optimization of resource utilization and exceptional <b>multi-modal</b> sensing performance across various use cases. This paper details DISAC&rsquo;s principles, architecture, and potential applications.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--165165-addressing-internally-disconnected-communities-in-leiden-and-louvain-community-detection-algorithms-subhajit-sahu-2024>(1/1 | 165/165) Addressing Internally-Disconnected Communities in Leiden and Louvain Community Detection Algorithms (Subhajit Sahu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subhajit Sahu. (2024)<br><strong>Addressing Internally-Disconnected Communities in Leiden and Louvain Community Detection Algorithms</strong><br><button class=copy-to-clipboard title="Addressing Internally-Disconnected Communities in Leiden and Louvain Community Detection Algorithms" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: G-2-2; I-5-3, cs-DC, cs-SI, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11454v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11454v2.pdf filename=2402.11454v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Community detection is the problem of identifying densely connected clusters of nodes within a network. The Louvain algorithm is a widely used method for this task, but it can produce communities that are internally disconnected. To address this, the Leiden algorithm was introduced. However, our analysis and empirical observations indicate that the Leiden algorithm still identifies disconnected communities, albeit to a lesser extent. To mitigate this issue, we propose two new parallel algorithms: GSP-Leiden and GSP-Louvain, based on the Leiden and Louvain algorithms, respectively. On a system with two 16-core Intel Xeon Gold 6226R processors, we demonstrate that GSP-Leiden/GSP-Louvain not only address this issue, but also outperform the original Leiden, igraph Leiden, and NetworKit Leiden by 190x/341x, 46x/83x, and 3.4x/6.1x respectively - achieving a processing rate of 195M/328M edges/s on a 3.8B edge <b>graph.</b> Furthermore, GSP-Leiden/GSP-Louvain improve performance at a rate of 1.6x/1.5x for every doubling of threads.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.19</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.21</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-61>cs.CL (61)</a><ul><li><a href=#161--1165-gnnavi-navigating-the-information-flow-in-large-language-models-by-graph-neural-network-shuzhou-yuan-et-al-2024>(1/61 | 1/165) GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network (Shuzhou Yuan et al., 2024)</a></li><li><a href=#261--2165-eventrl-enhancing-event-extraction-with-outcome-supervision-for-large-language-models-jun-gao-et-al-2024>(2/61 | 2/165) EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models (Jun Gao et al., 2024)</a></li><li><a href=#361--3165-decoding-news-narratives-a-critical-analysis-of-large-language-models-in-framing-bias-detection-valeria-pastorino-et-al-2024>(3/61 | 3/165) Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection (Valeria Pastorino et al., 2024)</a></li><li><a href=#461--4165-learning-from-failure-integrating-negative-examples-when-fine-tuning-large-language-models-as-agents-renxi-wang-et-al-2024>(4/61 | 4/165) Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents (Renxi Wang et al., 2024)</a></li><li><a href=#561--5165-why-lift-so-heavy-slimming-large-language-models-by-cutting-off-the-layers-shuzhou-yuan-et-al-2024>(5/61 | 5/165) Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers (Shuzhou Yuan et al., 2024)</a></li><li><a href=#661--6165-vision-flan-scaling-human-labeled-tasks-in-visual-instruction-tuning-zhiyang-xu-et-al-2024>(6/61 | 6/165) Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning (Zhiyang Xu et al., 2024)</a></li><li><a href=#761--7165-longagent-scaling-language-models-to-128k-context-through-multi-agent-collaboration-jun-zhao-et-al-2024>(7/61 | 7/165) LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration (Jun Zhao et al., 2024)</a></li><li><a href=#861--8165-dictllm-harnessing-key-value-data-structures-with-large-language-models-for-enhanced-medical-diagnostics-yiqiu-guo-et-al-2024>(8/61 | 8/165) DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics (YiQiu Guo et al., 2024)</a></li><li><a href=#961--9165-autoprm-automating-procedural-supervision-for-multi-step-reasoning-via-controllable-question-decomposition-zhaorun-chen-et-al-2024>(9/61 | 9/165) AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition (Zhaorun Chen et al., 2024)</a></li><li><a href=#1061--10165-counter-intuitive-large-language-models-can-better-understand-knowledge-graphs-than-we-thought-xinbang-dai-et-al-2024>(10/61 | 10/165) Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought (Xinbang Dai et al., 2024)</a></li><li><a href=#1161--11165-multi-task-inference-can-large-language-models-follow-multiple-instructions-at-once-guijin-son-et-al-2024>(11/61 | 11/165) Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once? (Guijin Son et al., 2024)</a></li><li><a href=#1261--12165-factpico-factuality-evaluation-for-plain-language-summarization-of-medical-evidence-sebastian-antony-joseph-et-al-2024>(12/61 | 12/165) FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence (Sebastian Antony Joseph et al., 2024)</a></li><li><a href=#1361--13165-one-prompt-to-rule-them-all-llms-for-opinion-summary-evaluation-tejpalsingh-siledar-et-al-2024>(13/61 | 13/165) One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation (Tejpalsingh Siledar et al., 2024)</a></li><li><a href=#1461--14165-can-llms-reason-with-rules-logic-scaffolding-for-stress-testing-and-improving-llms-siyuan-wang-et-al-2024>(14/61 | 14/165) Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs (Siyuan Wang et al., 2024)</a></li><li><a href=#1561--15165-multi-dimensional-evaluation-of-empathetic-dialog-responses-zhichao-xu-et-al-2024>(15/61 | 15/165) Multi-dimensional Evaluation of Empathetic Dialog Responses (Zhichao Xu et al., 2024)</a></li><li><a href=#1661--16165-matplotagent-method-and-evaluation-for-llm-based-agentic-scientific-data-visualization-zhiyu-yang-et-al-2024>(16/61 | 16/165) MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization (Zhiyu Yang et al., 2024)</a></li><li><a href=#1761--17165-kmmlu-measuring-massive-multitask-language-understanding-in-korean-guijin-son-et-al-2024>(17/61 | 17/165) KMMLU: Measuring Massive Multitask Language Understanding in Korean (Guijin Son et al., 2024)</a></li><li><a href=#1861--18165-modelling-political-coalition-negotiations-using-llm-based-agents-farhad-moghimifar-et-al-2024>(18/61 | 18/165) Modelling Political Coalition Negotiations Using LLM-based Agents (Farhad Moghimifar et al., 2024)</a></li><li><a href=#1961--19165-chain-of-instructions-compositional-instruction-tuning-on-large-language-models-shirley-anugrah-hayati-et-al-2024>(19/61 | 19/165) Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models (Shirley Anugrah Hayati et al., 2024)</a></li><li><a href=#2061--20165-leia-facilitating-cross-lingual-knowledge-transfer-in-language-models-with-entity-based-data-augmentation-ikuya-yamada-et-al-2024>(20/61 | 20/165) LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation (Ikuya Yamada et al., 2024)</a></li><li><a href=#2161--21165-in-context-example-ordering-guided-by-label-distributions-zhichao-xu-et-al-2024>(21/61 | 21/165) In-Context Example Ordering Guided by Label Distributions (Zhichao Xu et al., 2024)</a></li><li><a href=#2261--22165-perils-of-self-feedback-self-bias-amplifies-in-large-language-models-wenda-xu-et-al-2024>(22/61 | 22/165) Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models (Wenda Xu et al., 2024)</a></li><li><a href=#2361--23165-dont-go-to-extremes-revealing-the-excessive-sensitivity-and-calibration-limitations-of-llms-in-implicit-hate-speech-detection-min-zhang-et-al-2024>(23/61 | 23/165) Don&rsquo;t Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection (Min Zhang et al., 2024)</a></li><li><a href=#2461--24165-infuserki-enhancing-large-language-models-with-knowledge-graphs-via-infuser-guided-knowledge-integration-fali-wang-et-al-2024>(24/61 | 24/165) InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration (Fali Wang et al., 2024)</a></li><li><a href=#2561--25165-sciagent-tool-augmented-language-models-for-scientific-reasoning-yubo-ma-et-al-2024>(25/61 | 25/165) SciAgent: Tool-augmented Language Models for Scientific Reasoning (Yubo Ma et al., 2024)</a></li><li><a href=#2661--26165-can-deception-detection-go-deeper-dataset-evaluation-and-benchmark-for-deception-reasoning-kang-chen-et-al-2024>(26/61 | 26/165) Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning (Kang Chen et al., 2024)</a></li><li><a href=#2761--27165-morl-prompt-an-empirical-analysis-of-multi-objective-reinforcement-learning-for-discrete-prompt-optimization-yasaman-jafari-et-al-2024>(27/61 | 27/165) MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization (Yasaman Jafari et al., 2024)</a></li><li><a href=#2861--28165-autocorrect-for-estonian-texts-final-report-from-project-ektb25-agnes-luhtaru-et-al-2024>(28/61 | 28/165) Autocorrect for Estonian texts: final report from project EKTB25 (Agnes Luhtaru et al., 2024)</a></li><li><a href=#2961--29165-self-seeding-and-multi-intent-self-instructing-llms-for-generating-intent-aware-information-seeking-dialogs-arian-askari-et-al-2024>(29/61 | 29/165) Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs (Arian Askari et al., 2024)</a></li><li><a href=#3061--30165-metacognitive-retrieval-augmented-large-language-models-yujia-zhou-et-al-2024>(30/61 | 30/165) Metacognitive Retrieval-Augmented Large Language Models (Yujia Zhou et al., 2024)</a></li><li><a href=#3161--31165-preact-predicting-future-in-react-enhances-agents-planning-ability-dayuan-fu-et-al-2024>(31/61 | 31/165) PreAct: Predicting Future in ReAct Enhances Agent&rsquo;s Planning Ability (Dayuan Fu et al., 2024)</a></li><li><a href=#3261--32165-advancing-translation-preference-modeling-with-rlhf-a-step-towards-cost-effective-solution-nuo-xu-et-al-2024>(32/61 | 32/165) Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution (Nuo Xu et al., 2024)</a></li><li><a href=#3361--33165-knowledge-to-sql-enhancing-sql-generation-with-data-expert-llm-zijin-hong-et-al-2024>(33/61 | 33/165) Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM (Zijin Hong et al., 2024)</a></li><li><a href=#3461--34165-federated-fine-tuning-of-large-language-models-under-heterogeneous-language-tasks-and-client-resources-jiamu-bai-et-al-2024>(34/61 | 34/165) Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources (Jiamu Bai et al., 2024)</a></li><li><a href=#3561--35165-loretta-low-rank-economic-tensor-train-adaptation-for-ultra-low-parameter-fine-tuning-of-large-language-models-yifan-yang-et-al-2024>(35/61 | 35/165) LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models (Yifan Yang et al., 2024)</a></li><li><a href=#3661--36165-allava-harnessing-gpt4v-synthesized-data-for-a-lite-vision-language-model-guiming-hardy-chen-et-al-2024>(36/61 | 36/165) ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model (Guiming Hardy Chen et al., 2024)</a></li><li><a href=#3761--37165-benchmarking-knowledge-boundary-for-large-language-model-a-different-perspective-on-model-evaluation-xunjian-yin-et-al-2024>(37/61 | 37/165) Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation (Xunjian Yin et al., 2024)</a></li><li><a href=#3861--38165-question-answering-over-spatio-temporal-knowledge-graph-xinbang-dai-et-al-2024>(38/61 | 38/165) Question Answering Over Spatio-Temporal Knowledge Graph (Xinbang Dai et al., 2024)</a></li><li><a href=#3961--39165-deciphering-the-lmpact-of-pretraining-data-on-large-language-models-through-machine-unlearning-yang-zhao-et-al-2024>(39/61 | 39/165) Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning (Yang Zhao et al., 2024)</a></li><li><a href=#4061--40165-how-susceptible-are-large-language-models-to-ideological-manipulation-kai-chen-et-al-2024>(40/61 | 40/165) How Susceptible are Large Language Models to Ideological Manipulation? (Kai Chen et al., 2024)</a></li><li><a href=#4161--41165-a-multi-aspect-framework-for-counter-narrative-evaluation-using-large-language-models-jaylen-jones-et-al-2024>(41/61 | 41/165) A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models (Jaylen Jones et al., 2024)</a></li><li><a href=#4261--42165-competition-of-mechanisms-tracing-how-language-models-handle-facts-and-counterfactuals-francesco-ortu-et-al-2024>(42/61 | 42/165) Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals (Francesco Ortu et al., 2024)</a></li><li><a href=#4361--43165-stumbling-blocks-stress-testing-the-robustness-of-machine-generated-text-detectors-under-attacks-yichen-wang-et-al-2024>(43/61 | 43/165) Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks (Yichen Wang et al., 2024)</a></li><li><a href=#4461--44165-unveiling-the-secrets-of-engaging-conversations-factors-that-keep-users-hooked-on-role-playing-dialog-agents-shuai-zhang-et-al-2024>(44/61 | 44/165) Unveiling the Secrets of Engaging Conversations: Factors that Keep Users Hooked on Role-Playing Dialog Agents (Shuai Zhang et al., 2024)</a></li><li><a href=#4561--45165-when-do-llms-need-retrieval-augmentation-mitigating-llms-overconfidence-helps-retrieval-augmentation-shiyu-ni-et-al-2024>(45/61 | 45/165) When Do LLMs Need Retrieval Augmentation? Mitigating LLMs&rsquo; Overconfidence Helps Retrieval Augmentation (Shiyu Ni et al., 2024)</a></li><li><a href=#4661--46165-rethinking-the-roles-of-large-language-models-in-chinese-grammatical-error-correction-yinghui-li-et-al-2024>(46/61 | 46/165) Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction (Yinghui Li et al., 2024)</a></li><li><a href=#4761--47165-opening-the-black-box-of-language-acquisition-jérôme-michaud-et-al-2024>(47/61 | 47/165) Opening the black box of language acquisition (Jérôme Michaud et al., 2024)</a></li><li><a href=#4861--48165-benchmark-self-evolving-a-multi-agent-framework-for-dynamic-llm-evaluation-siyuan-wang-et-al-2024>(48/61 | 48/165) Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation (Siyuan Wang et al., 2024)</a></li><li><a href=#4961--49165-specrawler-generating-openapi-specifications-from-api-documentation-using-large-language-models-koren-lazar-et-al-2024>(49/61 | 49/165) SpeCrawler: Generating OpenAPI Specifications from API Documentation Using Large Language Models (Koren Lazar et al., 2024)</a></li><li><a href=#5061--50165-extensible-embedding-a-flexible-multipler-for-llms-context-length-ninglu-shao-et-al-2024>(50/61 | 50/165) Extensible Embedding: A Flexible Multipler For LLM&rsquo;s Context Length (Ninglu Shao et al., 2024)</a></li><li><a href=#5161--51165-bge-landmark-embedding-a-chunking-free-embedding-method-for-retrieval-augmented-long-context-large-language-models-kun-luo-et-al-2024>(51/61 | 51/165) BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models (Kun Luo et al., 2024)</a></li><li><a href=#5261--52165-from-prejudice-to-parity-a-new-approach-to-debiasing-large-language-model-word-embeddings-aishik-rakshit-et-al-2024>(52/61 | 52/165) From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings (Aishik Rakshit et al., 2024)</a></li><li><a href=#5361--53165-whats-the-plan-evaluating-and-developing-planning-aware-techniques-for-llms-eran-hirsch-et-al-2024>(53/61 | 53/165) What&rsquo;s the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs (Eran Hirsch et al., 2024)</a></li><li><a href=#5461--54165-lora-flow-dynamic-lora-fusion-for-large-language-models-in-generative-tasks-hanqing-wang-et-al-2024>(54/61 | 54/165) LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks (Hanqing Wang et al., 2024)</a></li><li><a href=#5561--55165-mitigating-catastrophic-forgetting-in-multi-domain-chinese-spelling-correction-by-multi-stage-knowledge-transfer-framework-peng-xing-et-al-2024>(55/61 | 55/165) Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling Correction by Multi-stage Knowledge Transfer Framework (Peng Xing et al., 2024)</a></li><li><a href=#5661--56165-cobra-effect-in-reference-free-image-captioning-metrics-zheng-ma-et-al-2024>(56/61 | 56/165) Cobra Effect in Reference-Free Image Captioning Metrics (Zheng Ma et al., 2024)</a></li><li><a href=#5761--57165-fine-grained-and-explainable-factuality-evaluation-for-multimodal-summarization-liqiang-jing-et-al-2024>(57/61 | 57/165) Fine-grained and Explainable Factuality Evaluation for Multimodal Summarization (Liqiang Jing et al., 2024)</a></li><li><a href=#5861--58165-numerical-claim-detection-in-finance-a-new-financial-dataset-weak-supervision-model-and-market-analysis-agam-shah-et-al-2024>(58/61 | 58/165) Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis (Agam Shah et al., 2024)</a></li><li><a href=#5961--59165-syntactic-language-change-in-english-and-german-metrics-parsers-and-convergences-yanran-chen-et-al-2024>(59/61 | 59/165) Syntactic Language Change in English and German: Metrics, Parsers, and Convergences (Yanran Chen et al., 2024)</a></li><li><a href=#6061--60165-a-note-on-bias-to-complete-jia-xu-et-al-2024>(60/61 | 60/165) A Note on Bias to Complete (Jia Xu et al., 2024)</a></li><li><a href=#6161--61165-metric-learning-encoding-models-identify-processing-profiles-of-linguistic-features-in-berts-representations-louis-jalouzot-et-al-2024>(61/61 | 61/165) Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT&rsquo;s Representations (Louis Jalouzot et al., 2024)</a></li></ul></li><li><a href=#csit-4>cs.IT (4)</a><ul><li><a href=#14--62165-integrating-pre-trained-language-model-with-physical-layer-communications-ju-hyung-lee-et-al-2024>(1/4 | 62/165) Integrating Pre-Trained Language Model with Physical Layer Communications (Ju-Hyung Lee et al., 2024)</a></li><li><a href=#24--63165-hybrid-ris-with-sub-connected-active-partitions-performance-analysis-and-transmission-design-konstantinos-ntougias-et-al-2024>(2/4 | 63/165) Hybrid RIS With Sub-Connected Active Partitions: Performance Analysis and Transmission Design (Konstantinos Ntougias et al., 2024)</a></li><li><a href=#34--64165-on-efficient-normal-bases-over-binary-fields-mohamadou-sall-et-al-2024>(3/4 | 64/165) On efficient normal bases over binary fields (Mohamadou Sall et al., 2024)</a></li><li><a href=#44--65165-age-of-kn-threshold-signature-scheme-on-a-gossip-network-erkan-bayram-et-al-2024>(4/4 | 65/165) Age of $(k,n)$-Threshold Signature Scheme on a Gossip Network (Erkan Bayram et al., 2024)</a></li></ul></li><li><a href=#csro-7>cs.RO (7)</a><ul><li><a href=#17--66165-aint-misbehavin----using-llms-to-generate-expressive-robot-behavior-in-conversations-with-the-tabletop-robot-haru-zining-wang-et-al-2024>(1/7 | 66/165) Ain&rsquo;t Misbehavin&rsquo; &ndash; Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru (Zining Wang et al., 2024)</a></li><li><a href=#27--67165-learning-to-learn-faster-from-human-feedback-with-language-model-predictive-control-jacky-liang-et-al-2024>(2/7 | 67/165) Learning to Learn Faster from Human Feedback with Language Model Predictive Control (Jacky Liang et al., 2024)</a></li><li><a href=#37--68165-lirafusion-deep-adaptive-lidar-radar-fusion-for-3d-object-detection-jingyu-song-et-al-2024>(3/7 | 68/165) LiRaFusion: Deep Adaptive LiDAR-Radar Fusion for 3D Object Detection (Jingyu Song et al., 2024)</a></li><li><a href=#47--69165-smooth-path-planning-with-subharmonic-artificial-potential-field-bo-peng-et-al-2024>(4/7 | 69/165) Smooth Path Planning with Subharmonic Artificial Potential Field (Bo Peng et al., 2024)</a></li><li><a href=#57--70165-imitation-learning-based-online-time-optimal-control-with-multiple-waypoint-constraints-for-quadrotors-jin-zhou-et-al-2024>(5/7 | 70/165) Imitation Learning-Based Online Time-Optimal Control with Multiple-Waypoint Constraints for Quadrotors (Jin Zhou et al., 2024)</a></li><li><a href=#67--71165-verifiably-following-complex-robot-instructions-with-foundation-models-benedict-quartey-et-al-2024>(6/7 | 71/165) Verifiably Following Complex Robot Instructions with Foundation Models (Benedict Quartey et al., 2024)</a></li><li><a href=#77--72165-developing-autonomous-robot-mediated-behavior-coaching-sessions-with-haru-matouš-jelínek-et-al-2024>(7/7 | 72/165) Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru (Matouš Jelínek et al., 2024)</a></li></ul></li><li><a href=#cslg-26>cs.LG (26)</a><ul><li><a href=#126--73165-a-curious-case-of-searching-for-the-correlation-between-training-data-and-adversarial-robustness-of-transformer-textual-models-cuong-dang-et-al-2024>(1/26 | 73/165) A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models (Cuong Dang et al., 2024)</a></li><li><a href=#226--74165-revisiting-zeroth-order-optimization-for-memory-efficient-llm-fine-tuning-a-benchmark-yihua-zhang-et-al-2024>(2/26 | 74/165) Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark (Yihua Zhang et al., 2024)</a></li><li><a href=#326--75165-large-language-model-driven-meta-structure-discovery-in-heterogeneous-information-network-lin-chen-et-al-2024>(3/26 | 75/165) Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network (Lin Chen et al., 2024)</a></li><li><a href=#426--76165-aligning-modalities-in-vision-large-language-models-via-preference-fine-tuning-yiyang-zhou-et-al-2024>(4/26 | 76/165) Aligning Modalities in Vision Large Language Models via Preference Fine-tuning (Yiyang Zhou et al., 2024)</a></li><li><a href=#526--77165-graph-out-of-distribution-generalization-via-causal-intervention-qitian-wu-et-al-2024>(5/26 | 77/165) Graph Out-of-Distribution Generalization via Causal Intervention (Qitian Wu et al., 2024)</a></li><li><a href=#626--78165-in-context-learning-with-transformers-softmax-attention-adapts-to-function-lipschitzness-liam-collins-et-al-2024>(6/26 | 78/165) In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness (Liam Collins et al., 2024)</a></li><li><a href=#726--79165-towards-versatile-graph-learning-approach-from-the-perspective-of-large-language-models-lanning-wei-et-al-2024>(7/26 | 79/165) Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models (Lanning Wei et al., 2024)</a></li><li><a href=#826--80165-self-evolving-autoencoder-embedded-q-network-j-senthilnath-et-al-2024>(8/26 | 80/165) Self-evolving Autoencoder Embedded Q-Network (J. Senthilnath et al., 2024)</a></li><li><a href=#926--81165-modelgpt-unleashing-llms-capabilities-for-tailored-model-generation-zihao-tang-et-al-2024>(9/26 | 81/165) ModelGPT: Unleashing LLM&rsquo;s Capabilities for Tailored Model Generation (Zihao Tang et al., 2024)</a></li><li><a href=#1026--82165-teacher-as-a-lenient-expert-teacher-agnostic-data-free-knowledge-distillation-hyunjune-shin-et-al-2024>(10/26 | 82/165) Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation (Hyunjune Shin et al., 2024)</a></li><li><a href=#1126--83165-continual-learning-on-graphs-challenges-solutions-and-opportunities-xikun-zhang-et-al-2024>(11/26 | 83/165) Continual Learning on Graphs: Challenges, Solutions, and Opportunities (Xikun Zhang et al., 2024)</a></li><li><a href=#1226--84165-the-effectiveness-of-random-forgetting-for-robust-generalization-vijaya-raghavan-t-ramkumar-et-al-2024>(12/26 | 84/165) The Effectiveness of Random Forgetting for Robust Generalization (Vijaya Raghavan T Ramkumar et al., 2024)</a></li><li><a href=#1326--85165-discrete-neural-algorithmic-reasoning-gleb-rodionov-et-al-2024>(13/26 | 85/165) Discrete Neural Algorithmic Reasoning (Gleb Rodionov et al., 2024)</a></li><li><a href=#1426--86165-temporal-disentangled-contrastive-diffusion-model-for-spatiotemporal-imputation-yakun-chen-et-al-2024>(14/26 | 86/165) Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal Imputation (Yakun Chen et al., 2024)</a></li><li><a href=#1526--87165-optex-expediting-first-order-optimization-with-approximately-parallelized-iterations-yao-shu-et-al-2024>(15/26 | 87/165) OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations (Yao Shu et al., 2024)</a></li><li><a href=#1626--88165-invertible-fourier-neural-operators-for-tackling-both-forward-and-inverse-problems-da-long-et-al-2024>(16/26 | 88/165) Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems (Da Long et al., 2024)</a></li><li><a href=#1726--89165-simplifying-hyperparameter-tuning-in-online-machine-learning----the-spotrivergui-thomas-bartz-beielstein-2024>(17/26 | 89/165) Simplifying Hyperparameter Tuning in Online Machine Learning &ndash; The spotRiverGUI (Thomas Bartz-Beielstein, 2024)</a></li><li><a href=#1826--90165-optimal-parallelization-strategies-for-active-flow-control-in-deep-reinforcement-learning-based-computational-fluid-dynamics-wang-jia-et-al-2024>(18/26 | 90/165) Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics (Wang Jia et al., 2024)</a></li><li><a href=#1926--91165-balanced-data-imbalanced-spectra-unveiling-class-disparities-with-spectral-imbalance-chiraag-kaushik-et-al-2024>(19/26 | 91/165) Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance (Chiraag Kaushik et al., 2024)</a></li><li><a href=#2026--92165-extraction-of-nonlinearity-in-neural-networks-and-model-compression-with-koopman-operator-naoki-sugishita-et-al-2024>(20/26 | 92/165) Extraction of nonlinearity in neural networks and model compression with Koopman operator (Naoki Sugishita et al., 2024)</a></li><li><a href=#2126--93165-compression-repair-for-feedforward-neural-networks-based-on-model-equivalence-evaluation-zihao-mo-et-al-2024>(21/26 | 93/165) Compression Repair for Feedforward Neural Networks Based on Model Equivalence Evaluation (Zihao Mo et al., 2024)</a></li><li><a href=#2226--94165-learning-conditional-invariances-through-non-commutativity-abhra-chaudhuri-et-al-2024>(22/26 | 94/165) Learning Conditional Invariances through Non-Commutativity (Abhra Chaudhuri et al., 2024)</a></li><li><a href=#2326--95165-theoretical-foundations-for-programmatic-reinforcement-learning-guruprerana-shabadi-et-al-2024>(23/26 | 95/165) Theoretical foundations for programmatic reinforcement learning (Guruprerana Shabadi et al., 2024)</a></li><li><a href=#2426--96165-improved-indoor-localization-with-machine-learning-techniques-for-iot-applications-m-w-p-maduranga-2024>(24/26 | 96/165) Improved Indoor Localization with Machine Learning Techniques for IoT applications (M. W. P. Maduranga, 2024)</a></li><li><a href=#2526--97165-learning-the-topology-and-behavior-of-discrete-dynamical-systems-zirou-qiu-et-al-2024>(25/26 | 97/165) Learning the Topology and Behavior of Discrete Dynamical Systems (Zirou Qiu et al., 2024)</a></li><li><a href=#2626--98165-prospector-heads-generalized-feature-attribution-for-large-models--data-gautam-machiraju-et-al-2024>(26/26 | 98/165) Prospector Heads: Generalized Feature Attribution for Large Models & Data (Gautam Machiraju et al., 2024)</a></li></ul></li><li><a href=#cscv-23>cs.CV (23)</a><ul><li><a href=#123--99165-visual-in-context-learning-for-large-vision-language-models-yucheng-zhou-et-al-2024>(1/23 | 99/165) Visual In-Context Learning for Large Vision-Language Models (Yucheng Zhou et al., 2024)</a></li><li><a href=#223--100165-data-distribution-distilled-generative-model-for-generalized-zero-shot-recognition-yijie-wang-et-al-2024>(2/23 | 100/165) Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition (Yijie Wang et al., 2024)</a></li><li><a href=#323--101165-boosting-semi-supervised-2d-human-pose-estimation-by-revisiting-data-augmentation-and-consistency-training-huayi-zhou-et-al-2024>(3/23 | 101/165) Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data Augmentation and Consistency Training (Huayi Zhou et al., 2024)</a></li><li><a href=#423--102165-efficient-multimodal-learning-from-data-centric-perspective-muyang-he-et-al-2024>(4/23 | 102/165) Efficient Multimodal Learning from Data-centric Perspective (Muyang He et al., 2024)</a></li><li><a href=#523--103165-momentor-advancing-video-large-language-model-with-fine-grained-temporal-reasoning-long-qian-et-al-2024>(5/23 | 103/165) Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning (Long Qian et al., 2024)</a></li><li><a href=#623--104165-logical-closed-loop-uncovering-object-hallucinations-in-large-vision-language-models-junfei-wu-et-al-2024>(6/23 | 104/165) Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models (Junfei Wu et al., 2024)</a></li><li><a href=#723--105165-polypnextlstm-a-lightweight-and-fast-polyp-video-segmentation-network-using-convnext-and-convlstm-debayan-bhattacharya-et-al-2024>(7/23 | 105/165) PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM (Debayan Bhattacharya et al., 2024)</a></li><li><a href=#823--106165-a-novel-fourier-neural-operator-framework-for-classification-of-multi-sized-images-application-to-3d-digital-porous-media-ali-kashefi-et-al-2024>(8/23 | 106/165) A novel Fourier neural operator framework for classification of multi-sized images: Application to 3D digital porous media (Ali Kashefi et al., 2024)</a></li><li><a href=#923--107165-mal-motion-aware-loss-with-temporal-and-distillation-hints-for-self-supervised-depth-estimation-yup-jiang-dong-et-al-2024>(9/23 | 107/165) MAL: Motion-Aware Loss with Temporal and Distillation Hints for Self-Supervised Depth Estimation (Yup-Jiang Dong et al., 2024)</a></li><li><a href=#1023--108165-genad-generative-end-to-end-autonomous-driving-wenzhao-zheng-et-al-2024>(10/23 | 108/165) GenAD: Generative End-to-End Autonomous Driving (Wenzhao Zheng et al., 2024)</a></li><li><a href=#1123--109165-3d-point-cloud-compression-with-recurrent-neural-network-and-image-compression-methods-till-beemelmanns-et-al-2024>(11/23 | 109/165) 3D Point Cloud Compression with Recurrent Neural Network and Image Compression Methods (Till Beemelmanns et al., 2024)</a></li><li><a href=#1223--110165-interactive-garment-recommendation-with-user-in-the-loop-federico-becattini-et-al-2024>(12/23 | 110/165) Interactive Garment Recommendation with User in the Loop (Federico Becattini et al., 2024)</a></li><li><a href=#1323--111165-sdit-spiking-diffusion-model-with-transformer-shu-yang-et-al-2024>(13/23 | 111/165) SDiT: Spiking Diffusion Model with Transformer (Shu Yang et al., 2024)</a></li><li><a href=#1423--112165-thyroid-ultrasound-diagnosis-improvement-via-multi-view-self-supervised-learning-and-two-stage-pre-training-jian-wang-et-al-2024>(14/23 | 112/165) Thyroid ultrasound diagnosis improvement via multi-view self-supervised learning and two-stage pre-training (Jian Wang et al., 2024)</a></li><li><a href=#1523--113165-multicorrupt-a-multi-modal-robustness-dataset-and-benchmark-of-lidar-camera-fusion-for-3d-object-detection-till-beemelmanns-et-al-2024>(15/23 | 113/165) MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection (Till Beemelmanns et al., 2024)</a></li><li><a href=#1623--114165-challenging-the-black-box-a-comprehensive-evaluation-of-attribution-maps-of-cnn-applications-in-agriculture-and-forestry-lars-nieradzik-et-al-2024>(16/23 | 114/165) Challenging the Black Box: A Comprehensive Evaluation of Attribution Maps of CNN Applications in Agriculture and Forestry (Lars Nieradzik et al., 2024)</a></li><li><a href=#1723--115165-cross-attention-fusion-of-visual-and-geometric-features-for-large-vocabulary-arabic-lipreading-samar-daou-et-al-2024>(17/23 | 115/165) Cross-Attention Fusion of Visual and Geometric Features for Large Vocabulary Arabic Lipreading (Samar Daou et al., 2024)</a></li><li><a href=#1823--116165-visual-concept-driven-image-generation-with-text-to-image-diffusion-model-tanzila-rahman-et-al-2024>(18/23 | 116/165) Visual Concept-driven Image Generation with Text-to-Image Diffusion Model (Tanzila Rahman et al., 2024)</a></li><li><a href=#1923--117165-endoood-uncertainty-aware-out-of-distribution-detection-in-capsule-endoscopy-diagnosis-qiaozhi-tan-et-al-2024>(19/23 | 117/165) EndoOOD: Uncertainty-aware Out-of-distribution Detection in Capsule Endoscopy Diagnosis (Qiaozhi Tan et al., 2024)</a></li><li><a href=#2023--118165-key-patch-proposer-key-patches-contain-rich-information-jing-xu-et-al-2024>(20/23 | 118/165) Key Patch Proposer: Key Patches Contain Rich Information (Jing Xu et al., 2024)</a></li><li><a href=#2123--119165-a-multispectral-automated-transfer-technique-matt-for-machine-driven-image-labeling-utilizing-the-segment-anything-model-sam-james-e-gallagher-et-al-2024>(21/23 | 119/165) A Multispectral Automated Transfer Technique (MATT) for machine-driven image labeling utilizing the Segment Anything Model (SAM) (James E. Gallagher et al., 2024)</a></li><li><a href=#2223--120165-neuromorphic-face-analysis-a-survey-federico-becattini-et-al-2024>(22/23 | 120/165) Neuromorphic Face Analysis: a Survey (Federico Becattini et al., 2024)</a></li><li><a href=#2323--121165-cpn-complementary-proposal-network-for-unconstrained-text-detection-longhuang-wu-et-al-2024>(23/23 | 121/165) CPN: Complementary Proposal Network for Unconstrained Text Detection (Longhuang Wu et al., 2024)</a></li></ul></li><li><a href=#cscr-2>cs.CR (2)</a><ul><li><a href=#12--122165-urlberta-contrastive-and-adversarial-pre-trained-model-for-url-classification-yujie-li-et-al-2024>(1/2 | 122/165) URLBERT:A Contrastive and Adversarial Pre-trained Model for URL Classification (Yujie Li et al., 2024)</a></li><li><a href=#22--123165-poisoning-federated-recommender-systems-with-fake-users-ming-yin-et-al-2024>(2/2 | 123/165) Poisoning Federated Recommender Systems with Fake Users (Ming Yin et al., 2024)</a></li></ul></li><li><a href=#csir-4>cs.IR (4)</a><ul><li><a href=#14--124165-neighborhood-enhanced-supervised-contrastive-learning-for-collaborative-filtering-peijie-sun-et-al-2024>(1/4 | 124/165) Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering (Peijie Sun et al., 2024)</a></li><li><a href=#24--125165-large-language-models-as-data-augmenters-for-cold-start-item-recommendation-jianling-wang-et-al-2024>(2/4 | 125/165) Large Language Models as Data Augmenters for Cold-Start Item Recommendation (Jianling Wang et al., 2024)</a></li><li><a href=#34--126165-pattern-wise-transparent-sequential-recommendation-kun-ma-et-al-2024>(3/4 | 126/165) Pattern-wise Transparent Sequential Recommendation (Kun Ma et al., 2024)</a></li><li><a href=#44--127165-search-engines-post-chatgpt-how-generative-artificial-intelligence-could-make-search-less-reliable-shahan-ali-memon-et-al-2024>(4/4 | 127/165) Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable (Shahan Ali Memon et al., 2024)</a></li></ul></li><li><a href=#q-biobm-2>q-bio.BM (2)</a><ul><li><a href=#12--128165-ddiprompt-drug-drug-interaction-event-prediction-based-on-graph-prompt-learning-yingying-wang-et-al-2024>(1/2 | 128/165) DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning (Yingying Wang et al., 2024)</a></li><li><a href=#22--129165-re-dock-towards-flexible-and-realistic-molecular-docking-with-diffusion-bridge-yufei-huang-et-al-2024>(2/2 | 129/165) Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge (Yufei Huang et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--130165-can-chatgpt-support-developers-an-empirical-evaluation-of-large-language-models-for-code-generation-kailun-jin-et-al-2024>(1/3 | 130/165) Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation (Kailun Jin et al., 2024)</a></li><li><a href=#23--131165-tool-augmented-llms-as-a-universal-interface-for-ides-yaroslav-zharov-et-al-2024>(2/3 | 131/165) Tool-Augmented LLMs as a Universal Interface for IDEs (Yaroslav Zharov et al., 2024)</a></li><li><a href=#33--132165-using-rule-engine-in-self-healing-systems-and-mape-model-zahra-yazdanparast-2024>(3/3 | 132/165) Using rule engine in self-healing systems and MAPE model (Zahra Yazdanparast, 2024)</a></li></ul></li><li><a href=#hep-ph-1>hep-ph (1)</a><ul><li><a href=#11--133165-pascl-supervised-contrastive-learning-with-perturbative-augmentation-for-particle-decay-reconstruction-junjian-lu-et-al-2024>(1/1 | 133/165) PASCL: Supervised Contrastive Learning with Perturbative Augmentation for Particle Decay Reconstruction (Junjian Lu et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--134165-solving-data-centric-tasks-using-large-language-models-shraddha-barke-et-al-2024>(1/1 | 134/165) Solving Data-centric Tasks using Large Language Models (Shraddha Barke et al., 2024)</a></li></ul></li><li><a href=#cshc-1>cs.HC (1)</a><ul><li><a href=#11--135165-shaping-human-ai-collaboration-varied-scaffolding-levels-in-co-writing-with-language-models-paramveer-s-dhillon-et-al-2024>(1/1 | 135/165) Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models (Paramveer S. Dhillon et al., 2024)</a></li></ul></li><li><a href=#csgt-2>cs.GT (2)</a><ul><li><a href=#12--136165-a-three-party-repeated-coalition-formation-game-for-pls-in-wireless-communications-with-irss-haipeng-zhou-et-al-2024>(1/2 | 136/165) A Three-Party Repeated Coalition Formation Game for PLS in Wireless Communications with IRSs (Haipeng Zhou et al., 2024)</a></li><li><a href=#22--137165-the-assignment-game-new-mechanisms-for-equitable-core-imputations-vijay-v-vazirani-2024>(2/2 | 137/165) The Assignment Game: New Mechanisms for Equitable Core Imputations (Vijay V. Vazirani, 2024)</a></li></ul></li><li><a href=#eesssy-8>eess.SY (8)</a><ul><li><a href=#18--138165-a-fisher-information-based-receding-horizon-control-method-for-signal-strength-model-estimation-yancheng-zhu-et-al-2024>(1/8 | 138/165) A Fisher Information based Receding Horizon Control Method for Signal Strength Model Estimation (Yancheng Zhu et al., 2024)</a></li><li><a href=#28--139165-balanced-truncation-of-linear-systems-with-quadratic-outputs-in-limited-time-and-frequency-intervals-qiu-yan-song-et-al-2024>(2/8 | 139/165) Balanced Truncation of Linear Systems with Quadratic Outputs in Limited Time and Frequency Intervals (Qiu-Yan Song et al., 2024)</a></li><li><a href=#38--140165-iterative-linear-quadratic-regulator-with-variational-equation-based-discretization-katsuya-shigematsu-et-al-2024>(3/8 | 140/165) Iterative Linear Quadratic Regulator With Variational Equation-Based Discretization (Katsuya Shigematsu et al., 2024)</a></li><li><a href=#48--141165-signed-perturbed-sums-estimation-of-arx-systems-exact-coverage-and-strong-consistency-extended-version-algo-carè-et-al-2024>(4/8 | 141/165) Signed-Perturbed Sums Estimation of ARX Systems: Exact Coverage and Strong Consistency (Extended Version) (Algo Carè et al., 2024)</a></li><li><a href=#58--142165-exponential-cluster-synchronization-in-fast-switching-network-topologies-a-pinning-control-approach-with-necessary-and-sufficient-conditions-ku-du-et-al-2024>(5/8 | 142/165) Exponential Cluster Synchronization in Fast Switching Network Topologies: A Pinning Control Approach with Necessary and Sufficient Conditions (Ku Du et al., 2024)</a></li><li><a href=#68--143165-specifying-and-analyzing-networked-and-layered-control-systems-operating-on-multiple-clocks-inigo-incer-et-al-2024>(6/8 | 143/165) Specifying and Analyzing Networked and Layered Control Systems Operating on Multiple Clocks (Inigo Incer et al., 2024)</a></li><li><a href=#78--144165-federated-reinforcement-learning-for-uplink-centric-broadband-communication-optimization-over-unlicensed-spectrum-hui-zhou-et-al-2024>(7/8 | 144/165) Federated Reinforcement Learning for Uplink Centric Broadband Communication Optimization over Unlicensed Spectrum (Hui Zhou et al., 2024)</a></li><li><a href=#88--145165-a-transition-system-abstraction-framework-for-neural-network-dynamical-system-models-yejiang-yang-et-al-2024>(8/8 | 145/165) A Transition System Abstraction Framework for Neural Network Dynamical System Models (Yejiang Yang et al., 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#12--146165-stochastic-nonlinear-dynamical-modelling-of-sram-bitcells-in-retention-mode-léopold-van-brandt-et-al-2024>(1/2 | 146/165) Stochastic Nonlinear Dynamical Modelling of SRAM Bitcells in Retention Mode (Léopold Van Brandt et al., 2024)</a></li><li><a href=#22--147165-variability-aware-noise-induced-dynamic-instability-of-ultra-low-voltage-sram-bitcells-léopold-van-brandt-et-al-2024>(2/2 | 147/165) Variability-Aware Noise-Induced Dynamic Instability of Ultra-Low-Voltage SRAM Bitcells (Léopold Van Brandt et al., 2024)</a></li></ul></li><li><a href=#cset-1>cs.ET (1)</a><ul><li><a href=#11--148165-a-fast-algorithm-to-simulate-nonlinear-resistive-networks-benjamin-scellier-2024>(1/1 | 148/165) A Fast Algorithm to Simulate Nonlinear Resistive Networks (Benjamin Scellier, 2024)</a></li></ul></li><li><a href=#csai-3>cs.AI (3)</a><ul><li><a href=#13--149165-combinatorial-client-master-multiagent-deep-reinforcement-learning-for-task-offloading-in-mobile-edge-computing-tesfay-zemuy-gebrekidan-et-al-2024>(1/3 | 149/165) Combinatorial Client-Master Multiagent Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing (Tesfay Zemuy Gebrekidan et al., 2024)</a></li><li><a href=#23--150165-fgeo-hypergnet-geometry-problem-solving-integrating-formal-symbolic-system-and-hypergraph-neural-network-xiaokai-zhang-et-al-2024>(2/3 | 150/165) FGeo-HyperGNet: Geometry Problem Solving Integrating Formal Symbolic System and Hypergraph Neural Network (Xiaokai Zhang et al., 2024)</a></li><li><a href=#33--151165-dynamic-planning-in-hierarchical-active-inference-matteo-priorelli-et-al-2024>(3/3 | 151/165) Dynamic planning in hierarchical active inference (Matteo Priorelli et al., 2024)</a></li></ul></li><li><a href=#econem-1>econ.EM (1)</a><ul><li><a href=#11--152165-doubly-robust-inference-in-causal-latent-factor-models-alberto-abadie-et-al-2024>(1/1 | 152/165) Doubly Robust Inference in Causal Latent Factor Models (Alberto Abadie et al., 2024)</a></li></ul></li><li><a href=#q-biogn-1>q-bio.GN (1)</a><ul><li><a href=#11--153165-scinterpreter-training-large-language-models-to-interpret-scrna-seq-data-for-cell-type-annotation-cong-li-et-al-2024>(1/1 | 153/165) scInterpreter: Training Large Language Models to Interpret scRNA-seq Data for Cell Type Annotation (Cong Li et al., 2024)</a></li></ul></li><li><a href=#cscc-2>cs.CC (2)</a><ul><li><a href=#12--154165-a-simple-proof-that-ricochet-robots-is-pspace-complete-jose-balanza-martinez-et-al-2024>(1/2 | 154/165) A Simple Proof that Ricochet Robots is PSPACE-Complete (Jose Balanza-Martinez et al., 2024)</a></li><li><a href=#22--155165-computational-complexity-of-the-weisfeiler-leman-dimension-moritz-lichter-et-al-2024>(2/2 | 155/165) Computational complexity of the Weisfeiler-Leman dimension (Moritz Lichter et al., 2024)</a></li></ul></li><li><a href=#statml-1>stat.ML (1)</a><ul><li><a href=#11--156165-empirical-density-estimation-based-on-spline-quasi-interpolation-with-applications-to-copulas-clustering-modeling-cristiano-tamborrino-et-al-2024>(1/1 | 156/165) Empirical Density Estimation based on Spline Quasi-Interpolation with applications to Copulas clustering modeling (Cristiano Tamborrino et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--157165-adaptive-decision-making-for-autonomous-vehicles-a-learning-enhanced-game-theoretic-approach-in-interactive-environments-heye-huang-et-al-2024>(1/2 | 157/165) Adaptive Decision-Making for Autonomous Vehicles: A Learning-Enhanced Game-Theoretic Approach in Interactive Environments (Heye Huang et al., 2024)</a></li><li><a href=#22--158165-on-the-limits-of-information-spread-by-memory-less-agents-niccolò-darchivio-et-al-2024>(2/2 | 158/165) On the Limits of Information Spread by Memory-less Agents (Niccolò D&rsquo;Archivio et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--159165-to-store-or-not-to-store-a-graph-theoretical-approach-for-dataset-versioning-anxin-guo-et-al-2024>(1/2 | 159/165) To Store or Not to Store: a graph theoretical approach for Dataset Versioning (Anxin Guo et al., 2024)</a></li><li><a href=#22--160165-odd-cycle-transversal-on-p_5-free-graphs-in-polynomial-time-akanksha-agrawal-et-al-2024>(2/2 | 160/165) Odd Cycle Transversal on $P_5$-free Graphs in Polynomial Time (Akanksha Agrawal et al., 2024)</a></li></ul></li><li><a href=#mathna-1>math.NA (1)</a><ul><li><a href=#11--161165-mixed-material-point-method-formulation-stabilization-and-validation-for-a-unified-analysis-of-free-surface-and-seepage-flow-bodhinanda-chandra-et-al-2024>(1/1 | 161/165) Mixed material point method formulation, stabilization, and validation for a unified analysis of free-surface and seepage flow (Bodhinanda Chandra et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--162165-model-free-μ-synthesis-a-nonsmooth-optimization-perspective-darioush-keivan-et-al-2024>(1/1 | 162/165) Model-Free $μ$-Synthesis: A Nonsmooth Optimization Perspective (Darioush Keivan et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--163165-spectral-independence-beyond-uniqueness-with-the-topological-method----an-extended-view-charilaos-efthymiou-2024>(1/1 | 163/165) Spectral Independence Beyond Uniqueness with. the topological method &ndash; An extended view (Charilaos Efthymiou, 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--164165-towards-distributed-and-intelligent-integrated-sensing-and-communications-for-6g-networks-emilio-calvanese-strinati-et-al-2024>(1/1 | 164/165) Towards Distributed and Intelligent Integrated Sensing and Communications for 6G Networks (Emilio Calvanese Strinati et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--165165-addressing-internally-disconnected-communities-in-leiden-and-louvain-community-detection-algorithms-subhajit-sahu-2024>(1/1 | 165/165) Addressing Internally-Disconnected Communities in Leiden and Louvain Community Detection Algorithms (Subhajit Sahu, 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>