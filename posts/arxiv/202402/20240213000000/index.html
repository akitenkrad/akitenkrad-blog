<!doctype html><html><head><title>arXiv @ 2024.02.13</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.13"><meta property="og:description" content="Primary Categories cond-mat.soft (1) cs.AI (12) cs.CC (1) cs.CE (1) cs.CL (9) cs.CR (1) cs.CV (12) cs.DB (1) cs.HC (2) cs.IR (1) cs.LG (24) cs.NE (2) cs.NI (1) cs.RO (3) cs.SE (3) eess.IV (4) eess.SY (1) math.NA (1) q-bio.GN (1) stat.ML (4) Keywords keyword cs.AI cs.CV cs.LG Active Learning 1 Anomaly Detection 1 Autoencoder 1 BERT 1 Bandit Algorithm 1 Benchmarking 4 4 8 ChatGPT 1 Clustering 2 Contrastive Learning 1 Convolution 1 1 Convolutional Neural Network 1 2 2 Distributional Reinforcement Learning 3 Emotion Recognition 2 Face Recognition 1 Fairness 1 Federated Learning 1 Few-shot 1 Fine-tuning 3 1 1 Foundation Model 1 GPT 1 Gaussian Process 1 Graph 4 5 Graph Contrastive Learning 2 Graph Convolutional Network 2 Graph Neural Network 1 4 GraphMAE 2 Human Intervention 1 Information Retrieval 1 Instruction Following 1 Karush-Kuhn-Tucker 1 Knowledge Distillation 2 4 Knowledge Graph 2 LSTM 4 Large Language Model 11 4 6 Logistic Regression 1 Markov Decision Process 2 Markov Game 1 Message-Passing 1 Multi-modal 2 1 1 Node Classification 1 1 Out-of-distribution 1 Outlier Detection 1 Prompt 3 Quantization 2 Question Answering 2 2 Reasoning 1 1 Recommendation 1 Reinforcement Learning 2 9 Reinforcement Learning from Human Feedback 4 Self-supervised Learning 4 2 2 Simulation 2 Simulator 2 Stochastic Gradient Descent 2 Supervised Learning 2 Text Classification 1 Text Generation 1 Transfer Learning 1 Transformer 4 2 3 Vision Transformer 2 2 Vision-and-Language 2 Visual Question Answering 3 Zero-shot 1 1 cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240213000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-13T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.13"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240213000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Tuesday, Feb 13, 2024</p></div><div class=title><h1>arXiv @ 2024.02.13</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#cond-matsoft-1>cond-mat.soft (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#csai-12>cs.AI (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#cscl-9>cs.CL (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#cscr-1>cs.CR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#cscv-12>cs.CV (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#cshc-2>cs.HC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#csir-1>cs.IR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#cslg-24>cs.LG (24)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#csro-3>cs.RO (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#eessiv-4>eess.IV (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#eesssy-1>eess.SY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#mathna-1>math.NA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#q-biogn-1>q-bio.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/#statml-4>stat.ML (4)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>1</td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td></tr><tr><td>BERT</td><td></td><td></td><td>1</td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td></tr><tr><td>Benchmarking</td><td>4</td><td>4</td><td>8</td></tr><tr><td>ChatGPT</td><td>1</td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>2</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>1</td></tr><tr><td>Convolution</td><td></td><td>1</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td>2</td><td>2</td></tr><tr><td>Distributional Reinforcement Learning</td><td></td><td></td><td>3</td></tr><tr><td>Emotion Recognition</td><td>2</td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td>1</td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td></tr><tr><td>Few-shot</td><td></td><td></td><td>1</td></tr><tr><td>Fine-tuning</td><td>3</td><td>1</td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td></td></tr><tr><td>GPT</td><td></td><td>1</td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>1</td></tr><tr><td>Graph</td><td>4</td><td></td><td>5</td></tr><tr><td>Graph Contrastive Learning</td><td></td><td></td><td>2</td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>2</td></tr><tr><td>Graph Neural Network</td><td>1</td><td></td><td>4</td></tr><tr><td>GraphMAE</td><td></td><td></td><td>2</td></tr><tr><td>Human Intervention</td><td>1</td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td></td><td></td></tr><tr><td>Karush-Kuhn-Tucker</td><td></td><td></td><td>1</td></tr><tr><td>Knowledge Distillation</td><td></td><td>2</td><td>4</td></tr><tr><td>Knowledge Graph</td><td></td><td></td><td>2</td></tr><tr><td>LSTM</td><td></td><td></td><td>4</td></tr><tr><td>Large Language Model</td><td>11</td><td>4</td><td>6</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td>1</td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>2</td></tr><tr><td>Markov Game</td><td></td><td></td><td>1</td></tr><tr><td>Message-Passing</td><td></td><td></td><td>1</td></tr><tr><td>Multi-modal</td><td>2</td><td>1</td><td>1</td></tr><tr><td>Node Classification</td><td>1</td><td></td><td>1</td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td></tr><tr><td>Outlier Detection</td><td></td><td></td><td>1</td></tr><tr><td>Prompt</td><td></td><td></td><td>3</td></tr><tr><td>Quantization</td><td></td><td>2</td><td></td></tr><tr><td>Question Answering</td><td>2</td><td>2</td><td></td></tr><tr><td>Reasoning</td><td>1</td><td>1</td><td></td></tr><tr><td>Recommendation</td><td></td><td></td><td>1</td></tr><tr><td>Reinforcement Learning</td><td>2</td><td></td><td>9</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td></td><td>4</td></tr><tr><td>Self-supervised Learning</td><td>4</td><td>2</td><td>2</td></tr><tr><td>Simulation</td><td></td><td></td><td>2</td></tr><tr><td>Simulator</td><td></td><td></td><td>2</td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>2</td></tr><tr><td>Supervised Learning</td><td></td><td>2</td><td></td></tr><tr><td>Text Classification</td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td></td></tr><tr><td>Transformer</td><td>4</td><td>2</td><td>3</td></tr><tr><td>Vision Transformer</td><td>2</td><td></td><td>2</td></tr><tr><td>Vision-and-Language</td><td></td><td>2</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td>3</td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cslg-24>cs.LG (24)</h2><h3 id=124--185-explainable-global-wildfire-prediction-models-using-graph-neural-networks-dayou-chen-et-al-2024>(1/24 | 1/85) Explainable Global Wildfire Prediction Models using Graph Neural Networks (Dayou Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dayou Chen, Sibo Cheng, Jinwei Hu, Matthew Kasoar, Rossella Arcucci. (2024)<br><strong>Explainable Global Wildfire Prediction Models using Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Explainable Global Wildfire Prediction Models using Graph Neural Networks" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 129<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Simulation, Simulator, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07152v1.pdf filename=2402.07152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wildfire prediction has become increasingly crucial due to the escalating impacts of climate change. Traditional <b>CNN-based</b> wildfire prediction models struggle with handling missing oceanic data and addressing the <b>long-range</b> <b>dependencies</b> <b>across</b> <b>distant</b> regions in meteorological data. In this paper, we introduce an innovative <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)-based</b> model for global wildfire prediction. We propose a hybrid model that combines the spatial prowess of <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCNs)</b> with the temporal depth of <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> networks. Our approach uniquely transforms global climate and wildfire data into a <b>graph</b> <b>representation,</b> <b>addressing</b> challenges such as null oceanic data locations and <b>long-range</b> <b>dependencies</b> <b>inherent</b> <b>in</b> traditional models. <b>Benchmarking</b> against established architectures using an unseen ensemble of JULES-INFERNO <b>simulations,</b> our model demonstrates superior predictive accuracy. Furthermore, we emphasise the model&rsquo;s explainability, unveiling potential wildfire correlation clusters through community detection and elucidating feature importance via Integrated Gradient analysis. Our findings not only advance the methodological domain of wildfire prediction but also underscore the importance of model transparency, offering valuable insights for stakeholders in wildfire management.</p></p class="citation"></blockquote><h3 id=224--285-rethinking-graph-masked-autoencoders-through-alignment-and-uniformity-liang-wang-et-al-2024>(2/24 | 2/85) Rethinking Graph Masked Autoencoders through Alignment and Uniformity (Liang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Wang, Xiang Tao, Qiang Liu, Shu Wu, Liang Wang. (2024)<br><strong>Rethinking Graph Masked Autoencoders through Alignment and Uniformity</strong><br><button class=copy-to-clipboard title="Rethinking Graph Masked Autoencoders through Alignment and Uniformity" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 86<br>Keywords: GraphMAE, GraphMAE, Graph, Graph Contrastive Learning, Graph Contrastive Learning, Autoencoder, Benchmarking, Contrastive Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07225v1.pdf filename=2402.07225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> on <b>graphs</b> <b>can</b> <b>be</b> bifurcated into <b>contrastive</b> <b>and</b> generative methods. <b>Contrastive</b> <b>methods,</b> also known as <b>graph</b> <b>contrastive</b> <b>learning</b> <b>(GCL),</b> have dominated <b>graph</b> <b>self-supervised</b> <b>learning</b> in the past few years, but the recent advent of <b>graph</b> <b>masked</b> <b>autoencoder</b> <b>(GraphMAE)</b> rekindles the momentum behind generative methods. Despite the empirical success of <b>GraphMAE,</b> there is still a dearth of theoretical understanding regarding its efficacy. Moreover, while both generative and <b>contrastive</b> <b>methods</b> have been shown to be effective, their connections and differences have yet to be thoroughly investigated. Therefore, we theoretically build a bridge between <b>GraphMAE</b> and <b>GCL,</b> and prove that the node-level reconstruction objective in <b>GraphMAE</b> implicitly performs context-level <b>GCL.</b> Based on our theoretical analysis, we further identify the limitations of the <b>GraphMAE</b> from the perspectives of alignment and uniformity, which have been considered as two key properties of high-quality representations in <b>GCL.</b> We point out that <b>GraphMAE&rsquo;s</b> alignment performance is restricted by the masking strategy, and the uniformity is not strictly guaranteed. To remedy the aforementioned limitations, we propose an Alignment-Uniformity enhanced <b>Graph</b> <b>Masked</b> <b>AutoEncoder,</b> named AUG-MAE. Specifically, we propose an easy-to-hard adversarial masking strategy to provide hard-to-align samples, which improves the alignment performance. Meanwhile, we introduce an explicit uniformity regularizer to ensure the uniformity of the learned representations. Experimental results on <b>benchmark</b> datasets demonstrate the superiority of our model over existing state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=324--385-a-theoretical-analysis-of-nash-learning-from-human-feedback-under-general-kl-regularized-preference-chenlu-ye-et-al-2024>(3/24 | 3/85) A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference (Chenlu Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenlu Ye, Wei Xiong, Yuheng Zhang, Nan Jiang, Tong Zhang. (2024)<br><strong>A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference</strong><br><button class=copy-to-clipboard title="A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07314v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07314v1.pdf filename=2402.07314v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> learns from the preference signal provided by a probabilistic preference model, which takes a <b>prompt</b> and two responses as input, and produces a score indicating the preference of one response against another. So far, the most popular <b>RLHF</b> paradigm is reward-based, which starts with an initial step of reward modeling, and the constructed reward is then used to provide a reward signal for the subsequent reward optimization stage. However, the existence of a reward function is a strong assumption and the reward-based <b>RLHF</b> is limited in expressivity and cannot capture the real-world complicated human preference. In this work, we provide theoretical insights for a recently proposed learning paradigm, Nash learning from human feedback (NLHF), which considered a general preference model and formulated the alignment process as a game between two competitive <b>LLMs.</b> The learning objective is to find a policy that consistently generates responses preferred over any competing policy while staying close to the initial model. The objective is defined as the Nash equilibrium (NE) of the KL-regularized preference model. We aim to make the first attempt to study the theoretical learnability of the KL-regularized NLHF by considering both offline and online settings. For the offline learning from a pre-collected dataset, we propose algorithms that are efficient under suitable coverage conditions of the dataset. For batch online learning from iterative interactions with a preference oracle, our proposed algorithm enjoys a finite sample guarantee under the structural condition of the underlying preference model. Our results connect the new NLHF paradigm with traditional RL theory, and validate the potential of reward-model-free learning under general preference.</p></p class="citation"></blockquote><h3 id=424--485-more-benefits-of-being-distributional-second-order-bounds-for-reinforcement-learning-kaiwen-wang-et-al-2024>(4/24 | 4/85) More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning (Kaiwen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiwen Wang, Owen Oertell, Alekh Agarwal, Nathan Kallus, Wen Sun. (2024)<br><strong>More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning</strong><br><button class=copy-to-clipboard title="More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Bandit Algorithm, Distributional Reinforcement Learning, Distributional Reinforcement Learning, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07198v1.pdf filename=2402.07198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we prove that <b>Distributional</b> <b>Reinforcement</b> <b>Learning</b> <b>(DistRL),</b> which learns the return distribution, can obtain second-order bounds in both online and offline RL in general settings with function approximation. Second-order bounds are instance-dependent bounds that scale with the variance of return, which we prove are tighter than the previously known small-loss bounds of <b>distributional</b> <b>RL.</b> <b>To</b> the best of our knowledge, our results are the first second-order bounds for low-rank <b>MDPs</b> and for offline RL. When specializing to contextual <b>bandits</b> (one-step RL problem), we show that a <b>distributional</b> <b>learning</b> <b>based</b> optimism algorithm achieves a second-order worst-case regret bound, and a second-order gap dependent bound, simultaneously. We also empirically demonstrate the benefit of <b>DistRL</b> in contextual <b>bandits</b> on real-world datasets. We highlight that our analysis with <b>DistRL</b> is relatively simple, follows the general framework of optimism in the face of uncertainty and does not require weighted regression. Our results suggest that <b>DistRL</b> is a promising framework for obtaining second-order bounds in general RL settings, thus further reinforcing the benefits of <b>DistRL.</b></p></p class="citation"></blockquote><h3 id=524--585-using-large-language-models-to-automate-and-expedite-reinforcement-learning-with-reward-machine-shayan-meshkat-alsadat-et-al-2024>(5/24 | 5/85) Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine (Shayan Meshkat Alsadat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shayan Meshkat Alsadat, Jean-Raphael Gaglione, Daniel Neider, Ufuk Topcu, Zhe Xu. (2024)<br><strong>Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine</strong><br><button class=copy-to-clipboard title="Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Few-shot, Reinforcement Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07069v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07069v1.pdf filename=2402.07069v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present LARL-RM <b>(Large</b> <b>language</b> <b>model-generated</b> Automaton for <b>Reinforcement</b> <b>Learning</b> with Reward Machine) algorithm in order to encode high-level knowledge into <b>reinforcement</b> <b>learning</b> using automaton to expedite the <b>reinforcement</b> <b>learning.</b> Our method uses <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> to obtain high-level domain-specific knowledge using <b>prompt</b> engineering instead of providing the <b>reinforcement</b> <b>learning</b> algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and <b>few-shot</b> methods for <b>prompt</b> engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop <b>reinforcement</b> <b>learning</b> without the need for an expert to guide and supervise the learning since LARL-RM can use the <b>LLM</b> directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-RM speeds up the convergence by 30% by implementing our method in two case studies.</p></p class="citation"></blockquote><h3 id=624--685-summing-up-the-facts-additive-mechanisms-behind-factual-recall-in-llms-bilal-chughtai-et-al-2024>(6/24 | 6/85) Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs (Bilal Chughtai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bilal Chughtai, Alan Cooney, Neel Nanda. (2024)<br><strong>Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs</strong><br><button class=copy-to-clipboard title="Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Transformer, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07321v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07321v1.pdf filename=2402.07321v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How do <b>transformer-based</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> store and retrieve knowledge? We focus on the most basic form of this task &ndash; factual recall, where the model is tasked with explicitly surfacing stored facts in <b>prompts</b> of form <code>Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions. Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer. In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens. We use this technique to unpack what we call </code>mixed heads&rsquo; &ndash; which are themselves a pair of two separate additive updates from different source tokens.</p></p class="citation"></blockquote><h3 id=724--785-odin-disentangled-reward-mitigates-hacking-in-rlhf-lichang-chen-et-al-2024>(7/24 | 7/85) ODIN: Disentangled Reward Mitigates Hacking in RLHF (Lichang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, Bryan Catanzaro. (2024)<br><strong>ODIN: Disentangled Reward Mitigates Hacking in RLHF</strong><br><button class=copy-to-clipboard title="ODIN: Disentangled Reward Mitigates Hacking in RLHF" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07319v1.pdf filename=2402.07319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we study the issue of reward hacking on the response length, a challenge emerging in <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> on <b>LLMs.</b> A well-formatted, verbose but less helpful response from the <b>LLMs</b> can often deceive <b>LLMs</b> or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between <b>LLM</b> evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the other trained to decorrelate with length and therefore focus more on the actual content. We then discard the length head in RL to prevent reward hacking on length. Experiments demonstrate that our approach almost eliminates the reward correlation with length, and improves the obtained policy by a significant margin.</p></p class="citation"></blockquote><h3 id=824--885-training-heterogeneous-client-models-using-knowledge-distillation-in-serverless-federated-learning-mohak-chadha-et-al-2024>(8/24 | 8/85) Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning (Mohak Chadha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohak Chadha, Pulkit Khera, Jianfeng Gu, Osama Abboud, Michael Gerndt. (2024)<br><strong>Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning</strong><br><button class=copy-to-clipboard title="Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Federated Learning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07295v1.pdf filename=2402.07295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, existing serverless FL systems implicitly assume a uniform global model architecture across all participating clients during training. This assumption fails to address fundamental challenges in practical FL due to the resource and statistical data heterogeneity among FL clients. To address these challenges and enable heterogeneous client models in serverless FL, we utilize <b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> in this paper. Towards this, we propose novel optimized serverless workflows for two popular conventional <b>federated</b> <b>KD</b> techniques, i.e., FedMD and FedDF. We implement these workflows by introducing several extensions to an open-source serverless FL system called FedLess. Moreover, we comprehensively evaluate the two strategies on multiple datasets across varying levels of client data heterogeneity using heterogeneous client models with respect to accuracy, fine-grained training times, and costs. Results from our experiments demonstrate that serverless FedDF is more robust to extreme non-IID data distributions, is faster, and leads to lower costs than serverless FedMD. In addition, compared to the original implementation, our optimizations for particular steps in FedMD and FedDF lead to an average speedup of 3.5x and 1.76x across all datasets.</p></p class="citation"></blockquote><h3 id=924--985-physics-informed-neural-networks-with-hard-linear-equality-constraints-hao-chen-et-al-2024>(9/24 | 9/85) Physics-Informed Neural Networks with Hard Linear Equality Constraints (Hao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Chen, Gonzalo E. Constante Flores, Can Li. (2024)<br><strong>Physics-Informed Neural Networks with Hard Linear Equality Constraints</strong><br><button class=copy-to-clipboard title="Physics-Informed Neural Networks with Hard Linear Equality Constraints" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 40<br>Keywords: Karush-Kuhn-Tucker, Knowledge Distillation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07251v1.pdf filename=2402.07251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Surrogate modeling is used to replace computationally expensive <b>simulations.</b> Neural networks have been widely applied as surrogate models that enable efficient evaluations over complex physical systems. Despite this, neural networks are data-driven models and devoid of any physics. The incorporation of physics into neural networks can improve generalization and data efficiency. The physics-informed neural network (PINN) is an approach to leverage known physical constraints present in the data, but it cannot strictly satisfy them in the predictions. This work proposes a novel physics-informed neural network, <b>KKT-hPINN,</b> which rigorously guarantees hard linear equality constraints through projection layers derived from <b>KKT</b> conditions. Numerical experiments on Aspen models of a continuous stirred-tank reactor (CSTR) unit, an extractive <b>distillation</b> subsystem, and a chemical plant demonstrate that this model can further enhance the prediction accuracy.</p></p class="citation"></blockquote><h3 id=1024--1085-power-transformer-fault-prediction-based-on-knowledge-graphs-chao-wang-et-al-2024>(10/24 | 10/85) Power Transformer Fault Prediction Based on Knowledge Graphs (Chao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Wang, Zhuo Chen, Ziyan Zhang, Chiyi Li, Kai Song. (2024)<br><strong>Power Transformer Fault Prediction Based on Knowledge Graphs</strong><br><button class=copy-to-clipboard title="Power Transformer Fault Prediction Based on Knowledge Graphs" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Logistic Regression, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07283v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07283v1.pdf filename=2402.07283v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the challenge of learning with limited fault data for power <b>transformers.</b> Traditional operation and maintenance tools lack effective predictive capabilities for potential faults. The scarcity of extensive fault data makes it difficult to apply machine learning techniques effectively. To solve this problem, we propose a novel approach that leverages the <b>knowledge</b> <b>graph</b> <b>(KG)</b> technology in combination with gradient boosting decision trees (GBDT). This method is designed to efficiently learn from a small set of high-dimensional data, integrating various factors influencing <b>transformer</b> faults and historical operational data. Our approach enables accurate safe state assessments and fault analyses of power <b>transformers</b> despite the limited fault characteristic data. Experimental results demonstrate that this method outperforms other learning approaches in prediction accuracy, such as artificial neural networks (ANN) and <b>logistic</b> <b>regression</b> (LR). Furthermore, it offers significant improvements in progressiveness, practicality, and potential for widespread application.</p></p class="citation"></blockquote><h3 id=1124--1185-rethinking-the-capacity-of-graph-neural-networks-for-branching-strategy-ziang-chen-et-al-2024>(11/24 | 11/85) Rethinking the Capacity of Graph Neural Networks for Branching Strategy (Ziang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziang Chen, Jialin Liu, Xiaohan Chen, Xinshang Wang, Wotao Yin. (2024)<br><strong>Rethinking the Capacity of Graph Neural Networks for Branching Strategy</strong><br><button class=copy-to-clipboard title="Rethinking the Capacity of Graph Neural Networks for Branching Strategy" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07099v1.pdf filename=2402.07099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers. This paper investigates the capacity of <b>GNNs</b> to represent strong branching (SB) scores that provide an efficient strategy in the branch-and-bound algorithm. Although <b>message-passing</b> <b>GNN</b> (MP-GNN), as the simplest <b>GNN</b> structure, is frequently employed in the existing literature to learn SB scores, we prove a fundamental limitation in its expressive power &ndash; there exist two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters. In addition, we establish a universal approximation theorem for another <b>GNN</b> structure called the second-order folklore <b>GNN</b> (2-FGNN). We show that for any data distribution over MILPs, there always exists a 2-FGNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability. A small-scale numerical experiment is conducted to directly validate our theoretical findings.</p></p class="citation"></blockquote><h3 id=1224--1285-geoformer-a-vision-and-sequence-transformer-based-approach-for-greenhouse-gas-monitoring-madhav-khirwar-et-al-2024>(12/24 | 12/85) GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring (Madhav Khirwar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Madhav Khirwar, Ankur Narang. (2024)<br><strong>GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring</strong><br><button class=copy-to-clipboard title="GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07164v1.pdf filename=2402.07164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Air pollution represents a pivotal environmental challenge globally, playing a major role in climate change via greenhouse gas emissions and negatively affecting the health of billions. However predicting the spatial and temporal patterns of pollutants remains challenging. The scarcity of ground-based monitoring facilities and the dependency of air pollution modeling on comprehensive datasets, often inaccessible for numerous areas, complicate this issue. In this work, we introduce GeoFormer, a compact model that combines a <b>vision</b> <b>transformer</b> module with a highly efficient time-series <b>transformer</b> module to predict surface-level nitrogen dioxide (NO2) concentrations from Sentinel-5P satellite imagery. We train the proposed model to predict surface-level NO2 measurements using a dataset we constructed with Sentinel-5P images of ground-level monitoring stations, and their corresponding NO2 concentration readings. The proposed model attains high accuracy (MAE 5.65), demonstrating the efficacy of combining <b>vision</b> <b>and</b> time-series <b>transformer</b> architectures to harness satellite-derived data for enhanced GHG emission insights, proving instrumental in advancing climate change monitoring and emission regulation efforts globally.</p></p class="citation"></blockquote><h3 id=1324--1385-towards-robust-car-following-dynamics-modeling-via-blackbox-models-methodology-analysis-and-recommendations-muhammad-bilal-shahid-et-al-2024>(13/24 | 13/85) Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations (Muhammad Bilal Shahid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Bilal Shahid, Cody Fleming. (2024)<br><strong>Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations</strong><br><button class=copy-to-clipboard title="Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs-SY, cs.LG, eess-SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Recommendation, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07139v1.pdf filename=2402.07139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The selection of the target variable is important while learning parameters of the classical car following models like GIPPS, IDM, etc. There is a vast body of literature on which target variable is optimal for classical car following models, but there is no study that empirically evaluates the selection of optimal target variables for black-box models, such as <b>LSTM,</b> etc. The black-box models, like <b>LSTM</b> and <b>Gaussian</b> <b>Process</b> (GP) are increasingly being used to model car following behavior without wise selection of target variables. The current work tests different target variables, like acceleration, velocity, and headway, for three black-box models, i.e., GP, <b>LSTM,</b> and Kernel Ridge Regression. These models have different objective functions and work in different vector spaces, e.g., GP works in function space, and <b>LSTM</b> works in parameter space. The experiments show that the optimal target variable <b>recommendations</b> for black-box models differ from classical car following models depending on the objective function and the vector space. It is worth mentioning that models and datasets used during evaluation are diverse in nature: the datasets contained both automated and human-driven vehicle trajectories; the black-box models belong to both parametric and non-parametric classes of models. This diversity is important during the analysis of variance, wherein we try to find the interaction between datasets, models, and target variables. It is shown that the models and target variables interact and recommended target variables don&rsquo;t depend on the dataset under consideration.</p></p class="citation"></blockquote><h3 id=1424--1485-echoes-of-socratic-doubt-embracing-uncertainty-in-calibrated-evidential-reinforcement-learning-alex-christopher-stutts-et-al-2024>(14/24 | 14/85) Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning (Alex Christopher Stutts et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Christopher Stutts, Danilo Erricolo, Theja Tulabandhula, Amit Ranjan Trivedi. (2024)<br><strong>Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Distributional Reinforcement Learning, Out-of-distribution, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07107v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07107v2.pdf filename=2402.07107v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel statistical approach to incorporating uncertainty awareness in model-free <b>distributional</b> <b>reinforcement</b> <b>learning</b> involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of <b>out-of-distribution</b> (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously evaluate uncertainty improves exploration strategies and can serve as a blueprint for other algorithms requiring uncertainty awareness.</p></p class="citation"></blockquote><h3 id=1524--1585-can-tree-based-approaches-surpass-deep-learning-in-anomaly-detection-a-benchmarking-study-santonu-sarkar-et-al-2024>(15/24 | 15/85) Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study (Santonu Sarkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Santonu Sarkar, Shanay Mehta, Nicole Fernandes, Jyotirmoy Sarkar, Snehanshu Saha. (2024)<br><strong>Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study</strong><br><button class=copy-to-clipboard title="Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Anomaly Detection, Benchmarking, Benchmarking, Outlier Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07281v1.pdf filename=2402.07281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detection of anomalous situations for complex mission-critical systems holds paramount importance when their service continuity needs to be ensured. A major challenge in detecting anomalies from the operational data arises due to the imbalanced class distribution problem since the anomalies are supposed to be rare events. This paper evaluates a diverse array of machine learning-based <b>anomaly</b> <b>detection</b> algorithms through a comprehensive <b>benchmark</b> study. The paper contributes significantly by conducting an unbiased comparison of various <b>anomaly</b> <b>detection</b> algorithms, spanning classical machine learning including various tree-based approaches to deep learning and <b>outlier</b> <b>detection</b> methods. The inclusion of 104 publicly available and a few proprietary industrial systems datasets enhances the diversity of the study, allowing for a more realistic evaluation of algorithm performance and emphasizing the importance of adaptability to real-world scenarios. The paper dispels the deep learning myth, demonstrating that though powerful, deep learning is not a universal solution in this case. We observed that recently proposed tree-based evolutionary algorithms outperform in many scenarios. We noticed that tree-based approaches catch a singleton <b>anomaly</b> <b>in</b> a dataset where deep learning methods fail. On the other hand, classical SVM performs the best on datasets with more than 10% anomalies, implying that such scenarios can be best modeled as a classification problem rather than <b>anomaly</b> <b>detection.</b> To our knowledge, such a study on a large number of state-of-the-art algorithms using diverse data sets, with the objective of guiding researchers and practitioners in making informed algorithmic choices, has not been attempted earlier.</p></p class="citation"></blockquote><h3 id=1624--1685-hyperbert-mixing-hypergraph-aware-layers-with-language-models-for-node-classification-on-text-attributed-hypergraphs-adrián-bazaga-et-al-2024>(16/24 | 16/85) HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs (Adrián Bazaga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrián Bazaga, Pietro Liò, Gos Micklem. (2024)<br><strong>HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs</strong><br><button class=copy-to-clipboard title="HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, stat-ML<br>Keyword Score: 23<br>Keywords: Node Classification, Benchmarking, BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07309v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07309v2.pdf filename=2402.07309v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of <b>node</b> <b>classification</b> on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the <b>nodes</b> <b>attributes,</b> which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained <b>BERT</b> model with specialized hypergraph-aware layers for the task of <b>node</b> <b>classification.</b> Such layers introduce higher-order structural inductive bias into the language model, thus improving the model&rsquo;s capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we propose a new architecture, HyperBERT, a mixed text-hypergraph model which simultaneously models hypergraph relational structure while maintaining the high-quality text encoding capabilities of a pre-trained <b>BERT.</b> Notably, HyperBERT presents results that achieve a new state-of-the-art on five challenging text-attributed hypergraph <b>node</b> <b>classification</b> <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1724--1785-towards-generalized-inverse-reinforcement-learning-chaosheng-dong-et-al-2024>(17/24 | 17/85) Towards Generalized Inverse Reinforcement Learning (Chaosheng Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaosheng Dong, Yijia Wang. (2024)<br><strong>Towards Generalized Inverse Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Towards Generalized Inverse Reinforcement Learning" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07246v1.pdf filename=2402.07246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies generalized inverse <b>reinforcement</b> <b>learning</b> (GIRL) in Markov decision processes <b>(MDPs),</b> that is, the problem of learning the basic components of an MDP given observed behavior (policy) that might not be optimal. These components include not only the reward function and transition probability matrices, but also the action space and state space that are not exactly known but are known to belong to given uncertainty sets. We address two key challenges in GIRL: first, the need to quantify the discrepancy between the observed policy and the underlying optimal policy; second, the difficulty of mathematically characterizing the underlying optimal policy when the basic components of an MDP are unobservable or partially observable. Then, we propose the mathematical formulation for GIRL and develop a fast heuristic algorithm. Numerical results on both finite and infinite state problems show the merit of our formulation and algorithm.</p></p class="citation"></blockquote><h3 id=1824--1885-the-implicit-bias-of-gradient-noise-a-symmetry-perspective-liu-ziyin-et-al-2024>(18/24 | 18/85) The Implicit Bias of Gradient Noise: A Symmetry Perspective (Liu Ziyin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liu Ziyin, Mingze Wang, Lei Wu. (2024)<br><strong>The Implicit Bias of Gradient Noise: A Symmetry Perspective</strong><br><button class=copy-to-clipboard title="The Implicit Bias of Gradient Noise: A Symmetry Perspective" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07193v1.pdf filename=2402.07193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We characterize the learning dynamics of <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> when continuous symmetry exists in the loss function, where the divergence between <b>SGD</b> and gradient descent is dramatic. We show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes. For one class of symmetry, <b>SGD</b> naturally converges to solutions that have a balanced and aligned gradient noise. For the other class of symmetry, <b>SGD</b> will almost always diverge. Then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function. Our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function. We demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, matrix factorization, and the use of warmup.</p></p class="citation"></blockquote><h3 id=1924--1985-refined-sample-complexity-for-markov-games-with-independent-linear-function-approximation-yan-dai-et-al-2024>(19/24 | 19/85) Refined Sample Complexity for Markov Games with Independent Linear Function Approximation (Yan Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Dai, Qiwen Cui, Simon S. Du. (2024)<br><strong>Refined Sample Complexity for Markov Games with Independent Linear Function Approximation</strong><br><button class=copy-to-clipboard title="Refined Sample Complexity for Markov Games with Independent Linear Function Approximation" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-GT, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Markov Game, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07082v1.pdf filename=2402.07082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Markov</b> <b>Games</b> (MG) is an important model for Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL). It was long believed that the &ldquo;curse of multi-agents&rdquo; (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023. While these works did resolve the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\max}$ &ndash; which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time (Dai et al., 2023). This paper first refines the <code>AVLPR</code> framework by Wang et al. (2023), with an insight of <em>data-dependent</em> (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. When specialized to MGs with independent linear function approximations, we propose novel <em>action-dependent bonuses</em> to cover occasionally extreme estimation errors. With the help of state-of-the-art techniques from the single-agent RL literature, we give the first algorithm that tackles the curse of multi-agents, attains the optimal $O(T^{-1/2})$ convergence rate, and avoids $\text{poly}(A_{\max})$ dependency simultaneously.</p></p class="citation"></blockquote><h3 id=2024--2085-future-prediction-can-be-a-strong-evidence-of-good-history-representation-in-partially-observable-environments-jeongyeol-kwon-et-al-2024>(20/24 | 20/85) Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments (Jeongyeol Kwon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeongyeol Kwon, Liu Yang, Robert Nowak, Josiah Hanna. (2024)<br><strong>Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments</strong><br><button class=copy-to-clipboard title="Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07102v1.pdf filename=2402.07102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning a good history representation is one of the core challenges of <b>reinforcement</b> <b>learning</b> (RL) in partially observable environments. Recent works have shown the advantages of various auxiliary tasks for facilitating representation learning. However, the effectiveness of such auxiliary tasks has not been fully convincing, especially in partially observable environments that require long-term memorization and inference. In this empirical study, we investigate the effectiveness of future prediction for learning the representations of histories, possibly of extensive length, in partially observable environments. We first introduce an approach that decouples the task of learning history representations from policy optimization via future prediction. Then, our main contributions are two-fold: (a) we demonstrate that the performance of <b>reinforcement</b> <b>learning</b> is strongly correlated with the prediction accuracy of future observations in partially observable environments, and (b) our approach can significantly improve the overall end-to-end approach by preventing high-variance noisy signals from <b>reinforcement</b> <b>learning</b> objectives to influence the representation learning. We illustrate our claims on three types of <b>benchmarks</b> that necessitate the ability to process long histories for high returns.</p></p class="citation"></blockquote><h3 id=2124--2185-genstl-general-sparse-trajectory-learning-via-auto-regressive-generation-of-feature-domains-yan-lin-et-al-2024>(21/24 | 21/85) GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains (Yan Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Lin, Jilin Hu, Shengnan Guo, Bin Yang, Christian S. Jensen, Youfang Lin, Huaiyu Wan. (2024)<br><strong>GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains</strong><br><button class=copy-to-clipboard title="GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07232v1.pdf filename=2402.07232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectories are sequences of timestamped location samples. In sparse trajectories, the locations are sampled infrequently; and while such trajectories are prevalent in real-world settings, they are challenging to use to enable high-quality transportation-related applications. Current methodologies either assume densely sampled and accurately map-matched trajectories, or they rely on two-stage schemes, yielding sub-optimal applications. To extend the utility of sparse trajectories, we propose a novel sparse trajectory learning framework, GenSTL. The framework is pre-trained to form connections between sparse trajectories and dense counterparts using auto-regressive generation of feature domains. GenSTL can subsequently be applied directly in downstream tasks, or it can be <b>fine-tuned</b> first. This way, GenSTL eliminates the reliance on the availability of large-scale dense and map-matched trajectory data. The inclusion of a well-crafted feature domain encoding layer and a hierarchical masked trajectory encoder enhances GenSTL&rsquo;s learning capabilities and adaptability. Experiments on two real-world trajectory datasets offer insight into the framework&rsquo;s ability to contend with sparse trajectories with different sampling intervals and its versatility across different downstream tasks, thus offering evidence of its practicality in real-world applications.</p></p class="citation"></blockquote><h3 id=2224--2285-divide-and-conquer-provably-unveiling-the-pareto-front-with-multi-objective-reinforcement-learning-willem-röpke-et-al-2024>(22/24 | 22/85) Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning (Willem Röpke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Willem Röpke, Mathieu Reymond, Patrick Mannion, Diederik M. Roijers, Ann Nowé, Roxana Rădulescu. (2024)<br><strong>Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07182v1.pdf filename=2402.07182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A significant challenge in multi-objective <b>reinforcement</b> <b>learning</b> is obtaining a Pareto front of policies that attain optimal performance under different preferences. We introduce Iterated Pareto Referent Optimisation (IPRO), a principled algorithm that decomposes the task of finding the Pareto front into a sequence of single-objective problems for which various solution methods exist. This enables us to establish convergence guarantees while providing an upper bound on the distance to undiscovered Pareto optimal solutions at each step. Empirical evaluations demonstrate that IPRO matches or outperforms methods that require additional domain knowledge. By leveraging problem-specific single-objective solvers, our approach also holds promise for applications beyond multi-objective <b>reinforcement</b> <b>learning,</b> such as in pathfinding and optimisation.</p></p class="citation"></blockquote><h3 id=2324--2385-the-impact-of-domain-knowledge-and-multi-modality-on-intelligent-molecular-property-prediction-a-systematic-survey-taojie-kuang-et-al-2024>(23/24 | 23/85) The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey (Taojie Kuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taojie Kuang, Pengfei Liu, Zhixiang Ren. (2024)<br><strong>The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey</strong><br><button class=copy-to-clipboard title="The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07249v1.pdf filename=2402.07249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The precise prediction of molecular properties is essential for advancements in drug development, particularly in virtual screening and compound optimization. The recent introduction of numerous deep learning-based methods has shown remarkable potential in enhancing molecular property prediction (MPP), especially improving accuracy and insights into molecular structures. Yet, two critical questions arise: does the integration of domain knowledge augment the accuracy of molecular property prediction and does employing <b>multi-modal</b> data fusion yield more precise results than unique data source methods? To explore these matters, we comprehensively review and quantitatively analyze recent deep learning methods based on various <b>benchmarks.</b> We discover that integrating molecular information will improve both MPP regression and classification tasks by upto 3.98% and 1.72%, respectively. We also discover that the utilizing 3-dimensional information with 1-dimensional and 2-dimensional information simultaneously can substantially enhance MPP upto 4.2%. The two consolidated insights offer crucial guidance for future advancements in drug discovery.</p></p class="citation"></blockquote><h3 id=2424--2485-gsina-improving-subgraph-extraction-for-graph-invariant-learning-via-graph-sinkhorn-attention-fangyu-ding-et-al-2024>(24/24 | 24/85) GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention (Fangyu Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangyu Ding, Haiyang Wang, Zhixuan Chu, Tianming Li, Zhaoping Hu, Junchi Yan. (2024)<br><strong>GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention</strong><br><button class=copy-to-clipboard title="GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07191v1.pdf filename=2402.07191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> invariant learning (GIL) has been an effective approach to discovering the invariant relationships between <b>graph</b> data and its labels for different <b>graph</b> learning tasks under various distribution shifts. Many recent endeavors of GIL focus on extracting the invariant subgraph from the input <b>graph</b> for prediction as a regularization strategy to improve the generalization performance of <b>graph</b> learning. Despite their success, such methods also have various limitations in obtaining their invariant subgraphs. In this paper, we provide in-depth analyses of the drawbacks of existing works and propose corresponding principles of our invariant subgraph extraction: 1) the sparsity, to filter out the variant features, 2) the softness, for a broader solution space, and 3) the differentiability, for a soundly end-to-end optimization. To meet these principles in one shot, we leverage the Optimal Transport (OT) theory and propose a novel <b>graph</b> attention mechanism called <b>Graph</b> Sinkhorn Attention (GSINA). This novel approach serves as a powerful regularization method for GIL tasks. By GSINA, we are able to obtain meaningful, differentiable invariant subgraphs with controllable sparsity and softness. Moreover, GSINA is a general <b>graph</b> learning framework that could handle GIL tasks of multiple data grain levels. Extensive experiments on both synthetic and real-world datasets validate the superiority of our GSINA, which outperforms the state-of-the-art GIL methods by large margins on both <b>graph-level</b> tasks and node-level tasks. Our code is publicly available at \url{https://github.com/dingfangyu/GSINA}.</p></p class="citation"></blockquote><h2 id=cscl-9>cs.CL (9)</h2><h3 id=19--2585-how-do-large-language-models-navigate-conflicts-between-honesty-and-helpfulness-ryan-liu-et-al-2024>(1/9 | 25/85) How do Large Language Models Navigate Conflicts between Honesty and Helpfulness? (Ryan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Liu, Theodore R. Sumers, Ishita Dasgupta, Thomas L. Griffiths. (2024)<br><strong>How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?</strong><br><button class=copy-to-clipboard title="How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 110<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Zero-shot, GPT, GPT-4, GPT-4 turbo, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07282v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07282v2.pdf filename=2402.07282v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze <b>LLMs.</b> We test a range of <b>LLMs</b> and explore how optimization for human preferences or inference-time <b>reasoning</b> affects these trade-offs. We find that <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> improves both honesty and helpfulness, while <b>chain-of-thought</b> <b>prompting</b> skews <b>LLMs</b> towards helpfulness over honesty. Finally, <b>GPT-4</b> <b>Turbo</b> demonstrates human-like response patterns including sensitivity to the conversational framing and listener&rsquo;s decision context. Our findings reveal the conversational values internalized by <b>LLMs</b> and suggest that even these abstract values can, to a degree, be steered by <b>zero-shot</b> <b>prompting.</b></p></p class="citation"></blockquote><h3 id=29--2685-prompt-perturbation-in-retrieval-augmented-generation-based-large-language-models-zhibo-hu-et-al-2024>(2/9 | 26/85) Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models (Zhibo Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhibo Hu, Chen Wang, Yanfeng Shu, Helen, Paik, Liming Zhu. (2024)<br><strong>Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models</strong><br><button class=copy-to-clipboard title="Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; H-3-3, cs-CL, cs-IR, cs.CL<br>Keyword Score: 70<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Text Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07179v1.pdf filename=2402.07179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The robustness of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> becomes increasingly important as their use rapidly grows in a wide range of domains. <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> is considered as a means to improve the trustworthiness of <b>text</b> <b>generation</b> from <b>LLMs.</b> However, how the outputs from <b>RAG-based</b> <b>LLMs</b> are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the <b>prompt</b> leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on <b>RAG</b> by introducing a novel optimization technique called Gradient Guided <b>Prompt</b> Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of <b>RAG-based</b> <b>LLMs</b> to targeted wrong answers. It can also cope with instructions in the <b>prompts</b> requesting to ignore irrelevant context. We also exploit <b>LLMs&rsquo;</b> neuron activation difference between <b>prompts</b> with and without GGPP perturbations to give a method that improves the robustness of <b>RAG-based</b> <b>LLMs</b> through a highly effective detector trained on neuron activation triggered by GGPP generated <b>prompts.</b> Our evaluation on open-sourced <b>LLMs</b> demonstrates the effectiveness of our methods.</p></p class="citation"></blockquote><h3 id=39--2785-natural-language-reinforcement-learning-xidong-feng-et-al-2024>(3/9 | 27/85) Natural Language Reinforcement Learning (Xidong Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xidong Feng, Ziyu Wan, Mengyue Yang, Ziyan Wang, Girish A. Koushik, Yali Du, Ying Wen, Jun Wang. (2024)<br><strong>Natural Language Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Natural Language Reinforcement Learning" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Markov Decision Process, Reinforcement Learning, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07157v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07157v2.pdf filename=2402.07157v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language <b>Reinforcement</b> <b>Learning</b> (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>GPT-4.</b> Initial experiments over tabular <b>MDPs</b> demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.</p></p class="citation"></blockquote><h3 id=49--2885-transgpt-multi-modal-generative-pre-trained-transformer-for-transportation-peng-wang-et-al-2024>(4/9 | 28/85) TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation (Peng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Wang, Xiang Wei, Fangxu Hu, Wenjuan Han. (2024)<br><strong>TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation</strong><br><button class=copy-to-clipboard title="TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Recommendation, Transformer, Neural Machine Translation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07233v1.pdf filename=2402.07233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural language processing (NLP) is a key component of intelligent transportation systems (ITS), but it faces many challenges in the transportation domain, such as domain-specific knowledge and data, and <b>multi-modal</b> inputs and outputs. This paper presents TransGPT, a novel <b>(multi-modal)</b> <b>large</b> <b>language</b> <b>model</b> for the transportation domain, which consists of two independent variants: TransGPT-SM for single-modal data and TransGPT-MM for <b>multi-modal</b> data. TransGPT-SM is <b>finetuned</b> on a single-modal Transportation dataset (STD) that contains textual data from various sources in the transportation domain. TransGPT-MM is <b>finetuned</b> on a <b>multi-modal</b> Transportation dataset <b>(MTD)</b> that we manually collected from three areas of the transportation domain: driving tests, traffic signs, and landmarks. We evaluate TransGPT on several <b>benchmark</b> datasets for different tasks in the transportation domain, and show that it outperforms baseline models on most tasks. We also showcase the potential applications of TransGPT for traffic analysis and modeling, such as generating synthetic traffic scenarios, explaining traffic phenomena, answering traffic-related questions, providing traffic <b>recommendations,</b> and generating traffic reports. This work advances the state-of-the-art of NLP in the transportation domain and provides a useful tool for ITS researchers and practitioners.</p></p class="citation"></blockquote><h3 id=59--2985-generalizing-conversational-dense-retrieval-via-llm-cognition-data-augmentation-haonan-chen-et-al-2024>(5/9 | 29/85) Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation (Haonan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haonan Chen, Zhicheng Dou, Kelong Mao, Jiongnan Liu, Ziliang Zhao. (2024)<br><strong>Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation</strong><br><button class=copy-to-clipboard title="Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 50<br>Keywords: Contrastive Learning, Data Augmentation, Dense Retrieval, Zero-shot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07092v1.pdf filename=2402.07092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational <b>dense</b> <b>retrieval</b> models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe <b>data</b> <b>sparsity</b> problem &ndash; that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational <b>dense</b> <b>retrieval</b> via <b>LLM-cognition</b> <b>data</b> <b>Augmentation</b> (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space. A <b>contrastive</b> <b>learning</b> objective is then employed to train a better conversational context encoder. Extensive experiments conducted on four public datasets, under both normal and <b>zero-shot</b> settings, demonstrate the effectiveness, generalizability, and applicability of ConvAug.</p></p class="citation"></blockquote><h3 id=69--3085-previously-on-the-stories-recap-snippet-identification-for-story-reading-jiangnan-li-et-al-2024>(6/9 | 30/85) Previously on the Stories: Recap Snippet Identification for Story Reading (Jiangnan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiangnan Li, Qiujing Wang, Liyan Xu, Wenjie Pang, Mo Yu, Zheng Lin, Weiping Wang, Jie Zhou. (2024)<br><strong>Previously on the Stories: Recap Snippet Identification for Story Reading</strong><br><button class=copy-to-clipboard title="Previously on the Stories: Recap Snippet Identification for Story Reading" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07271v1.pdf filename=2402.07271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Similar to the &ldquo;previously-on&rdquo; scenes in TV shows, recaps can help book reading by recalling the readers&rsquo; memory about the important elements in previous texts to better understand the ongoing plot. Despite its usefulness, this application has not been well studied in the NLP community. We propose the first <b>benchmark</b> on this useful task called Recap Snippet Identification with a hand-crafted evaluation dataset. Our experiments show that the proposed task is challenging to <b>PLMs,</b> <b>LLMs,</b> and proposed methods as the task requires a deep understanding of the plot correlation between snippets.</p></p class="citation"></blockquote><h3 id=79--3185-american-sign-language-video-to-text-translation-parsheeta-roy-et-al-2024>(7/9 | 31/85) American Sign Language Video to Text Translation (Parsheeta Roy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parsheeta Roy, Ji-Eun Han, Srishti Chouhan, Bhaavanaa Thumu. (2024)<br><strong>American Sign Language Video to Text Translation</strong><br><button class=copy-to-clipboard title="American Sign Language Video to Text Translation" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 20<br>Keywords: Label Smoothing, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07255v1.pdf filename=2402.07255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sign language to text is a crucial technology that can break down communication barriers for individuals with hearing difficulties. We replicate and try to improve on a recently published study. We evaluate models using <b>BLEU</b> and rBLEU metrics to ensure translation quality. During our ablation study, we found that the model&rsquo;s performance is significantly influenced by optimizers, activation functions, and <b>label</b> <b>smoothing.</b> Further research aims to refine visual feature capturing, enhance decoder utilization, and integrate pre-trained decoders for better translation outcomes. Our source code is available to facilitate replication of our results and encourage future research.</p></p class="citation"></blockquote><h3 id=89--3285-low-resource-counterspeech-generation-for-indic-languages-the-case-of-bengali-and-hindi-mithun-das-et-al-2024>(8/9 | 32/85) Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi (Mithun Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mithun Das, Saurabh Kumar Pandey, Shivansh Sethi, Punyajoy Saha, Animesh Mukherjee. (2024)<br><strong>Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi</strong><br><button class=copy-to-clipboard title="Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07262v1.pdf filename=2402.07262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rise of online abuse, the NLP community has begun investigating the use of neural architectures to generate counterspeech that can &ldquo;counter&rdquo; the vicious tone of such abusive speech and dilute/ameliorate their rippling effect over the social network. However, most of the efforts so far have been primarily focused on English. To bridge the gap for <b>low-resource</b> languages such as Bengali and Hindi, we create a <b>benchmark</b> dataset of 5,062 abusive speech/counterspeech pairs, of which 2,460 pairs are in Bengali and 2,602 pairs are in Hindi. We implement several baseline models considering various interlingual transfer mechanisms with different configurations to generate suitable counterspeech to set up an effective <b>benchmark.</b> We observe that the monolingual setup yields the best performance. Further, using synthetic transfer, language models can generate counterspeech to some extent; specifically, we notice that transferability is better when languages belong to the same language family.</p></p class="citation"></blockquote><h3 id=99--3385-using-large-language-models-for-student-code-guided-test-case-generation-in-computer-science-education-nischal-ashok-kumar-et-al-2024>(9/9 | 33/85) Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education (Nischal Ashok Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nischal Ashok Kumar, Andrew Lan. (2024)<br><strong>Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education</strong><br><button class=copy-to-clipboard title="Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SE, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07081v1.pdf filename=2402.07081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In computer science education, test cases are an integral part of programming assignments since they can be used as assessment items to test students&rsquo; programming knowledge and provide personalized feedback on student-written code. The goal of our work is to propose a fully automated approach for test case generation that can accurately measure student knowledge, which is important for two reasons. First, manually constructing test cases requires expert knowledge and is a labor-intensive process. Second, developing test cases for students, especially those who are novice programmers, is significantly different from those oriented toward professional-level software developers. Therefore, we need an automated process for test case generation to assess student knowledge and provide feedback. In this work, we propose a <b>large</b> <b>language</b> <b>model-based</b> approach to automatically generate test cases and show that they are good measures of student knowledge, using a publicly available dataset that contains student-written Java code. We also discuss future research directions centered on using test cases to help students.</p></p class="citation"></blockquote><h2 id=eessiv-4>eess.IV (4)</h2><h3 id=14--3485-semi-mamba-unet-pixel-level-contrastive-cross-supervised-visual-mamba-based-unet-for-semi-supervised-medical-image-segmentation-ziyang-wang-et-al-2024>(1/4 | 34/85) Semi-Mamba-UNet: Pixel-Level Contrastive Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image Segmentation (Ziyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyang Wang, Chao Ma. (2024)<br><strong>Semi-Mamba-UNet: Pixel-Level Contrastive Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Semi-Mamba-UNet: Pixel-Level Contrastive Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image Segmentation" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 100<br>Keywords: Vision Transformer, Contrastive Learning, Convolution, Convolutional Neural Network, Convolutional Neural Network, Self-supervised Learning, Semi-Supervised Learning, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07245v1.pdf filename=2402.07245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image segmentation is essential in diagnostics, treatment planning, and healthcare, with deep learning offering promising advancements. Notably, <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> excel in capturing local image features, whereas <b>Vision</b> <b>Transformer</b> (ViT) adeptly model long-range dependencies through multi-head <b>self-attention</b> mechanisms. Despite their strengths, both <b>CNN</b> and ViT face challenges in efficiently processing long-range dependencies within medical images, often requiring substantial computational resources. This issue, combined with the high cost and limited availability of expert annotations, poses significant obstacles to achieving precise segmentation. To address these challenges, this paper introduces the Semi-Mamba-UNet, which integrates a visual mamba-based UNet architecture with a conventional UNet into a <b>semi-supervised</b> <b>learning</b> (SSL) framework. This innovative SSL approach leverages dual networks to jointly generate pseudo labels and cross supervise each other, drawing inspiration from consistency regularization techniques. Furthermore, we introduce a <b>self-supervised</b> pixel-level <b>contrastive</b> <b>learning</b> strategy, employing a projector pair to further enhance feature learning capabilities. Our comprehensive evaluation on a publicly available MRI cardiac segmentation dataset, comparing against various SSL frameworks with different UNet-based segmentation networks, highlights the superior performance of Semi-Mamba-UNet. The source code has been made publicly accessible.</p></p class="citation"></blockquote><h3 id=24--3585-kvq-kaleidoscope-video-quality-assessment-for-short-form-videos-yiting-lu-et-al-2024>(2/4 | 35/85) KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos (Yiting Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiting Lu, Xin Li, Yajing Pei, Kun Yuan, Qizhi Xie, Yunpeng Qu, Ming Sun, Chao Zhou, Zhibo Chen. (2024)<br><strong>KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos</strong><br><button class=copy-to-clipboard title="KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07220v1.pdf filename=2402.07220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Short-form UGC video platforms, like Kwai and TikTok, have been an emerging and irreplaceable mainstream media form, thriving on user-friendly engagement, and kaleidoscope creation, etc. However, the advancing content-generation modes, e.g., special effects, and sophisticated processing workflows, e.g., de-artifacts, have introduced significant challenges to recent UGC video quality assessment: (i) the ambiguous contents hinder the identification of quality-determined regions. (ii) the diverse and complicated hybrid distortions are hard to distinguish. To tackle the above challenges and assist in the development of short-form videos, we establish the first large-scale Kaleidoscope short Video database for Quality assessment, termed KVQ, which comprises 600 user-uploaded short videos and 3600 processed videos through the diverse practical processing workflows, including pre-processing, transcoding, and enhancement. Among them, the absolute quality score of each video and partial ranking score among indistinguishable samples are provided by a team of professional researchers specializing in image processing. Based on this database, we propose the first short-form video quality evaluator, i.e., KSVQE, which enables the quality evaluator to identify the quality-determined semantics with the content understanding of large vision language models (i.e., CLIP) and distinguish the distortions with the distortion understanding module. Experimental results have shown the effectiveness of KSVQE on our KVQ database and popular <b>VQA</b> databases.</p></p class="citation"></blockquote><h3 id=34--3685-spatio-spectral-classification-of-hyperspectral-images-for-brain-cancer-detection-during-surgical-operations-h-fabelo-et-al-2024>(3/4 | 36/85) Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations (H. Fabelo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>H. Fabelo, S. Ortega, D. Ravi, B. R. Kiran, C. Sosa, D. Bulters, G. M. Callico, H. Bulstrode, A. Szolna, J. F. Pineiro, S. Kabwama, D. Madronal, R. Lazcano, A. J. OShanahan, S. Bisshopp, M. Hernandez, A. Baez-Quevedo, G. Z. Yang, B. Stanciulescu, R. Salvador, E. Juarez, R. Sarmiento. (2024)<br><strong>Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations</strong><br><button class=copy-to-clipboard title="Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07192v1.pdf filename=2402.07192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Surgery for brain cancer is a major problem in neurosurgery. The diffuse infiltration into the surrounding normal brain by these tumors makes their accurate identification by the naked eye difficult. Since surgery is the common treatment for brain cancer, an accurate radical resection of the tumor leads to improved survival rates for patients. However, the identification of the tumor boundaries during surgery is challenging. Hyperspectral imaging is a noncontact, non-ionizing and non-invasive technique suitable for medical diagnosis. This study presents the development of a novel classification method taking into account the spatial and spectral characteristics of the hyperspectral images to help neurosurgeons to accurately determine the tumor boundaries in surgical-time during the resection, avoiding excessive excision of normal tissue or unintentionally leaving residual tumor. The algorithm proposed in this study to approach an efficient solution consists of a hybrid framework that combines both <b>supervised</b> and <b>unsupervised</b> machine learning methods. To evaluate the proposed approach, five hyperspectral images of surface of the brain affected by glioblastoma tumor in vivo from five different patients have been used. The final classification maps obtained have been analyzed and validated by specialists. These preliminary results are promising, obtaining an accurate delineation of the tumor area.</p></p class="citation"></blockquote><h3 id=44--3785-supervised-reconstruction-for-silhouette-tomography-evan-bell-et-al-2024>(4/4 | 37/85) Supervised Reconstruction for Silhouette Tomography (Evan Bell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evan Bell, Michael T. McCann, Marc Klasky. (2024)<br><strong>Supervised Reconstruction for Silhouette Tomography</strong><br><button class=copy-to-clipboard title="Supervised Reconstruction for Silhouette Tomography" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07298v1.pdf filename=2402.07298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce silhouette tomography, a novel formulation of X-ray computed tomography that relies only on the geometry of the imaging system. We formulate silhouette tomography mathematically and provide a simple method for obtaining a particular solution to the problem, assuming that any solution exists. We then propose a <b>supervised</b> reconstruction approach that uses a deep neural network to solve the silhouette tomography problem. We present experimental results on a synthetic dataset that demonstrate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h2 id=csai-12>cs.AI (12)</h2><h3 id=112--3885-graphtranslator-aligning-graph-model-to-large-language-model-for-open-ended-tasks-mengmei-zhang-et-al-2024>(1/12 | 38/85) GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks (Mengmei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, Chuan Shi. (2024)<br><strong>GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks</strong><br><button class=copy-to-clipboard title="GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 73<br>Keywords: Node Classification, Graph, Zero-shot, ChatGPT, Instruction Following, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07197v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07197v2.pdf filename=2402.07197v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>ChatGPT,</b> exhibit powerful <b>zero-shot</b> and <b>instruction-following</b> <b>capabilities,</b> have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks. While the idea is less explored in the <b>graph</b> domain, despite the availability of numerous powerful <b>graph</b> models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying <b>LLMs</b> to <b>graphs</b> have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with <b>LLM</b> as a <b>node</b> <b>feature</b> enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and <b>LLM</b> by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of <b>LLMs</b> to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the <b>graph-text</b> alignment data along <b>node</b> <b>information,</b> neighbor information and model information. By treating the <b>node</b> <b>representation</b> as a type of language, the proposed GraphTranslator empowers an <b>LLM</b> to make predictions based on <b>node</b> <b>representation</b> and language <b>instructions,</b> <b>providing</b> a unified perspective for both pre-defined and open-ended tasks. Extensive results show that the proposed GraphTranslator effectively improves the results of <b>zero-shot</b> <b>node</b> <b>classification.</b> The <b>graph</b> <b>question</b> <b>answering</b> experiments reveal our GraphTranslator potential across a broad spectrum of open-ended applications through language instructions.</p></p class="citation"></blockquote><h3 id=212--3985-multi-modal-emotion-recognition-by-text-speech-and-video-using-pretrained-transformers-minoo-shayaninasab-et-al-2024>(2/12 | 39/85) Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers (Minoo Shayaninasab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minoo Shayaninasab, Bagher Babaali. (2024)<br><strong>Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers</strong><br><button class=copy-to-clipboard title="Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 66<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Self-supervised Learning, Self-supervised Learning, Transfer Learning, Transformer, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07327v1.pdf filename=2402.07327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the complex nature of human <b>emotions</b> <b>and</b> the diversity of <b>emotion</b> <b>representation</b> methods in humans, <b>emotion</b> <b>recognition</b> is a challenging field. In this research, three input modalities, namely text, audio (speech), and video, are employed to generate <b>multimodal</b> feature vectors. For generating features for each of these modalities, pre-trained <b>Transformer</b> models with <b>fine-tuning</b> are utilized. In each modality, a <b>Transformer</b> model is used with <b>transfer</b> <b>learning</b> to extract feature and <b>emotional</b> <b>structure.</b> These features are then fused together, and <b>emotion</b> <b>recognition</b> is performed using a classifier. To select an appropriate fusion method and classifier, various feature-level and decision-level fusion techniques have been experimented with, and ultimately, the best model, which combines feature-level fusion by concatenating feature vectors and classification using a Support Vector Machine on the IEMOCAP <b>multimodal</b> dataset, achieves an accuracy of 75.42%. Keywords: <b>Multimodal</b> <b>Emotion</b> <b>Recognition,</b> IEMOCAP, <b>Self-Supervised</b> <b>Learning,</b> <b>Transfer</b> <b>Learning,</b> <b>Transformer.</b></p></p class="citation"></blockquote><h3 id=312--4085-cpsdbench-a-large-language-model-evaluation-benchmark-and-baseline-for-chinese-public-security-domain-xin-tong-et-al-2024>(3/12 | 40/85) CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain (Xin Tong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Tong, Bo Jin, Zhi Lin, Binjun Wang, Ting Yu. (2024)<br><strong>CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain</strong><br><button class=copy-to-clipboard title="CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 63<br>Keywords: Benchmarking, Information Retrieval, Question Answering, Text Classification, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07234v1.pdf filename=2402.07234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream <b>LLMs</b> in public security tasks, this study aims to construct a specialized evaluation <b>benchmark</b> tailored to the Chinese public security domain&ndash;CPSDbench. CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of <b>LLMs</b> across four key dimensions: <b>text</b> <b>classification,</b> <b>information</b> <b>extraction,</b> <b>question</b> <b>answering,</b> and <b>text</b> <b>generation.</b> Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of <b>LLMs</b> in executing tasks related to public security. Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the future development of more accurate and customized <b>LLM</b> models targeted at applications in this field.</p></p class="citation"></blockquote><h3 id=412--4185-large-language-model-empowered-dose-volume-histogram-prediction-for-intensity-modulated-radiotherapy-zehao-dong-et-al-2024>(4/12 | 41/85) Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy (Zehao Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehao Dong, Yixin Chen, Hiram Gay, Yao Hao, Geoffrey D. Hugo, Pamela Samson, Tianyu Zhao. (2024)<br><strong>Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy</strong><br><button class=copy-to-clipboard title="Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 63<br>Keywords: Graph, Graph Neural Network, Convolutional Neural Network, Human Intervention, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07167v1.pdf filename=2402.07167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Treatment planning is currently a patient specific, time-consuming, and resource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction plays a critical role in automating this process. The geometric relationship between DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target volume (PTV) has been well established. This study explores the potential of deep learning models for predicting DVHs using images and subsequent <b>human</b> <b>intervention</b> facilitated by a <b>large-language</b> <b>model</b> <b>(LLM)</b> to enhance the planning quality. We propose a pipeline to convert unstructured images to a structured <b>graph</b> <b>consisting</b> <b>of</b> image-patch nodes and dose nodes. A novel Dose <b>Graph</b> <b>Neural</b> <b>Network</b> (DoseGNN) model is developed for predicting DVHs from the structured <b>graph.</b> <b>The</b> <b>proposed</b> DoseGNN is enhanced with the <b>LLM</b> to encode massive knowledge from prescriptions and interactive instructions from clinicians. In this study, we introduced an online <b>human-AI</b> <b>collaboration</b> (OHAC) system as a practical implementation of the concept proposed for the automation of intensity-modulated radiotherapy (IMRT) planning. In comparison to the widely-employed DL models used in radiotherapy, DoseGNN achieved mean square errors that were 80$%$, 76$%$ and 41.0$%$ of those predicted by Swin U-Net <b>Transformer,</b> 3D U-Net <b>CNN</b> and vanilla MLP, respectively. Moreover, the <b>LLM-empowered</b> DoseGNN model facilitates seamless adjustment to treatment plans through interaction with clinicians using natural language.</p></p class="citation"></blockquote><h3 id=512--4285-persian-speech-emotion-recognition-by-fine-tuning-transformers-minoo-shayaninasab-et-al-2024>(5/12 | 42/85) Persian Speech Emotion Recognition by Fine-Tuning Transformers (Minoo Shayaninasab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minoo Shayaninasab, Bagher Babaali. (2024)<br><strong>Persian Speech Emotion Recognition by Fine-Tuning Transformers</strong><br><button class=copy-to-clipboard title="Persian Speech Emotion Recognition by Fine-Tuning Transformers" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-SD, cs.AI, eess-AS<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Self-supervised Learning, Self-supervised Learning, Transformer, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07326v1.pdf filename=2402.07326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the significance of speech <b>emotion</b> <b>recognition,</b> numerous methods have been developed in recent years to create effective and efficient systems in this domain. One of these methods involves the use of pretrained <b>transformers,</b> <b>fine-tuned</b> to address this specific problem, resulting in high accuracy. Despite extensive discussions and global-scale efforts to enhance these systems, the application of this innovative and effective approach has received less attention in the context of Persian speech <b>emotion</b> <b>recognition.</b> In this article, we review the field of speech <b>emotion</b> <b>recognition</b> and its background, with an emphasis on the importance of employing <b>transformers</b> in this context. We present two models, one based on spectrograms and the other on the audio itself, <b>fine-tuned</b> using the shEMO dataset. These models significantly enhance the accuracy of previous systems, increasing it from approximately 65% to 80% on the mentioned dataset. Subsequently, to investigate the effect of multilinguality on the <b>fine-tuning</b> process, these same models are <b>fine-tuned</b> twice. First, they are <b>fine-tuned</b> using the English IEMOCAP dataset, and then they are <b>fine-tuned</b> with the Persian shEMO dataset. This results in an improved accuracy of 82% for the Persian <b>emotion</b> <b>recognition</b> system. Keywords: Persian Speech <b>Emotion</b> <b>Recognition,</b> shEMO, <b>Self-Supervised</b> <b>Learning</b></p></p class="citation"></blockquote><h3 id=612--4385-sequential-ordering-in-textual-descriptions-impact-on-spatial-perception-abilities-of-large-language-models-yuyao-ge-et-al-2024>(6/12 | 43/85) Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models (Yuyao Ge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuyao Ge, Shenghua Liu, Lingrui Mei, Lizhe Chen, Xueqi Cheng. (2024)<br><strong>Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models</strong><br><button class=copy-to-clipboard title="Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07140v1.pdf filename=2402.07140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>Large</b> <b>Language</b> <b>Models</b> have reached state-of-the-art performance across multiple domains. However, the progress in the field of <b>graph</b> <b>reasoning</b> remains limited. Our work delves into this gap by thoroughly investigating <b>graph</b> <b>reasoning</b> with <b>LLM.</b> In this work, we reveal the impact of text sequence on <b>LLM</b> spatial understanding, finding that <b>graph-descriptive</b> text sequences significantly affect <b>LLM</b> <b>reasoning</b> performance on <b>graphs.</b> By altering the <b>graph-descriptive</b> text sequences, we enhance the performance of <b>LLM</b> from 42.22% to 70%. Furthermore, we evaluate the relationship between <b>LLM</b> performance and <b>graph</b> size, discovering that the <b>reasoning</b> performance of <b>LLM</b> does not monotonically decrease with the increase in <b>graph</b> size. Conclusively, we introduce the Scaled <b>Graph</b> <b>Reasoning</b> <b>benchmark</b> for assessing <b>LLM</b> performance across varied <b>graph</b> sizes.</p></p class="citation"></blockquote><h3 id=712--4485-a-random-ensemble-of-encrypted-vision-transformers-for-adversarially-robust-defense-ryota-iijima-et-al-2024>(7/12 | 44/85) A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense (Ryota Iijima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryota Iijima, Sayaka Shiota, Hitoshi Kiya. (2024)<br><strong>A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense</strong><br><button class=copy-to-clipboard title="A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 33<br>Keywords: Vision Transformer, Benchmarking, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07183v1.pdf filename=2402.07183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks (DNNs) are well known to be vulnerable to adversarial examples (AEs). In previous studies, the use of models encrypted with a secret key was demonstrated to be robust against white-box attacks, but not against black-box ones. In this paper, we propose a novel method using the <b>vision</b> <b>transformer</b> (ViT) that is a random ensemble of encrypted models for enhancing robustness against both white-box and black-box attacks. In addition, a <b>benchmark</b> attack method, called AutoAttack, is applied to models to test adversarial robustness objectively. In experiments, the method was demonstrated to be robust against not only white-box attacks but also black-box ones in an image classification task on the CIFAR-10 and ImageNet datasets. The method was also compared with the state-of-the-art in a standardized <b>benchmark</b> for adversarial robustness, RobustBench, and it was verified to outperform conventional defenses in terms of clean accuracy and robust accuracy.</p></p class="citation"></blockquote><h3 id=812--4585-synergizing-spatial-optimization-with-large-language-models-for-open-domain-urban-itinerary-planning-yihong-tang-et-al-2024>(8/12 | 45/85) Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning (Yihong Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Kebing Hou, Dingyi Zhuang, Xiaotong Guo, Jinhua Zhao, Zhan Zhao, Wei Ma. (2024)<br><strong>Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning</strong><br><button class=copy-to-clipboard title="Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07204v1.pdf filename=2402.07204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users&rsquo; requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to provide services that customize urban itineraries based on users&rsquo; needs. Specifically, we develop an <b>LLM-based</b> pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage <b>LLM</b> in cooperation with an embedding-based module for retrieving candidate POIs from the user&rsquo;s POI database. Then, a spatial optimization module is used to order these POIs, followed by <b>LLM</b> crafting a personalized, spatially coherent itinerary. To the best of our knowledge, this study marks the first integration of <b>LLMs</b> to innovate itinerary planning solutions. Extensive experiments on offline datasets and online subjective evaluation have demonstrated the capacities of our system to deliver more responsive and spatially coherent itineraries than current <b>LLM-based</b> solutions. Our system has been deployed in production at the TuTu online travel service and has attracted thousands of users for their urban travel planning.</p></p class="citation"></blockquote><h3 id=912--4685-stitching-sub-trajectories-with-conditional-diffusion-model-for-goal-conditioned-offline-rl-sungyoon-kim-et-al-2024>(9/12 | 46/85) Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL (Sungyoon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungyoon Kim, Yunseon Choi, Daiki E. Matsunaga, Kee-Eung Kim. (2024)<br><strong>Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL</strong><br><button class=copy-to-clipboard title="Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07226v1.pdf filename=2402.07226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Offline Goal-Conditioned <b>Reinforcement</b> <b>Learning</b> (Offline GCRL) is an important problem in RL that focuses on acquiring diverse goal-oriented skills solely from pre-collected behavior datasets. In this setting, the reward feedback is typically absent except when the goal is achieved, which makes it difficult to learn policies especially from a finite dataset of suboptimal behaviors. In addition, realistic scenarios involve long-horizon planning, which necessitates the extraction of useful skills within sub-trajectories. Recently, the conditional diffusion model has been shown to be a promising approach to generate high-quality long-horizon plans for RL. However, their practicality for the goal-conditioned setting is still limited due to a number of technical assumptions made by the methods. In this paper, we propose SSD (Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method that leverages the conditional diffusion model to address these limitations. In summary, we use the diffusion model that generates future plans conditioned on the target goal and value, with the target value estimated from the goal-relabeled offline dataset. We report state-of-the-art performance in the standard <b>benchmark</b> set of GCRL tasks, and demonstrate the capability to successfully stitch the segments of suboptimal trajectories in the offline data to generate high-quality plans.</p></p class="citation"></blockquote><h3 id=1012--4785-the-reasons-that-agents-act-intention-and-instrumental-goals-francis-rhys-ward-et-al-2024>(10/12 | 47/85) The Reasons that Agents Act: Intention and Instrumental Goals (Francis Rhys Ward et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francis Rhys Ward, Matt MacDermott, Francesco Belardinelli, Francesca Toni, Tom Everitt. (2024)<br><strong>The Reasons that Agents Act: Intention and Instrumental Goals</strong><br><button class=copy-to-clipboard title="The Reasons that Agents Act: Intention and Instrumental Goals" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07221v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07221v2.pdf filename=2402.07221v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intention is an important and challenging concept in AI. It is important because it underlies many other concepts we care about, such as agency, manipulation, legal responsibility, and blame. However, ascribing intent to AI systems is contentious, and there is no universally accepted theory of intention applicable to AI agents. We operationalise the intention with which an agent acts, relating to the reasons it chooses its decision. We introduce a formal definition of intention in structural causal influence models, grounded in the philosophy literature on intent and applicable to real-world machine learning systems. Through a number of examples and results, we show that our definition captures the intuitive notion of intent and satisfies desiderata set-out by past work. In addition, we show how our definition relates to past concepts, including actual causality, and the notion of instrumental goals, which is a core idea in the literature on safe AI agents. Finally, we demonstrate how our definition can be used to infer the intentions of <b>reinforcement</b> <b>learning</b> agents and language models from their behaviour.</p></p class="citation"></blockquote><h3 id=1112--4885-social-evolution-of-published-text-and-the-emergence-of-artificial-intelligence-through-large-language-models-and-the-problem-of-toxicity-and-bias-arifa-khan-et-al-2024>(11/12 | 48/85) Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias (Arifa Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arifa Khan, P. Saravanan, S. K Venkatesan. (2024)<br><strong>Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias</strong><br><button class=copy-to-clipboard title="Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07166v1.pdf filename=2402.07166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We provide a birds eye view of the rapid developments in AI and Deep Learning that has led to the path-breaking emergence of AI in <b>Large</b> <b>Language</b> <b>Models.</b> The aim of this study is to place all these developments in a pragmatic broader historical social perspective without any exaggerations while at the same time without any pessimism that created the AI winter in the 1970s to 1990s. We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic. We note here that just as this emergence of AI seems to occur at a threshold point in the number of neural connections or weights, it has also been observed that human brain and especially the cortex region is nothing special or extraordinary but simply a case of scaled-up version of the primate brain and that even the human intelligence seems like an emergent phenomena of scale.</p></p class="citation"></blockquote><h3 id=1212--4985-link-aware-link-prediction-over-temporal-graph-by-pattern-recognition-bingqing-liu-et-al-2024>(12/12 | 49/85) Link-aware link prediction over temporal graph by pattern recognition (Bingqing Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingqing Liu, Xikun Huang. (2024)<br><strong>Link-aware link prediction over temporal graph by pattern recognition</strong><br><button class=copy-to-clipboard title="Link-aware link prediction over temporal graph by pattern recognition" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07199v1.pdf filename=2402.07199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A temporal <b>graph</b> can be considered as a stream of links, each of which represents an interaction between two nodes at a certain time. On temporal <b>graphs,</b> link prediction is a common task, which aims to answer whether the query link is true or not. To do this task, previous methods usually focus on the learning of representations of the two nodes in the query link. We point out that the learned representation by their models may encode too much information with side effects for link prediction because they have not utilized the information of the query link, i.e., they are link-unaware. Based on this observation, we propose a link-aware model: historical links and the query link are input together into the following model layers to distinguish whether this input implies a reasonable pattern that ends with the query link. During this process, we focus on the modeling of link evolution patterns rather than node representations. Experiments on six datasets show that our model achieves strong performances compared with state-of-the-art baselines, and the results of link prediction are interpretable. The code and datasets are available on the project website: <a href=https://github.com/lbq8942/TGACN>https://github.com/lbq8942/TGACN</a>.</p></p class="citation"></blockquote><h2 id=csro-3>cs.RO (3)</h2><h3 id=13--5085-does-chatgpt-and-whisper-make-humanoid-robots-more-relatable-xiaohui-chen-et-al-2024>(1/3 | 50/85) Does ChatGPT and Whisper Make Humanoid Robots More Relatable? (Xiaohui Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohui Chen, Katherine Luo, Trevor Gee, Mahla Nejati. (2024)<br><strong>Does ChatGPT and Whisper Make Humanoid Robots More Relatable?</strong><br><button class=copy-to-clipboard title="Does ChatGPT and Whisper Make Humanoid Robots More Relatable?" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: ChatGPT, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07095v1.pdf filename=2402.07095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humanoid robots are designed to be relatable to humans for applications such as customer support and helpdesk services. However, many such systems, including Softbank&rsquo;s Pepper, fall short because they fail to communicate effectively with humans. The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> shows the potential to solve the communication barrier for humanoid robotics. This paper outlines the comparison of different <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR)</b> APIs, the integration of Whisper <b>ASR</b> and <b>ChatGPT</b> with the Pepper robot and the evaluation of the system (Pepper-GPT) tested by 15 human users. The comparison result shows that, compared to the Google <b>ASR</b> and Google Cloud <b>ASR,</b> the Whisper <b>ASR</b> performed best as its average Word Error Rate (1.716%) and processing time (2.639 s) are both the lowest. The participants&rsquo; usability investigations show that 60% of the participants thought the performance of the Pepper-GPT was &ldquo;excellent&rdquo;, while the rest rated this system as &ldquo;good&rdquo; in the subsequent experiments. It is proved that while some problems still need to be overcome, such as the robot&rsquo;s multilingual ability and facial tracking capacity, users generally responded positively to the system, feeling like talking to an actual human.</p></p class="citation"></blockquote><h3 id=23--5185-learning-by-watching-a-review-of-video-based-learning-approaches-for-robot-manipulation-chrisantus-eze-et-al-2024>(2/3 | 51/85) Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation (Chrisantus Eze et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chrisantus Eze, Christopher Crick. (2024)<br><strong>Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation</strong><br><button class=copy-to-clipboard title="Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Self-supervised Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07127v1.pdf filename=2402.07127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale &ldquo;in-the-wild&rdquo; video datasets have driven progress in computer vision through <b>self-supervised</b> techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for robotic manipulation. The survey <b>summarizes</b> video-based learning approaches, analyses their benefits over standard datasets, survey metrics, and <b>benchmarks,</b> and discusses open challenges and future directions in this nascent domain at the intersection of computer vision, natural language processing, and robot learning.</p></p class="citation"></blockquote><h3 id=33--5285-clipper-robust-data-association-without-an-initial-guess-parker-c-lusk-et-al-2024>(3/3 | 52/85) CLIPPER: Robust Data Association without an Initial Guess (Parker C. Lusk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parker C. Lusk, Jonathan P. How. (2024)<br><strong>CLIPPER: Robust Data Association without an Initial Guess</strong><br><button class=copy-to-clipboard title="CLIPPER: Robust Data Association without an Initial Guess" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07284v1.pdf filename=2402.07284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying correspondences in noisy data is a critically important step in estimation processes. When an informative initial estimation guess is available, the data association challenge is less acute; however, the existence of a high-quality initial guess is rare in most contexts. We explore <b>graph-theoretic</b> formulations for data association, which do not require an initial estimation guess. Existing <b>graph-theoretic</b> approaches optimize over unweighted <b>graphs,</b> discarding important consistency information encoded in weighted edges, and frequently attempt to solve NP-hard problems exactly. In contrast, we formulate a new optimization problem that fully leverages weighted <b>graphs</b> and seeks the densest edge-weighted clique. We introduce two relaxations to this problem: a convex semidefinite relaxation which we find to be empirically tight, and a fast first-order algorithm called CLIPPER which frequently arrives at nearly-optimal solutions in milliseconds. When evaluated on point cloud registration problems, our algorithms remain robust up to at least 95% outliers while existing algorithms begin breaking down at 80% outliers. Code is available at <a href=https://mit-acl.github.io/clipper>https://mit-acl.github.io/clipper</a>.</p></p class="citation"></blockquote><h2 id=cond-matsoft-1>cond-mat.soft (1)</h2><h3 id=11--5385-x-lora-mixture-of-low-rank-adapter-experts-a-flexible-framework-for-large-language-models-with-applications-in-protein-mechanics-and-design-eric-l-buehler-et-al-2024>(1/1 | 53/85) X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design (Eric L. Buehler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric L. Buehler, Markus J. Buehler. (2024)<br><strong>X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design</strong><br><button class=copy-to-clipboard title="X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.soft<br>Categories: cond-mat-dis-nn, cond-mat-soft, cond-mat.soft, cs-AI, cs-CL, cs-LG, q-bio-QM<br>Keyword Score: 58<br>Keywords: Graph Attention Networks, Graph, Fine-tuning, Knowledge Graph, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07148v1.pdf filename=2402.07148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We report a mixture of expert strategy to create <b>fine-tuned</b> <b>large</b> <b>language</b> <b>models</b> using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a <b>gating</b> strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced <b>reasoning</b> capability, focused on biomaterial analysis, protein mechanics and design. The impact of this work include access to readily expandable, adaptable and changeable models with strong domain <b>knowledge</b> <b>and</b> the capability to integrate across areas of <b>knowledge.</b> <b>With</b> the X-LoRA model featuring experts in biology, mathematics, <b>reasoning,</b> bio-inspired materials, mechanics and materials, chemistry, and protein mechanics we conduct a series of physics-focused case studies. We examine <b>knowledge</b> <b>recall,</b> protein mechanics forward/inverse tasks, protein design, and adversarial agentic modeling including ontological <b>knowledge</b> <b>graphs.</b> The model is capable not only of making quantitative predictions of nanomechanical properties of proteins, but also reasons over the results and correctly predicts likely mechanisms that explain distinct molecular behaviors.</p></p class="citation"></blockquote><h2 id=cscv-12>cs.CV (12)</h2><h3 id=112--5485-open-ended-vqa-benchmarking-of-vision-language-models-by-exploiting-classification-datasets-and-their-semantic-hierarchy-simon-ging-et-al-2024>(1/12 | 54/85) Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy (Simon Ging et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Ging, María A. Bravo, Thomas Brox. (2024)<br><strong>Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy</strong><br><button class=copy-to-clipboard title="Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 56<br>Keywords: Benchmarking, Benchmarking, Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07270v1.pdf filename=2402.07270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evaluation of text-generative <b>vision-language</b> models is a challenging yet crucial endeavor. By addressing the limitations of existing <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> <b>benchmarks</b> and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models&rsquo; capabilities. We propose a novel <b>VQA</b> <b>benchmark</b> based on well-known <b>visual</b> <b>classification</b> <b>datasets</b> which allows a granular evaluation of text-generative <b>vision-language</b> models and their comparison with discriminative <b>vision-language</b> models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up <b>questions</b> <b>about</b> the ground-truth category. Finally, we compare traditional NLP and <b>LLM-based</b> metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our <b>benchmark</b> to a suite of <b>vision-language</b> models and show a detailed comparison of their abilities on object, action, and attribute classification. Our contributions aim to lay the foundation for more precise and meaningful assessments, facilitating targeted progress in the exciting field of <b>vision-language</b> modeling.</p></p class="citation"></blockquote><h3 id=212--5585-a-benchmark-for-multi-modal-foundation-models-on-low-level-vision-from-single-images-to-pairs-zicheng-zhang-et-al-2024>(2/12 | 55/85) A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs (Zicheng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zicheng Zhang, Haoning Wu, Erli Zhang, Guangtao Zhai, Weisi Lin. (2024)<br><strong>A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs</strong><br><button class=copy-to-clipboard title="A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Benchmarking, Foundation Model, Multi-modal, GPT, Question Answering, Visual Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07116v1.pdf filename=2402.07116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid development of Multi-modality <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) has navigated a paradigm shift in computer vision, moving towards versatile <b>foundational</b> <b>models.</b> However, evaluating MLLMs in low-level <b>visual</b> <b>perception</b> <b>and</b> understanding remains a yet-to-explore domain. To this end, we design <b>benchmark</b> settings to emulate human language responses related to low-level vision: the low-level <b>visual</b> <b>perception</b> <b>(A1)</b> via <b>visual</b> <b>question</b> <b>answering</b> related to low-level attributes (e.g. clarity, lighting); and the low-level <b>visual</b> <b>description</b> <b>(A2),</b> on evaluating MLLMs for low-level text descriptions. Furthermore, given that pairwise comparison can better avoid ambiguity of responses and has been adopted by many human experiments, we further extend the low-level perception-related <b>question-answering</b> <b>and</b> description evaluations of MLLMs from single images to image pairs. Specifically, for perception (A1), we carry out the LLVisionQA+ dataset, comprising 2,990 single images and 1,999 image pairs each accompanied by an open-ended <b>question</b> <b>about</b> its low-level features; for description (A2), we propose the LLDescribe+ dataset, evaluating MLLMs for low-level descriptions on 499 single images and 450 pairs. Additionally, we evaluate MLLMs on assessment (A3) ability, i.e. predicting score, by employing a softmax-based approach to enable all MLLMs to generate quantifiable quality ratings, tested against human opinions in 7 image quality assessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate that several MLLMs have decent low-level <b>visual</b> <b>competencies</b> <b>on</b> single images, but only <b>GPT-4V</b> exhibits higher accuracy on pairwise comparisons than single image evaluations (like humans). We hope that our <b>benchmark</b> will motivate further research into uncovering and enhancing these nascent capabilities of MLLMs. Datasets will be available at <a href=https://github.com/Q-Future/Q-Bench>https://github.com/Q-Future/Q-Bench</a>.</p></p class="citation"></blockquote><h3 id=312--5685-outlier-aware-training-for-low-bit-quantization-of-structural-re-parameterized-networks-muqun-niu-et-al-2024>(3/12 | 56/85) Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks (Muqun Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muqun Niu, Yuan Ren, Boyu Li, Chenchen Ding. (2024)<br><strong>Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks</strong><br><button class=copy-to-clipboard title="Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-NE, cs.CV<br>Keyword Score: 53<br>Keywords: Clustering, Convolution, Convolutional Neural Network, Convolutional Neural Network, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07200v1.pdf filename=2402.07200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lightweight design of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> requires co-design efforts in the model architectures and compression techniques. As a novel design paradigm that separates training and inference, a structural re-parameterized (SR) network such as the representative RepVGG revitalizes the simple VGG-like network with a high accuracy comparable to advanced and often more complicated networks. However, the merging process in SR networks introduces outliers into weights, making their distribution distinct from conventional networks and thus heightening difficulties in <b>quantization.</b> To address this, we propose an operator-level improvement for training called Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of limited bitwidths while upkeeping the inference accuracy, we develop a <b>clustering-based</b> non-uniform <b>quantization</b> framework for <b>Quantization-Aware</b> Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the <b>quantized</b> performance of RepVGG is largely enhanced, particularly when the bitwidth falls below 8.</p></p class="citation"></blockquote><h3 id=412--5785-two-stage-multi-task-self-supervised-learning-for-medical-image-segmentation-binyan-hu-et-al-2024>(4/12 | 57/85) Two-Stage Multi-task Self-Supervised Learning for Medical Image Segmentation (Binyan Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binyan Hu, A. K. Qin. (2024)<br><strong>Two-Stage Multi-task Self-Supervised Learning for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Two-Stage Multi-task Self-Supervised Learning for Medical Image Segmentation" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-NE, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Distillation, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07119v1.pdf filename=2402.07119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image segmentation has been significantly advanced by deep learning (DL) techniques, though the data scarcity inherent in medical applications poses a great challenge to DL-based segmentation methods. <b>Self-supervised</b> <b>learning</b> offers a solution by creating auxiliary learning tasks from the available dataset and then leveraging the <b>knowledge</b> <b>acquired</b> from solving auxiliary tasks to help better solve the target segmentation task. Different auxiliary tasks may have different properties and thus can help the target task to different extents. It is desired to leverage their complementary advantages to enhance the overall assistance to the target task. To achieve this, existing methods often adopt a joint training paradigm, which co-solves segmentation and auxiliary tasks by integrating their losses or intermediate gradients. However, direct coupling of losses or intermediate gradients risks undesirable interference because the <b>knowledge</b> <b>acquired</b> from solving each auxiliary task at every training step may not always benefit the target task. To address this issue, we propose a two-stage training approach. In the first stage, the target segmentation task will be independently co-solved with each auxiliary task in both joint training and pre-training modes, with the better model selected via validation performance. In the second stage, the models obtained with respect to each auxiliary task are converted into a single model using an ensemble <b>knowledge</b> <b>distillation</b> method. Our approach allows for making best use of each auxiliary task to create multiple elite segmentation models and then combine them into an even more powerful model. We employed five auxiliary tasks of different proprieties in our approach and applied it to train the U-Net model on an X-ray pneumothorax segmentation dataset. Experimental results demonstrate the superiority of our approach over several existing methods.</p></p class="citation"></blockquote><h3 id=512--5885-the-bias-of-harmful-label-associations-in-vision-language-models-caner-hazirbas-et-al-2024>(5/12 | 58/85) The Bias of Harmful Label Associations in Vision-Language Models (Caner Hazirbas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Caner Hazirbas, Alicia Sun, Yonathan Efroni, Mark Ibrahim. (2024)<br><strong>The Bias of Harmful Label Associations in Vision-Language Models</strong><br><button class=copy-to-clipboard title="The Bias of Harmful Label Associations in Vision-Language Models" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fairness, Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07329v1.pdf filename=2402.07329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the remarkable performance of foundation <b>vision-language</b> models, the shared representation space for text and vision can also encode harmful label associations detrimental to <b>fairness.</b> While prior work has uncovered bias in <b>vision-language</b> models&rsquo; (VLMs) classification performance across geography, work has been limited along the important axis of harmful label associations due to a lack of rich, labeled data. In this work, we investigate harmful label associations in the recently released Casual Conversations datasets containing more than 70,000 videos. We study bias in the frequency of harmful label associations across self-provided labels for age, gender, apparent skin tone, and physical adornments across several leading VLMs. We find that VLMs are $4-13$x more likely to harmfully classify individuals with darker skin tones. We also find scaling <b>transformer</b> encoder model size leads to higher confidence in harmful predictions. Finally, we find improvements on standard vision tasks across VLMs does not address disparities in harmful label associations.</p></p class="citation"></blockquote><h3 id=612--5985-towards-explainable-safe-autonomous-driving-with-language-embeddings-for-novelty-identification-and-active-learning-framework-and-experimental-analysis-with-real-world-data-sets-ross-greer-et-al-2024>(6/12 | 59/85) Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets (Ross Greer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ross Greer, Mohan Trivedi. (2024)<br><strong>Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets</strong><br><button class=copy-to-clipboard title="Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Active Learning, Clustering, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07320v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07320v1.pdf filename=2402.07320v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research explores the integration of language embeddings for <b>active</b> <b>learning</b> in autonomous driving datasets, with a focus on novelty detection. Novelty arises from unexpected scenarios that autonomous vehicles struggle to navigate, necessitating higher-level <b>reasoning</b> abilities. Our proposed method employs language-based representations to identify novel scenes, emphasizing the dual purpose of safety takeover responses and <b>active</b> <b>learning.</b> The research presents a <b>clustering</b> experiment using Contrastive Language-Image Pretrained (CLIP) embeddings to organize datasets and detect novelties. We find that the proposed algorithm effectively isolates novel scenes from a collection of subsets derived from two real-world driving datasets, one vehicle-mounted and one infrastructure-mounted. From the generated clusters, we further present methods for generating textual explanations of elements which differentiate scenes classified as novel from other scenes in the data pool, presenting qualitative examples from the clustered results. Our results demonstrate the effectiveness of language-driven embeddings in identifying novel elements and generating explanations of data, and we further discuss potential applications in safe takeovers, data curation, and multi-task <b>active</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=712--6085-lisr-learning-linear-3d-implicit-surface-representation-using-compactly-supported-radial-basis-functions-atharva-pandey-et-al-2024>(7/12 | 60/85) LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions (Atharva Pandey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atharva Pandey, Vishal Yadav, Rajendra Nagar, Santanu Chaudhury. (2024)<br><strong>LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions</strong><br><button class=copy-to-clipboard title="LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07301v1.pdf filename=2402.07301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit 3D surface reconstruction of an object from its partial and noisy 3D point cloud scan is the classical geometry processing and 3D computer vision problem. In the literature, various 3D shape representations have been developed, differing in memory efficiency and shape retrieval effectiveness, such as volumetric, parametric, and implicit surfaces. Radial basis functions provide memory-efficient parameterization of the implicit surface. However, we show that training a neural network using the mean squared error between the ground-truth implicit surface and the linear basis-based implicit surfaces does not converge to the global solution. In this work, we propose locally supported compact radial basis functions for a linear representation of the implicit surface. This representation enables us to generate 3D shapes with arbitrary topologies at any resolution due to their continuous nature. We then propose a neural network architecture for learning the linear implicit shape representation of the 3D surface of an object. We learn linear implicit shapes within a <b>supervised</b> <b>learning</b> framework using ground truth Signed-Distance Field (SDF) data for guidance. The classical strategies face difficulties in finding linear implicit shapes from a given 3D point cloud due to numerical issues (requires solving inverse of a large matrix) in basis and query point selection. The proposed approach achieves better Chamfer distance and comparable F-score than the state-of-the-art approach on the <b>benchmark</b> dataset. We also show the effectiveness of the proposed approach by using it for the 3D shape completion task.</p></p class="citation"></blockquote><h3 id=812--6185-gala3d-towards-text-to-3d-complex-scene-generation-via-layout-guided-generative-gaussian-splatting-xiaoyu-zhou-et-al-2024>(8/12 | 61/85) GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting (Xiaoyu Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang. (2024)<br><strong>GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting</strong><br><button class=copy-to-clipboard title="GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07207v1.pdf filename=2402.07207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present GALA3D, generative 3D GAussians with LAyout-guided control, for effective compositional text-to-3D generation. We first utilize <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to generate the initial layout and introduce a layout-guided 3D Gaussian representation for 3D content generation with adaptive geometric constraints. We then propose an object-scene compositional optimization mechanism with conditioned diffusion to collaboratively generate realistic 3D scenes with consistent geometry, texture, scale, and accurate interactions among multiple objects while simultaneously adjusting the coarse layout priors extracted from the <b>LLMs</b> to align with the generated scene. Experiments show that GALA3D is a user-friendly, end-to-end framework for state-of-the-art scene-level 3D content generation and controllable editing while ensuring the high fidelity of object-level entities within the scene. Source codes and models will be available at <a href=https://gala3d.github.io/>https://gala3d.github.io/</a>.</p></p class="citation"></blockquote><h3 id=912--6285-deep-learning-for-medical-image-segmentation-with-imprecise-annotation-binyan-hu-et-al-2024>(9/12 | 62/85) Deep Learning for Medical Image Segmentation with Imprecise Annotation (Binyan Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binyan Hu, A. K. Qin. (2024)<br><strong>Deep Learning for Medical Image Segmentation with Imprecise Annotation</strong><br><button class=copy-to-clipboard title="Deep Learning for Medical Image Segmentation with Imprecise Annotation" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-NE, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07330v1.pdf filename=2402.07330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image segmentation (MIS) plays an instrumental role in medical image analysis, where considerable efforts have been devoted to automating the process. Currently, mainstream MIS approaches are based on deep neural networks (DNNs) which are typically trained on a dataset that contains annotation masks produced by doctors. However, in the medical domain, the annotation masks generated by different doctors can inherently vary because a doctor may unnecessarily produce precise and unique annotations to meet the goal of diagnosis. Therefore, the DNN model trained on the data annotated by certain doctors, often just a single doctor, could undesirably favour those doctors who annotate the training data, leading to the unsatisfaction of a new doctor who will use the trained model. To address this issue, this work investigates the utilization of multi-expert annotation to enhance the adaptability of the model to a new doctor and we conduct a pilot study on the MRI brain segmentation task. Experimental results demonstrate that the model trained on a dataset with multi-expert annotation can efficiently cater for a new doctor, after lightweight <b>fine-tuning</b> on just a few annotations from the new doctor.</p></p class="citation"></blockquote><h3 id=1012--6385-trade-off-between-spatial-and-angular-resolution-in-facial-recognition-muhammad-zeshan-alam-et-al-2024>(10/12 | 63/85) Trade-off Between Spatial and Angular Resolution in Facial Recognition (Muhammad Zeshan Alam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Zeshan Alam, Sousso kelowani, Mohamed Elsaeidy. (2024)<br><strong>Trade-off Between Spatial and Angular Resolution in Facial Recognition</strong><br><button class=copy-to-clipboard title="Trade-off Between Spatial and Angular Resolution in Facial Recognition" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Face Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07263v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07263v1.pdf filename=2402.07263v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring robustness in <b>face</b> <b>recognition</b> systems across various challenging conditions is crucial for their versatility. State-of-the-art methods often incorporate additional information, such as depth, thermal, or angular data, to enhance performance. However, light field-based <b>face</b> <b>recognition</b> approaches that leverage angular information <b>face</b> <b>computational</b> limitations. This paper investigates the fundamental trade-off between spatio-angular resolution in light field representation to achieve improved <b>face</b> <b>recognition</b> performance. By utilizing macro-pixels with varying angular resolutions while maintaining the overall image size, we aim to quantify the impact of angular information at the expense of spatial resolution, while considering computational constraints. Our experimental results demonstrate a notable performance improvement in <b>face</b> <b>recognition</b> systems by increasing the angular resolution, up to a certain extent, at the cost of spatial resolution.</p></p class="citation"></blockquote><h3 id=1112--6485-pivot-net-heterogeneous-point-voxel-tree-based-framework-for-point-cloud-compression-jiahao-pang-et-al-2024>(11/12 | 64/85) PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression (Jiahao Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Pang, Kevin Bui, Dong Tian. (2024)<br><strong>PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression</strong><br><button class=copy-to-clipboard title="PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07243v1.pdf filename=2402.07243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The universality of the point cloud format enables many 3D applications, making the compression of point clouds a critical phase in practice. Sampled as discrete 3D points, a point cloud approximates 2D surface(s) embedded in 3D with a finite bit-depth. However, the point distribution of a practical point cloud changes drastically as its bit-depth increases, requiring different methodologies for effective consumption/analysis. In this regard, a heterogeneous point cloud compression (PCC) framework is proposed. We unify typical point cloud representations &ndash; point-based, voxel-based, and tree-based representations &ndash; and their associated backbones under a learning-based framework to compress an input point cloud at different bit-depth levels. Having recognized the importance of voxel-domain processing, we augment the framework with a proposed context-aware upsampling for decoding and an enhanced voxel <b>transformer</b> for feature aggregation. Extensive experimentation demonstrates the state-of-the-art performance of our proposal on a wide range of point clouds.</p></p class="citation"></blockquote><h3 id=1212--6585-a-novel-spatial-frequency-domain-network-for-zero-shot-incremental-learning-jie-ren-et-al-2024>(12/12 | 65/85) A novel spatial-frequency domain network for zero-shot incremental learning (Jie Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Ren, Yang Zhao, Weichuan Zhang, Changming Sun. (2024)<br><strong>A novel spatial-frequency domain network for zero-shot incremental learning</strong><br><button class=copy-to-clipboard title="A novel spatial-frequency domain network for zero-shot incremental learning" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07216v1.pdf filename=2402.07216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> incremental learning aims to enable the model to generalize to new classes without forgetting previously learned classes. However, the semantic gap between old and new sample classes can lead to catastrophic forgetting. Additionally, existing algorithms lack capturing significant information from each sample image domain, impairing models&rsquo; classification performance. Therefore, this paper proposes a novel Spatial-Frequency Domain Network (SFDNet) which contains a Spatial-Frequency Feature Extraction (SFFE) module and Attention Feature Alignment (AFA) module to improve the <b>Zero-Shot</b> Translation for Class Incremental algorithm. Firstly, SFFE module is designed which contains a dual attention mechanism for obtaining salient spatial-frequency feature information. Secondly, a novel feature fusion module is conducted for obtaining fused spatial-frequency domain features. Thirdly, the Nearest Class Mean classifier is utilized to select the most suitable category. Finally, iteration between tasks is performed using the <b>Zero-Shot</b> Translation model. The proposed SFDNet has the ability to effectively extract spatial-frequency feature representation from input images, improve the accuracy of image classification, and fundamentally alleviate catastrophic forgetting. Extensive experiments on the CUB 200-2011 and CIFAR100 datasets demonstrate that our proposed algorithm outperforms state-of-the-art incremental learning algorithms.</p></p class="citation"></blockquote><h2 id=cshc-2>cs.HC (2)</h2><h3 id=12--6685-insights-into-natural-language-database-query-errors-from-attention-misalignment-to-user-handling-strategies-zheng-ning-et-al-2024>(1/2 | 66/85) Insights into Natural Language Database Query Errors: From Attention Misalignment to User Handling Strategies (Zheng Ning et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Ning, Yuan Tian, Zheng Zhang, Tianyi Zhang, Toby Li. (2024)<br><strong>Insights into Natural Language Database Query Errors: From Attention Misalignment to User Handling Strategies</strong><br><button class=copy-to-clipboard title="Insights into Natural Language Database Query Errors: From Attention Misalignment to User Handling Strategies" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 33<br>Keywords: Benchmarking, Attention Alignment, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07304v1.pdf filename=2402.07304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Querying structured databases with natural language (NL2SQL) has remained a difficult problem for years. Recently, the advancement of machine learning (ML), natural language processing (NLP), and <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> have led to significant improvements in performance, with the best model achieving ~85% percent accuracy on the <b>benchmark</b> Spider dataset. However, there is a lack of a systematic understanding of the types, causes, and effectiveness of error-handling mechanisms of errors for erroneous queries nowadays. To bridge the gap, a taxonomy of errors made by four representative NL2SQL models was built in this work, along with an in-depth analysis of the errors. Second, the causes of model errors were explored by analyzing the model-human <b>attention</b> <b>alignment</b> to the natural language query. Last, a within-subjects user study with 26 participants was conducted to investigate the effectiveness of three interactive error-handling mechanisms in NL2SQL. Findings from this paper shed light on the design of model structure and error discovery and repair strategies for natural language data query interfaces in the future.</p></p class="citation"></blockquote><h3 id=22--6785-emowear-exploring-emotional-teasers-for-voice-message-interaction-on-smartwatches-pengcheng-an-et-al-2024>(2/2 | 67/85) EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches (Pengcheng An et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengcheng An, Jiawen Zhu, Zibo Zhang, Yifei Yin, Qingyuan Ma, Che Yan, Linghao Du, Jian Zhao. (2024)<br><strong>EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches</strong><br><button class=copy-to-clipboard title="EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07174v1.pdf filename=2402.07174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Voice messages, by nature, prevent users from gauging the emotional tone without fully diving into the audio content. This hinders the shared emotional experience at the pre-retrieval stage. Research scarcely explored &ldquo;Emotional Teasers&rdquo;-pre-retrieval cues offering a glimpse into an awaiting message&rsquo;s emotional tone without disclosing its content. We introduce EmoWear, a smartwatch voice messaging system enabling users to apply 30 animation teasers on message bubbles to reflect emotions. EmoWear eases senders&rsquo; choice by prioritizing emotions based on semantic and acoustic processing. EmoWear was evaluated in comparison with a mirroring system using color-coded message bubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced emotional communication experience in both receiving and sending messages. The animated teasers were considered intuitive and valued for diverse expressions. Desirable interaction qualities and practical implications are <b>distilled</b> for future design. We thereby contribute both a novel system and empirical knowledge concerning emotional teasers for voice messaging.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--6885-ml-framework-for-wireless-mac-protocol-design-navid-keshtiarast-et-al-2024>(1/1 | 68/85) ML Framework for Wireless MAC Protocol Design (Navid Keshtiarast et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Navid Keshtiarast, Marina Petrova. (2024)<br><strong>ML Framework for Wireless MAC Protocol Design</strong><br><button class=copy-to-clipboard title="ML Framework for Wireless MAC Protocol Design" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs-SY, cs.NI, eess-SY<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07208v1.pdf filename=2402.07208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adaptivity, reconfigurability and intelligence are key features of the next-generation wireless networks to meet the increasingly diverse quality of service (QoS) requirements of the future applications. Conventional protocol designs, however, struggle to provide flexibility and agility to changing radio environments, traffic types and different user service requirements. In this paper, we explore the potential of deep <b>reinforcement</b> <b>learning</b> (DRL), in particular Proximal Policy Optimization (PPO), to design and configure intelligent and application-specific medium access control (MAC) protocols. We propose a framework that enables the addition, removal, or modification of protocol features to meet individual application needs. The DRL channel access policy design empowers the protocol to adapt and optimize in accordance with the network and radio environment. Through extensive <b>simulations,</b> we demonstrate the superior performance of the learned protocols over legacy IEEE 802.11ac in terms of throughput and latency.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--6985-joint-source-channel-coding-for-wireless-image-transmission-a-deep-compressed-sensing-based-method-mohammad-amin-jarrahi-et-al-2024>(1/1 | 69/85) Joint Source-Channel Coding for Wireless Image Transmission: A Deep Compressed-Sensing Based Method (Mohammad Amin Jarrahi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Amin Jarrahi, Eirina Bourtsoulatze, Vahid Abolghasemi. (2024)<br><strong>Joint Source-Channel Coding for Wireless Image Transmission: A Deep Compressed-Sensing Based Method</strong><br><button class=copy-to-clipboard title="Joint Source-Channel Coding for Wireless Image Transmission: A Deep Compressed-Sensing Based Method" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07162v1.pdf filename=2402.07162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, the demand for image transmission over wireless networks has surged significantly. To meet the need for swift delivery of high-quality images through time-varying channels with limited bandwidth, the development of efficient transmission strategies and techniques for preserving image quality is of importance. This paper introduces an innovative approach to Joint Source-Channel Coding (JSCC) tailored for wireless image transmission. It capitalizes on the power of Compressed Sensing (CS) to achieve superior compression and resilience to channel noise. In this method, the process begins with the compression of images using a block-based CS technique implemented through a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> structure. Subsequently, the images are encoded by directly mapping image blocks to complex-valued channel input symbols. Upon reception, the data is decoded to recover the channel-encoded information, effectively removing the noise introduced during transmission. To finalize the process, a novel <b>CNN-based</b> reconstruction network is employed to restore the original image from the channel-decoded data. The performance of the proposed method is assessed using the CIFAR-10 and Kodak datasets. The results illustrate a substantial improvement over existing JSCC frameworks when assessed in terms of metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) across various channel Signal-to-Noise Ratios (SNRs) and channel bandwidth values. These findings underscore the potential of harnessing <b>CNN-based</b> CS for the development of deep JSCC algorithms tailored for wireless image transmission.</p></p class="citation"></blockquote><h2 id=q-biogn-1>q-bio.GN (1)</h2><h3 id=11--7085-highly-accurate-disease-diagnosis-and-highly-reproducible-biomarker-identification-with-pathformer-zehao-dong-et-al-2024>(1/1 | 70/85) Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer (Zehao Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehao Dong, Qihang Zhao, Philip R. O. Payne, Michael A Province, Carlos Cruchaga, Muhan Zhang, Tianyu Zhao, Yixin Chen, Fuhai Li. (2024)<br><strong>Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer</strong><br><button class=copy-to-clipboard title="Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-AI, cs-LG, q-bio-GN, q-bio.GN<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07268v1.pdf filename=2402.07268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Biomarker identification is critical for precise disease diagnosis and understanding disease pathogenesis in omics data analysis, like using fold change and regression analysis. <b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have been the dominant deep learning model for analyzing <b>graph-structured</b> <b>data.</b> <b>However,</b> we found two major limitations of existing <b>GNNs</b> in omics data analysis, i.e., limited-prediction (diagnosis) accuracy and limited-reproducible biomarker identification capacity across multiple datasets. The root of the challenges is the unique <b>graph</b> <b>structure</b> <b>of</b> biological signaling pathways, which consists of a large number of targets and intensive and complex signaling interactions among these targets. To resolve these two challenges, in this study, we presented a novel <b>GNN</b> model architecture, named PathFormer, which systematically integrate signaling network, priori knowledge and omics data to rank biomarkers and predict disease diagnosis. In the comparison results, PathFormer outperformed existing <b>GNN</b> models significantly in terms of highly accurate prediction capability ( 30% accuracy improvement in disease diagnosis compared with existing <b>GNN</b> models) and high reproducibility of biomarker ranking across different datasets. The improvement was confirmed using two independent Alzheimer&rsquo;s Disease (AD) and cancer transcriptomic datasets. The PathFormer model can be directly applied to other omics data analysis studies.</p></p class="citation"></blockquote><h2 id=cscr-1>cs.CR (1)</h2><h3 id=11--7185-differentially-private-training-of-mixture-of-experts-models-pierre-tholoniat-et-al-2024>(1/1 | 71/85) Differentially Private Training of Mixture of Experts Models (Pierre Tholoniat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Tholoniat, Huseyin A. Inan, Janardhan Kulkarni, Robert Sim. (2024)<br><strong>Differentially Private Training of Mixture of Experts Models</strong><br><button class=copy-to-clipboard title="Differentially Private Training of Mixture of Experts Models" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07334v1.pdf filename=2402.07334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This position paper investigates the integration of Differential Privacy (DP) in the training of Mixture of Experts (MoE) models within the field of natural language processing. As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> scale to billions of parameters, leveraging expansive datasets, they exhibit enhanced linguistic capabilities and emergent abilities. However, this growth raises significant computational and privacy concerns. Our study addresses these issues by exploring the potential of MoE models, known for their computational efficiency, and the application of DP, a standard for privacy preservation. We present the first known attempt to train MoE models under the constraints of DP, addressing the unique challenges posed by their architecture and the complexities of DP integration. Our initial experimental studies demonstrate that MoE models can be effectively trained with DP, achieving performance that is competitive with their non-private counterparts. This initial study aims to provide valuable insights and ignite further research in the domain of privacy-preserving MoE models, softly laying the groundwork for prospective developments in this evolving field.</p></p class="citation"></blockquote><h2 id=statml-4>stat.ML (4)</h2><h3 id=14--7285-self-consistent-conformal-prediction-lars-van-der-laan-et-al-2024>(1/4 | 72/85) Self-Consistent Conformal Prediction (Lars van der Laan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lars van der Laan, Ahmed M. Alaa. (2024)<br><strong>Self-Consistent Conformal Prediction</strong><br><button class=copy-to-clipboard title="Self-Consistent Conformal Prediction" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07307v1.pdf filename=2402.07307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes. Conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management. Inspired by this perspective, we introduce self-consistent conformal prediction, which yields both Venn-Abers calibrated predictions and conformal prediction intervals that are valid conditional on actions <b>prompted</b> by model predictions. Our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees. Numerical experiments show our approach strikes a balance between interval efficiency and conditional validity.</p></p class="citation"></blockquote><h3 id=24--7385-resampling-methods-for-private-statistical-inference-karan-chadha-et-al-2024>(2/4 | 73/85) Resampling methods for Private Statistical Inference (Karan Chadha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karan Chadha, John Duchi, Rohith Kuditipudi. (2024)<br><strong>Resampling methods for Private Statistical Inference</strong><br><button class=copy-to-clipboard title="Resampling methods for Private Statistical Inference" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CR, cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Logistic Regression, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07131v1.pdf filename=2402.07131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple ``little&rsquo;&rsquo; bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the <b>sample</b> <b>size</b> $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and <b>logistic</b> <b>regression</b> with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.</p></p class="citation"></blockquote><h3 id=34--7485-optimal-thresholding-linear-bandit-eduardo-ochoa-rivera-et-al-2024>(3/4 | 74/85) Optimal Thresholding Linear Bandit (Eduardo Ochoa Rivera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eduardo Ochoa Rivera, Ambuj Tewari. (2024)<br><strong>Optimal Thresholding Linear Bandit</strong><br><button class=copy-to-clipboard title="Optimal Thresholding Linear Bandit" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09467v1.pdf filename=2402.09467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a novel pure exploration problem: the $\epsilon$-Thresholding <b>Bandit</b> Problem (TBP) with fixed confidence in stochastic linear <b>bandits.</b> We prove a lower bound for the sample complexity and extend an algorithm designed for Best Arm Identification in the linear case to TBP that is asymptotically optimal.</p></p class="citation"></blockquote><h3 id=44--7585-improving-lsh-via-tensorized-random-projection-bhisham-dev-verma-et-al-2024>(4/4 | 75/85) Improving LSH via Tensorized Random Projection (Bhisham Dev Verma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhisham Dev Verma, Rameshwar Pratap. (2024)<br><strong>Improving LSH via Tensorized Random Projection</strong><br><button class=copy-to-clipboard title="Improving LSH via Tensorized Random Projection" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-DS, cs-LG, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07189v1.pdf filename=2402.07189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by data scientists for approximate nearest neighbour search problems that have been used extensively in many large scale data processing applications such as near duplicate detection, nearest neighbour search, <b>clustering,</b> etc. In this work, we aim to propose faster and space efficient locality sensitive hash functions for Euclidean distance and cosine similarity for tensor data. Typically, the naive approach for obtaining LSH for tensor data involves first reshaping the tensor into vectors, followed by applying existing LSH methods for vector data $E2LSH$ and $SRP$. However, this approach becomes impractical for higher order tensors because the size of the reshaped vector becomes exponential in the order of the tensor. Consequently, the size of LSH parameters increases exponentially. To address this problem, we suggest two methods for LSH for Euclidean distance and cosine similarity, namely $CP-E2LSH$, $TT-E2LSH$, and $CP-SRP$, $TT-SRP$, respectively, building on $CP$ and tensor train $(TT)$ decompositions techniques. Our approaches are space efficient and can be efficiently applied to low rank $CP$ or $TT$ tensors. We provide a rigorous theoretical analysis of our proposal on their correctness and efficacy.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--7685-effort-and-size-estimation-in-software-projects-with-large-language-model-based-intelligent-interfaces-claudionor-n-coelho-jr-et-al-2024>(1/3 | 76/85) Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces (Claudionor N. Coelho Jr et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Claudionor N. Coelho Jr, Hanchen Xiong, Tushar Karayil, Sree Koratala, Rex Shang, Jacob Bollinger, Mohamed Shabar, Syam Nair. (2024)<br><strong>Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces</strong><br><button class=copy-to-clipboard title="Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07158v1.pdf filename=2402.07158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> has also resulted in an equivalent proliferation in its applications. Software design, being one, has gained tremendous benefits in using <b>LLMs</b> as an interface component that extends fixed user stories. However, inclusion of <b>LLM-based</b> AI agents in software design often poses unexpected challenges, especially in the estimation of development efforts. Through the example of UI-based user stories, we provide a comparison against traditional methods and propose a new way to enhance specifications of natural language-based questions that allows for the estimation of development effort by taking into account data sources, interfaces and algorithms.</p></p class="citation"></blockquote><h3 id=23--7785-unprecedented-code-change-automation-the-fusion-of-llms-and-transformation-by-example-malinda-dilhara-et-al-2024>(2/3 | 77/85) Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example (Malinda Dilhara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Malinda Dilhara, Abhiram Bellur, Timofey Bryksin, Danny Dig. (2024)<br><strong>Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example</strong><br><button class=copy-to-clipboard title="Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07138v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07138v1.pdf filename=2402.07138v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software developers often repeat code changes, known as &ldquo;code change patterns&rdquo; (CPATs), within and across projects. Automating these CPATs accelerates development, but current Transformation by Example (TBE) techniques are limited by the input examples&rsquo; quality and quantity, missing variations with different syntax or flow yet semantically similar. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> trained on vast code datasets, can overcome these limitations by generating semantically equivalent, unseen CPAT variants, enhancing TBE effectiveness. We identified best practices for using <b>LLMs</b> to generate code variants meeting criteria of correctness, usefulness, and applicability. Implementing these in PyCraft, combining static and dynamic analysis with <b>LLMs,</b> we achieved an F-measure of 96.6% in identifying correct variants, expanding inputs by 58x on average, and automating changes to increase target codes by up to 39x. Patches from PyCraft were submitted to projects like microsoft/DeepSpeed and IBM/inFairness, with an 83% acceptance rate, validating our approach&rsquo;s usefulness.</p></p class="citation"></blockquote><h3 id=33--7885-on-the-effectiveness-of-machine-learning-based-call-graph-pruning-an-empirical-study-amir-m-mir-et-al-2024>(3/3 | 78/85) On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study (Amir M. Mir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir M. Mir, Mehdi Keshani, Sebastian Proksch. (2024)<br><strong>On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study</strong><br><button class=copy-to-clipboard title="On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-PL, cs-SE, cs.SE<br>Keyword Score: 13<br>Keywords: Graph, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07294v1.pdf filename=2402.07294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Static call <b>graph</b> (CG) construction often over-approximates call relations, leading to sound, but imprecise results. Recent research has explored machine learning (ML)-based CG <b>pruning</b> as a means to enhance precision by eliminating false edges. However, current methods suffer from a limited evaluation dataset, imbalanced training data, and reduced recall, which affects practical downstream analyses. Prior results were also not compared with advanced static CG construction techniques yet. This study tackles these issues. We introduce the NYXCorpus, a dataset of real-world Java programs with high test coverage and we collect traces from test executions and build a ground truth of dynamic CGs. We leverage these CGs to explore conservative <b>pruning</b> strategies during the training and inference of ML-based CG pruners. We conduct a comparative analysis of static CGs generated using zero control flow analysis (0-CFA) and those produced by a context-sensitive 1-CFA algorithm, evaluating both with and without <b>pruning.</b> We find that CG <b>pruning</b> is a difficult task for real-world Java projects and substantial improvements in the CG precision (+25%) meet reduced recall (-9%). However, our experiments show promising results: even when we favor recall over precision by using an F2 metric in our experiments, we can show that pruned CGs have comparable quality to a context-sensitive 1-CFA analysis while being computationally less demanding. Resulting CGs are much smaller (69%), and substantially faster (3.5x speed-up), with virtually unchanged results in our downstream analysis.</p></p class="citation"></blockquote><h2 id=mathna-1>math.NA (1)</h2><h3 id=11--7985-codpy-a-python-library-for-numerics-machine-learning-and-statistics-philippe-g-lefloch-et-al-2024>(1/1 | 79/85) CodPy: a Python library for numerics, machine learning, and statistics (Philippe G. LeFloch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philippe G. LeFloch, Jean-Marc Mercier, Shohruh Miryusupov. (2024)<br><strong>CodPy: a Python library for numerics, machine learning, and statistics</strong><br><button class=copy-to-clipboard title="CodPy: a Python library for numerics, machine learning, and statistics" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math-ST, math.NA, stat-CO, stat-TH<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07084v1.pdf filename=2402.07084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This monograph offers an introduction to a collection of numerical algorithms implemented in the library CodPy (an acronym that stands for the Curse Of Dimensionality in PYthon), which has found widespread applications across various areas, including machine learning, statistics, and computational physics. We develop here a strategy based on the theory of reproducing kernel Hilbert spaces (RKHS) and the theory of optimal transport. Initially designed for mathematical finance, this library has since been enhanced and broadened to be applicable to problems arising in engineering and industry. In order to present the general principles and techniques employed in CodPy and its applications, we have structured this monograph into two main parts. First of all, we focus on the fundamental principles of kernel-based representations of data and solutions, also that the presentation therein is supplemented with illustrative examples only. Next, we discuss the application of these principles to many classes of concrete problems, spanning from the numerical approximation of partial differential equations to <b>(supervised,</b> <b>unsupervised)</b> machine learning, extending to generative methods with a focus on stochastic aspects.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--8085-intent-based-access-control-using-llms-to-intelligently-manage-access-control-pranav-subramaniam-et-al-2024>(1/1 | 80/85) Intent-Based Access Control: Using LLMs to Intelligently Manage Access Control (Pranav Subramaniam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pranav Subramaniam, Sanjay Krishnan. (2024)<br><strong>Intent-Based Access Control: Using LLMs to Intelligently Manage Access Control</strong><br><button class=copy-to-clipboard title="Intent-Based Access Control: Using LLMs to Intelligently Manage Access Control" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-CR, cs-DB, cs.DB<br>Keyword Score: 13<br>Keywords: Benchmarking, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07332v1.pdf filename=2402.07332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In every enterprise database, administrators must define an access control policy that specifies which users have access to which assets. Access control straddles two worlds: policy (organization-level principles that define who should have access) and process (database-level primitives that actually implement the policy). Assessing and enforcing process compliance with a policy is a manual and ad-hoc task. This paper introduces a new paradigm for access control called Intent-Based Access Control for Databases (IBAC-DB). In IBAC-DB, access control policies are expressed more precisely using a novel format, the natural language access control matrix (NLACM). Database access control primitives are synthesized automatically from these NLACMs. These primitives can be used to generate new DB configurations and/or evaluate existing ones. This paper presents a reference architecture for an IBAC-DB interface, an initial implementation for PostgreSQL (which we call LLM4AC), and initial <b>benchmarks</b> that evaluate the accuracy and scope of such a system. We find that our chosen implementation, LLM4AC, vastly outperforms other baselines, achieving near-perfect F1 scores on our initial <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=eesssy-1>eess.SY (1)</h2><h3 id=11--8185-a-review-of-the-gic-blocker-placement-problem-arthur-k-barnes-et-al-2024>(1/1 | 81/85) A Review of the GIC Blocker Placement Problem (Arthur K. Barnes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arthur K. Barnes, Adam Mate, Russell Bent. (2024)<br><strong>A Review of the GIC Blocker Placement Problem</strong><br><button class=copy-to-clipboard title="A Review of the GIC Blocker Placement Problem" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07302v1.pdf filename=2402.07302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Space weather poses a tremendous threat to power systems: geomagnetic disturbances could result in widespread disruptions and long-duration blackouts, including severe damage to system components. To mitigate their impacts, a handful of strategies exist, with the most promising being the deployment of <b>transformer</b> neutral blocking devices. The high cost of these devices, however, precludes their installation at all substations; this motivates the development of effective solutions for the cost-effective placement of such devices. While the current state-of-the-art in blocker placement methods is insufficient to be applied to real-sized power grids, ongoing research continues to increase the size of networks for which the placement problem remains tractable. Along these lines, the contributions of this paper are two fold: first, a comprehensive overview of the current state-of-the-art in blocker placement methods is provided; and second, a complete optimization formulation - implemented and <b>benchmarked</b> in an open-source software - for the blocker placement problem is presented.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--8285-optimizing-genetically-driven-synaptogenesis-tommaso-boccato-et-al-2024>(1/2 | 82/85) Optimizing Genetically-Driven Synaptogenesis (Tommaso Boccato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tommaso Boccato, Matteo Ferrante, Nicola Toschi. (2024)<br><strong>Optimizing Genetically-Driven Synaptogenesis</strong><br><button class=copy-to-clipboard title="Optimizing Genetically-Driven Synaptogenesis" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE, q-bio-NC<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07242v1.pdf filename=2402.07242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we introduce SynaptoGen, a novel framework that aims to bridge the gap between genetic manipulations and neuronal network behavior by simulating synaptogenesis and guiding the development of neuronal networks capable of solving predetermined computational tasks. Drawing inspiration from recent advancements in the field, we propose SynaptoGen as a bio-plausible approach to modeling synaptogenesis through differentiable functions. To validate SynaptoGen, we conduct a preliminary experiment using <b>reinforcement</b> <b>learning</b> as a <b>benchmark</b> learning framework, demonstrating its effectiveness in generating neuronal networks capable of solving the OpenAI Gym&rsquo;s Cart Pole task, compared to carefully designed baselines. The results highlight the potential of SynaptoGen to inspire further advancements in neuroscience and computational modeling, while also acknowledging the need for incorporating more realistic genetic rules and synaptic conductances in future research. Overall, SynaptoGen represents a promising avenue for exploring the intersection of genetics, neuroscience, and artificial intelligence.</p></p class="citation"></blockquote><h3 id=22--8385-sais-a-novel-bio-inspired-artificial-immune-system-based-on-symbiotic-paradigm-junhao-song-et-al-2024>(2/2 | 83/85) SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic Paradigm (Junhao Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhao Song, Yingfang Yuan, Wei Pang. (2024)<br><strong>SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic Paradigm</strong><br><button class=copy-to-clipboard title="SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic Paradigm" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07244v1.pdf filename=2402.07244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel type of Artificial Immune System (AIS): Symbiotic Artificial Immune Systems (SAIS), drawing inspiration from symbiotic relationships in biology. SAIS parallels the three key stages (i.e., mutualism, commensalism and parasitism) of population updating from the Symbiotic Organisms Search (SOS) algorithm. This parallel approach effectively addresses the challenges of large population size and enhances population diversity in AIS, which traditional AIS and SOS struggle to resolve efficiently. We conducted a series of experiments, which demonstrated that our SAIS achieved comparable performance to the state-of-the-art approach SOS and outperformed other popular AIS approaches and evolutionary algorithms across 26 <b>benchmark</b> problems. Furthermore, we investigated the problem of parameter selection and found that SAIS performs better in handling larger population sizes while requiring fewer generations. Finally, we believe SAIS, as a novel bio-inspired and immune-inspired algorithm, paves the way for innovation in bio-inspired computing with the symbiotic paradigm.</p></p class="citation"></blockquote><h2 id=csir-1>cs.IR (1)</h2><h3 id=11--8485-enhancing-multi-field-b2b-cloud-solution-matching-via-contrastive-pre-training-haonan-chen-et-al-2024>(1/1 | 84/85) Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training (Haonan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haonan Chen, Zhicheng Dou, Xuetong Hao, Yunhao Tao, Shiren Song, Zhenli Sheng. (2024)<br><strong>Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training</strong><br><button class=copy-to-clipboard title="Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07076v1.pdf filename=2402.07076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems. However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address. In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction <b>data.</b> <b>To</b> tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three <b>data</b> <b>augmentation</b> strategies and a contrastive pre-training objective to compensate for the imperfections in the available <b>data.</b> <b>Through</b> extensive experiments on a real-world dataset, we demonstrate that CAMA outperforms several strong baseline matching models significantly. Furthermore, we have deployed our matching framework on a system of Huawei Cloud. Our observations indicate an improvement of about 30% compared to the previous online model in terms of Conversion Rate (CVR), which demonstrates its great business value.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--8585-fundamental-problems-on-bounded-treewidth-graphs-the-real-source-of-hardness-barış-can-esmer-et-al-2024>(1/1 | 85/85) Fundamental Problems on Bounded-Treewidth Graphs: The Real Source of Hardness (Barış Can Esmer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Barış Can Esmer, Jacob Focke, Dániel Marx, Paweł Rzążewski. (2024)<br><strong>Fundamental Problems on Bounded-Treewidth Graphs: The Real Source of Hardness</strong><br><button class=copy-to-clipboard title="Fundamental Problems on Bounded-Treewidth Graphs: The Real Source of Hardness" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DS, cs.CC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07331v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07331v1.pdf filename=2402.07331v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is known for many algorithmic problems that if a tree decomposition of width $t$ is given in the input, then the problem can be solved with exponential dependence on $t$. A line of research by Lokshtanov, Marx, and Saurabh [SODA 2011] produced lower bounds showing that in many cases known algorithms achieve the best possible exponential dependence on $t$, assuming the SETH. The main message of our paper is showing that the same lower bounds can be obtained in a more restricted setting: a <b>graph</b> consisting of a block of $t$ vertices connected to components of constant size already has the same hardness as a general tree decomposition of width $t$. Formally, a $(\sigma,\delta)$-hub is a set $Q$ of vertices such that every component of $Q$ has size at most $\sigma$ and is adjacent to at most $\delta$ vertices of $Q$. We show that $\bullet$ For every $\epsilon> 0$, there are $\sigma,\delta> 0$ such that Independent Set/Vertex Cover cannot be solved in time $(2-\epsilon)^p\cdot n$, even if a $(\sigma,\delta)$-hub of size $p$ is given in the input, assuming the SETH. This matches the earlier tight lower bounds parameterized by the width of the tree decomposition. Similar tight bounds are obtained for Odd Cycle Transversal, Max Cut, $q$-Coloring, and edge/vertex deletions versions of $q$-Coloring. $\bullet$ For every $\epsilon>0$, there are $\sigma,\delta> 0$ such that Triangle-Partition cannot be solved in time $(2-\epsilon)^p\cdot n$, even if a $(\sigma,\delta)$-hub of size $p$ is given in the input, assuming the Set Cover Conjecture (SCC). In fact, we prove that this statement is equivalent to the SCC, thus it is unlikely that this could be proved assuming the SETH. Our results reveal that, for many problems, the research on lower bounds on the dependence on tree width was never really about tree decompositions, but the real source of hardness comes from a much simpler structure.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.12</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.14</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cslg-24>cs.LG (24)</a><ul><li><a href=#124--185-explainable-global-wildfire-prediction-models-using-graph-neural-networks-dayou-chen-et-al-2024>(1/24 | 1/85) Explainable Global Wildfire Prediction Models using Graph Neural Networks (Dayou Chen et al., 2024)</a></li><li><a href=#224--285-rethinking-graph-masked-autoencoders-through-alignment-and-uniformity-liang-wang-et-al-2024>(2/24 | 2/85) Rethinking Graph Masked Autoencoders through Alignment and Uniformity (Liang Wang et al., 2024)</a></li><li><a href=#324--385-a-theoretical-analysis-of-nash-learning-from-human-feedback-under-general-kl-regularized-preference-chenlu-ye-et-al-2024>(3/24 | 3/85) A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference (Chenlu Ye et al., 2024)</a></li><li><a href=#424--485-more-benefits-of-being-distributional-second-order-bounds-for-reinforcement-learning-kaiwen-wang-et-al-2024>(4/24 | 4/85) More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning (Kaiwen Wang et al., 2024)</a></li><li><a href=#524--585-using-large-language-models-to-automate-and-expedite-reinforcement-learning-with-reward-machine-shayan-meshkat-alsadat-et-al-2024>(5/24 | 5/85) Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine (Shayan Meshkat Alsadat et al., 2024)</a></li><li><a href=#624--685-summing-up-the-facts-additive-mechanisms-behind-factual-recall-in-llms-bilal-chughtai-et-al-2024>(6/24 | 6/85) Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs (Bilal Chughtai et al., 2024)</a></li><li><a href=#724--785-odin-disentangled-reward-mitigates-hacking-in-rlhf-lichang-chen-et-al-2024>(7/24 | 7/85) ODIN: Disentangled Reward Mitigates Hacking in RLHF (Lichang Chen et al., 2024)</a></li><li><a href=#824--885-training-heterogeneous-client-models-using-knowledge-distillation-in-serverless-federated-learning-mohak-chadha-et-al-2024>(8/24 | 8/85) Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning (Mohak Chadha et al., 2024)</a></li><li><a href=#924--985-physics-informed-neural-networks-with-hard-linear-equality-constraints-hao-chen-et-al-2024>(9/24 | 9/85) Physics-Informed Neural Networks with Hard Linear Equality Constraints (Hao Chen et al., 2024)</a></li><li><a href=#1024--1085-power-transformer-fault-prediction-based-on-knowledge-graphs-chao-wang-et-al-2024>(10/24 | 10/85) Power Transformer Fault Prediction Based on Knowledge Graphs (Chao Wang et al., 2024)</a></li><li><a href=#1124--1185-rethinking-the-capacity-of-graph-neural-networks-for-branching-strategy-ziang-chen-et-al-2024>(11/24 | 11/85) Rethinking the Capacity of Graph Neural Networks for Branching Strategy (Ziang Chen et al., 2024)</a></li><li><a href=#1224--1285-geoformer-a-vision-and-sequence-transformer-based-approach-for-greenhouse-gas-monitoring-madhav-khirwar-et-al-2024>(12/24 | 12/85) GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring (Madhav Khirwar et al., 2024)</a></li><li><a href=#1324--1385-towards-robust-car-following-dynamics-modeling-via-blackbox-models-methodology-analysis-and-recommendations-muhammad-bilal-shahid-et-al-2024>(13/24 | 13/85) Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations (Muhammad Bilal Shahid et al., 2024)</a></li><li><a href=#1424--1485-echoes-of-socratic-doubt-embracing-uncertainty-in-calibrated-evidential-reinforcement-learning-alex-christopher-stutts-et-al-2024>(14/24 | 14/85) Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning (Alex Christopher Stutts et al., 2024)</a></li><li><a href=#1524--1585-can-tree-based-approaches-surpass-deep-learning-in-anomaly-detection-a-benchmarking-study-santonu-sarkar-et-al-2024>(15/24 | 15/85) Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study (Santonu Sarkar et al., 2024)</a></li><li><a href=#1624--1685-hyperbert-mixing-hypergraph-aware-layers-with-language-models-for-node-classification-on-text-attributed-hypergraphs-adrián-bazaga-et-al-2024>(16/24 | 16/85) HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs (Adrián Bazaga et al., 2024)</a></li><li><a href=#1724--1785-towards-generalized-inverse-reinforcement-learning-chaosheng-dong-et-al-2024>(17/24 | 17/85) Towards Generalized Inverse Reinforcement Learning (Chaosheng Dong et al., 2024)</a></li><li><a href=#1824--1885-the-implicit-bias-of-gradient-noise-a-symmetry-perspective-liu-ziyin-et-al-2024>(18/24 | 18/85) The Implicit Bias of Gradient Noise: A Symmetry Perspective (Liu Ziyin et al., 2024)</a></li><li><a href=#1924--1985-refined-sample-complexity-for-markov-games-with-independent-linear-function-approximation-yan-dai-et-al-2024>(19/24 | 19/85) Refined Sample Complexity for Markov Games with Independent Linear Function Approximation (Yan Dai et al., 2024)</a></li><li><a href=#2024--2085-future-prediction-can-be-a-strong-evidence-of-good-history-representation-in-partially-observable-environments-jeongyeol-kwon-et-al-2024>(20/24 | 20/85) Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments (Jeongyeol Kwon et al., 2024)</a></li><li><a href=#2124--2185-genstl-general-sparse-trajectory-learning-via-auto-regressive-generation-of-feature-domains-yan-lin-et-al-2024>(21/24 | 21/85) GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains (Yan Lin et al., 2024)</a></li><li><a href=#2224--2285-divide-and-conquer-provably-unveiling-the-pareto-front-with-multi-objective-reinforcement-learning-willem-röpke-et-al-2024>(22/24 | 22/85) Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning (Willem Röpke et al., 2024)</a></li><li><a href=#2324--2385-the-impact-of-domain-knowledge-and-multi-modality-on-intelligent-molecular-property-prediction-a-systematic-survey-taojie-kuang-et-al-2024>(23/24 | 23/85) The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey (Taojie Kuang et al., 2024)</a></li><li><a href=#2424--2485-gsina-improving-subgraph-extraction-for-graph-invariant-learning-via-graph-sinkhorn-attention-fangyu-ding-et-al-2024>(24/24 | 24/85) GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention (Fangyu Ding et al., 2024)</a></li></ul></li><li><a href=#cscl-9>cs.CL (9)</a><ul><li><a href=#19--2585-how-do-large-language-models-navigate-conflicts-between-honesty-and-helpfulness-ryan-liu-et-al-2024>(1/9 | 25/85) How do Large Language Models Navigate Conflicts between Honesty and Helpfulness? (Ryan Liu et al., 2024)</a></li><li><a href=#29--2685-prompt-perturbation-in-retrieval-augmented-generation-based-large-language-models-zhibo-hu-et-al-2024>(2/9 | 26/85) Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models (Zhibo Hu et al., 2024)</a></li><li><a href=#39--2785-natural-language-reinforcement-learning-xidong-feng-et-al-2024>(3/9 | 27/85) Natural Language Reinforcement Learning (Xidong Feng et al., 2024)</a></li><li><a href=#49--2885-transgpt-multi-modal-generative-pre-trained-transformer-for-transportation-peng-wang-et-al-2024>(4/9 | 28/85) TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation (Peng Wang et al., 2024)</a></li><li><a href=#59--2985-generalizing-conversational-dense-retrieval-via-llm-cognition-data-augmentation-haonan-chen-et-al-2024>(5/9 | 29/85) Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation (Haonan Chen et al., 2024)</a></li><li><a href=#69--3085-previously-on-the-stories-recap-snippet-identification-for-story-reading-jiangnan-li-et-al-2024>(6/9 | 30/85) Previously on the Stories: Recap Snippet Identification for Story Reading (Jiangnan Li et al., 2024)</a></li><li><a href=#79--3185-american-sign-language-video-to-text-translation-parsheeta-roy-et-al-2024>(7/9 | 31/85) American Sign Language Video to Text Translation (Parsheeta Roy et al., 2024)</a></li><li><a href=#89--3285-low-resource-counterspeech-generation-for-indic-languages-the-case-of-bengali-and-hindi-mithun-das-et-al-2024>(8/9 | 32/85) Low-Resource Counterspeech Generation for Indic Languages: The Case of Bengali and Hindi (Mithun Das et al., 2024)</a></li><li><a href=#99--3385-using-large-language-models-for-student-code-guided-test-case-generation-in-computer-science-education-nischal-ashok-kumar-et-al-2024>(9/9 | 33/85) Using Large Language Models for Student-Code Guided Test Case Generation in Computer Science Education (Nischal Ashok Kumar et al., 2024)</a></li></ul></li><li><a href=#eessiv-4>eess.IV (4)</a><ul><li><a href=#14--3485-semi-mamba-unet-pixel-level-contrastive-cross-supervised-visual-mamba-based-unet-for-semi-supervised-medical-image-segmentation-ziyang-wang-et-al-2024>(1/4 | 34/85) Semi-Mamba-UNet: Pixel-Level Contrastive Cross-Supervised Visual Mamba-based UNet for Semi-Supervised Medical Image Segmentation (Ziyang Wang et al., 2024)</a></li><li><a href=#24--3585-kvq-kaleidoscope-video-quality-assessment-for-short-form-videos-yiting-lu-et-al-2024>(2/4 | 35/85) KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos (Yiting Lu et al., 2024)</a></li><li><a href=#34--3685-spatio-spectral-classification-of-hyperspectral-images-for-brain-cancer-detection-during-surgical-operations-h-fabelo-et-al-2024>(3/4 | 36/85) Spatio-spectral classification of hyperspectral images for brain cancer detection during surgical operations (H. Fabelo et al., 2024)</a></li><li><a href=#44--3785-supervised-reconstruction-for-silhouette-tomography-evan-bell-et-al-2024>(4/4 | 37/85) Supervised Reconstruction for Silhouette Tomography (Evan Bell et al., 2024)</a></li></ul></li><li><a href=#csai-12>cs.AI (12)</a><ul><li><a href=#112--3885-graphtranslator-aligning-graph-model-to-large-language-model-for-open-ended-tasks-mengmei-zhang-et-al-2024>(1/12 | 38/85) GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks (Mengmei Zhang et al., 2024)</a></li><li><a href=#212--3985-multi-modal-emotion-recognition-by-text-speech-and-video-using-pretrained-transformers-minoo-shayaninasab-et-al-2024>(2/12 | 39/85) Multi-Modal Emotion Recognition by Text, Speech and Video Using Pretrained Transformers (Minoo Shayaninasab et al., 2024)</a></li><li><a href=#312--4085-cpsdbench-a-large-language-model-evaluation-benchmark-and-baseline-for-chinese-public-security-domain-xin-tong-et-al-2024>(3/12 | 40/85) CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain (Xin Tong et al., 2024)</a></li><li><a href=#412--4185-large-language-model-empowered-dose-volume-histogram-prediction-for-intensity-modulated-radiotherapy-zehao-dong-et-al-2024>(4/12 | 41/85) Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy (Zehao Dong et al., 2024)</a></li><li><a href=#512--4285-persian-speech-emotion-recognition-by-fine-tuning-transformers-minoo-shayaninasab-et-al-2024>(5/12 | 42/85) Persian Speech Emotion Recognition by Fine-Tuning Transformers (Minoo Shayaninasab et al., 2024)</a></li><li><a href=#612--4385-sequential-ordering-in-textual-descriptions-impact-on-spatial-perception-abilities-of-large-language-models-yuyao-ge-et-al-2024>(6/12 | 43/85) Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models (Yuyao Ge et al., 2024)</a></li><li><a href=#712--4485-a-random-ensemble-of-encrypted-vision-transformers-for-adversarially-robust-defense-ryota-iijima-et-al-2024>(7/12 | 44/85) A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense (Ryota Iijima et al., 2024)</a></li><li><a href=#812--4585-synergizing-spatial-optimization-with-large-language-models-for-open-domain-urban-itinerary-planning-yihong-tang-et-al-2024>(8/12 | 45/85) Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning (Yihong Tang et al., 2024)</a></li><li><a href=#912--4685-stitching-sub-trajectories-with-conditional-diffusion-model-for-goal-conditioned-offline-rl-sungyoon-kim-et-al-2024>(9/12 | 46/85) Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL (Sungyoon Kim et al., 2024)</a></li><li><a href=#1012--4785-the-reasons-that-agents-act-intention-and-instrumental-goals-francis-rhys-ward-et-al-2024>(10/12 | 47/85) The Reasons that Agents Act: Intention and Instrumental Goals (Francis Rhys Ward et al., 2024)</a></li><li><a href=#1112--4885-social-evolution-of-published-text-and-the-emergence-of-artificial-intelligence-through-large-language-models-and-the-problem-of-toxicity-and-bias-arifa-khan-et-al-2024>(11/12 | 48/85) Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias (Arifa Khan et al., 2024)</a></li><li><a href=#1212--4985-link-aware-link-prediction-over-temporal-graph-by-pattern-recognition-bingqing-liu-et-al-2024>(12/12 | 49/85) Link-aware link prediction over temporal graph by pattern recognition (Bingqing Liu et al., 2024)</a></li></ul></li><li><a href=#csro-3>cs.RO (3)</a><ul><li><a href=#13--5085-does-chatgpt-and-whisper-make-humanoid-robots-more-relatable-xiaohui-chen-et-al-2024>(1/3 | 50/85) Does ChatGPT and Whisper Make Humanoid Robots More Relatable? (Xiaohui Chen et al., 2024)</a></li><li><a href=#23--5185-learning-by-watching-a-review-of-video-based-learning-approaches-for-robot-manipulation-chrisantus-eze-et-al-2024>(2/3 | 51/85) Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation (Chrisantus Eze et al., 2024)</a></li><li><a href=#33--5285-clipper-robust-data-association-without-an-initial-guess-parker-c-lusk-et-al-2024>(3/3 | 52/85) CLIPPER: Robust Data Association without an Initial Guess (Parker C. Lusk et al., 2024)</a></li></ul></li><li><a href=#cond-matsoft-1>cond-mat.soft (1)</a><ul><li><a href=#11--5385-x-lora-mixture-of-low-rank-adapter-experts-a-flexible-framework-for-large-language-models-with-applications-in-protein-mechanics-and-design-eric-l-buehler-et-al-2024>(1/1 | 53/85) X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design (Eric L. Buehler et al., 2024)</a></li></ul></li><li><a href=#cscv-12>cs.CV (12)</a><ul><li><a href=#112--5485-open-ended-vqa-benchmarking-of-vision-language-models-by-exploiting-classification-datasets-and-their-semantic-hierarchy-simon-ging-et-al-2024>(1/12 | 54/85) Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy (Simon Ging et al., 2024)</a></li><li><a href=#212--5585-a-benchmark-for-multi-modal-foundation-models-on-low-level-vision-from-single-images-to-pairs-zicheng-zhang-et-al-2024>(2/12 | 55/85) A Benchmark for Multi-modal Foundation Models on Low-level Vision: from Single Images to Pairs (Zicheng Zhang et al., 2024)</a></li><li><a href=#312--5685-outlier-aware-training-for-low-bit-quantization-of-structural-re-parameterized-networks-muqun-niu-et-al-2024>(3/12 | 56/85) Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks (Muqun Niu et al., 2024)</a></li><li><a href=#412--5785-two-stage-multi-task-self-supervised-learning-for-medical-image-segmentation-binyan-hu-et-al-2024>(4/12 | 57/85) Two-Stage Multi-task Self-Supervised Learning for Medical Image Segmentation (Binyan Hu et al., 2024)</a></li><li><a href=#512--5885-the-bias-of-harmful-label-associations-in-vision-language-models-caner-hazirbas-et-al-2024>(5/12 | 58/85) The Bias of Harmful Label Associations in Vision-Language Models (Caner Hazirbas et al., 2024)</a></li><li><a href=#612--5985-towards-explainable-safe-autonomous-driving-with-language-embeddings-for-novelty-identification-and-active-learning-framework-and-experimental-analysis-with-real-world-data-sets-ross-greer-et-al-2024>(6/12 | 59/85) Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets (Ross Greer et al., 2024)</a></li><li><a href=#712--6085-lisr-learning-linear-3d-implicit-surface-representation-using-compactly-supported-radial-basis-functions-atharva-pandey-et-al-2024>(7/12 | 60/85) LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions (Atharva Pandey et al., 2024)</a></li><li><a href=#812--6185-gala3d-towards-text-to-3d-complex-scene-generation-via-layout-guided-generative-gaussian-splatting-xiaoyu-zhou-et-al-2024>(8/12 | 61/85) GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting (Xiaoyu Zhou et al., 2024)</a></li><li><a href=#912--6285-deep-learning-for-medical-image-segmentation-with-imprecise-annotation-binyan-hu-et-al-2024>(9/12 | 62/85) Deep Learning for Medical Image Segmentation with Imprecise Annotation (Binyan Hu et al., 2024)</a></li><li><a href=#1012--6385-trade-off-between-spatial-and-angular-resolution-in-facial-recognition-muhammad-zeshan-alam-et-al-2024>(10/12 | 63/85) Trade-off Between Spatial and Angular Resolution in Facial Recognition (Muhammad Zeshan Alam et al., 2024)</a></li><li><a href=#1112--6485-pivot-net-heterogeneous-point-voxel-tree-based-framework-for-point-cloud-compression-jiahao-pang-et-al-2024>(11/12 | 64/85) PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression (Jiahao Pang et al., 2024)</a></li><li><a href=#1212--6585-a-novel-spatial-frequency-domain-network-for-zero-shot-incremental-learning-jie-ren-et-al-2024>(12/12 | 65/85) A novel spatial-frequency domain network for zero-shot incremental learning (Jie Ren et al., 2024)</a></li></ul></li><li><a href=#cshc-2>cs.HC (2)</a><ul><li><a href=#12--6685-insights-into-natural-language-database-query-errors-from-attention-misalignment-to-user-handling-strategies-zheng-ning-et-al-2024>(1/2 | 66/85) Insights into Natural Language Database Query Errors: From Attention Misalignment to User Handling Strategies (Zheng Ning et al., 2024)</a></li><li><a href=#22--6785-emowear-exploring-emotional-teasers-for-voice-message-interaction-on-smartwatches-pengcheng-an-et-al-2024>(2/2 | 67/85) EmoWear: Exploring Emotional Teasers for Voice Message Interaction on Smartwatches (Pengcheng An et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--6885-ml-framework-for-wireless-mac-protocol-design-navid-keshtiarast-et-al-2024>(1/1 | 68/85) ML Framework for Wireless MAC Protocol Design (Navid Keshtiarast et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--6985-joint-source-channel-coding-for-wireless-image-transmission-a-deep-compressed-sensing-based-method-mohammad-amin-jarrahi-et-al-2024>(1/1 | 69/85) Joint Source-Channel Coding for Wireless Image Transmission: A Deep Compressed-Sensing Based Method (Mohammad Amin Jarrahi et al., 2024)</a></li></ul></li><li><a href=#q-biogn-1>q-bio.GN (1)</a><ul><li><a href=#11--7085-highly-accurate-disease-diagnosis-and-highly-reproducible-biomarker-identification-with-pathformer-zehao-dong-et-al-2024>(1/1 | 70/85) Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer (Zehao Dong et al., 2024)</a></li></ul></li><li><a href=#cscr-1>cs.CR (1)</a><ul><li><a href=#11--7185-differentially-private-training-of-mixture-of-experts-models-pierre-tholoniat-et-al-2024>(1/1 | 71/85) Differentially Private Training of Mixture of Experts Models (Pierre Tholoniat et al., 2024)</a></li></ul></li><li><a href=#statml-4>stat.ML (4)</a><ul><li><a href=#14--7285-self-consistent-conformal-prediction-lars-van-der-laan-et-al-2024>(1/4 | 72/85) Self-Consistent Conformal Prediction (Lars van der Laan et al., 2024)</a></li><li><a href=#24--7385-resampling-methods-for-private-statistical-inference-karan-chadha-et-al-2024>(2/4 | 73/85) Resampling methods for Private Statistical Inference (Karan Chadha et al., 2024)</a></li><li><a href=#34--7485-optimal-thresholding-linear-bandit-eduardo-ochoa-rivera-et-al-2024>(3/4 | 74/85) Optimal Thresholding Linear Bandit (Eduardo Ochoa Rivera et al., 2024)</a></li><li><a href=#44--7585-improving-lsh-via-tensorized-random-projection-bhisham-dev-verma-et-al-2024>(4/4 | 75/85) Improving LSH via Tensorized Random Projection (Bhisham Dev Verma et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--7685-effort-and-size-estimation-in-software-projects-with-large-language-model-based-intelligent-interfaces-claudionor-n-coelho-jr-et-al-2024>(1/3 | 76/85) Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces (Claudionor N. Coelho Jr et al., 2024)</a></li><li><a href=#23--7785-unprecedented-code-change-automation-the-fusion-of-llms-and-transformation-by-example-malinda-dilhara-et-al-2024>(2/3 | 77/85) Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example (Malinda Dilhara et al., 2024)</a></li><li><a href=#33--7885-on-the-effectiveness-of-machine-learning-based-call-graph-pruning-an-empirical-study-amir-m-mir-et-al-2024>(3/3 | 78/85) On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study (Amir M. Mir et al., 2024)</a></li></ul></li><li><a href=#mathna-1>math.NA (1)</a><ul><li><a href=#11--7985-codpy-a-python-library-for-numerics-machine-learning-and-statistics-philippe-g-lefloch-et-al-2024>(1/1 | 79/85) CodPy: a Python library for numerics, machine learning, and statistics (Philippe G. LeFloch et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--8085-intent-based-access-control-using-llms-to-intelligently-manage-access-control-pranav-subramaniam-et-al-2024>(1/1 | 80/85) Intent-Based Access Control: Using LLMs to Intelligently Manage Access Control (Pranav Subramaniam et al., 2024)</a></li></ul></li><li><a href=#eesssy-1>eess.SY (1)</a><ul><li><a href=#11--8185-a-review-of-the-gic-blocker-placement-problem-arthur-k-barnes-et-al-2024>(1/1 | 81/85) A Review of the GIC Blocker Placement Problem (Arthur K. Barnes et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--8285-optimizing-genetically-driven-synaptogenesis-tommaso-boccato-et-al-2024>(1/2 | 82/85) Optimizing Genetically-Driven Synaptogenesis (Tommaso Boccato et al., 2024)</a></li><li><a href=#22--8385-sais-a-novel-bio-inspired-artificial-immune-system-based-on-symbiotic-paradigm-junhao-song-et-al-2024>(2/2 | 83/85) SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic Paradigm (Junhao Song et al., 2024)</a></li></ul></li><li><a href=#csir-1>cs.IR (1)</a><ul><li><a href=#11--8485-enhancing-multi-field-b2b-cloud-solution-matching-via-contrastive-pre-training-haonan-chen-et-al-2024>(1/1 | 84/85) Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training (Haonan Chen et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--8585-fundamental-problems-on-bounded-treewidth-graphs-the-real-source-of-hardness-barış-can-esmer-et-al-2024>(1/1 | 85/85) Fundamental Problems on Bounded-Treewidth Graphs: The Real Source of Hardness (Barış Can Esmer et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>