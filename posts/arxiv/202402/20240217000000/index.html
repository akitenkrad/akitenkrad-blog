<!doctype html><html><head><title>arXiv @ 2024.02.17</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.17"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cond-mat.dis-nn (1) cs.AI (24) cs.AR (2) cs.CE (1) cs.CG (1) cs.CL (49) cs.CR (5) cs.CV (29) cs.CY (1) cs.DC (1) cs.DS (4) cs.GT (1) cs.HC (6) cs.IR (3) cs.IT (3) cs.LG (66) cs.NE (3) cs.NI (1) cs.OS (1) cs.RO (10) cs.SC (1) cs.SD (4) cs.SE (2) cs.SI (2) eess.IV (7) eess.SP (1) eess.SY (1) math.NA (1) math.OC (3) physics.ao-ph (1) physics.app-ph (1) physics.chem-ph (1) physics.soc-ph (1) q-bio."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240217000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-17T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-17T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.17"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240217000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Feb 17, 2024</p></div><div class=title><h1>arXiv @ 2024.02.17</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#cond-matdis-nn-1>cond-mat.dis-nn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csai-24>cs.AI (24)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#cscl-49>cs.CL (49)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#cscr-5>cs.CR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#cscv-29>cs.CV (29)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csds-4>cs.DS (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#cshc-6>cs.HC (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csir-3>cs.IR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csit-3>cs.IT (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#cslg-66>cs.LG (66)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csne-3>cs.NE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csos-1>cs.OS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csro-10>cs.RO (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#cssc-1>cs.SC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#cssd-4>cs.SD (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#csse-2>cs.SE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#eessiv-7>eess.IV (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#eesssy-1>eess.SY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#mathna-1>math.NA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#mathoc-3>math.OC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#physicsao-ph-1>physics.ao-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#physicsapp-ph-1>physics.app-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#physicschem-ph-1>physics.chem-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#q-biogn-2>q-bio.GN (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#q-fincp-1>q-fin.CP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#quant-ph-2>quant-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>2</td><td></td><td>1</td><td>1</td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Alpaca</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Automatic Evaluation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>BERTScore</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Benchmarking</td><td>7</td><td>15</td><td>7</td><td>16</td><td></td></tr><tr><td>Black Box</td><td>2</td><td>2</td><td></td><td>3</td><td></td></tr><tr><td>ChatGPT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>2</td><td></td><td>3</td><td></td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Content Detection</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>7</td><td>1</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>10</td><td>1</td><td></td></tr><tr><td>Counter-factual</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fake News Detection</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td>2</td><td></td><td>2</td><td>8</td><td></td></tr><tr><td>Few-shot</td><td>2</td><td>2</td><td>1</td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>4</td><td>15</td><td>4</td><td>9</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td>1</td><td>2</td><td>1</td></tr><tr><td>GPT</td><td>4</td><td>6</td><td></td><td>1</td><td></td></tr><tr><td>GPT-3</td><td>2</td><td>2</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>2</td><td>2</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>2</td><td>6</td><td></td><td>1</td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Generative AI</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Generative Adversarial Network</td><td>1</td><td></td><td>4</td><td>4</td><td></td></tr><tr><td>Geometry</td><td>1</td><td></td><td>2</td><td></td><td>1</td></tr><tr><td>Grammatical Error Correction</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph</td><td>3</td><td>2</td><td>2</td><td>11</td><td>1</td></tr><tr><td>Graph Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Embedding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td>1</td><td></td><td>3</td><td>6</td><td></td></tr><tr><td>Grounding</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Image2text</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>2</td><td>13</td><td></td><td>1</td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>4</td><td></td><td>1</td><td></td></tr><tr><td>Knowledge Graph</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td></td><td>5</td><td></td><td>1</td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Language Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>23</td><td>61</td><td>2</td><td>22</td><td>2</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Low-Resource</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>MNIST</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Message-Passing</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Mistral</td><td></td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Model Compression</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>3</td><td>5</td><td>6</td><td>4</td><td></td></tr><tr><td>Mutual Information</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Natural Language Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Node Embedding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Open-Domain Question Answering</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Out-of-domain</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Perplexity</td><td>1</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Prompt</td><td>6</td><td>12</td><td>5</td><td>3</td><td>1</td></tr><tr><td>Prompt Learning</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Quantization</td><td></td><td>4</td><td></td><td>4</td><td></td></tr><tr><td>Question Answering</td><td></td><td>8</td><td>1</td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>4</td><td>6</td><td></td><td>3</td><td></td></tr><tr><td>Recommendation</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Recommender System</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>3</td><td>5</td><td></td></tr><tr><td>Reinforcement Learning</td><td>4</td><td>1</td><td></td><td>15</td><td></td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td>2</td><td></td><td>5</td><td></td></tr><tr><td>Representation Learning</td><td>1</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td>6</td><td></td><td></td><td></td></tr><tr><td>RoBERTa</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Rouge-L</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Self-Distillation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>1</td><td>3</td><td>2</td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td></td><td>6</td><td>4</td></tr><tr><td>Simulator</td><td></td><td></td><td></td><td>6</td><td>4</td></tr><tr><td>Stemming</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>1</td><td>3</td><td>7</td><td>1</td></tr><tr><td>Text Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Text Embedding</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Mining</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>3</td><td>2</td><td></td></tr><tr><td>Textual Entailment</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Transformer</td><td>3</td><td>5</td><td>4</td><td>5</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>2</td><td>1</td><td>2</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td>2</td><td>1</td><td>1</td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>3</td><td>2</td><td>2</td><td>2</td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td></td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-49>cs.CL (49)</h2><h3 id=149--1249-prompt-based-bias-calibration-for-better-zerofew-shot-learning-of-language-models-kang-he-et-al-2024>(1/49 | 1/249) Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models (Kang He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kang He, Yinghan Long, Kaushik Roy. (2024)<br><strong>Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models</strong><br><button class=copy-to-clipboard title="Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 100<br>Keywords: Fairness, Fine-tuning, GPT, GPT-4, Sentiment Analysis, In-context Learning, In-context Learning, Pre-trained Language Model, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10353v1.pdf filename=2402.10353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt</b> <b>learning</b> is susceptible to intrinsic bias present in <b>pre-trained</b> <b>language</b> <b>models</b> (LMs), resulting in sub-optimal performance of <b>prompt-based</b> <b>zero/few-shot</b> learning. In this work, we propose a null-input <b>prompting</b> <b>method</b> to calibrate intrinsic bias encoded in <b>pre-trained</b> <b>LMs.</b> <b>Different</b> from prior efforts that address intrinsic bias primarily for social <b>fairness</b> and often involve excessive computational cost, our objective is to explore enhancing LMs&rsquo; performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from <b>GPT-4</b> to <b>prompt</b> <b>pre-trained</b> <b>LMs</b> <b>for</b> intrinsic bias probing. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1%$ of total parameters) of LMs towards equal probability distribution. Experimental results show that the calibration promotes an equitable starting point for LMs while preserving language modeling abilities. Across a wide range of datasets, including <b>sentiment</b> <b>analysis</b> and topic classification, our method significantly improves zero/few-shot learning performance of LMs for both <b>in-context</b> <b>learning</b> and <b>prompt-based</b> <b>fine-tuning</b> (on average $9%$ and $2%$, respectively).</p></p class="citation"></blockquote><h3 id=249--2249-self-augmented-in-context-learning-for-unsupervised-word-translation-yaoyiran-li-et-al-2024>(2/49 | 2/249) Self-Augmented In-Context Learning for Unsupervised Word Translation (Yaoyiran Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaoyiran Li, Anna Korhonen, Ivan Vulić. (2024)<br><strong>Self-Augmented In-Context Learning for Unsupervised Word Translation</strong><br><button class=copy-to-clipboard title="Self-Augmented In-Context Learning for Unsupervised Word Translation" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Few-shot, Unsupervised Learning, Zero-shot, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10024v1.pdf filename=2402.10024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has shown that, while <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in <b>few-shot</b> setups, they still cannot match the performance of &rsquo;traditional&rsquo; mapping-based approaches in the <b>unsupervised</b> scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with <b>LLMs,</b> we propose self-augmented <b>in-context</b> <b>learning</b> (SAIL) for <b>unsupervised</b> BLI: starting from a <b>zero-shot</b> <b>prompt,</b> SAIL iteratively induces a set of high-confidence word translation pairs for <b>in-context</b> <b>learning</b> <b>(ICL)</b> from an <b>LLM,</b> which it then reapplies to the same <b>LLM</b> in the <b>ICL</b> fashion. Our method shows substantial gains over <b>zero-shot</b> <b>prompting</b> of <b>LLMs</b> on two established BLI <b>benchmarks</b> spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art <b>unsupervised</b> BLI performance, we also conduct comprehensive analyses on SAIL and discuss its limitations.</p></p class="citation"></blockquote><h3 id=349--3249-unlocking-structure-measuring-introducing-pdd-an-automatic-metric-for-positional-discourse-coherence-yinhong-liu-et-al-2024>(3/49 | 3/249) Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence (Yinhong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinhong Liu, Yixuan Su, Ehsan Shareghi, Nigel Collier. (2024)<br><strong>Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence</strong><br><button class=copy-to-clipboard title="Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Automatic Evaluation, GPT, GPT-4, Text Generation, BERTScore, BLEU, Large Language Model, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10175v1.pdf filename=2402.10175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown remarkable performance in aligning generated <b>text</b> <b>with</b> user intentions across various tasks. When it comes to long-form <b>text</b> <b>generation,</b> there has been a growing interest in generation from a discourse coherence perspective. However, existing lexical or semantic metrics such as <b>BLEU,</b> <b>ROUGE,</b> <b>BertScore</b> cannot effectively capture the discourse coherence. The development of discourse-specific <b>automatic</b> <b>evaluation</b> methods for assessing the output of <b>LLMs</b> warrants greater focus and exploration. In this paper, we present a novel <b>automatic</b> <b>metric</b> designed to quantify the discourse divergence between two long-form articles. Extensive experiments on three datasets from representative domains demonstrate that our metric aligns more closely with human preferences and <b>GPT-4</b> coherence evaluation, outperforming existing evaluation methods.</p></p class="citation"></blockquote><h3 id=449--4249-rs-dpo-a-hybrid-rejection-sampling-and-direct-preference-optimization-method-for-alignment-of-large-language-models-saeed-khaki-et-al-2024>(4/49 | 4/249) RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models (Saeed Khaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, Prathap Ramachandra. (2024)<br><strong>RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models</strong><br><button class=copy-to-clipboard title="RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Supervised Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10038v1.pdf filename=2402.10038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF)</b> has been extensively employed to align <b>large</b> <b>language</b> <b>models</b> with user intent. However, proximal policy optimization (PPO) based <b>RLHF</b> is occasionally unstable requiring significant hyperparameter <b>finetuning,</b> and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative <b>LLM,</b> instead of the policy model, limiting the effectiveness of the <b>RLHF.</b> In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a <b>supervised</b> <b>fine-tuned</b> policy model (SFT). A varied set of k responses per <b>prompt</b> are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively <b>fine-tunes</b> <b>LLMs</b> with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.</p></p class="citation"></blockquote><h3 id=549--5249-pal-proxy-guided-black-box-attack-on-large-language-models-chawin-sitawarin-et-al-2024>(5/49 | 5/249) PAL: Proxy-Guided Black-Box Attack on Large Language Models (Chawin Sitawarin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chawin Sitawarin, Norman Mu, David Wagner, Alexandre Araujo. (2024)<br><strong>PAL: Proxy-Guided Black-Box Attack on Large Language Models</strong><br><button class=copy-to-clipboard title="PAL: Proxy-Guided Black-Box Attack on Large Language Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 85<br>Keywords: Black Box, Fine-tuning, GPT, GPT-3, GPT-3.5, LLaMA, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09674v1.pdf filename=2402.09674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety <b>fine-tuning</b> aim to minimize harmful use, recent works have shown that <b>LLMs</b> remain vulnerable to attacks that elicit toxic responses. In this work, we introduce the Proxy-Guided Attack on <b>LLMs</b> (PAL), the first optimization-based attack on <b>LLMs</b> in a <b>black-box</b> <b>query-only</b> setting. In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world <b>LLM</b> APIs. Our attack achieves 84% attack success rate <b>(ASR)</b> on <b>GPT-3.5-Turbo</b> and 48% on <b>Llama-2-7B,</b> compared to 4% for the current state of the art. We also propose GCG++, an improvement to the GCG attack that reaches 94% <b>ASR</b> on white-box <b>Llama-2-7B,</b> and the Random-Search Attack on <b>LLMs</b> (RAL), a strong but simple baseline for query-based attacks. We believe the techniques proposed in this work will enable more comprehensive safety testing of <b>LLMs</b> and, in the long term, the development of better security guardrails. The code can be found at <a href=https://github.com/chawins/pal>https://github.com/chawins/pal</a>.</p></p class="citation"></blockquote><h3 id=649--6249-answer-is-all-you-need-instruction-following-text-embedding-via-answering-the-question-letian-peng-et-al-2024>(6/49 | 6/249) Answer is All You Need: Instruction-following Text Embedding via Answering the Question (Letian Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, Jingbo Shang. (2024)<br><strong>Answer is All You Need: Instruction-following Text Embedding via Answering the Question</strong><br><button class=copy-to-clipboard title="Answer is All You Need: Instruction-following Text Embedding via Answering the Question" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Clustering, Fine-tuning, LLaMA, RoBERTa, Instruction Following, Question Answering, Large Language Model, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09642v1.pdf filename=2402.09642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work aims to build a <b>text</b> <b>embedder</b> that can capture characteristics of <b>texts</b> <b>specified</b> by user <b>instructions.</b> <b>Despite</b> its tremendous potential to deploy user-oriented embeddings, none of previous approaches provides a concrete solution for it. This paper offers a new viewpoint, which treats the <b>instruction</b> <b>as</b> a <b>question</b> <b>about</b> the input <b>text</b> <b>and</b> encodes the expected answers to obtain the representation accordingly. Intuitively, <b>texts</b> <b>with</b> the same (implicit) semantics would share similar answers following the <b>instruction,</b> <b>thus</b> leading to more similar embeddings. Specifically, we propose InBedder that instantiates this embed-via-answering idea by only <b>fine-tuning</b> language models on abstractive <b>question</b> <b>answering</b> tasks. InBedder demonstrates significantly improved <b>instruction-following</b> <b>capabilities</b> according to our proposed <b>instruction</b> <b>awareness</b> tests and <b>instruction</b> <b>robustness</b> tests, when applied to both <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> (e.g., <b>llama-2-7b)</b> and smaller encoder-based LMs (e.g., <b>roberta-large).</b> Additionally, our qualitative analysis of <b>clustering</b> outcomes, achieved by applying different <b>instructions</b> <b>to</b> the same corpus, demonstrates a high degree of interpretability.</p></p class="citation"></blockquote><h3 id=749--7249-selective-reflection-tuning-student-selected-data-recycling-for-llm-instruction-tuning-ming-li-et-al-2024>(7/49 | 7/249) Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning (Ming Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, Tianyi Zhou. (2024)<br><strong>Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning</strong><br><button class=copy-to-clipboard title="Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Data Augmentation, Fine-tuning, Fine-tuning, Alpaca, Instruction Following, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10110v1.pdf filename=2402.10110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> is critical to <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for achieving better <b>instruction</b> <b>following</b> and task adaptation capabilities but its success heavily relies on the training <b>data</b> <b>quality.</b> Many recent methods focus on improving the <b>data</b> <b>quality</b> but often overlook the compatibility of the <b>data</b> <b>with</b> the student model being <b>finetuned.</b> This paper introduces Selective Reflection-Tuning, a novel paradigm that synergizes a teacher <b>LLM&rsquo;s</b> reflection and introspection for improving existing <b>data</b> <b>quality</b> with the <b>data</b> <b>selection</b> capability of the student <b>LLM,</b> to automatically refine existing <b>instruction-tuning</b> <b>data.</b> <b>This</b> teacher-student collaboration produces high-quality and student-compatible <b>instruction-response</b> <b>pairs,</b> resulting in sample-efficient <b>instruction</b> <b>tuning</b> and <b>LLMs</b> of superior performance. Selective Reflection-Tuning is a <b>data</b> <b>augmentation</b> and synthesis that generally improves <b>LLM</b> <b>finetuning</b> and self-improvement without collecting brand-new <b>data.</b> <b>We</b> apply our method to <b>Alpaca</b> and WizardLM <b>data</b> <b>and</b> achieve much stronger and top-tier 7B and 13B <b>LLMs.</b> Our codes, models, and <b>data</b> <b>will</b> be released at <a href=https://github.com/tianyi-lab/Reflection_Tuning>https://github.com/tianyi-lab/Reflection_Tuning</a>.</p></p class="citation"></blockquote><h3 id=849--8249-model-compression-and-efficient-inference-for-large-language-models-a-survey-wenxiao-wang-et-al-2024>(8/49 | 8/249) Model Compression and Efficient Inference for Large Language Models: A Survey (Wenxiao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, Xiaofei He. (2024)<br><strong>Model Compression and Efficient Inference for Large Language Models: A Survey</strong><br><button class=copy-to-clipboard title="Model Compression and Efficient Inference for Large Language Models: A Survey" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs-PF, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Knowledge Distillation, Knowledge Distillation, Model Compression, Pruning, Quantization, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09748v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09748v1.pdf filename=2402.09748v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer</b> based <b>large</b> <b>language</b> <b>models</b> <b>have</b> achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy <b>large</b> <b>models</b> <b>on</b> resource-constrained devices. In this paper, we investigate compression and efficient inference methods for <b>large</b> <b>language</b> <b>models</b> <b>from</b> an algorithmic perspective. Regarding taxonomy, similar to smaller <b>models,</b> <b>compression</b> and acceleration algorithms for <b>large</b> <b>language</b> <b>models</b> <b>can</b> still be categorized into <b>quantization,</b> <b>pruning,</b> <b>distillation,</b> compact architecture design, dynamic networks. However, <b>Large</b> <b>language</b> <b>models</b> <b>have</b> two prominent characteristics compared to smaller <b>models:</b> <b>(1)</b> Most of compression algorithms require <b>finetuning</b> or even retraining the <b>model</b> <b>after</b> compression. The most notable aspect of <b>large</b> <b>models</b> <b>is</b> the very high cost associated with <b>model</b> <b>finetuning</b> or training. Therefore, many algorithms for <b>large</b> <b>models,</b> <b>such</b> as <b>quantization</b> and <b>pruning,</b> start to explore tuning-free algorithms. (2) <b>Large</b> <b>models</b> <b>emphasize</b> versatility and generalization rather than performance on a single task. Hence, many algorithms, such as <b>knowledge</b> <b>distillation,</b> focus on how to preserving their versatility and generalization after compression. Since these two characteristics were not very pronounced in early <b>large</b> <b>models,</b> <b>we</b> further distinguish <b>large</b> <b>language</b> <b>models</b> <b>into</b> medium <b>models</b> <b>and</b> ``real&rsquo;&rsquo; <b>large</b> <b>models.</b> <b>Additionally,</b> we also provide an introduction to some mature frameworks for efficient inference of <b>large</b> <b>models,</b> <b>which</b> can support basic compression or acceleration algorithms, greatly facilitating <b>model</b> <b>deployment</b> for users.</p></p class="citation"></blockquote><h3 id=949--9249-entaile-introducing-textual-entailment-in-commonsense-knowledge-graph-completion-ying-su-et-al-2024>(9/49 | 9/249) EntailE: Introducing Textual Entailment in Commonsense Knowledge Graph Completion (Ying Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Su, Tianqing Fang, Huiru Xiao, Weiqi Wang, Yangqiu Song, Tong Zhang, Lei Chen. (2024)<br><strong>EntailE: Introducing Textual Entailment in Commonsense Knowledge Graph Completion</strong><br><button class=copy-to-clipboard title="EntailE: Introducing Textual Entailment in Commonsense Knowledge Graph Completion" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 78<br>Keywords: Graph, Graph Embedding, Node Embedding, Fine-tuning, Knowledge Graph, Transformer, Natural Language Inference, Natural Language Inference, Textual Entailment<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09666v1.pdf filename=2402.09666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Commonsense <b>knowledge</b> <b>graph</b> <b>completion</b> is a new challenge for commonsense <b>knowledge</b> <b>graph</b> <b>construction</b> and application. In contrast to factual <b>knowledge</b> <b>graphs</b> <b>such</b> as Freebase and YAGO, commonsense <b>knowledge</b> <b>graphs</b> <b>(CSKGs;</b> e.g., ConceptNet) utilize free-form text to represent named entities, short phrases, and events as their <b>nodes.</b> <b>Such</b> a loose structure results in large and sparse CSKGs, which makes the semantic understanding of these <b>nodes</b> <b>more</b> critical for learning rich commonsense <b>knowledge</b> <b>graph</b> <b>embedding.</b> While current methods leverage semantic similarities to increase the <b>graph</b> <b>density,</b> the semantic plausibility of the <b>nodes</b> <b>and</b> their relations are under-explored. Previous works adopt conceptual abstraction to improve the consistency of modeling (event) plausibility, but they are not scalable enough and still suffer from data sparsity. In this paper, we propose to adopt <b>textual</b> <b>entailment</b> to find implicit entailment relations between CSKG <b>nodes,</b> <b>to</b> effectively densify the subgraph connecting <b>nodes</b> <b>within</b> the same conceptual class, which indicates a similar level of plausibility. Each <b>node</b> <b>in</b> CSKG finds its top entailed <b>nodes</b> <b>using</b> a <b>finetuned</b> <b>transformer</b> over <b>natural</b> <b>language</b> <b>inference</b> <b>(NLI)</b> tasks, which sufficiently capture <b>textual</b> <b>entailment</b> signals. The entailment relation between these <b>nodes</b> <b>are</b> further utilized to: 1) build new connections between source triplets and entailed <b>nodes</b> <b>to</b> densify the sparse CSKGs; 2) enrich the generalization ability of <b>node</b> <b>representations</b> by comparing the <b>node</b> <b>embeddings</b> with a contrastive loss. Experiments on two standard CSKGs demonstrate that our proposed framework EntailE can improve the performance of CSKG completion tasks under both transductive and inductive settings.</p></p class="citation"></blockquote><h3 id=1049--10249-lapdoc-layout-aware-prompting-for-documents-marcel-lamott-et-al-2024>(10/49 | 10/249) LAPDoc: Layout-Aware Prompting for Documents (Marcel Lamott et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcel Lamott, Yves-Noel Weweler, Adrian Ulges, Faisal Shafait, Dirk Krechel, Darko Obradovic. (2024)<br><strong>LAPDoc: Layout-Aware Prompting for Documents</strong><br><button class=copy-to-clipboard title="LAPDoc: Layout-Aware Prompting for Documents" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 76<br>Keywords: Optical Character Recognition, Benchmarking, Fine-tuning, Multi-modal, ChatGPT, Transformer, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09841v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09841v1.pdf filename=2402.09841v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in training <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> using massive amounts of solely textual data lead to strong generalization across many domains and tasks, including document-specific tasks. Opposed to that there is a trend to train <b>multi-modal</b> <b>transformer</b> architectures tailored for document understanding that are designed specifically to fuse textual inputs with the corresponding document layout. This involves a separate <b>fine-tuning</b> step for which additional training data is required. At present, no document <b>transformers</b> with comparable generalization to <b>LLMs</b> are available That raises the question which type of model is to be preferred for document understanding tasks. In this paper we investigate the possibility to use purely text-based <b>LLMs</b> for document-specific tasks by using layout enrichment. We explore drop-in modifications and rule-based methods to enrich purely textual <b>LLM</b> <b>prompts</b> with layout information. In our experiments we investigate the effects on the commercial <b>ChatGPT</b> model and the open-source <b>LLM</b> Solar. We demonstrate that using our approach both <b>LLMs</b> show improved performance on various standard document <b>benchmarks.</b> In addition, we study the impact of noisy <b>OCR</b> and layout errors, as well as the limitations of <b>LLMs</b> when it comes to utilizing document layout. Our results indicate that layout enrichment can improve the performance of purely text-based <b>LLMs</b> for document understanding by up to 15% compared to just using plain document text. In conclusion, this approach should be considered for the best model choice between text-based <b>LLM</b> or <b>multi-modal</b> document <b>transformers.</b></p></p class="citation"></blockquote><h3 id=1149--11249-biomistral-a-collection-of-open-source-pretrained-large-language-models-for-medical-domains-yanis-labrak-et-al-2024>(11/49 | 11/249) BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains (Yanis Labrak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, Richard Dufour. (2024)<br><strong>BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains</strong><br><button class=copy-to-clipboard title="BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Foundation Model, Quantization, Mistral, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10373v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10373v1.pdf filename=2402.10373v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source <b>LLMs</b> tailored for health contexts, adapting general-purpose <b>LLMs</b> to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source <b>LLM</b> tailored for the biomedical domain, utilizing <b>Mistral</b> as its <b>foundation</b> <b>model</b> and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a <b>benchmark</b> comprising 10 established medical <b>question-answering</b> <b>(QA)</b> tasks in English. We also explore lightweight models obtained through <b>quantization</b> and model merging approaches. Our results demonstrate BioMistral&rsquo;s superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address the limited availability of data beyond English and to assess the multilingual generalization of medical <b>LLMs,</b> we automatically translated and evaluated this <b>benchmark</b> into 7 other languages. This marks the first <b>large-scale</b> <b>multilingual</b> <b>evaluation</b> of <b>LLMs</b> in the medical domain. Datasets, multilingual evaluation <b>benchmarks,</b> scripts, and all the models obtained during our experiments are freely released.</p></p class="citation"></blockquote><h3 id=1249--12249-openmathinstruct-1-a-18-million-math-instruction-tuning-dataset-shubham-toshniwal-et-al-2024>(12/49 | 12/249) OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset (Shubham Toshniwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman. (2024)<br><strong>OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset</strong><br><button class=copy-to-clipboard title="OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, GPT-4, Reasoning, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10176v1.pdf filename=2402.10176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has shown the immense potential of synthetically generated datasets for training <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> especially for acquiring targeted skills. Current <b>large-scale</b> <b>math</b> <b>instruction</b> <b>tuning</b> datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source <b>LLMs</b> with commercially restrictive licenses. A key reason limiting the use of open-source <b>LLMs</b> in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source <b>LLMs,</b> such as <b>GPT-4,</b> and the best open-source <b>LLMs.</b> Building on the recent progress in open-source <b>LLMs,</b> our proposed <b>prompting</b> novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math <b>instruction</b> <b>tuning</b> dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math <b>reasoning</b> <b>benchmarks,</b> using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best <b>gpt-distilled</b> models. We release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.</p></p class="citation"></blockquote><h3 id=1349--13249-quantized-embedding-vectors-for-controllable-diffusion-language-models-cheng-kang-et-al-2024>(13/49 | 13/249) Quantized Embedding Vectors for Controllable Diffusion Language Models (Cheng Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Kang, Xinye Chen, Yong Hu, Daniel Novak. (2024)<br><strong>Quantized Embedding Vectors for Controllable Diffusion Language Models</strong><br><button class=copy-to-clipboard title="Quantized Embedding Vectors for Controllable Diffusion Language Models" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Quantization, Quantization, Language Generation, Natural Language Generation, Text Generation, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10107v1.pdf filename=2402.10107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Improving the controllability, portability, and inference speed of diffusion <b>language</b> <b>models</b> (DLMs) is a key challenge in <b>natural</b> <b>language</b> <b>generation.</b> While recent research has shown significant success in complex <b>text</b> <b>generation</b> with <b>language</b> <b>models,</b> the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models. To mitigate these issues, numerous well-established methods were proposed for neural network <b>quantization.</b> To further enhance their portability of independent deployment as well as improve their stability evaluated by <b>language</b> <b>perplexity,</b> we propose a novel approach called the <b>Quantized</b> Embedding Controllable Diffusion <b>Language</b> <b>Model</b> (QE-CDLM). QE-CDLM builds upon the recent successful controllable DLMs by remodeling the task-specific embedding space via <b>quantization.</b> This leads to a gradient-based controller for the generation tasks, and more stable intermediate latent variables are obtained, which naturally brings in an accelerated convergence as well as better controllability. Additionally, the adaption <b>fine-tuning</b> method is employed to reduce tunable weights. Experimental results on five challenging fine-grained control tasks demonstrate that QE-CDLM compares favorably to existing methods in terms of quality and feasibility, achieving better <b>perplexity</b> and lightweight <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=1449--14249-crafting-a-good-prompt-or-providing-exemplary-dialogues-a-study-of-in-context-learning-for-persona-based-dialogue-generation-jiashu-pu-et-al-2024>(14/49 | 14/249) Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation (Jiashu Pu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiashu Pu, Yajing Wan, Yuru Zhang, Jing Chen, Ling Cheng, Qian Shao, Yongzhu Chang, Tangjie Lv, Rongsheng Zhang. (2024)<br><strong>Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation</strong><br><button class=copy-to-clipboard title="Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Neural Machine Translation, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09954v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09954v2.pdf filename=2402.09954v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous <b>in-context</b> <b>learning</b> <b>(ICL)</b> research has focused on tasks such as classification, <b>machine</b> <b>translation,</b> text2table, etc., while studies on whether <b>ICL</b> can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the <b>ICL</b> capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting <b>prompt</b> instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demos, increasing the number of demos still improves dialogue performance, proving that <b>LLMs</b> can learn from corrupted dialogue demos. Previous explanations of the <b>ICL</b> mechanism, such as $n$-gram induction head, cannot fully account for this phenomenon.</p></p class="citation"></blockquote><h3 id=1549--15249-nuteprune-efficient-progressive-pruning-with-numerous-teachers-for-large-language-models-shengrui-li-et-al-2024>(15/49 | 15/249) NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models (Shengrui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengrui Li, Xueting Han, Jing Bai. (2024)<br><strong>NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models</strong><br><button class=copy-to-clipboard title="NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Knowledge Distillation, Knowledge Distillation, Pruning, Zero-shot, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09773v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09773v1.pdf filename=2402.09773v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The considerable size of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> presents notable deployment challenges, particularly on resource-constrained hardware. Structured <b>pruning,</b> offers an effective means to compress <b>LLMs,</b> thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure <b>pruning</b> methods to obtain smaller yet still powerful models. <b>Knowledge</b> <b>Distillation</b> is well-suited for <b>pruning,</b> as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of <b>LLMs</b> due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher <b>pruning</b> method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach allows us to leverage numerous teachers with varying capacities to progressively guide the pruned model, enhancing overall performance. Extensive experiments across various tasks demonstrate the effectiveness of NutePrune. In <b>LLaMA-7B</b> <b>zero-shot</b> experiments, NutePrune retains 97.17% of the performance of the original model at 20% sparsity and 95.07% at 25% sparsity.</p></p class="citation"></blockquote><h3 id=1649--16249-chain-of-thought-reasoning-without-prompting-xuezhi-wang-et-al-2024>(16/49 | 16/249) Chain-of-Thought Reasoning Without Prompting (Xuezhi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuezhi Wang, Denny Zhou. (2024)<br><strong>Chain-of-Thought Reasoning Without Prompting</strong><br><button class=copy-to-clipboard title="Chain-of-Thought Reasoning Without Prompting" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Few-shot, Zero-shot, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10200v1.pdf filename=2402.10200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In enhancing the <b>reasoning</b> capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> prior research primarily focuses on specific <b>prompting</b> techniques such as <b>few-shot</b> or <b>zero-shot</b> chain-of-thought (CoT) <b>prompting.</b> These methods, while effective, often involve manually intensive <b>prompt</b> engineering. Our study takes a novel approach by asking: Can <b>LLMs</b> reason effectively without prompting? Our findings reveal that, intriguingly, CoT <b>reasoning</b> paths can be elicited from pre-trained <b>LLMs</b> by simply altering the \textit{decoding} process. Rather than conventional greedy decoding, we investigate the top-$k$ alternative tokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not only bypasses the confounders of <b>prompting</b> but also allows us to assess the <b>LLMs&rsquo;</b> \textit{intrinsic} <b>reasoning</b> abilities. Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model&rsquo;s decoded answer. This confidence metric effectively differentiates between CoT and non-CoT paths. Extensive empirical studies on various <b>reasoning</b> <b>benchmarks</b> show that the proposed CoT-decoding substantially outperforms the standard greedy decoding.</p></p class="citation"></blockquote><h3 id=1749--17249-enhancing-large-language-models-with-pseudo--and-multisource--knowledge-graphs-for-open-ended-question-answering-jiaxiang-liu-et-al-2024>(17/49 | 17/249) Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering (Jiaxiang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao. (2024)<br><strong>Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering</strong><br><button class=copy-to-clipboard title="Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Question Answering, Large Language Model, Large Language Model, Rouge, Rouge-L<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09911v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09911v1.pdf filename=2402.09911v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mitigating the hallucinations of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using <b>Knowledge</b> <b>Graph</b> <b>(KG)</b> enhancement approaches fails to address the generalization across different <b>KG</b> sources and the enhancement of open-ended answer <b>questions</b> <b>simultaneously.</b> To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic <b>Knowledge</b> <b>Verification</b> proposed. The enhancement of <b>LLM</b> using <b>KG</b> in an open-ended <b>question-answering</b> <b>setting</b> is implemented by leveraging the Pseudo-Graph Generation. Atomic <b>Knowledge</b> <b>Verification</b> utilizes atomic-level <b>knowledge</b> <b>querying</b> and verification to achieve generalizability under different <b>KG</b> sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the <b>ROUGE-L</b> score for open-ended <b>questions.</b> <b>For</b> precise <b>questions,</b> <b>we</b> observe a minimum accuracy improvement of 7.5. Moreover, there is also demonstration that this framework exhibits generalizability across different <b>KG</b> sources. In summary, our results pave the way for enhancing <b>LLMs</b> by incorporating Pseudo- and Multisource-KGs, particularly in the context of open-ended questions.</p></p class="citation"></blockquote><h3 id=1849--18249-generative-representational-instruction-tuning-niklas-muennighoff-et-al-2024>(18/49 | 18/249) Generative Representational Instruction Tuning (Niklas Muennighoff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela. (2024)<br><strong>Generative Representational Instruction Tuning</strong><br><button class=copy-to-clipboard title="Generative Representational Instruction Tuning" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Instruction Tuning, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09906v1.pdf filename=2402.09906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>All <b>text-based</b> <b>language</b> problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational <b>instruction</b> <b>tuning</b> (GRIT) whereby a <b>large</b> <b>language</b> <b>model</b> is trained to handle both generative and embedding tasks by distinguishing between them through <b>instructions.</b> <b>Compared</b> to other open models, our resulting GritLM 7B sets a new state of the art on the Massive <b>Text</b> <b>Embedding</b> <b>Benchmark</b> (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> by > 60% for long documents, by no longer requiring separate <b>retrieval</b> <b>and</b> <b>generation</b> models. Models, code, etc. are freely available at <a href=https://github.com/ContextualAI/gritlm>https://github.com/ContextualAI/gritlm</a>.</p></p class="citation"></blockquote><h3 id=1949--19249-uncertainty-decomposition-and-quantification-for-in-context-learning-of-large-language-models-chen-ling-et-al-2024>(19/49 | 19/249) Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models (Chen Ling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen. (2024)<br><strong>Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models</strong><br><button class=copy-to-clipboard title="Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Unsupervised Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10189v1.pdf filename=2402.10189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>learning</b> has emerged as a groundbreaking ability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and revolutionized various fields by providing a few task-relevant demonstrations in the <b>prompt.</b> However, trustworthy issues with <b>LLM&rsquo;s</b> response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in <b>LLM&rsquo;s</b> response, but they often overlook the complex nature of <b>LLMs</b> and the uniqueness of <b>in-context</b> <b>learning.</b> In this work, we delve into the predictive uncertainty of <b>LLMs</b> associated with <b>in-context</b> <b>learning,</b> highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model&rsquo;s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an <b>unsupervised</b> way to understand the prediction of <b>in-context</b> <b>learning</b> in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: \url{https://github.com/lingchen0331/UQ_ICL}.</p></p class="citation"></blockquote><h3 id=2049--20249-grounding-language-model-with-chunking-free-in-context-retrieval-hongjin-qian-et-al-2024>(20/49 | 20/249) Grounding Language Model with Chunking-Free In-Context Retrieval (Hongjin Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, Zhicheng Dou. (2024)<br><strong>Grounding Language Model with Chunking-Free In-Context Retrieval</strong><br><button class=copy-to-clipboard title="Grounding Language Model with Chunking-Free In-Context Retrieval" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 60<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Grounding, Question Answering, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09760v1.pdf filename=2402.09760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel Chunking-Free <b>In-Context</b> (CFIC) <b>retrieval</b> <b>approach,</b> <b>specifically</b> tailored for <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> systems. Traditional <b>RAG</b> systems often struggle with <b>grounding</b> responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence <b>retrieval.</b> <b>CFIC</b> <b>addresses</b> these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for <b>in-context</b> <b>retrieval,</b> <b>employing</b> <b>auto-aggressive</b> decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further enhanced by incorporating two decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies not only improve the efficiency of the <b>retrieval</b> <b>process</b> <b>but</b> also ensure that the fidelity of the generated <b>grounding</b> text evidence is maintained. Our evaluations of CFIC on a range of open <b>QA</b> datasets demonstrate its superiority in retrieving relevant and accurate evidence, offering a significant improvement over traditional methods. By doing away with the need for document chunking, CFIC presents a more streamlined, effective, and efficient <b>retrieval</b> <b>solution,</b> <b>making</b> it a valuable advancement in the field of <b>RAG</b> systems.</p></p class="citation"></blockquote><h3 id=2149--21249-an-analysis-of-language-frequency-and-error-correction-for-esperanto-junhong-liang-2024>(21/49 | 21/249) An Analysis of Language Frequency and Error Correction for Esperanto (Junhong Liang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhong Liang. (2024)<br><strong>An Analysis of Language Frequency and Error Correction for Esperanto</strong><br><button class=copy-to-clipboard title="An Analysis of Language Frequency and Error Correction for Esperanto" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Low-Resource, GPT, GPT-3, GPT-3.5, GPT-4, Grammatical Error Correction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09696v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09696v2.pdf filename=2402.09696v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current Grammar Error Correction <b>(GEC)</b> initiatives tend to focus on major languages, with less attention given to <b>low-resource</b> languages like Esperanto. In this article, we begin to bridge this gap by first conducting a comprehensive frequency analysis using the Eo-GP dataset, created explicitly for this purpose. We then introduce the Eo-GEC dataset, derived from authentic user cases and annotated with fine-grained linguistic details for error identification. Leveraging <b>GPT-3.5</b> and <b>GPT-4,</b> our experiments show that <b>GPT-4</b> outperforms <b>GPT-3.5</b> in both automated and human evaluations, highlighting its efficacy in addressing Esperanto&rsquo;s grammatical peculiarities and illustrating the potential of advanced language models to enhance <b>GEC</b> strategies for less commonly studied languages.</p></p class="citation"></blockquote><h3 id=2249--22249-unmemorization-in-large-language-models-via-self-distillation-and-deliberate-imagination-yijiang-river-dong-et-al-2024>(22/49 | 22/249) Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination (Yijiang River Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vulić. (2024)<br><strong>Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination</strong><br><button class=copy-to-clipboard title="Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Self-Distillation, Natural Language Understanding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10052v1.pdf filename=2402.10052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While displaying impressive generation capabilities across many tasks, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data. This raises an essential question: how should we prevent such undesired behavior of <b>LLMs</b> while maintaining their strong generation and <b>natural</b> <b>language</b> <b>understanding</b> (NLU) capabilities? In this work, we introduce a novel approach termed deliberate imagination in the context of <b>LLM</b> unlearning. Instead of trying to forget memorized data, we employ a <b>self-distillation</b> framework, guiding <b>LLMs</b> to deliberately imagine alternative scenarios. As demonstrated in a wide range of experiments, the proposed method not only effectively unlearns targeted text but also preserves the <b>LLMs&rsquo;</b> capabilities in open-ended generation tasks as well as in NLU tasks. Our results demonstrate the usefulness of this approach across different models and sizes, and also with parameter-efficient <b>fine-tuning,</b> offering a novel pathway to addressing the challenges with private and sensitive data in <b>LLM</b> applications.</p></p class="citation"></blockquote><h3 id=2349--23249-improving-non-autoregressive-machine-translation-with-error-exposure-and-consistency-regularization-xinran-chen-et-al-2024>(23/49 | 23/249) Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization (Xinran Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinran Chen, Sufeng Duan, Gongshen Liu. (2024)<br><strong>Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization</strong><br><button class=copy-to-clipboard title="Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Transformer, Neural Machine Translation, BLEU, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09725v1.pdf filename=2402.09725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional <b>Masked</b> <b>Language</b> <b>Model</b> (CMLM) adopts the mask-predict paradigm to re-predict the <b>masked</b> <b>low-confidence</b> <b>tokens.</b> However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases. In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR). We construct the mixed sequences based on model prediction during training, and propose to optimize over the <b>masked</b> <b>tokens</b> <b>under</b> imperfect observation conditions. We also design a consistency learning method to constrain the data distribution for the <b>masked</b> <b>tokens</b> <b>under</b> different observing situations to narrow down the gap between training and inference. The experiments on five translation <b>benchmarks</b> obtains an average improvement of 0.68 and 0.40 <b>BLEU</b> scores compared to the base models, respectively, and our CMLMC-EECR achieves the best performance with a comparable translation quality with the <b>Transformer.</b> The experiments results demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=2449--24249-both-matter-enhancing-the-emotional-intelligence-of-large-language-models-without-compromising-the-general-intelligence-weixiang-zhao-et-al-2024>(24/49 | 24/249) Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence (Weixiang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weixiang Zhao, Zhuojun Li, Shilong Wang, Yang Wang, Yulin Hu, Yanyan Zhao, Chen Wei, Bing Qin. (2024)<br><strong>Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence</strong><br><button class=copy-to-clipboard title="Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10073v1.pdf filename=2402.10073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive <b>fine-tuning</b> on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \textsc{EiBench}, a <b>large-scale</b> <b>collection</b> <b>of</b> EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of <b>LLMs.</b> Then a novel \underline{\textbf{Mo}}dular \underline{\textbf{E}}motional \underline{\textbf{I}}ntelligence enhancement method (\textbf{MoEI}), consisting of Modular Parameter Expansion and intra-inter modulation, is proposed to comprehensively enhance the EI of <b>LLMs</b> without compromise their GI. Extensive experiments on two representative <b>LLM-based</b> assistants, Flan-T5 and <b>LLaMA-2-Chat,</b> demonstrate the effectiveness of MoEI to improving EI while maintain GI.</p></p class="citation"></blockquote><h3 id=2549--25249-towards-safer-large-language-models-through-machine-unlearning-zheyuan-liu-et-al-2024>(25/49 | 25/249) Towards Safer Large Language Models through Machine Unlearning (Zheyuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, Meng Jiang. (2024)<br><strong>Towards Safer Large Language Models through Machine Unlearning</strong><br><button class=copy-to-clipboard title="Towards Safer Large Language Models through Machine Unlearning" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Machine Unlearning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10058v1.pdf filename=2402.10058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, <b>LLMs</b> often encounter challenges in generating harmful content when faced with problematic <b>prompts.</b> To address this problem, existing work attempted to implement a gradient ascent based approach to prevent <b>LLMs</b> from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal <b>prompts.</b> To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for <b>LLMs,</b> designed to eliminate harmful knowledge while preserving utility on normal <b>prompts.</b> Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model&rsquo;s performance remains robust on normal <b>prompts.</b> Our experiments conducted across various <b>LLM</b> architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.</p></p class="citation"></blockquote><h3 id=2649--26249-efficient-language-adaptive-pre-training-extending-state-of-the-art-large-language-models-for-polish-szymon-ruciński-2024>(26/49 | 26/249) Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish (Szymon Ruciński, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Szymon Ruciński. (2024)<br><strong>Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish</strong><br><button class=copy-to-clipboard title="Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09759v1.pdf filename=2402.09759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores the potential of <b>fine-tuning</b> foundational English <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for generating Polish text. The first step involves Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB, consisting of 276 million Polish tokens. The LAPT is followed by additional <b>fine-tuning</b> aimed at solving nine KLEJ challenges. Our trained model Curie-7B-v1 not only generates Polish text with the lowest <b>perplexity</b> of 3.02 among decoder-based Polish models but also closely rivals the performance of the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9 tasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn Polish. The LAPT was completed in less than five days using a consumer GPU, highlighting the method&rsquo;s efficiency. The proficiency of the model in Polish was significantly enhanced, demonstrating the viability of this approach for adding new languages to existing <b>LLMs</b> by training just 1.2% of its parameters. To contribute to the community&rsquo;s collaborative progress, the model has been released as open-source.</p></p class="citation"></blockquote><h3 id=2749--27249-qurating-selecting-high-quality-data-for-training-language-models-alexander-wettig-et-al-2024>(27/49 | 27/249) QuRating: Selecting High-Quality Data for Training Language Models (Alexander Wettig et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen. (2024)<br><strong>QuRating: Selecting High-Quality Data for Training Language Models</strong><br><button class=copy-to-clipboard title="QuRating: Selecting High-Quality Data for Training Language Models" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: In-context Learning, In-context Learning, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09739v1.pdf filename=2402.09739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value. We find that <b>LLMs</b> are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity, as selecting only the highest-rated documents leads to poor results. When we sample using quality ratings as logits over documents, our models achieve lower <b>perplexity</b> and stronger <b>in-context</b> <b>learning</b> performance than baselines. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.</p></p class="citation"></blockquote><h3 id=2849--28249-align-before-attend-aligning-visual-and-textual-features-for-multimodal-hateful-content-detection-eftekhar-hossain-et-al-2024>(28/49 | 28/249) Align before Attend: Aligning Visual and Textual Features for Multimodal Hateful Content Detection (Eftekhar Hossain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eftekhar Hossain, Omar Sharif, Mohammed Moshiul Hoque, Sarah M. Preum. (2024)<br><strong>Align before Attend: Aligning Visual and Textual Features for Multimodal Hateful Content Detection</strong><br><button class=copy-to-clipboard title="Align before Attend: Aligning Visual and Textual Features for Multimodal Hateful Content Detection" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 39<br>Keywords: Benchmarking, Low-Resource, Multi-modal, Multi-modal, Content Detection, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09738v1.pdf filename=2402.09738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> hateful <b>content</b> <b>detection</b> is a challenging task that requires complex <b>reasoning</b> across visual and textual modalities. Therefore, creating a meaningful <b>multimodal</b> representation that effectively captures the interplay between visual and textual features through intermediate fusion is critical. Conventional fusion techniques are unable to attend to the modality-specific features effectively. Moreover, most studies exclusively concentrated on English and overlooked other <b>low-resource</b> languages. This paper proposes a context-aware attention framework for <b>multimodal</b> hateful <b>content</b> <b>detection</b> and assesses it for both English and non-English languages. The proposed approach incorporates an attention layer to meaningfully align the visual and textual features. This alignment enables selective focus on modality-specific features before fusing them. We evaluate the proposed approach on two <b>benchmark</b> hateful meme datasets, viz. MUTE (Bengali code-mixed) and MultiOFF (English). Evaluation results demonstrate our proposed approach&rsquo;s effectiveness with F1-scores of $69.7$% and $70.3$% for the MUTE and MultiOFF datasets. The scores show approximately $2.5$% and $3.2$% performance improvement over the state-of-the-art systems on these datasets. Our implementation is available at <a href=https://github.com/eftekhar-hossain/Bengali-Hateful-Memes>https://github.com/eftekhar-hossain/Bengali-Hateful-Memes</a>.</p></p class="citation"></blockquote><h3 id=2949--29249-sportsmetrics-blending-text-and-numerical-data-to-understand-information-fusion-in-llms-yebowen-hu-et-al-2024>(29/49 | 29/249) SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs (Yebowen Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, Dong Yu, Fei Liu. (2024)<br><strong>SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs</strong><br><button class=copy-to-clipboard title="SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10979v1.pdf filename=2402.10979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. <b>LLMs</b> need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical <b>reasoning</b> and information fusion capabilities of <b>LLMs.</b> These tasks involve providing <b>LLMs</b> with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of <b>LLMs</b> on these tasks. Our <b>benchmark,</b> SportsMetrics, introduces a new mechanism for assessing <b>LLMs&rsquo;</b> numerical <b>reasoning</b> and fusion skills.</p></p class="citation"></blockquote><h3 id=3049--30249-tdag-a-multi-agent-framework-based-on-dynamic-task-decomposition-and-agent-generation-yaoxiang-wang-et-al-2024>(30/49 | 30/249) TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation (Yaoxiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, Jinsong Su. (2024)<br><strong>TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation</strong><br><button class=copy-to-clipboard title="TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10178v1.pdf filename=2402.10178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>ChatGPT</b> has inspired the development of <b>LLM-based</b> agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing <b>benchmarks</b> often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench is designed to assess agents&rsquo; abilities in memory, planning, and tool usage across tasks of varying complexity. Our experimental results reveal that TDAG significantly outperforms established baselines, showcasing its superior adaptability and context awareness in complex task scenarios.</p></p class="citation"></blockquote><h3 id=3149--31249-ai-hospital-interactive-evaluation-and-collaboration-of-llms-as-intern-doctors-for-clinical-diagnosis-zhihao-fan-et-al-2024>(31/49 | 31/249) AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis (Zhihao Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou. (2024)<br><strong>AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis</strong><br><button class=copy-to-clipboard title="AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09742v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09742v2.pdf filename=2402.09742v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The incorporation of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in healthcare marks a significant advancement. However, the application has predominantly been limited to discriminative and <b>question-answering</b> <b>tasks,</b> which does not fully leverage their interactive potential. To address this limitation, our paper presents AI Hospital, a framework designed to build a real-time interactive diagnosis environment. To simulate the procedure, we collect high-quality medical records to create patient, examiner, and medical director agents. AI Hospital is then utilized for the interactive evaluation and collaboration of <b>LLMs.</b> Initially, we create a Multi-View Medical Evaluation (MVME) <b>benchmark</b> where various <b>LLMs</b> serve as intern doctors for interactive diagnosis. Subsequently, to improve diagnostic accuracy, we introduce a collaborative mechanism that involves iterative discussions and a dispute resolution process under the supervision of the medical director. In our experiments, we validate the reliability of AI Hospital. The results not only explore the feasibility of apply <b>LLMs</b> in clinical consultation but also confirm the effectiveness of the dispute resolution focused collaboration method.</p></p class="citation"></blockquote><h3 id=3249--32249-a-trembling-house-of-cards-mapping-adversarial-attacks-against-language-agents-lingbo-mo-et-al-2024>(32/49 | 32/249) A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents (Lingbo Mo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao, Huan Sun. (2024)<br><strong>A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents</strong><br><button class=copy-to-clipboard title="A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10196v1.pdf filename=2402.10196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language agents powered by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have seen exploding development. Their capability of using language as a vehicle for thought and communication lends an incredible level of flexibility and versatility. People have quickly capitalized on this capability to connect <b>LLMs</b> to a wide range of external components and environments: databases, tools, the Internet, robotic embodiment, etc. Many believe an unprecedentedly powerful automation technology is emerging. However, new automation technologies come with new safety risks, especially for intricate systems like language agents. There is a surprisingly <b>large</b> <b>gap</b> <b>between</b> the speed and scale of their development and deployment and our understanding of their safety risks. Are we building a house of cards? In this position paper, we present the first systematic effort in mapping <b>adversarial</b> <b>attacks</b> against language agents. We first present a unified conceptual framework for agents with three major components: Perception, Brain, and Action. Under this framework, we present a comprehensive discussion and propose 12 potential attack scenarios against different components of an agent, covering different attack strategies (e.g., input manipulation, <b>adversarial</b> <b>demonstrations,</b> jailbreaking, backdoors). We also draw connections to successful attack strategies previously applied to <b>LLMs.</b> We emphasize the urgency to gain a thorough understanding of language agent risks before their widespread deployment.</p></p class="citation"></blockquote><h3 id=3349--33249-knowledge-infused-llm-powered-conversational-health-agent-a-case-study-for-diabetes-patients-mahyar-abbasian-et-al-2024>(33/49 | 33/249) Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients (Mahyar Abbasian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahyar Abbasian, Zhongqi Yang, Elahe Khatibi, Pengfei Zhang, Nitish Nagesh, Iman Azimi, Ramesh Jain, Amir M. Rahmani. (2024)<br><strong>Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients</strong><br><button class=copy-to-clipboard title="Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10153v1.pdf filename=2402.10153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective diabetes management is crucial for maintaining health in diabetic patients. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have opened new avenues for diabetes management, facilitating their efficacy. However, current <b>LLM-based</b> approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused <b>LLM-powered</b> conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with <b>GPT4.</b> Our evaluation includes 100 diabetes-related questions on daily meal choices and assessing the potential risks associated with the suggested diet. Our findings show that the proposed agent demonstrates superior performance in generating responses to manage essential nutrients.</p></p class="citation"></blockquote><h3 id=3449--34249-controllm-crafting-diverse-personalities-for-language-models-yixuan-weng-et-al-2024>(34/49 | 34/249) ControlLM: Crafting Diverse Personalities for Language Models (Yixuan Weng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixuan Weng, Shizhu He, Kang Liu, Shengping Liu, Jun Zhao. (2024)<br><strong>ControlLM: Crafting Diverse Personalities for Language Models</strong><br><button class=copy-to-clipboard title="ControlLM: Crafting Diverse Personalities for Language Models" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10151v1.pdf filename=2402.10151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As language models continue to scale in size and capability, they display an array of emerging behaviors, both beneficial and concerning. This heightens the need to control model behaviors. We hope to be able to control the personality traits of language models at the inference-time so as to have various character features, on top of which the requirements of different types of tasks can be met. Personality is a higher-level and more abstract behavioral representation for language models. We introduce ControlLM, which leverages differential activation patterns, derived from contrasting behavioral <b>prompts</b> in the model&rsquo;s latent space, to influence the model&rsquo;s personality traits at inference. This approach allows for the precise, real-time adjustment of model behavior. First, we demonstrate ControlLM&rsquo;s capacity to elicit diverse persona behaviors without any training, while precision control allows personality traits to closely match average human values. Subsequently, we showcase improved <b>reasoning</b> and <b>question</b> <b>answering</b> through selective amplification of beneficial attributes like conscientiousness and friendliness. We hope that this work will inspire research on controlling human-like behaviors of language models and provide insights for future research. Our code is publicly available at: <a href=https://github.com/wengsyx/ControlLM>https://github.com/wengsyx/ControlLM</a>.</p></p class="citation"></blockquote><h3 id=3549--35249-case-study-testing-model-capabilities-in-some-reasoning-tasks-min-zhang-et-al-2024>(35/49 | 35/249) Case Study: Testing Model Capabilities in Some Reasoning Tasks (Min Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Min Zhang, Sato Takumi, Jack Zhang, Jun Wang. (2024)<br><strong>Case Study: Testing Model Capabilities in Some Reasoning Tasks</strong><br><button class=copy-to-clipboard title="Case Study: Testing Model Capabilities in Some Reasoning Tasks" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09967v1.pdf filename=2402.09967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> excel in generating personalized content and facilitating interactive dialogues, showcasing their remarkable aptitude for a myriad of applications. However, their capabilities in <b>reasoning</b> and providing explainable outputs, especially within the context of <b>reasoning</b> abilities, remain areas for improvement. In this study, we delve into the <b>reasoning</b> abilities of <b>LLMs,</b> highlighting the current challenges and limitations that hinder their effectiveness in complex <b>reasoning</b> scenarios.</p></p class="citation"></blockquote><h3 id=3649--36249-camouflage-is-all-you-need-evaluating-and-enhancing-language-model-robustness-against-camouflage-adversarial-attacks-álvaro-huertas-garcía-et-al-2024>(36/49 | 36/249) Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks (Álvaro Huertas-García et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Álvaro Huertas-García, Alejandro Martín, Javier Huertas-Tato, David Camacho. (2024)<br><strong>Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks</strong><br><button class=copy-to-clipboard title="Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Adversarial Learning, Transformer, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09874v1.pdf filename=2402.09874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>attacks</b> represent a substantial challenge in Natural Language Processing (NLP). This study undertakes a systematic exploration of this challenge in two distinct phases: vulnerability evaluation and resilience enhancement of <b>Transformer-based</b> models under <b>adversarial</b> <b>attacks.</b> In the evaluation phase, we assess the susceptibility of three <b>Transformer</b> configurations, encoder-decoder, encoder-only, and decoder-only setups, to <b>adversarial</b> <b>attacks</b> of escalating complexity across datasets containing offensive language and misinformation. Encoder-only models manifest a 14% and 21% performance drop in offensive language detection and misinformation detection tasks, respectively. Decoder-only models register a 16% decrease in both tasks, while encoder-decoder models exhibit a maximum performance drop of 14% and 26% in the respective tasks. The resilience-enhancement phase employs <b>adversarial</b> <b>training,</b> integrating pre-camouflaged and dynamically altered data. This approach effectively reduces the performance drop in encoder-only models to an average of 5% in offensive language detection and 2% in misinformation detection tasks. Decoder-only models, occasionally exceeding original performance, limit the performance drop to 7% and 2% in the respective tasks. Although not surpassing the original performance, Encoder-decoder models can reduce the drop to an average of 6% and 2% respectively. Results suggest a trade-off between performance and robustness, with some models maintaining similar performance while gaining robustness. Our study and <b>adversarial</b> <b>training</b> techniques have been incorporated into an open-source tool for generating camouflaged datasets. However, methodology effectiveness depends on the specific camouflage technique and data encountered, emphasizing the need for continued exploration.</p></p class="citation"></blockquote><h3 id=3749--37249-do-llms-know-about-hallucination-an-empirical-investigation-of-llms-hidden-states-hanyu-duan-et-al-2024>(37/49 | 37/249) Do LLMs Know about Hallucination? An Empirical Investigation of LLM&rsquo;s Hidden States (Hanyu Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanyu Duan, Yi Yang, Kar Yan Tam. (2024)<br><strong>Do LLMs Know about Hallucination? An Empirical Investigation of LLM&rsquo;s Hidden States</strong><br><button class=copy-to-clipboard title="Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09733v1.pdf filename=2402.09733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent <b>LLMs</b> are aware of hallucination. More specifically, we check whether and how an <b>LLM</b> reacts differently in its hidden states when it answers a question right versus when it hallucinates. To do this, we introduce an experimental framework which allows examining <b>LLM&rsquo;s</b> hidden states in different hallucination situations. Building upon this framework, we conduct a series of experiments with language models in the <b>LLaMA</b> family (Touvron et al., 2023). Our empirical findings suggest that <b>LLMs</b> react differently when processing a genuine response versus a fabricated one. We then apply various model interpretation techniques to help understand and explain the findings better. Moreover, informed by the empirical observations, we show great potential of using the guidance derived from <b>LLM&rsquo;s</b> hidden representation space to mitigate hallucination. We believe this work provides insights into how <b>LLMs</b> produce hallucinated answers and how to make them occur less often.</p></p class="citation"></blockquote><h3 id=3849--38249-a-human-inspired-reading-agent-with-gist-memory-of-very-long-contexts-kuang-huei-lee-et-al-2024>(38/49 | 38/249) A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts (Kuang-Huei Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, Ian Fischer. (2024)<br><strong>A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts</strong><br><button class=copy-to-clipboard title="A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09727v1.pdf filename=2402.09727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an <b>LLM</b> agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple <b>prompting</b> system that uses the advanced language capabilities of <b>LLMs</b> to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum. ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3-20x.</p></p class="citation"></blockquote><h3 id=3949--39249-efuf-efficient-fine-grained-unlearning-framework-for-mitigating-hallucinations-in-multimodal-large-language-models-shangyu-xing-et-al-2024>(39/49 | 39/249) EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models (Shangyu Xing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, Xinyu Dai. (2024)<br><strong>EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 26<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09801v1.pdf filename=2402.09801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the <b>finetuning</b> stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.</p></p class="citation"></blockquote><h3 id=4049--40249-toad-task-oriented-automatic-dialogs-with-diverse-response-styles-yinhong-liu-et-al-2024>(40/49 | 40/249) TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles (Yinhong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinhong Liu, Yimai Fang, David Vandyke, Nigel Collier. (2024)<br><strong>TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles</strong><br><button class=copy-to-clipboard title="TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10137v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10137v2.pdf filename=2402.10137v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In light of recent advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios. However, the creation of high-quality annotated data for Task-Oriented Dialog (TOD) is recognized to be slow and costly. To address these challenges, we introduce Task-Oriented Automatic Dialogs (TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline. The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options. Two aspects of system response styles are considered, verbosity level and users&rsquo; expression mirroring. We <b>benchmark</b> TOAD on two response generation tasks and the results show that modelling more verbose or responses without user expression mirroring is more challenging.</p></p class="citation"></blockquote><h3 id=4149--41249-a-dataset-of-open-domain-question-answering-with-multiple-span-answers-zhiyi-luo-et-al-2024>(41/49 | 41/249) A Dataset of Open-Domain Question Answering with Multiple-Span Answers (Zhiyi Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyi Luo, Yingying Zhang, Shuyun Luo, Ying Zhao, Wentao Lyu. (2024)<br><strong>A Dataset of Open-Domain Question Answering with Multiple-Span Answers</strong><br><button class=copy-to-clipboard title="A Dataset of Open-Domain Question Answering with Multiple-Span Answers" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Open-Domain Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09923v1.pdf filename=2402.09923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-span answer extraction, also known as the task of multi-span <b>question</b> <b>answering</b> (MSQA), is critical for real-world applications, as it requires extracting multiple pieces of information from a text to answer complex <b>questions.</b> <b>Despite</b> the active studies and rapid progress in English MSQA research, there is a notable lack of publicly available MSQA <b>benchmark</b> in Chinese. Previous efforts for constructing MSQA datasets predominantly emphasized entity-centric contextualization, resulting in a bias towards collecting factoid <b>questions</b> <b>and</b> potentially overlooking <b>questions</b> <b>requiring</b> more detailed descriptive responses. To overcome these limitations, we present CLEAN, a comprehensive Chinese multi-span <b>question</b> <b>answering</b> dataset that involves a wide range of <b>open-domain</b> <b>subjects</b> <b>with</b> a substantial number of instances requiring descriptive answers. Additionally, we provide established models from relevant literature as baselines for CLEAN. Experimental results and analysis show the characteristics and challenge of the newly proposed CLEAN dataset for the community. Our dataset, CLEAN, will be publicly released at zhiyiluo.site/misc/clean_v1.0_ sample.json.</p></p class="citation"></blockquote><h3 id=4249--42249-data-engineering-for-scaling-language-models-to-128k-context-yao-fu-et-al-2024>(42/49 | 42/249) Data Engineering for Scaling Language Models to 128K Context (Yao Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, Hao Peng. (2024)<br><strong>Data Engineering for Scaling Language Models to 128K Context</strong><br><button class=copy-to-clipboard title="Data Engineering for Scaling Language Models to 128K Context" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10171v1.pdf filename=2402.10171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the continual pretraining recipe for scaling language models&rsquo; context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \textit{quantity} and \textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \textit{domain balance} and \textit{length upsampling}. Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like <b>GPT-4</b> 128K.</p></p class="citation"></blockquote><h3 id=4349--43249-multi-word-tokenization-for-sequence-compression-leonidas-gee-et-al-2024>(43/49 | 43/249) Multi-Word Tokenization for Sequence Compression (Leonidas Gee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonidas Gee, Leonardo Rigutini, Marco Ernandes, Andrea Zugarini. (2024)<br><strong>Multi-Word Tokenization for Sequence Compression</strong><br><button class=copy-to-clipboard title="Multi-Word Tokenization for Sequence Compression" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Tokenization, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09949v1.pdf filename=2402.09949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this pa005 per, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient <b>tokenization</b> that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length and budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.</p></p class="citation"></blockquote><h3 id=4449--44249-de-cop-detecting-copyrighted-content-in-language-models-training-data-andré-v-duarte-et-al-2024>(44/49 | 44/249) DE-COP: Detecting Copyrighted Content in Language Models Training Data (André V. Duarte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>André V. Duarte, Xuandong Zhao, Arlindo L. Oliveira, Lei Li. (2024)<br><strong>DE-COP: Detecting Copyrighted Content in Language Models Training Data</strong><br><button class=copy-to-clipboard title="DE-COP: Detecting Copyrighted Content in Language Models Training Data" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2, cs-CL, cs-LG, cs.CL<br>Keyword Score: 18<br>Keywords: Benchmarking, Black Box, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09910v1.pdf filename=2402.09910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP&rsquo;s core approach is to probe an <b>LLM</b> with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a <b>benchmark</b> with excerpts from 165 books published prior and subsequent to a model&rsquo;s training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully <b>black-box</b> <b>models</b> where prior methods give $\approx$ 4% accuracy. Our code and datasets are available at <a href=https://github.com/avduarte333/DE-COP_Method>https://github.com/avduarte333/DE-COP_Method</a></p></p class="citation"></blockquote><h3 id=4549--45249-how-to-discern-important-urgent-news-oleg-vasilyev-et-al-2024>(45/49 | 45/249) How to Discern Important Urgent News? (Oleg Vasilyev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oleg Vasilyev, John Bohannon. (2024)<br><strong>How to Discern Important Urgent News?</strong><br><button class=copy-to-clipboard title="How to Discern Important Urgent News?" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Clustering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10302v1.pdf filename=2402.10302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We found that a simple property of clusters in a clustered dataset of news correlate strongly with importance and urgency of news (IUN) as assessed by <b>LLM.</b> We verified our finding across different news datasets, dataset sizes, <b>clustering</b> algorithms and embeddings. The found correlation should allow using <b>clustering</b> (as an alternative to <b>LLM)</b> for identifying the most important urgent news, or for filtering out unimportant articles.</p></p class="citation"></blockquote><h3 id=4649--46249-fast-vocabulary-transfer-for-language-model-compression-leonidas-gee-et-al-2024>(46/49 | 46/249) Fast Vocabulary Transfer for Language Model Compression (Leonidas Gee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, Paolo Torroni. (2024)<br><strong>Fast Vocabulary Transfer for Language Model Compression</strong><br><button class=copy-to-clipboard title="Fast Vocabulary Transfer for Language Model Compression" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Model Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09977v1.pdf filename=2402.09977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world business applications require a trade-off between language <b>model</b> <b>performance</b> and size. We propose a new method for <b>model</b> <b>compression</b> that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in <b>model</b> <b>size</b> and inference time while marginally compromising on performance.</p></p class="citation"></blockquote><h3 id=4749--47249-paying-attention-to-deflections-mining-pragmatic-nuances-for-whataboutism-detection-in-online-discourse-khiem-phi-et-al-2024>(47/49 | 47/249) Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse (Khiem Phi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khiem Phi, Noushin Salek Faramarzi, Chenlu Wang, Ritwik Banerjee. (2024)<br><strong>Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse</strong><br><button class=copy-to-clipboard title="Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09934v1.pdf filename=2402.09934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about&rsquo; lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, <b>prompting</b> the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.</p></p class="citation"></blockquote><h3 id=4849--48249-knowledge-of-pretrained-language-models-on-surface-information-of-tokens-tatsuya-hiraoka-et-al-2024>(48/49 | 48/249) Knowledge of Pretrained Language Models on Surface Information of Tokens (Tatsuya Hiraoka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tatsuya Hiraoka, Naoaki Okazaki. (2024)<br><strong>Knowledge of Pretrained Language Models on Surface Information of Tokens</strong><br><button class=copy-to-clipboard title="Knowledge of Pretrained Language Models on Surface Information of Tokens" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09808v1.pdf filename=2402.09808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Do <b>pretrained</b> <b>language</b> <b>models</b> have knowledge regarding the surface information of tokens? We examined the surface information stored in word or subword embeddings acquired by <b>pretrained</b> <b>language</b> <b>models</b> from the perspectives of token length, substrings, and token constitution. Additionally, we evaluated the ability of models to generate knowledge regarding token surfaces. We focused on 12 <b>pretrained</b> <b>language</b> <b>models</b> that were mainly trained on English and Japanese corpora. Experimental results demonstrate that <b>pretrained</b> <b>language</b> <b>models</b> have knowledge regarding token length and substrings but not token constitution. Additionally, the results imply that there is a bottleneck on the decoder side in terms of effectively utilizing acquired knowledge.</p></p class="citation"></blockquote><h3 id=4949--49249-buster-a-business-transaction-entity-recognition-dataset-andrea-zugarini-et-al-2024>(49/49 | 49/249) BUSTER: a &lsquo;BUSiness Transaction Entity Recognition&rsquo; dataset (Andrea Zugarini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Zugarini, Andrew Zamai, Marco Ernandes, Leonardo Rigutini. (2024)<br><strong>BUSTER: a &lsquo;BUSiness Transaction Entity Recognition&rsquo; dataset</strong><br><button class=copy-to-clipboard title="BUSTER: a 'BUSiness Transaction Entity Recognition' dataset" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09916v1.pdf filename=2402.09916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Albeit Natural Language Processing has seen major breakthroughs in the last few years, transferring such advances into real-world business cases can be challenging. One of the reasons resides in the displacement between popular <b>benchmarks</b> and actual data. Lack of supervision, unbalanced classes, noisy data and long documents often affect real problems in vertical domains such as finance, law and health. To support industry-oriented research, we present BUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists of 3779 manually annotated documents on financial transactions. We establish several baselines exploiting both general-purpose and domain-specific language models. The best performing model is also used to automatically annotate 6196 documents, which we release as an additional silver corpus to BUSTER.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--50249-best-arm-identification-for-prompt-learning-under-a-limited-budget-chengshuai-shi-et-al-2024>(1/3 | 50/249) Best Arm Identification for Prompt Learning under a Limited Budget (Chengshuai Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengshuai Shi, Kun Yang, Jing Yang, Cong Shen. (2024)<br><strong>Best Arm Identification for Prompt Learning under a Limited Budget</strong><br><button class=copy-to-clipboard title="Best Arm Identification for Prompt Learning under a Limited Budget" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-CL, cs-LG, stat-ML, stat.ML<br>Keyword Score: 93<br>Keywords: Bandit Algorithm, Clustering, GPT, GPT-3, GPT-3.5, Instruction Following, Large Language Model, Large Language Model, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09723v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09723v2.pdf filename=2402.09723v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The remarkable <b>instruction-following</b> <b>capability</b> of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has sparked a growing interest in automatically learning suitable <b>prompts.</b> <b>However,</b> while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing <b>LLM</b> and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into <b>prompt</b> <b>learning.</b> Towards developing principled solutions, a novel connection is established between <b>prompt</b> <b>learning</b> and fixed-budget best arm identification (BAI-FB) in multi-armed <b>bandits</b> (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for <b>Prompt</b> <b>LEarning)</b> is proposed to harness the power of BAI-FB in <b>prompt</b> <b>learning</b> systematically. Unique characteristics of <b>prompt</b> <b>learning</b> further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of <b>clustering</b> and function approximation. Extensive experiments on multiple well-adopted tasks using both <b>GPT</b> 3.5 and Llama2 demonstrate the significant performance improvement of TRIPLE over the previous baselines while satisfying the limited budget constraints.</p></p class="citation"></blockquote><h3 id=23--51249-thompson-sampling-in-partially-observable-contextual-bandits-hongju-park-et-al-2024>(2/3 | 51/249) Thompson Sampling in Partially Observable Contextual Bandits (Hongju Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongju Park, Mohamad Kazem Shirani Faradonbeh. (2024)<br><strong>Thompson Sampling in Partially Observable Contextual Bandits</strong><br><button class=copy-to-clipboard title="Thompson Sampling in Partially Observable Contextual Bandits" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10289v1.pdf filename=2402.10289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contextual <b>bandits</b> constitute a classical framework for decision-making under uncertainty. In this setting, the goal is to learn the arms of highest reward subject to contextual information, while the unknown reward parameters of each arm need to be learned by experimenting that specific arm. Accordingly, a fundamental problem is that of balancing exploration (i.e., pulling different arms to learn their parameters), versus exploitation (i.e., pulling the best arms to gain reward). To study this problem, the existing literature mostly considers perfectly observed contexts. However, the setting of partial context observations remains unexplored to date, despite being theoretically more general and practically more versatile. We study <b>bandit</b> policies for learning to select optimal arms based on the data of observations, which are noisy linear functions of the unobserved context vectors. Our theoretical analysis shows that the Thompson sampling policy successfully balances exploration and exploitation. Specifically, we establish the followings: (i) regret bounds that grow poly-logarithmically with time, (ii) square-root consistency of parameter estimation, and (iii) scaling of the regret with other quantities including dimensions and number of arms. Extensive numerical experiments with both real and synthetic data are presented as well, corroborating the efficacy of Thompson sampling. To establish the results, we introduce novel martingale techniques and concentration inequalities to address partially observed dependent random variables generated from unspecified distributions, and also leverage problem-dependent information to sharpen probabilistic bounds for time-varying suboptimality gaps. These techniques pave the road towards studying other decision-making problems with contextual information as well as partial observations.</p></p class="citation"></blockquote><h3 id=33--52249-nonlinear-spiked-covariance-matrices-and-signal-propagation-in-deep-neural-networks-zhichao-wang-et-al-2024>(3/3 | 52/249) Nonlinear spiked covariance matrices and signal propagation in deep neural networks (Zhichao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhichao Wang, Denny Wu, Zhou Fan. (2024)<br><strong>Nonlinear spiked covariance matrices and signal propagation in deep neural networks</strong><br><button class=copy-to-clipboard title="Nonlinear spiked covariance matrices and signal propagation in deep neural networks" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-PR, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10127v1.pdf filename=2402.10127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many recent works have studied the eigenvalue spectrum of the Conjugate Kernel (CK) defined by the nonlinear feature map of a feedforward neural network. However, existing results only establish weak convergence of the empirical eigenvalue distribution, and fall short of providing precise quantitative characterizations of the &lsquo;&lsquo;spike&rsquo;&rsquo; eigenvalues and eigenvectors that often capture the low-dimensional signal structure of the learning problem. In this work, we characterize these signal eigenvalues and eigenvectors for a nonlinear version of the spiked covariance model, including the CK as a special case. Using this general result, we give a quantitative description of how spiked eigenstructure in the input data propagates through the hidden layers of a neural network with random weights. As a second application, we study a simple regime of <b>representation</b> <b>learning</b> where the weight matrix develops a rank-one signal component over training and characterize the alignment of the target function with the spike eigenvector of the CK on test data.</p></p class="citation"></blockquote><h2 id=cslg-66>cs.LG (66)</h2><h3 id=166--53249-self-play-fine-tuning-of-diffusion-models-for-text-to-image-generation-huizhuo-yuan-et-al-2024>(1/66 | 53/249) Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation (Huizhuo Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, Quanquan Gu. (2024)<br><strong>Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Supervised Learning, Text2image, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10210v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10210v1.pdf filename=2402.10210v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in <b>fine-tuning</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on <b>supervised</b> <b>fine-tuning,</b> their performance inevitably plateaus after seeing a certain volume of data. Recently, <b>reinforcement</b> <b>learning</b> (RL) has been employed to <b>fine-tune</b> diffusion models with human preference data, but it requires at least two images (&ldquo;winner&rdquo; and &ldquo;loser&rdquo; images) for each text <b>prompt.</b> In this paper, we introduce an innovative technique called self-play <b>fine-tuning</b> for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional <b>supervised</b> <b>fine-tuning</b> and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing <b>supervised</b> <b>fine-tuning</b> method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of <b>RLHF-based</b> methods across all metrics, achieving these results with less data.</p></p class="citation"></blockquote><h3 id=266--54249-rewards-in-context-multi-objective-alignment-of-foundation-models-with-dynamic-preference-adjustment-rui-yang-et-al-2024>(2/66 | 54/249) Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment (Rui Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, Jianshu Chen. (2024)<br><strong>Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment</strong><br><button class=copy-to-clipboard title="Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, Reinforcement Learning, Supervised Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10207v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10207v2.pdf filename=2402.10207v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of multi-objective alignment of <b>foundation</b> <b>models</b> with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to <b>fine-tune</b> <b>large</b> <b>foundation</b> <b>models</b> using <b>reinforcement</b> <b>learning</b> (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a <b>foundation</b> <b>model</b> on multiple rewards in its <b>prompt</b> context and applies <b>supervised</b> <b>fine-tuning</b> for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires <b>supervised</b> <b>fine-tuning</b> of a single <b>foundation</b> <b>model</b> and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline.</p></p class="citation"></blockquote><h3 id=366--55249-bitdelta-your-fine-tune-may-only-be-worth-one-bit-james-liu-et-al-2024>(3/66 | 55/249) BitDelta: Your Fine-Tune May Only Be Worth One Bit (James Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao, Tianle Cai. (2024)<br><strong>BitDelta: Your Fine-Tune May Only Be Worth One Bit</strong><br><button class=copy-to-clipboard title="BitDelta: Your Fine-Tune May Only Be Worth One Bit" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Quantization, LLaMA, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10193v1.pdf filename=2402.10193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are typically trained in two phases: pre-training on <b>large</b> <b>internet-scale</b> <b>datasets,</b> and <b>fine-tuning</b> for downstream tasks. Given the higher computational demand of pre-training, it&rsquo;s intuitive to assume that <b>fine-tuning</b> adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of <b>fine-tuned</b> models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully <b>quantizes</b> this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during <b>fine-tuning,</b> but also has significant implications for the multi-tenant serving and multi-tenant storage of <b>fine-tuned</b> models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which can also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through experiments across <b>Llama-2</b> and <b>Mistral</b> model families, and on models up to 70B parameters, showcasing minimal performance degradation over all tested settings.</p></p class="citation"></blockquote><h3 id=466--56249-a-strongreject-for-empty-jailbreaks-alexandra-souly-et-al-2024>(4/66 | 56/249) A StrongREJECT for Empty Jailbreaks (Alexandra Souly et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, Sam Toyer. (2024)<br><strong>A StrongREJECT for Empty Jailbreaks</strong><br><button class=copy-to-clipboard title="A StrongREJECT for Empty Jailbreaks" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, Zero-shot, GPT, GPT-4, Massive Multitask Language Understanding (MMLU), Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10260v1.pdf filename=2402.10260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has drawn attention to the existence of &ldquo;jailbreaks&rdquo; that allow the models to be used maliciously. However, there is no standard <b>benchmark</b> for measuring the severity of a jailbreak, leaving authors of jailbreak papers to create their own. We show that these <b>benchmarks</b> often include vague or unanswerable questions and use grading criteria that are biased towards overestimating the misuse potential of low-quality model responses. Some jailbreak techniques make the problem worse by decreasing the quality of model responses even on benign questions: we show that several jailbreaking techniques substantially reduce the <b>zero-shot</b> performance of <b>GPT-4</b> on <b>MMLU.</b> Jailbreaks can also make it harder to elicit harmful responses from an &ldquo;uncensored&rdquo; open-source model. We present a new <b>benchmark,</b> StrongREJECT, which better discriminates between effective and ineffective jailbreaks by using a higher-quality question set and a more accurate response grading algorithm. We show that our new grading scheme better accords with human judgment of response quality and overall jailbreak effectiveness, especially on the sort of low-quality responses that contribute the most to over-estimation of jailbreak performance on existing <b>benchmarks.</b> We release our code and data at <a href=https://github.com/alexandrasouly/strongreject>https://github.com/alexandrasouly/strongreject</a>.</p></p class="citation"></blockquote><h3 id=566--57249-class-balanced-and-reinforced-active-learning-on-graphs-chengcheng-yu-et-al-2024>(5/66 | 57/249) Class-Balanced and Reinforced Active Learning on Graphs (Chengcheng Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengcheng Yu, Jiapeng Zhu, Xiang Li. (2024)<br><strong>Class-Balanced and Reinforced Active Learning on Graphs</strong><br><button class=copy-to-clipboard title="Class-Balanced and Reinforced Active Learning on Graphs" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Graph Classification, Node Classification, Graph, Graph Neural Network, Graph Neural Network, Active Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10074v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10074v2.pdf filename=2402.10074v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have demonstrated significant success in various applications, such as <b>node</b> <b>classification,</b> link prediction, and <b>graph</b> <b>classification.</b> <b>Active</b> <b>learning</b> for <b>GNNs</b> aims to query the valuable samples from the unlabeled data for annotation to maximize the <b>GNNs&rsquo;</b> performance at a lower cost. However, most existing algorithms for reinforced <b>active</b> <b>learning</b> in <b>GNNs</b> may lead to a highly imbalanced class distribution, especially in highly skewed class scenarios. <b>GNNs</b> trained with class-imbalanced labeled data are susceptible to bias toward majority classes, and the lower performance of minority classes may lead to a decline in overall performance. To tackle this issue, we propose a novel class-balanced and reinforced <b>active</b> <b>learning</b> framework for <b>GNNs,</b> namely, GCBR. It learns an optimal policy to acquire class-balanced and informative <b>nodes</b> <b>for</b> annotation, maximizing the performance of <b>GNNs</b> trained with selected labeled <b>nodes.</b> <b>GCBR</b> designs class-balance-aware states, as well as a reward function that achieves trade-off between model performance and class balance. The <b>reinforcement</b> <b>learning</b> algorithm Advantage Actor-Critic (A2C) is employed to learn an optimal policy stably and efficiently. We further upgrade GCBR to GCBR++ by introducing a punishment mechanism to obtain a more class-balanced labeled set. Extensive experiments on multiple datasets demonstrate the effectiveness of the proposed approaches, achieving superior performance over state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=666--58249-covidhealth-a-benchmark-twitter-dataset-and-machine-learning-based-web-application-for-classifying-covid-19-discussions-mahathir-mohammad-bishal-et-al-2024>(6/66 | 58/249) COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web Application for Classifying COVID-19 Discussions (Mahathir Mohammad Bishal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahathir Mohammad Bishal, Md. Rakibul Hassan Chowdory, Anik Das, Muhammad Ashad Kabir. (2024)<br><strong>COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web Application for Classifying COVID-19 Discussions</strong><br><button class=copy-to-clipboard title="COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web Application for Classifying COVID-19 Discussions" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, Convolutional Neural Network, Logistic Regression, Stochastic Gradient Descent, BERT, LSTM, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09897v1.pdf filename=2402.09897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The COVID-19 pandemic has had adverse effects on both physical and mental health. During this pandemic, numerous studies have focused on gaining insights into health-related perspectives from social media. In this study, our primary objective is to develop a machine learning-based web application for automatically classifying COVID-19-related discussions on social media. To achieve this, we label COVID-19-related Twitter data, provide <b>benchmark</b> classification results, and develop a web application. We collected data using the Twitter API and labeled a total of 6,667 tweets into five different classes: health risks, prevention, symptoms, transmission, and treatment. We extracted features using various feature extraction methods and applied them to seven different traditional machine learning algorithms, including Decision Tree, Random Forest, <b>Stochastic</b> <b>Gradient</b> <b>Descent,</b> Adaboost, K-Nearest Neighbour, <b>Logistic</b> <b>Regression,</b> and Linear SVC. Additionally, we used four deep learning algorithms: <b>LSTM,</b> <b>CNN,</b> <b>RNN,</b> and <b>BERT,</b> for classification. Overall, we achieved a maximum F1 score of 90.43% with the <b>CNN</b> algorithm in deep learning. The Linear SVC algorithm exhibited the highest F1 score at 86.13%, surpassing other traditional machine learning approaches. Our study not only contributes to the field of health-related data analysis but also provides a valuable resource in the form of a web-based tool for efficient data classification, which can aid in addressing public health challenges and increasing awareness during pandemics. We made the dataset and application publicly available, which can be downloaded from this link <a href=https://github.com/Bishal16/COVID19-Health-Related-Data-Classification-Website>https://github.com/Bishal16/COVID19-Health-Related-Data-Classification-Website</a>.</p></p class="citation"></blockquote><h3 id=766--59249-multi-fidelity-methods-for-optimization-a-survey-ke-li-et-al-2024>(7/66 | 59/249) Multi-Fidelity Methods for Optimization: A Survey (Ke Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ke Li, Fan Li. (2024)<br><strong>Multi-Fidelity Methods for Optimization: A Survey</strong><br><button class=copy-to-clipboard title="Multi-Fidelity Methods for Optimization: A Survey" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 61<br>Keywords: Benchmarking, Benchmarking, Black Box, Simulation, Simulator, human-in-the-loop, Text Mining, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09638v1.pdf filename=2402.09638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world <b>black-box</b> <b>optimization</b> often involves time-consuming or costly experiments and <b>simulations.</b> Multi-fidelity optimization (MFO) stands out as a cost-effective strategy that balances high-fidelity accuracy with computational efficiency through a hierarchical fidelity approach. This survey presents a systematic exploration of MFO, underpinned by a novel <b>text</b> <b>mining</b> framework based on a <b>pre-trained</b> <b>language</b> <b>model.</b> We delve deep into the foundational principles and methodologies of MFO, focusing on three core components &ndash; multi-fidelity surrogate models, fidelity management strategies, and optimization techniques. Additionally, this survey highlights the diverse applications of MFO across several key domains, including machine learning, engineering design optimization, and scientific discovery, showcasing the adaptability and effectiveness of MFO in tackling complex computational challenges. Furthermore, we also envision several emerging challenges and prospects in the MFO landscape, spanning scalability, the composition of lower fidelities, and the integration of <b>human-in-the-loop</b> approaches at the algorithmic level. We also address critical issues related to <b>benchmarking</b> and the advancement of open science within the MFO community. Overall, this survey aims to catalyze further research and foster collaborations in MFO, setting the stage for future innovations and breakthroughs in the field.</p></p class="citation"></blockquote><h3 id=866--60249-can-we-soft-prompt-llms-for-graph-learning-tasks-zheyuan-liu-et-al-2024>(8/66 | 60/249) Can we soft prompt LLMs for graph learning tasks? (Zheyuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheyuan Liu, Xiaoxin He, Yijun Tian, Nitesh V. Chawla. (2024)<br><strong>Can we soft prompt LLMs for graph learning tasks?</strong><br><button class=copy-to-clipboard title="Can we soft prompt LLMs for graph learning tasks?" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 56<br>Keywords: Node Classification, Graph, Graph Neural Network, Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10359v1.pdf filename=2402.10359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>plays</b> <b>an</b> important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have achieved tremendous success in various domains, which makes applying <b>LLMs</b> to <b>graphs</b> <b>particularly</b> <b>appealing.</b> However, directly applying <b>LLMs</b> to <b>graph</b> <b>modalities</b> <b>presents</b> unique challenges due to the discrepancy and mismatch between the <b>graph</b> <b>and</b> <b>text</b> modalities. Hence, to further investigate <b>LLMs&rsquo;</b> potential for comprehending <b>graph</b> <b>information,</b> <b>we</b> introduce GraphPrompter, a novel framework designed to align <b>graph</b> <b>information</b> <b>with</b> <b>LLMs</b> via soft <b>prompts.</b> Specifically, GraphPrompter consists of two main components: a <b>graph</b> <b>neural</b> <b>network</b> to encode complex <b>graph</b> <b>information</b> <b>and</b> an <b>LLM</b> that effectively processes textual information. Comprehensive experiments on various <b>benchmark</b> datasets under <b>node</b> <b>classification</b> and link prediction tasks demonstrate the effectiveness of our proposed method. The GraphPrompter framework unveils the substantial capabilities of <b>LLMs</b> as predictors in <b>graph-related</b> <b>tasks,</b> <b>enabling</b> researchers to utilize <b>LLMs</b> across a spectrum of real-world <b>graph</b> <b>scenarios</b> <b>more</b> effectively.</p></p class="citation"></blockquote><h3 id=966--61249-bridging-associative-memory-and-probabilistic-modeling-rylan-schaeffer-et-al-2024>(9/66 | 61/249) Bridging Associative Memory and Probabilistic Modeling (Rylan Schaeffer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rylan Schaeffer, Nika Zahedi, Mikail Khona, Dhruv Pai, Sang Truong, Yilun Du, Mitchell Ostrow, Sarthak Chandra, Andres Carranza, Ila Rani Fiete, Andrey Gromov, Sanmi Koyejo. (2024)<br><strong>Bridging Associative Memory and Probabilistic Modeling</strong><br><button class=copy-to-clipboard title="Bridging Associative Memory and Probabilistic Modeling" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Clustering, Probabilistic Model, Recurrent Neural Network, Transformer, In-context Learning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10202v1.pdf filename=2402.10202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Associative memory and <b>probabilistic</b> <b>modeling</b> are two fundamental topics in artificial intelligence. The first studies <b>recurrent</b> <b>neural</b> <b>networks</b> designed to denoise, complete and retrieve data, whereas the second studies learning and sampling from probability distributions. Based on the observation that associative memory&rsquo;s energy functions can be seen as <b>probabilistic</b> <b>modeling&rsquo;s</b> negative log likelihoods, we build a bridge between the two that enables useful flow of ideas in both directions. We showcase four examples: First, we propose new energy-based models that flexibly adapt their energy functions to new <b>in-context</b> datasets, an approach we term \textit{in-context learning of energy functions}. Second, we propose two new associative memory models: one that dynamically creates new memories as necessitated by the training data using Bayesian nonparametrics, and another that explicitly computes proportional memory assignments using the evidence lower bound. Third, using tools from associative memory, we analytically and numerically characterize the memory capacity of Gaussian kernel density estimators, a widespread tool in probababilistic modeling. Fourth, we study a widespread implementation choice in <b>transformers</b> &ndash; normalization followed by self attention &ndash; to show it performs <b>clustering</b> on the hypersphere. Altogether, this work urges further exchange of useful ideas between these two continents of artificial intelligence.</p></p class="citation"></blockquote><h3 id=1066--62249-rethinking-information-structures-in-rlhf-reward-generalization-from-a-graph-theory-perspective-tianyi-qiu-et-al-2024>(10/66 | 62/249) Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective (Tianyi Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou, Han Yang, Josef Dai, Xuehai Pan, Yaodong Yang. (2024)<br><strong>Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective</strong><br><button class=copy-to-clipboard title="Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-DM, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Graph, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10184v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10184v3.pdf filename=2402.10184v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a trilemma in <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF):</b> the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling, and meanwhile propose new, generalizable methods of analysis that have wider applications, including potentially shedding light on goal misgeneralization. Specifically, we first reexamine the <b>RLHF</b> process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the <b>RLHF</b> objective of ensuring distributional consistency between human preference and <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> behavior. Based on this framework, we introduce a new method to model generalization in the reward modeling stage of <b>RLHF,</b> the induced Bayesian network (IBN). Drawing from random <b>graph</b> theory and causal analysis, it enables empirically grounded derivation of generalization error bounds, a key improvement over classical methods of generalization analysis. An insight from our analysis is the superiority of the tree-based information structure in reward modeling, compared to chain-based baselines in conventional <b>RLHF</b> methods. We derive that in complex contexts with limited data, the tree-based reward model (RM) induces up to $\Theta(\log n/\log\log n)$ times less variance than chain-based RM where $n$ is the dataset size. As validation, we demonstrate that on three NLP tasks, the tree-based RM achieves 65% win rate on average against chain-based baselines. Looking ahead, we hope to extend the IBN analysis to help understand the phenomenon of goal misgeneralization.</p></p class="citation"></blockquote><h3 id=1166--63249-all-in-one-and-one-for-all-a-simple-yet-effective-method-towards-cross-domain-graph-pretraining-haihong-zhao-et-al-2024>(11/66 | 63/249) All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining (Haihong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, Jia Li. (2024)<br><strong>All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining</strong><br><button class=copy-to-clipboard title="All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Graph, Few-shot, Few-shot Learning, Knowledge Distillation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09834v1.pdf filename=2402.09834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of <b>LLMs</b> is that a single model is trained on vast and diverse datasets spanning multiple domains &ndash; a paradigm we term <code>All in One'. This methodology empowers &lt;b>LLMs&lt;/b> with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single &lt;b>LLM&lt;/b> demonstrates remarkable versatility across a variety of domains -- a paradigm we term </code>One for All&rsquo;. However, applying this idea to the <b>graph</b> field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in <b>few-shot</b> <b>learning</b> scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called <b>Graph</b> COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse <b>graph</b> datasets to enhance <b>few-shot</b> <b>learning.</b> Our novel methodology involves a unification framework that amalgamates disparate <b>graph</b> datasets during the pretraining phase to <b>distill</b> and transfer meaningful knowledge to target tasks. Extensive experiments across multiple <b>graph</b> datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple <b>graph</b> datasets for pretraining, our work stands as a pioneering contribution to the realm of <b>graph</b> foundational model.</p></p class="citation"></blockquote><h3 id=1266--64249-non-orthogonal-age-optimal-information-dissemination-in-vehicular-networks-a-meta-multi-objective-reinforcement-learning-approach-a-a-habob-et-al-2024>(12/66 | 64/249) Non-orthogonal Age-Optimal Information Dissemination in Vehicular Networks: A Meta Multi-Objective Reinforcement Learning Approach (A. A. Habob et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>A. A. Habob, H. Tabassum, O. Waqar. (2024)<br><strong>Non-orthogonal Age-Optimal Information Dissemination in Vehicular Networks: A Meta Multi-Objective Reinforcement Learning Approach</strong><br><button class=copy-to-clipboard title="Non-orthogonal Age-Optimal Information Dissemination in Vehicular Networks: A Meta Multi-Objective Reinforcement Learning Approach" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12260v1.pdf filename=2402.12260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers minimizing the age-of-information (AoI) and transmit power consumption in a vehicular network, where a roadside unit (RSU) provides timely updates about a set of physical processes to vehicles. We consider non-orthogonal <b>multi-modal</b> information dissemination, which is based on superposed message transmission from RSU and successive interference cancellation (SIC) at vehicles. The formulated problem is a multi-objective mixed-integer nonlinear programming problem; thus, a Pareto-optimal front is very challenging to obtain. First, we leverage the weighted-sum approach to decompose the multi-objective problem into a set of multiple single-objective sub-problems corresponding to each predefined objective preference weight. Then, we develop a hybrid deep Q-network (DQN)-deep deterministic policy gradient (DDPG) model to solve each optimization sub-problem respective to predefined objective-preference weight. The DQN optimizes the decoding order, while the DDPG solves the continuous power allocation. The model needs to be retrained for each sub-problem. We then present a two-stage meta-multi-objective <b>reinforcement</b> <b>learning</b> solution to estimate the Pareto front with a few <b>fine-tuning</b> update steps without retraining the model for each sub-problem. <b>Simulation</b> results illustrate the efficacy of the proposed solutions compared to the existing <b>benchmarks</b> and that the meta-multi-objective <b>reinforcement</b> <b>learning</b> model estimates a high-quality Pareto frontier with reduced training time.</p></p class="citation"></blockquote><h3 id=1366--65249-generative-ai-and-process-systems-engineering-the-next-frontier-benjamin-decardi-nelson-et-al-2024>(13/66 | 65/249) Generative AI and Process Systems Engineering: The Next Frontier (Benjamin Decardi-Nelson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Decardi-Nelson, Abdulelah S. Alshehri, Akshay Ajagekar, Fengqi You. (2024)<br><strong>Generative AI and Process Systems Engineering: The Next Frontier</strong><br><button class=copy-to-clipboard title="Generative AI and Process Systems Engineering: The Next Frontier" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY, math-OC<br>Keyword Score: 43<br>Keywords: Benchmarking, Foundation Model, Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10977v1.pdf filename=2402.10977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article explores how emerging <b>generative</b> <b>artificial</b> intelligence (GenAI) models, such as <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> can enhance solution methodologies within process systems engineering (PSE). These cutting-edge GenAI models, particularly <b>foundation</b> <b>models</b> (FMs), which are pre-trained on extensive, general-purpose datasets, offer versatile adaptability for a broad range of tasks, including responding to queries, image generation, and complex decision-making. Given the close relationship between advancements in PSE and developments in computing and systems technologies, exploring the synergy between GenAI and PSE is essential. We begin our discussion with a compact overview of both classic and emerging GenAI models, including FMs, and then dive into their applications within key PSE domains: synthesis and design, optimization and integration, and process monitoring and control. In each domain, we explore how GenAI models could potentially advance PSE methodologies, providing insights and prospects for each area. Furthermore, the article identifies and discusses potential challenges in fully leveraging GenAI within PSE, including multiscale modeling, data requirements, evaluation metrics and <b>benchmarks,</b> and trust and safety, thereby deepening the discourse on effective GenAI integration into systems analysis, design, optimization, operations, monitoring, and control. This paper provides a guide for future research focused on the applications of emerging GenAI in PSE.</p></p class="citation"></blockquote><h3 id=1466--66249-node-duplication-improves-cold-start-link-prediction-zhichun-guo-et-al-2024>(14/66 | 66/249) Node Duplication Improves Cold-start Link Prediction (Zhichun Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhichun Guo, Tong Zhao, Yozen Liu, Kaiwen Dong, William Shiao, Neil Shah, Nitesh V. Chawla. (2024)<br><strong>Node Duplication Improves Cold-start Link Prediction</strong><br><button class=copy-to-clipboard title="Node Duplication Improves Cold-start Link Prediction" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Recommendation, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09711v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09711v1.pdf filename=2402.09711v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> are prominent in <b>graph</b> <b>machine</b> <b>learning</b> and have shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless, recent studies show that <b>GNNs</b> struggle to produce good results on low-degree nodes despite their overall strong performance. In practical applications of LP, like <b>recommendation</b> systems, improving performance on low-degree nodes is critical, as it amounts to tackling the cold-start problem of improving the experiences of users with few observed interactions. In this paper, we investigate improving <b>GNNs&rsquo;</b> LP performance on low-degree nodes while preserving their performance on high-degree nodes and propose a simple yet surprisingly effective augmentation technique called NodeDup. Specifically, NodeDup duplicates low-degree nodes and creates links between nodes and their own duplicates before following the standard <b>supervised</b> LP training scheme. By leveraging a &lsquo;&lsquo;multi-view&rsquo;&rsquo; perspective for low-degree nodes, NodeDup shows significant LP performance improvements on low-degree nodes without compromising any performance on high-degree nodes. Additionally, as a plug-and-play augmentation module, NodeDup can be easily applied to existing <b>GNNs</b> with very light computational cost. Extensive experiments show that NodeDup achieves 38.49%, 13.34%, and 6.76% improvements on isolated, low-degree, and warm nodes, respectively, on average across all datasets compared to <b>GNNs</b> and state-of-the-art cold-start methods.</p></p class="citation"></blockquote><h3 id=1566--67249-quick-quantization-aware-interleaving-and-conflict-free-kernel-for-efficient-llm-inference-taesu-kim-et-al-2024>(15/66 | 67/249) QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference (Taesu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taesu Kim, Jongho Lee, Daehyun Ahn, Sarang Kim, Jiwoong Choi, Minkyu Kim, Hyungjun Kim. (2024)<br><strong>QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference</strong><br><button class=copy-to-clipboard title="QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Quantization, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10076v1.pdf filename=2402.10076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce QUICK, a group of novel optimized CUDA kernels for the efficient inference of <b>quantized</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> QUICK addresses the shared memory bank-conflict problem of state-of-the-art mixed precision matrix multiplication kernels. Our method interleaves the <b>quantized</b> weight matrices of <b>LLMs</b> offline to skip the shared memory write-back after the dequantization. We demonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger batches and up to 1.94x throughput gain on representative <b>LLM</b> models on various NVIDIA GPU devices.</p></p class="citation"></blockquote><h3 id=1666--68249-how-to-train-data-efficient-llms-noveen-sachdeva-et-al-2024>(16/66 | 68/249) How to Train Data-Efficient LLMs (Noveen Sachdeva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed H. Chi, James Caverlee, Julian McAuley, Derek Zhiyuan Cheng. (2024)<br><strong>How to Train Data-Efficient LLMs</strong><br><button class=copy-to-clipboard title="How to Train Data-Efficient LLMs" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Zero-shot, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09668v1.pdf filename=2402.09668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The training of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is expensive. In this paper, we study data-efficient approaches for pre-training <b>LLMs,</b> i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. Our first technique, Ask-LLM, leverages the <b>zero-shot</b> <b>reasoning</b> capabilities of instruction-tuned <b>LLMs</b> to directly assess the quality of a training example. To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample. In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover the performance of the full data, while models trained on Ask-LLM data consistently outperform full-data training &ndash; even when we reject 90% of the original dataset, while converging up to 70% faster.</p></p class="citation"></blockquote><h3 id=1766--69249-large-language-models-for-forecasting-and-anomaly-detection-a-systematic-literature-review-jing-su-et-al-2024>(17/66 | 69/249) Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review (Jing Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Su, Chufeng Jiang, Xin Jin, Yuxin Qiao, Tingsong Xiao, Hongda Ma, Rong Wei, Zhi Jing, Jiajun Xu, Junhong Lin. (2024)<br><strong>Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review</strong><br><button class=copy-to-clipboard title="Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Anomaly Detection, Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10350v1.pdf filename=2402.10350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This systematic literature review comprehensively examines the application of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in forecasting and <b>anomaly</b> <b>detection,</b> highlighting the current state of research, inherent challenges, and prospective future directions. <b>LLMs</b> have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models&rsquo; knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating <b>multimodal</b> data, advancements in learning methodologies, and emphasizing model explainability and computational efficiency. Moreover, this review outlines critical trends that are likely to shape the evolution of <b>LLMs</b> in these fields, including the push toward real-time processing, the importance of sustainable modeling practices, and the value of interdisciplinary collaboration. Conclusively, this review underscores the transformative impact <b>LLMs</b> could have on forecasting and <b>anomaly</b> <b>detection</b> while emphasizing the need for continuous innovation, ethical considerations, and practical solutions to realize their full potential.</p></p class="citation"></blockquote><h3 id=1866--70249-smart-information-exchange-for-unsupervised-federated-learning-via-reinforcement-learning-seohyun-lee-et-al-2024>(18/66 | 70/249) Smart Information Exchange for Unsupervised Federated Learning via Reinforcement Learning (Seohyun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seohyun Lee, Anindya Bijoy Das, Satyavrat Wagle, Christopher G. Brinton. (2024)<br><strong>Smart Information Exchange for Unsupervised Federated Learning via Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Smart Information Exchange for Unsupervised Federated Learning via Reinforcement Learning" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Federated Learning, Reinforcement Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09629v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09629v1.pdf filename=2402.09629v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the main challenges of decentralized machine learning paradigms such as <b>Federated</b> <b>Learning</b> (FL) is the presence of local non-i.i.d. datasets. Device-to-device transfers (D2D) between distributed devices has been shown to be an effective tool for dealing with this problem and robust to stragglers. In an <b>unsupervised</b> case, however, it is not obvious how data exchanges should take place due to the absence of labels. In this paper, we propose an approach to create an optimal <b>graph</b> for data transfer using <b>Reinforcement</b> <b>Learning.</b> The goal is to form links that will provide the most benefit considering the environment&rsquo;s constraints and improve convergence speed in an <b>unsupervised</b> FL environment. Numerical analysis shows the advantages in terms of convergence speed and straggler resilience of the proposed method to different available FL schemes and <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=1966--71249-multi-excitation-projective-simulation-with-a-many-body-physics-inspired-inductive-bias-philip-a-lemaitre-et-al-2024>(19/66 | 71/249) Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias (Philip A. LeMaitre et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip A. LeMaitre, Marius Krumm, Hans J. Briegel. (2024)<br><strong>Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias</strong><br><button class=copy-to-clipboard title="Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DM, cs-LG, cs.LG, quant-ph<br>Keyword Score: 33<br>Keywords: Graph, Quantization, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10192v1.pdf filename=2402.10192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI). One method in this field known as Projective <b>Simulation</b> (PS) models a chain-of-thought as a random walk of a particle on a <b>graph</b> with vertices that have concepts attached to them. While this description has various benefits, including the possibility of <b>quantization,</b> it cannot be naturally used to model thoughts that combine several concepts simultaneously. To overcome this limitation, we introduce Multi-Excitation Projective <b>Simulation</b> (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph. A definition for a dynamic hypergraph is put forward to describe the agent&rsquo;s training history along with applications to AI and hypergraph visualization. An inductive bias inspired by the remarkably successful few-body interaction models used in quantum many-body physics is formalized for our classical mePS framework and employed to tackle the exponential complexity associated with naive implementations of hypergraphs. We prove that our inductive bias reduces the complexity from exponential to polynomial, with the exponent representing the cutoff on how many particles can interact. We numerically apply our method to two toy environments and a more complex scenario modelling the diagnosis of a broken computer. These environments demonstrate the resource savings provided by an appropriate choice of inductive bias, as well as showcasing aspects of interpretability. A quantum model for mePS is also briefly outlined and some future directions for it are discussed.</p></p class="citation"></blockquote><h3 id=2066--72249-f-micl-understanding-and-generalizing-infonce-based-contrastive-learning-yiwei-lu-et-al-2024>(20/66 | 72/249) $f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning (Yiwei Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwei Lu, Guojun Zhang, Sun Sun, Hongyu Guo, Yaoliang Yu. (2024)<br><strong>$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning</strong><br><button class=copy-to-clipboard title="$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Contrastive Learning, Mutual Information, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10150v1.pdf filename=2402.10150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>self-supervised</b> <b>contrastive</b> <b>learning,</b> a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback-Leibler (KL)-based <b>mutual</b> <b>information.</b> In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides the popular cosine similarity, can we design a better similarity function? We provide answers to both questions by generalizing the KL-based <b>mutual</b> <b>information</b> to the $f$-Mutual Information in <b>Contrastive</b> <b>Learning</b> ($f$-MICL) using the $f$-divergences. To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance. For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel, we derive an $f$-Gaussian similarity with better interpretability and empirical performance. Finally, we identify close relationships between the $f$-MICL objective and several popular InfoNCE-based objectives. Using <b>benchmark</b> tasks from both vision and natural language, we empirically evaluate $f$-MICL with different $f$-divergences on various architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that $f$-MICL generally outperforms the <b>benchmarks</b> and the best-performing $f$-divergence is task and dataset dependent.</p></p class="citation"></blockquote><h3 id=2166--73249-utilizing-gans-for-fraud-detection-model-training-with-synthetic-transaction-data-mengran-zhu-et-al-2024>(21/66 | 73/249) Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data (Mengran Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengran Zhu, Yulu Gong, Yafei Xiang, Hanyi Yu, Shuning Huo. (2024)<br><strong>Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data</strong><br><button class=copy-to-clipboard title="Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CE, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Anomaly Detection, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09830v1.pdf filename=2402.09830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Anomaly</b> <b>detection</b> is a critical challenge across various research domains, aiming to identify instances that deviate from normal data distributions. This paper explores the application of <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> in fraud detection, comparing their advantages with traditional methods. <b>GANs,</b> a type of Artificial Neural Network (ANN), have shown promise in modeling complex data distributions, making them effective tools for <b>anomaly</b> <b>detection.</b> The paper systematically describes the principles of <b>GANs</b> and their derivative models, emphasizing their application in fraud detection across different datasets. And by building a collection of adversarial verification <b>graphs,</b> we will effectively prevent fraud caused by bots or automated systems and ensure that the users in the transaction are real. The objective of the experiment is to design and implement a fake face verification code and fraud detection system based on <b>Generative</b> <b>Adversarial</b> <b>network</b> <b>(GANs)</b> algorithm to enhance the security of the transaction process.The study demonstrates the potential of <b>GANs</b> in enhancing transaction security through deep learning techniques.</p></p class="citation"></blockquote><h3 id=2266--74249-revisiting-experience-replayable-conditions-taisuke-kobayashi-2024>(22/66 | 74/249) Revisiting Experience Replayable Conditions (Taisuke Kobayashi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taisuke Kobayashi. (2024)<br><strong>Revisiting Experience Replayable Conditions</strong><br><button class=copy-to-clipboard title="Revisiting Experience Replayable Conditions" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10374v1.pdf filename=2402.10374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Experience replay (ER) used in (deep) <b>reinforcement</b> <b>learning</b> is considered to be applicable only to off-policy algorithms. However, there have been some cases in which ER has been applied for on-policy algorithms, suggesting that off-policyness might be a sufficient condition for applying ER. This paper reconsiders more strict &ldquo;experience replayable conditions&rdquo; (ERC) and proposes the way of modifying the existing algorithms to satisfy ERC. To this end, instability of policy improvements is assumed to be a key in ERC. The instability factors are revealed from the viewpoint of metric learning as i) repulsive forces from negative samples and ii) replays of inappropriate experiences. Accordingly, the corresponding stabilization tricks are derived. As a result, it is confirmed through numerical <b>simulations</b> that the proposed stabilization tricks make ER applicable to an advantage actor-critic, an on-policy algorithm. In addition, its learning performance is comparable to that of a soft actor-critic, a state-of-the-art off-policy algorithm.</p></p class="citation"></blockquote><h3 id=2366--75249-exploration-driven-policy-optimization-in-rlhf-theoretical-insights-on-efficient-data-utilization-yihan-du-et-al-2024>(23/66 | 75/249) Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization (Yihan Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihan Du, Anna Winnicki, Gal Dalal, Shie Mannor, R. Srikant. (2024)<br><strong>Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization</strong><br><button class=copy-to-clipboard title="Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10342v1.pdf filename=2402.10342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an <b>RLHF</b> algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with <b>RLHF.</b> A key novelty is our trajectory-level elliptical potential analysis technique used to infer reward function parameters when comparison queries rather than reward observations are used. We provide and analyze algorithms in two settings: linear and neural function approximation, PG-RLHF and NN-PG-RLHF, respectively.</p></p class="citation"></blockquote><h3 id=2466--76249-hierarchical-state-space-models-for-continuous-sequence-to-sequence-modeling-raunaq-bhirangi-et-al-2024>(24/66 | 76/249) Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling (Raunaq Bhirangi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raunaq Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi, Abhinav Gupta, Tess Hellebrekers, Lerrel Pinto. (2024)<br><strong>Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling</strong><br><button class=copy-to-clipboard title="Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG, eess-SP<br>Keyword Score: 30<br>Keywords: LSTM, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10211v1.pdf filename=2402.10211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reasoning</b> from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal <b>Transformers,</b> <b>LSTMs,</b> S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on <a href=https://hiss-csp.github.io>https://hiss-csp.github.io</a>.</p></p class="citation"></blockquote><h3 id=2566--77249-recovering-the-pre-fine-tuning-weights-of-generative-models-eliahu-horwitz-et-al-2024>(25/66 | 77/249) Recovering the Pre-Fine-Tuning Weights of Generative Models (Eliahu Horwitz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eliahu Horwitz, Jonathan Kahana, Yedid Hoshen. (2024)<br><strong>Recovering the Pre-Fine-Tuning Weights of Generative Models</strong><br><button class=copy-to-clipboard title="Recovering the Pre-Fine-Tuning Weights of Generative Models" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Mistral<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10208v1.pdf filename=2402.10208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via <b>fine-tuning.</b> This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) <b>fine-tuned</b> models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned <b>Mistral.</b></p></p class="citation"></blockquote><h3 id=2666--78249-fedanchor-enhancing-federated-semi-supervised-learning-with-label-contrastive-loss-for-unlabeled-clients-xinchi-qiu-et-al-2024>(26/66 | 78/249) FedAnchor: Enhancing Federated Semi-Supervised Learning with Label Contrastive Loss for Unlabeled Clients (Xinchi Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinchi Qiu, Yan Gao, Lorenzo Sani, Heng Pan, Wanru Zhao, Pedro P. B. Gusmao, Mina Alibeigi, Alex Iacob, Nicholas D. Lane. (2024)<br><strong>FedAnchor: Enhancing Federated Semi-Supervised Learning with Label Contrastive Loss for Unlabeled Clients</strong><br><button class=copy-to-clipboard title="FedAnchor: Enhancing Federated Semi-Supervised Learning with Label Contrastive Loss for Unlabeled Clients" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Semi-Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10191v1.pdf filename=2402.10191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is a distributed learning paradigm that facilitates collaborative training of a shared global model across devices while keeping data localized. The deployment of FL in numerous real-world applications faces delays, primarily due to the prevalent reliance on <b>supervised</b> tasks. Generating detailed labels at edge devices, if feasible, is demanding, given resource constraints and the imperative for continuous data updates. In addressing these challenges, solutions such as <b>federated</b> <b>semi-supervised</b> <b>learning</b> (FSSL), which relies on unlabeled clients&rsquo; data and a limited amount of labeled data on the server, become pivotal. In this paper, we propose FedAnchor, an innovative FSSL method that introduces a unique double-head structure, called anchor head, paired with the classification head trained exclusively on labeled anchor data on the server. The anchor head is empowered with a newly designed label contrastive loss based on the cosine similarity metric. Our approach mitigates the confirmation bias and overfitting issues associated with pseudo-labeling techniques based on high-confidence model prediction samples. Extensive experiments on CIFAR10/100 and SVHN datasets demonstrate that our method outperforms the state-of-the-art method by a significant margin in terms of convergence rate and model accuracy.</p></p class="citation"></blockquote><h3 id=2766--79249-revisiting-recurrent-reinforcement-learning-with-memory-monoids-steven-morad-et-al-2024>(27/66 | 79/249) Revisiting Recurrent Reinforcement Learning with Memory Monoids (Steven Morad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Steven Morad, Chris Lu, Ryan Kortvelesy, Stephan Liwicki, Jakob Foerster, Amanda Prorok. (2024)<br><strong>Revisiting Recurrent Reinforcement Learning with Memory Monoids</strong><br><button class=copy-to-clipboard title="Revisiting Recurrent Reinforcement Learning with Memory Monoids" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09900v1.pdf filename=2402.09900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In RL, memory models such as <b>RNNs</b> and <b>transformers</b> address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.</p></p class="citation"></blockquote><h3 id=2866--80249-performative-reinforcement-learning-in-gradually-shifting-environments-ben-rank-et-al-2024>(28/66 | 80/249) Performative Reinforcement Learning in Gradually Shifting Environments (Ben Rank et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Rank, Stelios Triantafyllou, Debmalya Mandal, Goran Radanovic. (2024)<br><strong>Performative Reinforcement Learning in Gradually Shifting Environments</strong><br><button class=copy-to-clipboard title="Performative Reinforcement Learning in Gradually Shifting Environments" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09838v1.pdf filename=2402.09838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When <b>Reinforcement</b> <b>Learning</b> (RL) agents are deployed in practice, they might impact their environment and change its dynamics. Ongoing research attempts to formally model this phenomenon and to analyze learning algorithms in these models. To this end, we propose a framework where the current environment depends on the deployed policy as well as its previous dynamics. This is a generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our framework allows to model scenarios where the environment gradually adjusts to a deployed policy. We adapt two algorithms from the performative prediction literature to our setting and propose a novel algorithm called Mixed Delayed Repeated Retraining (MDRR). We provide conditions under which these algorithms converge and compare them using three metrics: number of retrainings, approximation guarantee, and number of samples per deployment. Unlike previous approaches, MDRR combines samples from multiple deployments in its training. This makes MDRR particularly suitable for scenarios where the environment&rsquo;s response strongly depends on its previous dynamics, which are common in practice. We experimentally compare the algorithms using a <b>simulation-based</b> testbed and our results show that MDRR converges significantly faster than previous approaches.</p></p class="citation"></blockquote><h3 id=2966--81249-language-models-with-conformal-factuality-guarantees-christopher-mohri-et-al-2024>(29/66 | 81/249) Language Models with Conformal Factuality Guarantees (Christopher Mohri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher Mohri, Tatsunori Hashimoto. (2024)<br><strong>Language Models with Conformal Factuality Guarantees</strong><br><button class=copy-to-clipboard title="Language Models with Conformal Factuality Guarantees" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Black Box, Question Answering, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10978v1.pdf filename=2402.10978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. We observe that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM&rsquo;s output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any <b>black-box</b> <b>LM</b> and requires very few human-annotated samples. Evaluations of our approach on closed book <b>QA</b> (FActScore, NaturalQuestions) and <b>reasoning</b> tasks (MATH) show that our approach can provide 80-90% correctness guarantees while retaining the majority of the LM&rsquo;s original output.</p></p class="citation"></blockquote><h3 id=3066--82249-reward-poisoning-attack-against-offline-reinforcement-learning-yinglun-xu-et-al-2024>(30/66 | 82/249) Reward Poisoning Attack Against Offline Reinforcement Learning (Yinglun Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinglun Xu, Rohan Gumaste, Gagandeep Singh. (2024)<br><strong>Reward Poisoning Attack Against Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Reward Poisoning Attack Against Offline Reinforcement Learning" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Black Box, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09695v1.pdf filename=2402.09695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of reward poisoning attacks against general <b>offline</b> <b>reinforcement</b> <b>learning</b> with deep neural networks for function approximation. We consider a <b>black-box</b> <b>threat</b> model where the attacker is completely oblivious to the learning algorithm and its budget is limited by constraining both the amount of corruption at each data point, and the total perturbation. We propose an attack strategy called `policy contrast attack&rsquo;. The high-level idea is to make some low-performing policies appear as high-performing while making high-performing policies appear as low-performing. To the best of our knowledge, we propose the first <b>black-box</b> <b>reward</b> poisoning attack in the general <b>offline</b> <b>RL</b> <b>setting.</b> We provide theoretical insights on the attack design and empirically show that our attack is efficient against current state-of-the-art <b>offline</b> <b>RL</b> <b>algorithms</b> in different kinds of learning datasets.</p></p class="citation"></blockquote><h3 id=3166--83249-ising-on-the-graph-task-specific-graph-subsampling-via-the-ising-model-maria-bånkestad-et-al-2024>(31/66 | 83/249) Ising on the Graph: Task-specific Graph Subsampling via the Ising Model (Maria Bånkestad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maria Bånkestad, Jennifer Andersson, Sebastian Mair, Jens Sjölund. (2024)<br><strong>Ising on the Graph: Task-specific Graph Subsampling via the Ising Model</strong><br><button class=copy-to-clipboard title="Ising on the Graph: Task-specific Graph Subsampling via the Ising Model" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10206v1.pdf filename=2402.10206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reducing a <b>graph</b> <b>while</b> <b>preserving</b> its overall structure is an important problem with many applications. Typically, the reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an <b>unsupervised</b> way with no specific downstream task in mind. In this paper, we present an approach for subsampling <b>graph</b> <b>structures</b> <b>using</b> an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a <b>graph</b> <b>neural</b> <b>network.</b> Our approach is task-specific as it can learn how to reduce a <b>graph</b> <b>for</b> <b>a</b> specific downstream task in an end-to-end fashion. The utilized loss function of the task does not even have to be differentiable. We showcase the versatility of our approach on three distinct applications: image segmentation, 3D shape sparsification, and sparse approximate matrix inverse determination.</p></p class="citation"></blockquote><h3 id=3266--84249-learnability-is-a-compact-property-julian-asilis-et-al-2024>(32/66 | 84/249) Learnability is a Compact Property (Julian Asilis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julian Asilis, Siddartha Devic, Shaddin Dughmi, Vatsal Sharan, Shang-Hua Teng. (2024)<br><strong>Learnability is a Compact Property</strong><br><button class=copy-to-clipboard title="Learnability is a Compact Property" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CC, cs-DS, cs-LG, cs-LO, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10360v1.pdf filename=2402.10360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work on learning has yielded a striking result: the learnability of various problems can be undecidable, or independent of the standard ZFC axioms of set theory. Furthermore, the learnability of such problems can fail to be a property of finite character: informally, it cannot be detected by examining finite projections of the problem. On the other hand, learning theory abounds with notions of dimension that characterize learning and consider only finite restrictions of the problem, i.e., are properties of finite character. How can these results be reconciled? More precisely, which classes of learning problems are vulnerable to logical undecidability, and which are within the grasp of finite characterizations? We demonstrate that the difficulty of <b>supervised</b> <b>learning</b> with metric losses admits a tight finite characterization. In particular, we prove that the sample complexity of learning a hypothesis class can be detected by examining its finite projections. For realizable and agnostic learning with respect to a wide class of proper loss functions, we demonstrate an exact compactness result: a class is learnable with a given sample complexity precisely when the same is true of all its finite projections. For realizable learning with improper loss functions, we show that exact compactness of sample complexity can fail, and provide matching upper and lower bounds of a factor of 2 on the extent to which such sample complexities can differ. We conjecture that larger gaps are possible for the agnostic case. At the heart of our technical work is a compactness result concerning assignments of variables that maintain a class of functions below a target value, which generalizes Hall&rsquo;s classic matching theorem and may be of independent interest.</p></p class="citation"></blockquote><h3 id=3366--85249-what-to-do-when-your-discrete-optimization-is-the-size-of-a-neural-network-hugo-silva-et-al-2024>(33/66 | 85/249) What to Do When Your Discrete Optimization Is the Size of a Neural Network? (Hugo Silva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugo Silva, Martha White. (2024)<br><strong>What to Do When Your Discrete Optimization Is the Size of a Neural Network?</strong><br><button class=copy-to-clipboard title="What to Do When Your Discrete Optimization Is the Size of a Neural Network?" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Continual Learning, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10339v1.pdf filename=2402.10339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Oftentimes, machine learning applications using neural networks involve solving discrete optimization problems, such as in <b>pruning,</b> parameter-isolation-based <b>continual</b> <b>learning</b> and training of binary networks. Still, these discrete problems are combinatorial in nature and are also not amenable to gradient-based optimization. Additionally, classical approaches used in discrete settings do not scale well to large neural networks, forcing scientists and empiricists to rely on alternative methods. Among these, two main distinct sources of top-down information can be used to lead the model to good solutions: (1) extrapolating gradient information from points outside of the solution set (2) comparing evaluations between members of a subset of the valid solutions. We take continuation path (CP) methods to represent using purely the former and Monte Carlo (MC) methods to represent the latter, while also noting that some hybrid methods combine the two. The main goal of this work is to compare both approaches. For that purpose, we first overview the two classes while also discussing some of their drawbacks analytically. Then, on the experimental section, we compare their performance, starting with smaller microworld experiments, which allow more fine-grained control of problem variables, and gradually moving towards larger problems, including neural network regression and neural network <b>pruning</b> for image classification, where we additionally compare against magnitude-based <b>pruning.</b></p></p class="citation"></blockquote><h3 id=3466--86249-interpretable-generative-adversarial-imitation-learning-wenliang-liu-et-al-2024>(34/66 | 86/249) Interpretable Generative Adversarial Imitation Learning (Wenliang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenliang Liu, Danyang Li, Erfan Aasi, Roberto Tron, Calin Belta. (2024)<br><strong>Interpretable Generative Adversarial Imitation Learning</strong><br><button class=copy-to-clipboard title="Interpretable Generative Adversarial Imitation Learning" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10310v1.pdf filename=2402.10310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation learning methods have demonstrated considerable success in teaching autonomous systems complex tasks through expert demonstrations. However, a limitation of these methods is their lack of interpretability, particularly in understanding the specific task the learning agent aims to accomplish. In this paper, we propose a novel imitation learning method that combines Signal Temporal Logic (STL) inference and control synthesis, enabling the explicit representation of the task as an STL formula. This approach not only provides a clear understanding of the task but also allows for the incorporation of human knowledge and adaptation to new scenarios through manual adjustments of the STL formulae. Additionally, we employ a <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN)-inspired</b> training approach for both the inference and the control policy, effectively narrowing the gap between the expert and learned policies. The effectiveness of our algorithm is demonstrated through two case studies, showcasing its practical applicability and adaptability.</p></p class="citation"></blockquote><h3 id=3566--87249-discrete-probabilistic-inference-as-control-in-multi-path-environments-tristan-deleu-et-al-2024>(35/66 | 87/249) Discrete Probabilistic Inference as Control in Multi-path Environments (Tristan Deleu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, Yoshua Bengio. (2024)<br><strong>Discrete Probabilistic Inference as Control in Multi-path Environments</strong><br><button class=copy-to-clipboard title="Discrete Probabilistic Inference as Control in Multi-path Environments" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10309v1.pdf filename=2402.10309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of sampling from a discrete and structured distribution as a sequential decision problem, where the objective is to find a stochastic policy such that objects are sampled at the end of this sequential process proportionally to some predefined reward. While we could use maximum entropy <b>Reinforcement</b> <b>Learning</b> (MaxEnt RL) to solve this problem for some distributions, it has been shown that in general, the distribution over states induced by the optimal policy may be biased in cases where there are multiple ways to generate the same object. To address this issue, Generative Flow Networks (GFlowNets) learn a stochastic policy that samples objects proportionally to their reward by approximately enforcing a conservation of flows across the whole <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP). In this paper, we extend recent methods correcting the reward in order to guarantee that the marginal distribution induced by the optimal MaxEnt RL policy is proportional to the original reward, regardless of the structure of the underlying MDP. We also prove that some flow-matching objectives found in the GFlowNet literature are in fact equivalent to well-established MaxEnt RL algorithms with a corrected reward. Finally, we study empirically the performance of multiple MaxEnt RL and GFlowNet algorithms on multiple problems involving sampling from discrete distributions.</p></p class="citation"></blockquote><h3 id=3666--88249-an-evaluation-of-real-time-adaptive-sampling-change-point-detection-algorithm-using-kcusum-vijayalakshmi-saravanan-et-al-2024>(36/66 | 88/249) An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM (Vijayalakshmi Saravanan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vijayalakshmi Saravanan, Perry Siehien, Shinjae Yoo, Hubertus Van Dam, Thomas Flynn, Christopher Kelly, Khaled Z Ibrahim. (2024)<br><strong>An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM</strong><br><button class=copy-to-clipboard title="An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: CCS, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10291v1.pdf filename=2402.10291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting abrupt changes in real-time data streams from scientific <b>simulations</b> presents a challenging task, demanding the deployment of accurate and efficient algorithms. Identifying change points in live data stream involves continuous scrutiny of incoming observations for deviations in their statistical characteristics, particularly in high-volume data scenarios. Maintaining a balance between sudden change detection and minimizing false alarms is vital. Many existing algorithms for this purpose rely on known probability distributions, limiting their feasibility. In this study, we introduce the Kernel-based Cumulative Sum (KCUSUM) algorithm, a non-parametric extension of the traditional Cumulative Sum (CUSUM) method, which has gained prominence for its efficacy in online change point detection under less restrictive conditions. KCUSUM splits itself by comparing incoming samples directly with reference samples and computes a statistic grounded in the Maximum Mean Discrepancy (MMD) non-parametric framework. This approach extends KCUSUM&rsquo;s pertinence to scenarios where only reference samples are available, such as atomic trajectories of proteins in vacuum, facilitating the detection of deviations from the reference sample without prior knowledge of the data&rsquo;s underlying distribution. Furthermore, by harnessing MMD&rsquo;s inherent random-walk structure, we can theoretically analyze KCUSUM&rsquo;s performance across various use cases, including metrics like expected delay and mean runtime to false alarms. Finally, we discuss real-world use cases from scientific <b>simulations</b> such as NWChem CODAR and protein folding data, demonstrating KCUSUM&rsquo;s practical effectiveness in online change point detection.</p></p class="citation"></blockquote><h3 id=3766--89249-susfl-energy-aware-federated-learning-based-monitoring-for-sustainable-smart-farms-dian-chen-et-al-2024>(37/66 | 89/249) SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms (Dian Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dian Chen, Paul Yang, Ing-Ray Chen, Dong Sam Ha, Jin-Hee Cho. (2024)<br><strong>SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms</strong><br><button class=copy-to-clipboard title="SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10280v1.pdf filename=2402.10280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel energy-aware <b>federated</b> <b>learning</b> (FL)-based system, namely SusFL, for sustainable smart farming to address the challenge of inconsistent health monitoring due to fluctuating energy levels of solar sensors. This system equips animals, such as cattle, with solar sensors with computational capabilities, including Raspberry Pis, to train a local deep-learning model on health data. These sensors periodically update Long Range (LoRa) gateways, forming a wireless sensor network (WSN) to detect diseases like mastitis. Our proposed SusFL system incorporates mechanism design, a game theory concept, for intelligent client selection to optimize monitoring quality while minimizing energy use. This strategy ensures the system&rsquo;s sustainability and resilience against <b>adversarial</b> <b>attacks,</b> including data poisoning and privacy threats, that could disrupt FL operations. Through extensive comparative analysis using real-time datasets, we demonstrate that our FL-based monitoring system significantly outperforms existing methods in prediction accuracy, operational efficiency, system reliability (i.e., mean time between failures or MTBF), and social welfare maximization by the mechanism designer. Our findings validate the superiority of our system for effective and sustainable animal health monitoring in smart farms. The experimental results show that SusFL significantly improves system performance, including a $10%$ reduction in energy consumption, a $15%$ increase in social welfare, and a $34%$ rise in Mean Time Between Failures (MTBF), alongside a marginal increase in the global model&rsquo;s prediction accuracy.</p></p class="citation"></blockquote><h3 id=3866--90249-tracking-changing-probabilities-via-dynamic-learners-omid-madani-2024>(38/66 | 90/249) Tracking Changing Probabilities via Dynamic Learners (Omid Madani, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omid Madani. (2024)<br><strong>Tracking Changing Probabilities via Dynamic Learners</strong><br><button class=copy-to-clipboard title="Tracking Changing Probabilities via Dynamic Learners" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T05, I-2-6, cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10142v1.pdf filename=2402.10142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Consider a predictor, a learner, whose input is a stream of discrete items. The predictor&rsquo;s task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation. To output probabilities, the predictor keeps track of the proportions of the items it has seen. The predictor has constant (limited) space and we seek efficient prediction and update techniques: The stream is unbounded, the set of items is unknown to the predictor and their totality can also grow unbounded. Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time. For instance, new items may start appearing and a few currently frequent items may cease to occur again. The predictor, being space-bounded, need only provide probabilities for those items with (currently) sufficiently high frequency, i.e., the salient items. This problem is motivated in the setting of prediction games, a <b>self-supervised</b> <b>learning</b> regime where concepts serve as both the predictors and the predictands, and the set of concepts grows over time, resulting in non-stationarities as new concepts are generated and used. We develop moving average techniques designed to respond to such non-stationarities in a timely manner, and explore their properties. One is a simple technique based on queuing of count snapshots, and another is a combination of queuing together with an extended version of sparse EMA. The latter combination supports predictand-specific dynamic learning rates. We find that this flexibility allows for a more accurate and timely convergence.</p></p class="citation"></blockquote><h3 id=3966--91249-balancing-the-causal-effects-in-class-incremental-learning-junhao-zheng-et-al-2024>(39/66 | 91/249) Balancing the Causal Effects in Class-Incremental Learning (Junhao Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhao Zheng, Ruiyan Wang, Chongzhi Zhang, Huawen Feng, Qianli Ma. (2024)<br><strong>Balancing the Causal Effects in Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="Balancing the Causal Effects in Class-Incremental Learning" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Named Entity Recognition, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10063v1.pdf filename=2402.10063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Class-Incremental Learning (CIL) is a practical and challenging problem for achieving general artificial intelligence. Recently, Pre-Trained Models (PTMs) have led to breakthroughs in both visual and natural language processing tasks. Despite recent studies showing PTMs&rsquo; potential ability to learn sequentially, a plethora of work indicates the necessity of alleviating the catastrophic forgetting of PTMs. Through a pilot study and a causal analysis of CIL, we reveal that the crux lies in the imbalanced causal effects between new and old data. Specifically, the new data encourage models to adapt to new classes while hindering the adaptation of old classes. Similarly, the old data encourages models to adapt to old classes while hindering the adaptation of new classes. In other words, the adaptation process between new and old classes conflicts from the causal perspective. To alleviate this problem, we propose Balancing the Causal Effects (BaCE) in CIL. Concretely, BaCE proposes two objectives for building causal paths from both new and old data to the prediction of new and classes, respectively. In this way, the model is encouraged to adapt to all classes with causal effects from both new and old data and thus alleviates the causal imbalance problem. We conduct extensive experiments on continual image classification, continual <b>text</b> <b>classification,</b> and continual <b>named</b> <b>entity</b> <b>recognition.</b> Empirical results show that BaCE outperforms a series of CIL methods on different tasks and settings.</p></p class="citation"></blockquote><h3 id=4066--92249-explaining-probabilistic-models-with-distributional-values-luca-franceschi-et-al-2024>(40/66 | 92/249) Explaining Probabilistic Models with Distributional Values (Luca Franceschi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Franceschi, Michele Donini, Cédric Archambeau, Matthias Seeger. (2024)<br><strong>Explaining Probabilistic Models with Distributional Values</strong><br><button class=copy-to-clipboard title="Explaining Probabilistic Models with Distributional Values" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09947v1.pdf filename=2402.09947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A large branch of explainable machine learning is grounded in cooperative game theory. However, research indicates that game-theoretic explanations may mislead or be hard to interpret. We argue that often there is a critical mismatch between what one wishes to explain (e.g. the output of a classifier) and what current methods such as SHAP explain (e.g. the scalar probability of a class). This paper addresses such gap for <b>probabilistic</b> <b>models</b> by generalising cooperative games and value operators. We introduce the distributional values, random variables that track changes in the model output (e.g. flipping of the predicted class) and derive their analytic expressions for games with Gaussian, Bernoulli and Categorical payoffs. We further establish several characterising properties, and show that our framework provides fine-grained and insightful explanations with case studies on vision and language models.</p></p class="citation"></blockquote><h3 id=4166--93249-a-data-driven-supervised-machine-learning-approach-to-estimating-global-ambient-air-pollution-concentrations-with-associated-prediction-intervals-liam-j-berrisford-et-al-2024>(41/66 | 93/249) A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals (Liam J Berrisford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liam J Berrisford, Hugo Barbosa, Ronaldo Menezes. (2024)<br><strong>A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals</strong><br><button class=copy-to-clipboard title="A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Recommendation, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10248v1.pdf filename=2402.10248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Global ambient air pollution, a transboundary challenge, is typically addressed through interventions relying on data from spatially sparse and heterogeneously placed monitoring stations. These stations often encounter temporal data gaps due to issues such as power outages. In response, we have developed a scalable, data-driven, <b>supervised</b> machine learning framework. This model is designed to impute missing temporal and spatial measurements, thereby generating a comprehensive dataset for pollutants including NO$<em>2$, O$<em>3$, PM$</em>{10}$, PM$</em>{2.5}$, and SO$_2$. The dataset, with a fine granularity of 0.25$^{\circ}$ at hourly intervals and accompanied by prediction intervals for each estimate, caters to a wide range of stakeholders relying on outdoor air pollution data for downstream assessments. This enables more detailed studies. Additionally, the model&rsquo;s performance across various geographical locations is examined, providing insights and <b>recommendations</b> for strategic placement of future monitoring stations to further enhance the model&rsquo;s accuracy.</p></p class="citation"></blockquote><h3 id=4266--94249-tinycl-an-efficient-hardware-architecture-for-continual-learning-on-autonomous-systems-eugenio-ressa-et-al-2024>(42/66 | 94/249) TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems (Eugenio Ressa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eugenio Ressa, Alberto Marchisio, Maurizio Martina, Guido Masera, Muhammad Shafique. (2024)<br><strong>TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems</strong><br><button class=copy-to-clipboard title="TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Continual Learning, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09780v1.pdf filename=2402.09780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Continuous Learning (CL) paradigm consists of continuously evolving the parameters of the Deep Neural Network (DNN) model to progressively learn to perform new tasks without reducing the performance on previous tasks, i.e., avoiding the so-called catastrophic forgetting. However, the DNN parameter update in CL-based autonomous systems is extremely resource-hungry. The existing DNN accelerators cannot be directly employed in CL because they only support the execution of the forward propagation. Only a few prior architectures execute the backpropagation and weight update, but they lack the control and management for CL. Towards this, we design a hardware architecture, TinyCL, to perform CL on resource-constrained autonomous systems. It consists of a processing unit that executes both forward and backward propagation, and a control unit that manages memory-based CL workload. To minimize the memory accesses, the sliding window of the <b>convolutional</b> layer moves in a snake-like fashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at runtime to execute different operations. As per our knowledge, our proposed TinyCL represents the first hardware accelerator that executes CL on autonomous systems. We synthesize the complete TinyCL architecture in a 65 nm CMOS technology node with the conventional ASIC design flow. It executes 1 epoch of training on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while 1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s, thus achieving a 58 x speedup, consuming 86 mW in a 4.74 mm2 die.</p></p class="citation"></blockquote><h3 id=4366--95249-dform-diffeomorphic-vector-field-alignment-for-assessing-dynamics-across-learned-models-ruiqi-chen-et-al-2024>(43/66 | 95/249) DFORM: Diffeomorphic vector field alignment for assessing dynamics across learned models (Ruiqi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiqi Chen, Giacomo Vedovati, Todd Braver, ShiNung Ching. (2024)<br><strong>DFORM: Diffeomorphic vector field alignment for assessing dynamics across learned models</strong><br><button class=copy-to-clipboard title="DFORM: Diffeomorphic vector field alignment for assessing dynamics across learned models" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY, q-bio-NC<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09735v1.pdf filename=2402.09735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamical system models such as <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs)</b> have become increasingly popular as hypothesis-generating tools in scientific research. Evaluating the dynamics in such networks is key to understanding their learned generative mechanisms. However, comparison of learned dynamics across models is challenging due to their inherent nonlinearity and because a priori there is no enforced equivalence of their coordinate systems. Here, we propose the DFORM (Diffeomorphic vector field alignment for comparing dynamics across learned models) framework. DFORM learns a nonlinear coordinate transformation which provides a continuous, maximally one-to-one mapping between the trajectories of learned models, thus approximating a diffeomorphism between them. The mismatch between DFORM-transformed vector fields defines the orbital similarity between two models, thus providing a generalization of the concepts of smooth orbital and topological equivalence. As an example, we apply DFORM to models trained on a canonical neuroscience task, showing that learned dynamics may be functionally similar, despite overt differences in attractor landscapes.</p></p class="citation"></blockquote><h3 id=4466--96249-benchmarking-federated-strategies-in-peer-to-peer-federated-learning-for-biomedical-data-jose-l-salmeron-et-al-2024>(44/66 | 96/249) Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data (Jose L. Salmeron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jose L. Salmeron, Irina Arévalo, Antonio Ruiz-Celma. (2024)<br><strong>Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data</strong><br><button class=copy-to-clipboard title="Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10135v1.pdf filename=2402.10135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing requirements for data protection and privacy has attracted a huge research interest on distributed artificial intelligence and specifically on <b>federated</b> <b>learning,</b> an emerging machine learning approach that allows the construction of a model between several participants who hold their own private data. In the initial proposal of <b>federated</b> <b>learning</b> the architecture was centralised and the aggregation was done with <b>federated</b> <b>averaging,</b> meaning that a central server will orchestrate the federation using the most straightforward averaging strategy. This research is focused on testing different <b>federated</b> <b>strategies</b> in a peer-to-peer environment. The authors propose various aggregation strategies for <b>federated</b> <b>learning,</b> including weighted averaging aggregation, using different factors and strategies based on participant contribution. The strategies are tested with varying data sizes to identify the most robust ones. This research tests the strategies with several biomedical datasets and the results of the experiments show that the accuracy-based weighted average outperforms the classical <b>federated</b> <b>averaging</b> method.</p></p class="citation"></blockquote><h3 id=4566--97249-recommendations-for-baselines-and-benchmarking-approximate-gaussian-processes-sebastian-w-ober-et-al-2024>(45/66 | 97/249) Recommendations for Baselines and Benchmarking Approximate Gaussian Processes (Sebastian W. Ober et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian W. Ober, Artem Artemev, Marcel Wagenländer, Rudolfs Grobins, Mark van der Wilk. (2024)<br><strong>Recommendations for Baselines and Benchmarking Approximate Gaussian Processes</strong><br><button class=copy-to-clipboard title="Recommendations for Baselines and Benchmarking Approximate Gaussian Processes" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09849v1.pdf filename=2402.09849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gaussian processes (GPs) are a mature and widely-used component of the ML toolbox. One of their desirable qualities is automatic hyperparameter selection, which allows for training without user intervention. However, in many realistic settings, approximations are typically needed, which typically do require tuning. We argue that this requirement for tuning complicates evaluation, which has led to a lack of a clear <b>recommendations</b> on which method should be used in which situation. To address this, we make <b>recommendations</b> for comparing GP approximations based on a specification of what a user should expect from a method. In addition, we develop a training procedure for the variational method of Titsias [2009] that leaves no choices to the user, and show that this is a strong baseline that meets our specification. We conclude that <b>benchmarking</b> according to our suggestions gives a clearer view of the current state of the field, and uncovers problems that are still open that future papers should address.</p></p class="citation"></blockquote><h3 id=4666--98249-large-scale-constrained-clustering-with-reinforcement-learning-benedikt-schesch-et-al-2024>(46/66 | 98/249) Large Scale Constrained Clustering With Reinforcement Learning (Benedikt Schesch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benedikt Schesch, Marco Caserta. (2024)<br><strong>Large Scale Constrained Clustering With Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Large Scale Constrained Clustering With Reinforcement Learning" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10177v1.pdf filename=2402.10177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a network, allocating resources at clusters level, rather than at each node, enhances efficiency in resource allocation and usage. In this paper, we study the problem of finding fully connected disjoint clusters to minimize the intra-cluster distances and maximize the number of nodes assigned to the clusters, while also ensuring that no two nodes within a cluster exceed a threshold distance. While the problem can easily be formulated using a binary linear model, traditional combinatorial optimization solvers struggle when dealing with large-scale instances. We propose an approach to solve this constrained <b>clustering</b> problem via <b>reinforcement</b> <b>learning.</b> Our method involves training an agent to generate both feasible and (near) optimal solutions. The agent learns problem-specific heuristics, tailored to the instances encountered in this task. In the results section, we show that our algorithm finds near optimal solutions, even for large scale instances.</p></p class="citation"></blockquote><h3 id=4766--99249-is-continual-learning-ready-for-real-world-challenges-theodora-kontogianni-et-al-2024>(47/66 | 99/249) Is Continual Learning Ready for Real-world Challenges? (Theodora Kontogianni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Theodora Kontogianni, Yuanwen Yue, Siyu Tang, Konrad Schindler. (2024)<br><strong>Is Continual Learning Ready for Real-world Challenges?</strong><br><button class=copy-to-clipboard title="Is Continual Learning Ready for Real-world Challenges?" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10130v1.pdf filename=2402.10130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite <b>continual</b> <b>learning&rsquo;s</b> long and well-established academic history, its application in real-world scenarios remains rather limited. This paper contends that this gap is attributable to a misalignment between the actual challenges of <b>continual</b> <b>learning</b> and the evaluation protocols in use, rendering proposed solutions ineffective for addressing the complexities of real-world setups. We validate our hypothesis and assess progress to date, using a new 3D semantic segmentation <b>benchmark,</b> OCL-3DSS. We investigate various <b>continual</b> <b>learning</b> schemes from the literature by utilizing more realistic protocols that necessitate online and <b>continual</b> <b>learning</b> for dynamic, real-world scenarios (eg., in robotics and 3D vision applications). The outcomes are sobering: all considered methods perform poorly, significantly deviating from the upper bound of joint offline training. This raises questions about the applicability of existing methods in realistic settings. Our paper aims to initiate a paradigm shift, advocating for the adoption of <b>continual</b> <b>learning</b> methods through new experimental protocols that better emulate real-world conditions to facilitate breakthroughs in the field.</p></p class="citation"></blockquote><h3 id=4866--100249-risk-sensitive-soft-actor-critic-for-robust-deep-reinforcement-learning-under-distribution-shifts-tobias-enders-et-al-2024>(48/66 | 100/249) Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under Distribution Shifts (Tobias Enders et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Enders, James Harrison, Maximilian Schiffer. (2024)<br><strong>Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under Distribution Shifts</strong><br><button class=copy-to-clipboard title="Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under Distribution Shifts" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09992v1.pdf filename=2402.09992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the robustness of deep <b>reinforcement</b> <b>learning</b> algorithms against distribution shifts within contextual multi-stage stochastic combinatorial optimization problems from the operations research domain. In this context, risk-sensitive algorithms promise to learn robust policies. While this field is of general interest to the <b>reinforcement</b> <b>learning</b> community, most studies up-to-date focus on theoretical results rather than real-world performance. With this work, we aim to bridge this gap by formally deriving a novel risk-sensitive deep <b>reinforcement</b> <b>learning</b> algorithm while providing numerical evidence for its efficacy. Specifically, we introduce discrete Soft Actor-Critic for the entropic risk measure by deriving a version of the Bellman equation for the respective Q-values. We establish a corresponding policy improvement result and infer a practical algorithm. We introduce an environment that represents typical contextual multi-stage stochastic combinatorial optimization problems and perform numerical experiments to empirically validate our algorithm&rsquo;s robustness against realistic distribution shifts, without compromising performance on the training distribution. We show that our algorithm is superior to risk-neutral Soft Actor-Critic as well as to two <b>benchmark</b> approaches for robust deep <b>reinforcement</b> <b>learning.</b> Thereby, we provide the first structured analysis on the robustness of <b>reinforcement</b> <b>learning</b> under distribution shifts in the realm of contextual multi-stage stochastic combinatorial optimization problems.</p></p class="citation"></blockquote><h3 id=4966--101249-fedlion-faster-adaptive-federated-optimization-with-fewer-communication-zhiwei-tang-et-al-2024>(49/66 | 101/249) FedLion: Faster Adaptive Federated Optimization with Fewer Communication (Zhiwei Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiwei Tang, Tsung-Hui Chang. (2024)<br><strong>FedLion: Faster Adaptive Federated Optimization with Fewer Communication</strong><br><button class=copy-to-clipboard title="FedLion: Faster Adaptive Federated Optimization with Fewer Communication" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Benchmarking, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09941v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09941v1.pdf filename=2402.09941v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>Federated</b> <b>Learning</b> (FL), a framework to train machine learning models across distributed data, well-known algorithms like FedAvg tend to have slow convergence rates, resulting in high communication costs during training. To address this challenge, we introduce FedLion, an adaptive <b>federated</b> <b>optimization</b> algorithm that seamlessly incorporates key elements from the recently proposed centralized adaptive algorithm, Lion (Chen et al. 2o23), into the FL framework. Through comprehensive evaluations on two widely adopted FL <b>benchmarks,</b> we demonstrate that FedLion outperforms previous state-of-the-art adaptive algorithms, including FAFED (Wu et al. 2023) and FedDA. Moreover, thanks to the use of signed gradients in local training, FedLion substantially reduces data transmission requirements during uplink communication when compared to existing adaptive algorithms, further reducing communication costs. Last but not least, this work also includes a novel theoretical analysis, showcasing that FedLion attains faster convergence rate than established FL algorithms like FedAvg.</p></p class="citation"></blockquote><h3 id=5066--102249-hypermagnet-a-magnetic-laplacian-based-hypergraph-neural-network-tatyana-benko-et-al-2024>(50/66 | 102/249) HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network (Tatyana Benko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tatyana Benko, Martin Buck, Ilya Amburg, Stephen J. Young, Sinan G. Aksoy. (2024)<br><strong>HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network</strong><br><button class=copy-to-clipboard title="HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Node Classification, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09676v1.pdf filename=2402.09676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In data science, hypergraphs are natural models for data exhibiting multi-way relations, whereas <b>graphs</b> only capture pairwise. Nonetheless, many proposed hypergraph neural networks effectively reduce hypergraphs to undirected <b>graphs</b> via symmetrized matrix representations, potentially losing important information. We propose an alternative approach to hypergraph neural networks in which the hypergraph is represented as a non-reversible Markov chain. We use this Markov chain to construct a complex Hermitian Laplacian matrix - the magnetic Laplacian - which serves as the input to our proposed hypergraph neural network. We study HyperMagNet for the task of <b>node</b> <b>classification,</b> and demonstrate its effectiveness over <b>graph-reduction</b> based hypergraph neural networks.</p></p class="citation"></blockquote><h3 id=5166--103249-backdoor-attack-against-one-class-sequential-anomaly-detection-models-he-cheng-et-al-2024>(51/66 | 103/249) Backdoor Attack against One-Class Sequential Anomaly Detection Models (He Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>He Cheng, Shuhan Yuan. (2024)<br><strong>Backdoor Attack against One-Class Sequential Anomaly Detection Models</strong><br><button class=copy-to-clipboard title="Backdoor Attack against One-Class Sequential Anomaly Detection Models" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10283v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10283v1.pdf filename=2402.10283v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>anomaly</b> <b>detection</b> on sequential data has garnered significant attention due to the wide application scenarios. However, deep learning-based models face a critical security threat - their vulnerability to backdoor attacks. In this paper, we explore compromising deep sequential <b>anomaly</b> <b>detection</b> models by proposing a novel backdoor attack strategy. The attack approach comprises two primary steps, trigger generation and backdoor injection. Trigger generation is to derive imperceptible triggers by crafting perturbed samples from the benign normal data, of which the perturbed samples are still normal. The backdoor injection is to properly inject the backdoor triggers to comprise the model only for the samples with triggers. The experimental results demonstrate the effectiveness of our proposed attack strategy by injecting backdoors on two well-established one-class <b>anomaly</b> <b>detection</b> models.</p></p class="citation"></blockquote><h3 id=5266--104249-information-capacity-regret-bounds-for-bandits-with-mediator-feedback-khaled-eldowa-et-al-2024>(52/66 | 104/249) Information Capacity Regret Bounds for Bandits with Mediator Feedback (Khaled Eldowa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khaled Eldowa, Nicolò Cesa-Bianchi, Alberto Maria Metelli, Marcello Restelli. (2024)<br><strong>Information Capacity Regret Bounds for Bandits with Mediator Feedback</strong><br><button class=copy-to-clipboard title="Information Capacity Regret Bounds for Bandits with Mediator Feedback" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10282v1.pdf filename=2402.10282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work addresses the mediator feedback problem, a <b>bandit</b> game where the decision set consists of a number of policies, each associated with a probability distribution over a common space of outcomes. Upon choosing a policy, the learner observes an outcome sampled from its distribution and incurs the loss assigned to this outcome in the present round. We introduce the policy set capacity as an information-theoretic measure for the complexity of the policy set. Adopting the classical EXP4 algorithm, we provide new regret bounds depending on the policy set capacity in both the adversarial and the stochastic settings. For a selection of policy set families, we prove nearly-matching lower bounds, scaling similarly with the capacity. We also consider the case when the policies&rsquo; distributions can vary between rounds, thus addressing the related <b>bandits</b> with expert advice problem, which we improve upon its prior results. Additionally, we prove a lower bound showing that exploiting the similarity between the policies is not possible in general under linear <b>bandit</b> feedback. Finally, for a full-information variant, we provide a regret bound scaling with the information radius of the policy set.</p></p class="citation"></blockquote><h3 id=5366--105249-unlocking-the-potential-of-transformers-in-time-series-forecasting-with-sharpness-aware-minimization-and-channel-wise-attention-romain-ilbert-et-al-2024>(53/66 | 105/249) Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention (Romain Ilbert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Romain Ilbert, Ambroise Odonnat, Vasilii Feofanov, Aladin Virmaux, Giuseppe Paolo, Themis Palpanas, Ievgen Redko. (2024)<br><strong>Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention</strong><br><button class=copy-to-clipboard title="Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10198v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10198v2.pdf filename=2402.10198v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that <b>transformers</b> are incapable of converging to their true solution despite their high expressive power. We further identify the attention of <b>transformers</b> as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight <b>transformer</b> model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times fewer parameters. The code is available at <a href=https://github.com/romilbert/samformer>https://github.com/romilbert/samformer</a>.</p></p class="citation"></blockquote><h3 id=5466--106249-self-consistent-validation-for-machine-learning-electronic-structure-gengyuan-hu-et-al-2024>(54/66 | 106/249) Self-consistent Validation for Machine Learning Electronic Structure (Gengyuan Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gengyuan Hu, Gengchen Wei, Zekun Lou, Philip H. S. Torr, Wanli Ouyang, Han-sen Zhong, Chen Lin. (2024)<br><strong>Self-consistent Validation for Machine Learning Electronic Structure</strong><br><button class=copy-to-clipboard title="Self-consistent Validation for Machine Learning Electronic Structure" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-chem-ph, physics-comp-ph<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10186v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10186v1.pdf filename=2402.10186v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning has emerged as a significant approach to efficiently tackle electronic structure problems. Despite its potential, there is less guarantee for the model to generalize to unseen data that hinders its application in real-world scenarios. To address this issue, a technique has been proposed to estimate the accuracy of the predictions. This method integrates machine learning with self-consistent field methods to achieve both low validation cost and interpret-ability. This, in turn, enables exploration of the model&rsquo;s ability with <b>active</b> <b>learning</b> and instills confidence in its integration into real-world studies.</p></p class="citation"></blockquote><h3 id=5566--107249-a-chaotic-maps-based-privacy-preserving-distributed-deep-learning-for-incomplete-and-non-iid-datasets-irina-arévalo-et-al-2024>(55/66 | 107/249) A chaotic maps-based privacy-preserving distributed deep learning for incomplete and Non-IID datasets (Irina Arévalo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Irina Arévalo, Jose L. Salmeron. (2024)<br><strong>A chaotic maps-based privacy-preserving distributed deep learning for incomplete and Non-IID datasets</strong><br><button class=copy-to-clipboard title="A chaotic maps-based privacy-preserving distributed deep learning for incomplete and Non-IID datasets" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10145v1.pdf filename=2402.10145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> is a machine learning approach that enables the training of a deep learning model among several participants with sensitive data that wish to share their own knowledge without compromising the privacy of their data. In this research, the authors employ a secured <b>Federated</b> <b>Learning</b> method with an additional layer of privacy and proposes a method for addressing the non-IID challenge. Moreover, differential privacy is compared with chaotic-based encryption as layer of privacy. The experimental approach assesses the performance of the <b>federated</b> <b>deep</b> learning model with differential privacy using both IID and non-IID data. In each experiment, the <b>Federated</b> <b>Learning</b> process improves the average performance metrics of the deep neural network, even in the case of non-IID data.</p></p class="citation"></blockquote><h3 id=5666--108249-adaptive-federated-learning-in-heterogeneous-wireless-networks-with-independent-sampling-jiaxiang-geng-et-al-2024>(56/66 | 108/249) Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling (Jiaxiang Geng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxiang Geng, Yanzhao Hou, Xiaofeng Tao, Juncheng Wang, Bing Luo. (2024)<br><strong>Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling</strong><br><button class=copy-to-clipboard title="Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10097v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10097v2.pdf filename=2402.10097v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) algorithms commonly sample a random subset of clients to address the straggler issue and improve communication efficiency. While recent works have proposed various client sampling methods, they have limitations in joint system and data heterogeneity design, which may not align with practical heterogeneous wireless networks. In this work, we advocate a new independent client sampling strategy to minimize the wall-clock training time of FL, while considering data heterogeneity and system heterogeneity in both communication and computation. We first derive a new convergence bound for non-convex loss functions with independent client sampling and then propose an adaptive bandwidth allocation scheme. Furthermore, we propose an efficient independent client sampling algorithm based on the upper bounds on the convergence rounds and the expected per-round training time, to minimize the wall-clock time of FL, while considering both the data and system heterogeneity. Experimental results under practical wireless network settings with real-world prototype demonstrate that the proposed independent sampling scheme substantially outperforms the current best sampling schemes under various training models and datasets.</p></p class="citation"></blockquote><h3 id=5766--109249-fedrdf-a-robust-and-dynamic-aggregation-function-against-poisoning-attacks-in-federated-learning-enrique-mármol-campos-et-al-2024>(57/66 | 109/249) FedRDF: A Robust and Dynamic Aggregation Function against Poisoning Attacks in Federated Learning (Enrique Mármol Campos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enrique Mármol Campos, Aurora González Vidal, José Luis Hernández Ramos, Antonio Skarmeta. (2024)<br><strong>FedRDF: A Robust and Dynamic Aggregation Function against Poisoning Attacks in Federated Learning</strong><br><button class=copy-to-clipboard title="FedRDF: A Robust and Dynamic Aggregation Function against Poisoning Attacks in Federated Learning" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10082v1.pdf filename=2402.10082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) represents a promising approach to typical privacy concerns associated with centralized Machine Learning (ML) deployments. Despite its well-known advantages, FL is vulnerable to security attacks such as Byzantine behaviors and poisoning attacks, which can significantly degrade model performance and hinder convergence. The effectiveness of existing approaches to mitigate complex attacks, such as median, trimmed mean, or Krum aggregation functions, has been only partially demonstrated in the case of specific attacks. Our study introduces a novel robust aggregation mechanism utilizing the Fourier Transform (FT), which is able to effectively handling sophisticated attacks without prior knowledge of the number of attackers. Employing this data technique, weights generated by FL clients are projected into the frequency domain to ascertain their density function, selecting the one exhibiting the highest frequency. Consequently, malicious clients&rsquo; weights are excluded. Our proposed approach was tested against various model poisoning attacks, demonstrating superior performance over state-of-the-art aggregation methods.</p></p class="citation"></blockquote><h3 id=5866--110249-diffusion-models-meet-contextual-bandits-with-large-action-spaces-imad-aouali-2024>(58/66 | 110/249) Diffusion Models Meet Contextual Bandits with Large Action Spaces (Imad Aouali, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Imad Aouali. (2024)<br><strong>Diffusion Models Meet Contextual Bandits with Large Action Spaces</strong><br><button class=copy-to-clipboard title="Diffusion Models Meet Contextual Bandits with Large Action Spaces" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10028v1.pdf filename=2402.10028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient exploration is a key challenge in contextual <b>bandits</b> due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies. Fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently. In this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion Thompson sampling (dTS). Both theoretical and algorithmic foundations are developed for dTS, and empirical evaluation also shows its favorable performance.</p></p class="citation"></blockquote><h3 id=5966--111249-accelerating-parallel-sampling-of-diffusion-models-zhiwei-tang-et-al-2024>(59/66 | 111/249) Accelerating Parallel Sampling of Diffusion Models (Zhiwei Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiwei Tang, Jiasheng Tang, Hao Luo, Fan Wang, Tsung-Hui Chang. (2024)<br><strong>Accelerating Parallel Sampling of Diffusion Models</strong><br><button class=copy-to-clipboard title="Accelerating Parallel Sampling of Diffusion Models" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09970v1.pdf filename=2402.09970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion models have emerged as state-of-the-art generative models for image generation. However, sampling from diffusion models is usually time-consuming due to the inherent autoregressive nature of their sampling process. In this work, we propose a novel approach that accelerates the sampling of diffusion models by parallelizing the autoregressive process. Specifically, we reformulate the sampling process as solving a system of triangular nonlinear equations through fixed-point iteration. With this innovative formulation, we explore several systematic techniques to further reduce the iteration steps required by the solving process. Applying these techniques, we introduce ParaTAA, a universal and training-free parallel sampling algorithm that can leverage extra computational and memory resources to increase the sampling speed. Our experiments demonstrate that ParaTAA can decrease the inference steps required by common sequential sampling algorithms such as DDIM and DDPM by a factor of 4~14 times. Notably, when applying ParaTAA with 100 steps DDIM for Stable Diffusion, a widely-used <b>text-to-image</b> diffusion model, it can produce the same images as the sequential sampling in only 7 inference steps.</p></p class="citation"></blockquote><h3 id=6066--112249-why-are-sensitive-functions-hard-for-transformers-michael-hahn-et-al-2024>(60/66 | 112/249) Why are Sensitive Functions Hard for Transformers? (Michael Hahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Hahn, Mark Rofin. (2024)<br><strong>Why are Sensitive Functions Hard for Transformers?</strong><br><button class=copy-to-clipboard title="Why are Sensitive Functions Hard for Transformers?" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09963v1.pdf filename=2402.09963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Empirical studies have identified a range of learnability biases and limitations of <b>transformers,</b> such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the <b>transformer</b> architecture, the loss landscape is constrained by the input-space sensitivity: <b>Transformers</b> whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of <b>transformers,</b> such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalization for PARITY. This shows that understanding <b>transformers&rsquo;</b> inductive biases requires studying not just their in-principle expressivity, but also their loss landscape.</p></p class="citation"></blockquote><h3 id=6166--113249-enhancing-courier-scheduling-in-crowdsourced-last-mile-delivery-through-dynamic-shift-extensions-a-deep-reinforcement-learning-approach-zead-saleh-et-al-2024>(61/66 | 113/249) Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through Dynamic Shift Extensions: A Deep Reinforcement Learning Approach (Zead Saleh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zead Saleh, Ahmad Al Hanbali, Ahmad Baubaid. (2024)<br><strong>Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through Dynamic Shift Extensions: A Deep Reinforcement Learning Approach</strong><br><button class=copy-to-clipboard title="Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through Dynamic Shift Extensions: A Deep Reinforcement Learning Approach" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09961v1.pdf filename=2402.09961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crowdsourced delivery platforms face complex scheduling challenges to match couriers and customer orders. We consider two types of crowdsourced couriers, namely, committed and occasional couriers, each with different compensation schemes. Crowdsourced delivery platforms usually schedule committed courier shifts based on predicted demand. Therefore, platforms may devise an offline schedule for committed couriers before the planning period. However, due to the unpredictability of demand, there are instances where it becomes necessary to make online adjustments to the offline schedule. In this study, we focus on the problem of dynamically adjusting the offline schedule through shift extensions for committed couriers. This problem is modeled as a sequential decision process. The objective is to maximize platform profit by determining the shift extensions of couriers and the assignments of requests to couriers. To solve the model, a Deep Q-Network (DQN) learning approach is developed. Comparing this model with the baseline policy where no extensions are allowed demonstrates the benefits that platforms can gain from allowing shift extensions in terms of reward, reduced lost order costs, and lost requests. Additionally, sensitivity analysis showed that the total extension compensation increases in a nonlinear manner with the arrival rate of requests, and in a linear manner with the arrival rate of occasional couriers. On the compensation sensitivity, the results showed that the normal scenario exhibited the highest average number of shift extensions and, consequently, the fewest average number of lost requests. These findings serve as evidence of the successful learning of such dynamics by the DQN algorithm.</p></p class="citation"></blockquote><h3 id=6266--114249-predictors-from-causal-features-do-not-generalize-better-to-new-domains-vivian-y-nastl-et-al-2024>(62/66 | 114/249) Predictors from causal features do not generalize better to new domains (Vivian Y. Nastl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vivian Y. Nastl, Moritz Hardt. (2024)<br><strong>Predictors from causal features do not generalize better to new domains</strong><br><button class=copy-to-clipboard title="Predictors from causal features do not generalize better to new domains" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09891v1.pdf filename=2402.09891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study how well machine learning models trained on causal features generalize across domains. We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. For each prediction task, we select features that have a causal influence on the target of prediction. Our goal is to test the hypothesis that models trained on causal features generalize better across domains. Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and <b>out-of-domain</b> accuracy than predictors using causal features. Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features. If the goal is to generalize to new domains, practitioners might as well train the best possible model on all available features.</p></p class="citation"></blockquote><h3 id=6366--115249-mimic-minimally-modified-counterfactuals-in-the-representation-space-shashwat-singh-et-al-2024>(63/66 | 115/249) MiMiC: Minimally Modified Counterfactuals in the Representation Space (Shashwat Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, Ponnurangam Kumaraguru. (2024)<br><strong>MiMiC: Minimally Modified Counterfactuals in the Representation Space</strong><br><button class=copy-to-clipboard title="MiMiC: Minimally Modified Counterfactuals in the Representation Space" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09631v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09631v2.pdf filename=2402.09631v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity. We then propose a novel intervention methodology for generating expressive <b>counterfactuals</b> in the representation space, aiming to make representations of a source class (e.g., &ldquo;toxic&rdquo;) resemble those of a target class (e.g., &ldquo;non-toxic&rdquo;). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover&rsquo;s problem under Gaussian assumptions and provides theoretical guarantees on the representation space&rsquo;s geometric organization. We further build on this technique and derive a nonlinear intervention that enables controlled generation. We demonstrate the effectiveness of the proposed approaches in mitigating bias in multiclass classification and in reducing the generation of toxic language, outperforming strong baselines.</p></p class="citation"></blockquote><h3 id=6466--116249-privacy-attacks-in-decentralized-learning-abdellah-el-mrini-et-al-2024>(64/66 | 116/249) Privacy Attacks in Decentralized Learning (Abdellah El Mrini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdellah El Mrini, Edwige Cyffers, Aurélien Bellet. (2024)<br><strong>Privacy Attacks in Decentralized Learning</strong><br><button class=copy-to-clipboard title="Privacy Attacks in Decentralized Learning" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10001v1.pdf filename=2402.10001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decentralized Gradient Descent (D-GD) allows a set of users to perform collaborative learning without sharing their data by iteratively averaging local model updates with their neighbors in a network <b>graph.</b> The absence of direct communication between non-neighbor nodes might lead to the belief that users cannot infer precise information about the data of others. In this work, we demonstrate the opposite, by proposing the first attack against D-GD that enables a user (or set of users) to reconstruct the private data of other users outside their immediate neighborhood. Our approach is based on a reconstruction attack against the gossip averaging protocol, which we then extend to handle the additional challenges raised by D-GD. We validate the effectiveness of our attack on real <b>graphs</b> and datasets, showing that the number of users compromised by a single or a handful of attackers is often surprisingly large. We empirically investigate some of the factors that affect the performance of the attack, namely the <b>graph</b> topology, the number of attackers, and their position in the <b>graph.</b></p></p class="citation"></blockquote><h3 id=6566--117249-explaining-kernel-clustering-via-decision-trees-maximilian-fleissner-et-al-2024>(65/66 | 117/249) Explaining Kernel Clustering via Decision Trees (Maximilian Fleissner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maximilian Fleissner, Leena Chennuru Vankadara, Debarghya Ghoshdastidar. (2024)<br><strong>Explaining Kernel Clustering via Decision Trees</strong><br><button class=copy-to-clipboard title="Explaining Kernel Clustering via Decision Trees" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09881v1.pdf filename=2402.09881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable <b>clustering</b> methods. Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees. However, interpretable variants of k-means have limited applicability in practice, where more flexible <b>clustering</b> methods are often needed to obtain useful partitions of the data. In this work, we investigate interpretable kernel <b>clustering,</b> and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means. We further build on previous work on explainable k-means and demonstrate how a suitable choice of features allows preserving interpretability without sacrificing approximation guarantees on the interpretable model.</p></p class="citation"></blockquote><h3 id=6666--118249-mc-dbn-a-deep-belief-network-based-model-for-modality-completion-zihong-luo-et-al-2024>(66/66 | 118/249) MC-DBN: A Deep Belief Network-Based Model for Modality Completion (Zihong Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihong Luo, Haochen Xue, Mingyu Jin, Chengzhi Liu, Zile Huang, Chong Zhang, Shuliang Zhao. (2024)<br><strong>MC-DBN: A Deep Belief Network-Based Model for Modality Completion</strong><br><button class=copy-to-clipboard title="MC-DBN: A Deep Belief Network-Based Model for Modality Completion" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09782v1.pdf filename=2402.09782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>multi-modal</b> artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring. Utilizing diverse data sources can substantially improve prediction accuracy. Nonetheless, additional data may not always align with the original dataset. Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information. Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data. It ensures that the enhanced <b>multi-modal</b> data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model. We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate monitoring domains. Comprehensive experiments showcase the model&rsquo;s capacity to bridge the semantic divide present in <b>multi-modal</b> data, subsequently enhancing its performance. The source code is available at: <a href=https://github.com/logan-0623/DBN-generate>https://github.com/logan-0623/DBN-generate</a></p></p class="citation"></blockquote><h2 id=csai-24>cs.AI (24)</h2><h3 id=124--119249-fine-tuning-large-language-model-llm-artificial-intelligence-chatbots-in-ophthalmology-and-llm-based-evaluation-using-gpt-4-ting-fang-tan-et-al-2024>(1/24 | 119/249) Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4 (Ting Fang Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting. (2024)<br><strong>Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4</strong><br><button class=copy-to-clipboard title="Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10083v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10083v1.pdf filename=2402.10083v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: To assess the alignment of <b>GPT-4-based</b> evaluation to human clinician experts, for the evaluation of responses to ophthalmology-related patient queries generated by <b>fine-tuned</b> <b>LLM</b> <b>chatbots.</b> Methods: 400 ophthalmology questions and paired answers were created by ophthalmologists to represent commonly asked patient questions, divided into <b>fine-tuning</b> (368; 92%), and testing (40; 8%). We find-tuned 5 different <b>LLMs,</b> including LLAMA2-7b, LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset, additional 8 glaucoma QnA pairs were included. 200 responses to the testing dataset were generated by 5 <b>fine-tuned</b> <b>LLMs</b> for evaluation. A customized clinical evaluation rubric was used to guide <b>GPT-4</b> evaluation, grounded on clinical accuracy, relevance, patient safety, and ease of understanding. <b>GPT-4</b> evaluation was then compared against ranking by 5 clinicians for clinical alignment. Results: Among all <b>fine-tuned</b> <b>LLMs,</b> <b>GPT-3.5</b> scored the highest (87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%), LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the <b>GPT-4</b> evaluation. <b>GPT-4</b> evaluation demonstrated significant agreement with human clinician rankings, with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80 respectively; while correlation based on Cohen Kappa was more modest at 0.50. Notably, qualitative analysis and the glaucoma sub-analysis revealed clinical inaccuracies in the <b>LLM-generated</b> responses, which were appropriately identified by the <b>GPT-4</b> evaluation. Conclusion: The notable clinical alignment of <b>GPT-4</b> evaluation highlighted its potential to streamline the clinical evaluation of <b>LLM</b> <b>chatbot</b> responses to healthcare-related queries. By complementing the existing clinician-dependent manual grading, this efficient and automated evaluation could assist the validation of future developments in <b>LLM</b> applications for healthcare.</p></p class="citation"></blockquote><h3 id=224--120249-beyond-imitation-generating-human-mobility-from-context-aware-reasoning-with-large-language-models-chenyang-shao-et-al-2024>(2/24 | 120/249) Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models (Chenyang Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyang Shao, Fengli Xu, Bingbing Fan, Jingtao Ding, Yuan Yuan, Meng Wang, Yong Li. (2024)<br><strong>Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models</strong><br><button class=copy-to-clipboard title="Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 80<br>Keywords: Few-shot, Common-sense Reasoning, Reasoning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09836v1.pdf filename=2402.09836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human mobility behaviours are closely linked to various important societal problems such as traffic congestion, and epidemic control. However, collecting mobility data can be prohibitively expensive and involves serious privacy issues, posing a pressing need for high-quality generative mobility models. Previous efforts focus on learning the behaviour distribution from training samples, and generate new mobility data by sampling the learned distributions. They cannot effectively capture the coherent intentions that drive mobility behavior, leading to low sample efficiency and semantic-awareness. Inspired by the emergent <b>reasoning</b> ability in <b>LLMs,</b> we propose a radical perspective shift that reformulates mobility generation as a <b>commonsense</b> <b>reasoning</b> problem. In this paper, we design a novel Mobility Generation as <b>Reasoning</b> (MobiGeaR) framework that <b>prompts</b> <b>LLM</b> to recursively generate mobility behaviour. Specifically, we design a context-aware chain-of-thoughts <b>prompting</b> technique to align <b>LLMs</b> with context-aware mobility behaviour by <b>few-shot</b> <b>in-context</b> <b>learning.</b> Besides, MobiGeaR employ a divide-and-coordinate mechanism to exploit the synergistic effect between <b>LLM</b> <b>reasoning</b> and mechanistic gravity model. It leverages the step-by-step <b>LLM</b> <b>reasoning</b> to recursively generate a temporal template of activity intentions, which are then mapped to physical locations with a mechanistic gravity model. Experiments on two real-world datasets show MobiGeaR achieves state-of-the-art performance across all metrics, and substantially reduces the size of training samples at the same time. Besides, MobiGeaR also significantly improves the semantic-awareness of mobility generation by improving the intention accuracy by 62.23% and the generated mobility data is proven effective in boosting the performance of downstream applications. The implementation of our approach is available in the paper.</p></p class="citation"></blockquote><h3 id=324--121249-the-butterfly-effect-of-model-editing-few-edits-can-trigger-large-language-models-collapse-wanli-yang-et-al-2024>(3/24 | 121/249) The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse (Wanli Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanli Yang, Fei Sun, Xinyu Ma, Xun Liu, Dawei Yin, Xueqi Cheng. (2024)<br><strong>The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse</strong><br><button class=copy-to-clipboard title="The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 66<br>Keywords: Benchmarking, Benchmarking, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09656v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09656v2.pdf filename=2402.09656v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although model editing has shown promise in revising knowledge in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> its impact on the inherent capabilities of <b>LLMs</b> is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various <b>benchmark</b> tasks. However, <b>benchmarking</b> <b>LLMs</b> after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using <b>perplexity</b> as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream tasks performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and <b>LLMs,</b> focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse after only few edits. To facilitate further research, we have utilized <b>GPT-3.5</b> to develop a new dataset, HardEdit, based on those hard cases. This dataset aims to establish the foundation for pioneering research in reliable model editing and the mechanisms underlying editing-induced model collapse. We hope this work can draw the community&rsquo;s attention to the potential risks inherent in model editing practices.</p></p class="citation"></blockquote><h3 id=424--122249-generative-ai-in-the-construction-industry-a-state-of-the-art-analysis-ridwan-taiwo-et-al-2024>(4/24 | 122/249) Generative AI in the Construction Industry: A State-of-the-art Analysis (Ridwan Taiwo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai, Abdul-Mugis Yussif, Babatunde Abiodun Salami, Abdullahi Saka, Tarek Zayed. (2024)<br><strong>Generative AI in the Construction Industry: A State-of-the-art Analysis</strong><br><button class=copy-to-clipboard title="Generative AI in the Construction Industry: A State-of-the-art Analysis" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-HC, cs-IR, cs-LG, cs.AI<br>Keyword Score: 60<br>Keywords: Generative AI, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09939v1.pdf filename=2402.09939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. <b>Generative</b> <b>artificial</b> intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of <b>generative</b> <b>AI</b> in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of <b>generative</b> <b>AI</b> in construction, with three objectives: (1) to review and categorize the existing and emerging <b>generative</b> <b>AI</b> opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized <b>generative</b> <b>AI</b> solutions using their own data, comprising steps such as data collection, dataset curation, training custom <b>large</b> <b>language</b> <b>model</b> <b>(LLM),</b> model evaluation, and deployment; and (3) to demonstrate the framework via a case study of developing a <b>generative</b> <b>model</b> for querying contract documents. The results show that <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> improves the baseline <b>LLM</b> by 5.2, 9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study provides academics and construction professionals with a comprehensive analysis and practical framework to guide the adoption of <b>generative</b> <b>AI</b> techniques to enhance productivity, quality, safety, and sustainability across the construction industry.</p></p class="citation"></blockquote><h3 id=524--123249-user-modeling-and-user-profiling-a-comprehensive-survey-erasmo-purificato-et-al-2024>(5/24 | 123/249) User Modeling and User Profiling: A Comprehensive Survey (Erasmo Purificato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erasmo Purificato, Ludovico Boratto, Ernesto William De Luca. (2024)<br><strong>User Modeling and User Profiling: A Comprehensive Survey</strong><br><button class=copy-to-clipboard title="User Modeling and User Profiling: A Comprehensive Survey" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2, cs-AI, cs-HC, cs-IR, cs-LG, cs-SI, cs.AI<br>Keyword Score: 53<br>Keywords: Graph, Fairness, Recommender System, Fake News Detection, Information Retrieval, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09660v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09660v2.pdf filename=2402.09660v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of artificial intelligence (AI) into daily life, particularly through <b>information</b> <b>retrieval</b> and <b>recommender</b> <b>systems,</b> has necessitated advanced user modeling and profiling techniques to deliver personalized experiences. These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems. This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research. We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends. Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of <b>graph</b> data structures. We also address the critical need for privacy-preserving techniques and the push towards explainability and <b>fairness</b> in user modeling approaches. By examining the definitions of core terminology, we aim to clarify ambiguities and foster a clearer understanding of the field by proposing two novel encyclopedic definitions of the main terms. Furthermore, we explore the application of user modeling in various domains, such as <b>fake</b> <b>news</b> <b>detection,</b> cybersecurity, and personalized education. This survey serves as a comprehensive resource for researchers and practitioners, offering insights into the evolution of user modeling and profiling and guiding the development of more personalized, ethical, and effective AI systems.</p></p class="citation"></blockquote><h3 id=624--124249-geoeval-benchmark-for-evaluating-llms-and-multi-modal-models-on-geometry-problem-solving-jiaxin-zhang-et-al-2024>(6/24 | 124/249) GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving (Jiaxin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxin Zhang, Zhongzhi Li, Mingliang Zhang, Fei Yin, Chenglin Liu, Yashar Moshfeghi. (2024)<br><strong>GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving</strong><br><button class=copy-to-clipboard title="GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 51<br>Keywords: Benchmarking, Geometry, Multi-modal, GPT, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10104v1.pdf filename=2402.10104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>Multi-Modal</b> Models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling <b>geometry</b> math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval <b>benchmark,</b> a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward <b>reasoning,</b> an augmented subset of 2000 problems, and a hard subset of 300 problems. This <b>benchmark</b> facilitates a deeper investigation into the performance of <b>LLMs</b> and MMs on solving <b>geometry</b> math problems. Our evaluation of ten <b>LLMs</b> and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67% accuracy rate on the main subset but only a 6.00% accuracy on the challenging subset. This highlights the critical need for testing models against datasets on which they have not been pre-trained. Additionally, our findings indicate that <b>GPT-series</b> models perform more effectively on problems they have rephrased, suggesting a promising method for enhancing model capabilities.</p></p class="citation"></blockquote><h3 id=724--125249-inadequacies-of-large-language-model-benchmarks-in-the-era-of-generative-artificial-intelligence-timothy-r-mcintosh-et-al-2024>(7/24 | 125/249) Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence (Timothy R. McIntosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, Malka N. Halgamuge. (2024)<br><strong>Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence</strong><br><button class=copy-to-clipboard title="Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CY, cs-HC, cs.AI<br>Keyword Score: 43<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09880v1.pdf filename=2402.09880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid rise in popularity of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with emerging capabilities has spurred public curiosity to evaluate and compare different <b>LLMs,</b> leading many researchers to propose their <b>LLM</b> <b>benchmarks.</b> Noticing preliminary inadequacies in those <b>benchmarks,</b> we embarked on a study to critically assess 23 state-of-the-art <b>LLM</b> <b>benchmarks,</b> using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of functionality and security. Our research uncovered significant limitations, including biases, difficulties in measuring genuine <b>reasoning,</b> adaptability, implementation inconsistencies, <b>prompt</b> engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligence (AI) advancements, including advocating for an evolution from static <b>benchmarks</b> to dynamic behavioral profiling to accurately capture <b>LLMs&rsquo;</b> complex behaviors and potential risks. Our study highlighted the necessity for a paradigm shift in <b>LLM</b> evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted <b>benchmarks</b> and the enhancement of AI systems&rsquo; integration into society.</p></p class="citation"></blockquote><h3 id=824--126249-loraretriever-input-aware-lora-retrieval-and-composition-for-mixed-tasks-in-the-wild-ziyu-zhao-et-al-2024>(8/24 | 126/249) LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild (Ziyu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang, Kun Kuang, Fei Wu. (2024)<br><strong>LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild</strong><br><button class=copy-to-clipboard title="LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09997v1.pdf filename=2402.09997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for <b>fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLM).</b> The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of <b>LLMs.</b> Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, <b>LLMs</b> receive diverse <b>prompts</b> covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input <b>prompts.</b> LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing efficient batch inference to accommodate heterogeneous requests. Experimental results indicate that LoraRetriever consistently outperforms the baselines, highlighting its practical effectiveness and versatility.</p></p class="citation"></blockquote><h3 id=924--127249-aligning-crowd-feedback-via-distributional-preference-reward-modeling-dexun-li-et-al-2024>(9/24 | 127/249) Aligning Crowd Feedback via Distributional Preference Reward Modeling (Dexun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang, Yong Liu. (2024)<br><strong>Aligning Crowd Feedback via Distributional Preference Reward Modeling</strong><br><button class=copy-to-clipboard title="Aligning Crowd Feedback via Distributional Preference Reward Modeling" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: Fine-tuning, Reinforcement Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09764v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09764v2.pdf filename=2402.09764v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>Reinforcement</b> <b>Learning</b> is widely used for aligning <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> with human preference. However, the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately. In this paper, we introduce the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align <b>large</b> <b>language</b> <b>models</b> with a diverse set of human preferences. To this end, we characterize the preferences by a beta distribution, which can dynamically adapt to fluctuations in preference trends. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to <b>fine-tune</b> an <b>LLM</b> policy to generate responses favoured by the population. Our experiments show that DPRM significantly enhances the alignment of <b>LLMs</b> with population preference, yielding more accurate, unbiased, and contextually appropriate responses.</p></p class="citation"></blockquote><h3 id=1024--128249-federated-prompt-based-decision-transformer-for-customized-vr-services-in-mobile-edge-computing-system-tailin-zhou-et-al-2024>(10/24 | 128/249) Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System (Tailin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tailin Zhou, Jiadong Yu, Jun Zhang, Danny H. K. Tsang. (2024)<br><strong>Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System</strong><br><button class=copy-to-clipboard title="Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-SY, cs.AI, eess-SY<br>Keyword Score: 40<br>Keywords: Federated Learning, Reinforcement Learning, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09729v1.pdf filename=2402.09729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates resource allocation to provide heterogeneous users with customized virtual reality (VR) services in a mobile edge computing (MEC) system. We first introduce a quality of experience (QoE) metric to measure user experience, which considers the MEC system&rsquo;s latency, user attention levels, and preferred resolutions. Then, a QoE maximization problem is formulated for resource allocation to ensure the highest possible user experience,which is cast as a <b>reinforcement</b> <b>learning</b> problem, aiming to learn a generalized policy applicable across diverse user environments for all MEC servers. To learn the generalized policy, we propose a framework that employs <b>federated</b> <b>learning</b> (FL) and <b>prompt-based</b> sequence modeling to pre-train a common decision model across MEC servers, which is named FedPromptDT. Using FL solves the problem of insufficient local MEC data while protecting user privacy during offline training. The design of <b>prompts</b> integrating user-environment cues and user-preferred allocation improves the model&rsquo;s adaptability to various user environments during online execution.</p></p class="citation"></blockquote><h3 id=1124--129249-gpt-4s-assessment-of-its-performance-in-a-usmle-based-case-study-uttam-dhakal-et-al-2024>(11/24 | 129/249) GPT-4&rsquo;s assessment of its performance in a USMLE-based case study (Uttam Dhakal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Uttam Dhakal, Aniket Kumar Singh, Suman Devkota, Yogesh Sapkota, Bishal Lamichhane, Suprinsa Paudyal, Chandra Dhakal. (2024)<br><strong>GPT-4&rsquo;s assessment of its performance in a USMLE-based case study</strong><br><button class=copy-to-clipboard title="GPT-4's assessment of its performance in a USMLE-based case study" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-HC, cs-MA, cs.AI, stat-ML<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09654v1.pdf filename=2402.09654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates <b>GPT-4&rsquo;s</b> assessment of its performance in healthcare applications. A simple <b>prompting</b> technique was used to <b>prompt</b> the <b>LLM</b> with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question. The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The model was asked to provide absolute and relative confidence scores before and after each question. The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups. Results indicate that feedback influences relative confidence but doesn&rsquo;t consistently increase or decrease it. Understanding the performance of <b>LLM</b> is paramount in exploring its utility in sensitive areas like healthcare. This study contributes to the ongoing discourse on the reliability of AI, particularly of <b>LLMs</b> like <b>GPT-4,</b> within healthcare, offering insights into how feedback mechanisms might be optimized to enhance AI-assisted medical education and decision support.</p></p class="citation"></blockquote><h3 id=1224--130249-optimus-scalable-optimization-modeling-with-milp-solvers-and-large-language-models-ali-ahmaditeshnizi-et-al-2024>(12/24 | 130/249) OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models (Ali AhmadiTeshnizi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali AhmadiTeshnizi, Wenzhi Gao, Madeleine Udell. (2024)<br><strong>OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models</strong><br><button class=copy-to-clipboard title="OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-MA, cs.AI<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10172v1.pdf filename=2402.10172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)-based</b> agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long <b>prompts.</b> Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20%$ and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30%$.</p></p class="citation"></blockquote><h3 id=1324--131249-zero-shot-reasoning-personalized-content-generation-without-the-cold-start-problem-davor-hafnar-et-al-2024>(13/24 | 131/249) Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem (Davor Hafnar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davor Hafnar, Jure Demšar. (2024)<br><strong>Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem</strong><br><button class=copy-to-clipboard title="Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Zero-shot, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10133v1.pdf filename=2402.10133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Procedural content generation uses algorithmic techniques to create <b>large</b> <b>amounts</b> <b>of</b> new content for games at much lower production costs. In newer approaches, procedural content generation utilizes machine learning. However, these methods usually require expensive collection of <b>large</b> <b>amounts</b> <b>of</b> data, as well as the development and training of fairly complex learning models, which can be both extremely time-consuming and expensive. The core of our research is to explore whether we can lower the barrier to the use of personalized procedural content generation through a more practical and generalizable approach with <b>large</b> <b>language</b> <b>models.</b> Matching game content with player preferences benefits both players, who enjoy the game more, and developers, who increasingly depend on players enjoying the game before being able to monetize it. Therefore, this paper presents a novel approach to achieving personalization by using <b>large</b> <b>language</b> <b>models</b> to propose levels based on the gameplay data continuously collected from individual players. We compared the levels generated using our approach with levels generated with more traditional procedural generation techniques. Our easily reproducible method has proven viable in a production setting and outperformed levels generated by traditional methods in the probability that a player will not quit the game mid-level.</p></p class="citation"></blockquote><h3 id=1424--132249-jack-of-all-trades-master-of-some-a-multi-purpose-transformer-agent-quentin-gallouédec-et-al-2024>(14/24 | 132/249) Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent (Quentin Gallouédec et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quentin Gallouédec, Edward Beeching, Clément Romac, Emmanuel Dellandréa. (2024)<br><strong>Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent</strong><br><button class=copy-to-clipboard title="Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09844v1.pdf filename=2402.09844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The search for a general model that can operate seamlessly across multiple domains remains a key goal in machine learning research. The prevailing methodology in <b>Reinforcement</b> <b>Learning</b> (RL) typically limits models to a single task within a unimodal framework, a limitation that contrasts with the broader vision of a versatile, multi-domain model. In this paper, we present Jack of All Trades (JAT), a <b>transformer-based</b> model with a unique design optimized for handling sequential decision-making tasks and <b>multimodal</b> data types. The JAT model demonstrates its robust capabilities and versatility by achieving strong performance on very different RL <b>benchmarks,</b> along with promising results on Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a single set of weights. The JAT model marks a significant step towards more general, cross-domain AI model design, and notably, it is the first model of its kind to be fully open-sourced (see <a href=https://huggingface.co/jat-project/jat)>https://huggingface.co/jat-project/jat)</a>, including a pioneering general-purpose dataset.</p></p class="citation"></blockquote><h3 id=1524--133249-swissnyf-tool-grounded-llm-agents-for-black-box-setting-somnath-sendhil-kumar-et-al-2024>(15/24 | 133/249) SwissNYF: Tool Grounded LLM Agents for Black Box Setting (Somnath Sendhil Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Somnath Sendhil Kumar, Dhruv Jain, Eshaan Agarwal, Raunak Pandey. (2024)<br><strong>SwissNYF: Tool Grounded LLM Agents for Black Box Setting</strong><br><button class=copy-to-clipboard title="SwissNYF: Tool Grounded LLM Agents for Black Box Setting" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 25<br>Keywords: Black Box, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10051v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10051v1.pdf filename=2402.10051v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated enhanced capabilities in function-calling, these advancements primarily rely on accessing the functions&rsquo; responses. This methodology is practical for simpler APIs but faces scalability issues with irreversible APIs that significantly impact the system, such as a database deletion API. Similarly, processes requiring extensive time for each API call and those necessitating forward planning, like automated action pipelines, present complex challenges. Furthermore, scenarios often arise where a generalized approach is needed because algorithms lack direct access to the specific implementations of these functions or secrets to use them. Traditional tool planning methods are inadequate in these cases, compelling the need to operate within <b>black-box</b> <b>environments.</b> Unlike their performance in tool manipulation, <b>LLMs</b> excel in <b>black-box</b> <b>tasks,</b> such as program synthesis. Therefore, we harness the program synthesis capabilities of <b>LLMs</b> to strategize tool usage in <b>black-box</b> <b>settings,</b> ensuring solutions are verified prior to implementation. We introduce TOPGUN, an ingeniously crafted approach leveraging program synthesis for <b>black</b> <b>box</b> tool planning. Accompanied by SwissNYF, a comprehensive suite that integrates <b>black-box</b> <b>algorithms</b> for planning and verification tasks, addressing the aforementioned challenges and enhancing the versatility and effectiveness of <b>LLMs</b> in complex API interactions. The public code for SwissNYF is available at <a href=https://github.com/iclr-dummy-user/SwissNYF>https://github.com/iclr-dummy-user/SwissNYF</a>.</p></p class="citation"></blockquote><h3 id=1624--134249-representation-learning-using-a-single-forward-pass-aditya-somasundaram-et-al-2024>(16/24 | 134/249) Representation Learning Using a Single Forward Pass (Aditya Somasundaram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aditya Somasundaram, Pushkal Mishra, Ayon Borthakur. (2024)<br><strong>Representation Learning Using a Single Forward Pass</strong><br><button class=copy-to-clipboard title="Representation Learning Using a Single Forward Pass" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 25<br>Keywords: MNIST, Few-shot, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09769v1.pdf filename=2402.09769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a neuroscience-inspired Solo Pass Embedded Learning Algorithm (SPELA). SPELA is a prime candidate for training and inference applications in Edge AI devices. At the same time, SPELA can optimally cater to the need for a framework to study perceptual <b>representation</b> <b>learning</b> and formation. SPELA has distinctive features such as neural priors (in the form of embedded vectors), no weight transport, no update locking of weights, complete local Hebbian learning, single forward pass with no storage of activations, and single weight update per sample. Juxtaposed with traditional approaches, SPELA operates without the need for backpropagation. We show that our algorithm can perform nonlinear classification on a noisy boolean operation dataset. Additionally, we exhibit high performance using SPELA across <b>MNIST,</b> KMNIST, and Fashion <b>MNIST.</b> Lastly, we show the <b>few-shot</b> and 1-epoch learning capabilities of SPELA on <b>MNIST,</b> KMNIST, and Fashion <b>MNIST,</b> where it consistently outperforms backpropagation.</p></p class="citation"></blockquote><h3 id=1724--135249-clifford-group-equivariant-simplicial-message-passing-networks-cong-liu-et-al-2024>(17/24 | 135/249) Clifford Group Equivariant Simplicial Message Passing Networks (Cong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cong Liu, David Ruhe, Floor Eijkelboom, Patrick Forré. (2024)<br><strong>Clifford Group Equivariant Simplicial Message Passing Networks</strong><br><button class=copy-to-clipboard title="Clifford Group Equivariant Simplicial Message Passing Networks" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Message-Passing, Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10011v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10011v2.pdf filename=2402.10011v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular <b>graph</b> <b>message</b> <b>passing.</b> Clifford algebras include higher-order objects such as bivectors and trivectors, which express geometric features (e.g., areas, volumes) derived from vectors. Using this knowledge, we represent simplex features through geometric products of their vertices. To achieve efficient simplicial message passing, we share the parameters of the message network across different dimensions. Additionally, we restrict the final message to an aggregation of the incoming messages from different dimensions, leading to what we term shared simplicial message passing. Experimental results show that our method is able to outperform both equivariant and simplicial <b>graph</b> <b>neural</b> <b>networks</b> on a variety of geometric tasks.</p></p class="citation"></blockquote><h3 id=1824--136249-generating-visual-stimuli-from-eeg-recordings-using-transformer-encoder-based-eeg-encoder-and-gan-rahul-mishra-et-al-2024>(18/24 | 136/249) Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN (Rahul Mishra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rahul Mishra, Arnav Bhavsar. (2024)<br><strong>Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN</strong><br><button class=copy-to-clipboard title="Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI, eess-SP, q-bio-NC<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10115v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10115v1.pdf filename=2402.10115v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we tackle a modern research challenge within the field of perceptual brain decoding, which revolves around synthesizing images from EEG signals using an adversarial deep learning framework. The specific objective is to recreate images belonging to various object categories by leveraging EEG recordings obtained while subjects view those images. To achieve this, we employ a <b>Transformer-encoder</b> based EEG encoder to produce EEG encodings, which serve as inputs to the generator component of the <b>GAN</b> network. Alongside the adversarial loss, we also incorporate perceptual loss to enhance the quality of the generated images.</p></p class="citation"></blockquote><h3 id=1924--137249-towards-reducing-diagnostic-errors-with-interpretable-risk-prediction-denis-jered-mcinerney-et-al-2024>(19/24 | 137/249) Towards Reducing Diagnostic Errors with Interpretable Risk Prediction (Denis Jered McInerney et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Denis Jered McInerney, William Dickinson, Lucy Flynn, Andrea Young, Geoffrey Young, Jan-Willem van de Meent, Byron C. Wallace. (2024)<br><strong>Towards Reducing Diagnostic Errors with Interpretable Risk Prediction</strong><br><button class=copy-to-clipboard title="Towards Reducing Diagnostic Errors with Interpretable Risk Prediction" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Stemming, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10109v1.pdf filename=2402.10109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use <b>LLMs</b> to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors <b>stemming</b> from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual &ldquo;true&rdquo; diagnoses. We do so with <b>LLMs,</b> to ensure that the input text is from before a confident diagnosis can be made. We use an <b>LLM</b> to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model. We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses.</p></p class="citation"></blockquote><h3 id=2024--138249-reinforcement-learning-for-solving-stochastic-vehicle-routing-problem-with-time-windows-zangir-iklassov-et-al-2024>(20/24 | 138/249) Reinforcement Learning for Solving Stochastic Vehicle Routing Problem with Time Windows (Zangir Iklassov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zangir Iklassov, Ikboljon Sobirov, Ruben Solozabal, Martin Takac. (2024)<br><strong>Reinforcement Learning for Solving Stochastic Vehicle Routing Problem with Time Windows</strong><br><button class=copy-to-clipboard title="Reinforcement Learning for Solving Stochastic Vehicle Routing Problem with Time Windows" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09765v1.pdf filename=2402.09765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a <b>reinforcement</b> <b>learning</b> approach to optimize the Stochastic Vehicle Routing Problem with Time Windows (SVRP), focusing on reducing travel costs in goods delivery. We develop a novel SVRP formulation that accounts for uncertain travel costs and demands, alongside specific customer time windows. An attention-based neural network trained through <b>reinforcement</b> <b>learning</b> is employed to minimize routing costs. Our approach addresses a gap in SVRP research, which traditionally relies on heuristic methods, by leveraging machine learning. The model outperforms the Ant-Colony Optimization algorithm, achieving a 1.73% reduction in travel costs. It uniquely integrates external information, demonstrating robustness in diverse environments, making it a valuable <b>benchmark</b> for future SVRP studies and industry application.</p></p class="citation"></blockquote><h3 id=2124--139249-a-privacy-preserving-distributed-and-cooperative-fcm-based-learning-approach-for-cancer-research-jose-l-salmeron-et-al-2024>(21/24 | 139/249) A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research (Jose L. Salmeron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jose L. Salmeron, Irina Arévalo. (2024)<br><strong>A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research</strong><br><button class=copy-to-clipboard title="A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-DC, cs.AI<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10102v1.pdf filename=2402.10102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed Artificial Intelligence is attracting interest day by day. In this paper, the authors introduce an innovative methodology for distributed learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a privacy-preserving way. The authors design a training scheme for collaborative FCM learning that offers data privacy compliant with the current regulation. This method is applied to a cancer detection problem, proving that the performance of the model is improved by the <b>Federated</b> <b>Learning</b> process, and obtaining similar results to the ones that can be found in the literature.</p></p class="citation"></blockquote><h3 id=2224--140249-agents-need-not-know-their-purpose-paulo-garcia-2024>(22/24 | 140/249) Agents Need Not Know Their Purpose (Paulo Garcia, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paulo Garcia. (2024)<br><strong>Agents Need Not Know Their Purpose</strong><br><button class=copy-to-clipboard title="Agents Need Not Know Their Purpose" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09734v1.pdf filename=2402.09734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring artificial intelligence behaves in such a way that is aligned with human values is commonly referred to as the alignment challenge. Prior work has shown that rational agents, behaving in such a way that maximizes a utility function, will inevitably behave in such a way that is not aligned with human values, especially as their level of intelligence goes up. Prior work has also shown that there is no &ldquo;one true utility function&rdquo;; solutions must include a more holistic approach to alignment. This paper describes oblivious agents: agents that are architected in such a way that their effective utility function is an aggregation of a known and hidden sub-functions. The hidden component, to be maximized, is internally implemented as a <b>black</b> <b>box,</b> preventing the agent from examining it. The known component, to be minimized, is knowledge of the hidden sub-function. Architectural constraints further influence how agent actions can evolve its internal environment model. We show that an oblivious agent, behaving rationally, constructs an internal approximation of designers&rsquo; intentions (i.e., infers alignment), and, as a consequence of its architecture and effective utility function, behaves in such a way that maximizes alignment; i.e., maximizing the approximated intention function. We show that, paradoxically, it does this for whatever utility function is used as the hidden component and, in contrast with extant techniques, chances of alignment actually improve as agent intelligence grows.</p></p class="citation"></blockquote><h3 id=2324--141249-road-graph-generator-mapping-roads-at-construction-sites-from-gps-data-katarzyna-michałowska-et-al-2024>(23/24 | 141/249) Road Graph Generator: Mapping roads at construction sites from GPS data (Katarzyna Michałowska et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Katarzyna Michałowska, Helga Margrete Bodahl Holmestad, Signe Riemer-Sørensen. (2024)<br><strong>Road Graph Generator: Mapping roads at construction sites from GPS data</strong><br><button class=copy-to-clipboard title="Road Graph Generator: Mapping roads at construction sites from GPS data" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09919v1.pdf filename=2402.09919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a method for road inference from GPS trajectories to map construction sites. This task introduces a unique challenge due to the erratic and non-standard movement patterns of construction machinery, which diverge significantly from typical vehicular traffic on established roads. Our method first identifies intersections in the road network that serve as critical decision points, and later connects them with edges, producing a <b>graph,</b> which subsequently can be used for planning and task-allocation. We demonstrate the effectiveness of our approach by mapping roads at a real-life construction site in Norway.</p></p class="citation"></blockquote><h3 id=2424--142249-on-computing-plans-with-uniform-action-costs-alberto-pozanco-et-al-2024>(24/24 | 142/249) On Computing Plans with Uniform Action Costs (Alberto Pozanco et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Pozanco, Daniel Borrajo, Manuela Veloso. (2024)<br><strong>On Computing Plans with Uniform Action Costs</strong><br><button class=copy-to-clipboard title="On Computing Plans with Uniform Action Costs" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09877v1.pdf filename=2402.09877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many real-world planning applications, agents might be interested in finding plans whose actions have costs that are as uniform as possible. Such plans provide agents with a sense of stability and predictability, which are key features when humans are the agents executing plans suggested by planning tools. This paper adapts three uniformity metrics to automated planning, and introduce planning-based compilations that allow to lexicographically optimize sum of action costs and action costs uniformity. Experimental results both in well-known and novel planning <b>benchmarks</b> show that the reformulated tasks can be effectively solved in practice to generate uniform plans.</p></p class="citation"></blockquote><h2 id=cscv-29>cs.CV (29)</h2><h3 id=129--143249-vigeo-an-assessment-of-vision-gnns-in-earth-observation-luca-colomba-et-al-2024>(1/29 | 143/249) ViGEO: an Assessment of Vision GNNs in Earth Observation (Luca Colomba et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Colomba, Paolo Garza. (2024)<br><strong>ViGEO: an Assessment of Vision GNNs in Earth Observation</strong><br><button class=copy-to-clipboard title="ViGEO: an Assessment of Vision GNNs in Earth Observation" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 86<br>Keywords: Vision Transformer, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09962v1.pdf filename=2402.09962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Satellite missions and Earth Observation (EO) systems represent fundamental assets for environmental monitoring and the timely identification of catastrophic events, long-term monitoring of both natural resources and human-made assets, such as vegetation, water bodies, forests as well as buildings. Different EO missions enables the collection of information on several spectral bandwidths, such as MODIS, Sentinel-1 and Sentinel-2. Thus, given the recent advances of machine learning, computer <b>vision</b> <b>and</b> the availability of labeled data, researchers demonstrated the feasibility and the precision of land-use monitoring systems and remote sensing image classification through the use of deep neural networks. Such systems may help domain experts and governments in constant environmental monitoring, enabling timely intervention in case of catastrophic events (e.g., forest wildfire in a remote area). Despite the recent advances in the field of computer <b>vision,</b> <b>many</b> works limit their analysis on <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and, more recently, to <b>vision</b> <b>transformers</b> (ViTs). Given the recent successes of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> on non-graph data, such as time-series and images, we investigate the performances of a recent <b>Vision</b> <b>GNN</b> architecture (ViG) applied to the task of land cover classification. The experimental results show that ViG achieves state-of-the-art performances in multiclass and multilabel classification contexts, surpassing both ViT and ResNet on large-scale <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=229--144249-llms-as-bridges-reformulating-grounded-multimodal-named-entity-recognition-jinyuan-li-et-al-2024>(2/29 | 144/249) LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition (Jinyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyuan Li, Han Li, Di Sun, Jiahao Wang, Wenkun Zhang, Zan Wang, Gang Pan. (2024)<br><strong>LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition</strong><br><button class=copy-to-clipboard title="LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Object Detection, Multi-modal, Multi-modal, Grounding, Image2text, Named Entity Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09989v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09989v2.pdf filename=2402.09989v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Grounded <b>Multimodal</b> <b>Named</b> <b>Entity</b> <b>Recognition</b> (GMNER) is a nascent <b>multimodal</b> task that aims to identify <b>named</b> <b>entities,</b> <b>entity</b> types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between <b>image-text</b> pairs in social media results in a significant portion of <b>named</b> <b>entities</b> <b>being</b> ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained <b>named</b> <b>entities.</b> <b>In</b> this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing <b>object</b> <b>detection</b> methods to pre-extract regional features, thereby naturally addressing two major limitations of existing GMNER methods. 2) The introduction of entity expansion expression and Visual Entailment (VE) Module unifies Visual <b>Grounding</b> (VG) and Entity <b>Grounding</b> (EG). It enables RiVEG to effortlessly inherit the Visual Entailment and Visual <b>Grounding</b> capabilities of any current or prospective <b>multimodal</b> pretraining models. Extensive experiments demonstrate that RiVEG outperforms state-of-the-art methods on the existing GMNER dataset and achieves absolute leads of 10.65%, 6.21%, and 8.83% in all three subtasks.</p></p class="citation"></blockquote><h3 id=329--145249-data-augmentation-and-transfer-learning-approaches-applied-to-facial-expressions-recognition-enrico-randellini-et-al-2024>(3/29 | 145/249) Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition (Enrico Randellini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enrico Randellini, Leonardo Rigutini, Claudio Sacca&rsquo;. (2024)<br><strong>Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition</strong><br><button class=copy-to-clipboard title="Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Data Augmentation, Fine-tuning, Generative Adversarial Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09982v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09982v1.pdf filename=2402.09982v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The face expression is the first thing we pay attention to when we want to understand a person&rsquo;s state of mind. Thus, the ability to recognize facial expressions in an automatic way is a very interesting research field. In this paper, because the small size of available training datasets, we propose a novel <b>data</b> <b>augmentation</b> technique that improves the performances in the recognition task. We apply geometrical transformations and build from scratch <b>GAN</b> models able to generate new synthetic images for each emotion type. Thus, on the augmented datasets we fine tune pretrained <b>convolutional</b> <b>neural</b> <b>networks</b> with different architectures. To measure the generalization ability of the models, we apply extra-database protocol approach, namely we train models on the augmented versions of training dataset and test them on two different databases. The combination of these techniques allows to reach average accuracy values of the order of 85% for the InceptionResNetV2 model.</p></p class="citation"></blockquote><h3 id=429--146249-prompt-based-personalized-federated-learning-for-medical-visual-question-answering-he-zhu-et-al-2024>(4/29 | 146/249) Prompt-based Personalized Federated Learning for Medical Visual Question Answering (He Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>He Zhu, Ren Togo, Takahiro Ogawa, Miki Haseyama. (2024)<br><strong>Prompt-based Personalized Federated Learning for Medical Visual Question Answering</strong><br><button class=copy-to-clipboard title="Prompt-based Personalized Federated Learning for Medical Visual Question Answering" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Federated Learning, Transformer, Question Answering, Visual Question Answering, Visual Question Answering, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09677v1.pdf filename=2402.09677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel <b>prompt-based</b> personalized <b>federated</b> <b>learning</b> (pFL) method to address data heterogeneity and privacy concerns in traditional medical <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA)</b> methods. Specifically, we regard medical datasets from different organs as clients and use pFL to train personalized <b>transformer-based</b> <b>VQA</b> models for each client. To address the high computational complexity of client-to-client communication in previous pFL methods, we propose a succinct information sharing system by introducing <b>prompts</b> that are small learnable parameters. In addition, the proposed method introduces a reliability parameter to prevent the negative effects of low performance and irrelevant clients. Finally, extensive evaluations on various heterogeneous medical datasets attest to the effectiveness of our proposed method.</p></p class="citation"></blockquote><h3 id=529--147249-mind-the-modality-gap-towards-a-remote-sensing-vision-language-model-via-cross-modal-alignment-angelos-zavras-et-al-2024>(5/29 | 147/249) Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment (Angelos Zavras et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angelos Zavras, Dimitrios Michail, Begüm Demir, Ioannis Papoutsis. (2024)<br><strong>Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment</strong><br><button class=copy-to-clipboard title="Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Foundation Model, Supervised Learning, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09816v1.pdf filename=2402.09816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning (DL) is undergoing a paradigm shift with the emergence of <b>foundation</b> <b>models,</b> aptly named by their crucial, yet incomplete nature. In this work, we focus on Contrastive Language-Image Pre-training (CLIP), an open-vocabulary <b>foundation</b> <b>model,</b> which achieves high accuracy across many image classification tasks and is often competitive with a fully <b>supervised</b> baseline without being explicitly trained. Nevertheless, there are still domains where <b>zero-shot</b> CLIP performance is far from optimal, such as Remote Sensing (RS) and medical imagery. These domains do not only exhibit fundamentally different distributions compared to natural images, but also commonly rely on complementary modalities, beyond RGB, to derive meaningful insights. To this end, we propose a methodology for the purpose of aligning distinct RS imagery modalities with the visual and textual modalities of CLIP. Our two-stage procedure, comprises of robust <b>fine-tuning</b> CLIP in order to deal with the distribution shift, accompanied by the cross-modal alignment of a RS modality encoder, in an effort to extend the <b>zero-shot</b> capabilities of CLIP. We ultimately demonstrate our method on the tasks of RS imagery classification and cross-modal retrieval. We empirically show that both robust <b>fine-tuning</b> and cross-modal alignment translate to significant performance gains, across several RS <b>benchmark</b> datasets. Notably, these enhancements are achieved without the reliance on textual descriptions, without introducing any task-specific parameters, without training from scratch and without catastrophic forgetting.</p></p class="citation"></blockquote><h3 id=629--148249-deep-spectral-meshes-multi-frequency-facial-mesh-processing-with-graph-neural-networks-robert-kosk-et-al-2024>(6/29 | 148/249) Deep Spectral Meshes: Multi-Frequency Facial Mesh Processing with Graph Neural Networks (Robert Kosk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Kosk, Richard Southern, Lihua You, Shaojun Bian, Willem Kokke, Greg Maguire. (2024)<br><strong>Deep Spectral Meshes: Multi-Frequency Facial Mesh Processing with Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Deep Spectral Meshes: Multi-Frequency Facial Mesh Processing with Graph Neural Networks" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T10, 68T45, 68U05, I-5-4; I-5-1; I-3-5; I-3-7; I-4-5; I-4-2; I-5-1; I-5-2, cs-CG, cs-CV, cs-GR, cs.CV<br>Keyword Score: 48<br>Keywords: Graph Convolutional Network, Graph, Graph Neural Network, Convolution, Convolutional Neural Network, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10365v1.pdf filename=2402.10365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rising popularity of virtual worlds, the importance of data-driven parametric models of 3D meshes has grown rapidly. Numerous applications, such as computer vision, procedural generation, and mesh editing, vastly rely on these models. However, current approaches do not allow for independent editing of deformations at different frequency levels. They also do not benefit from representing deformations at different frequencies with dedicated <b>representations,</b> <b>which</b> would better expose their properties and improve the generated meshes&rsquo; geometric and perceptual quality. In this work, spectral meshes are introduced as a method to decompose mesh deformations into low-frequency and high-frequency deformations. These features of low- and high-frequency deformations are used for <b>representation</b> <b>learning</b> with <b>graph</b> <b>convolutional</b> <b>networks.</b> A parametric model for 3D facial mesh synthesis is built upon the proposed framework, exposing user parameters that control disentangled high- and low-frequency deformations. Independent control of deformations at different frequencies and generation of plausible synthetic examples are mutually exclusive objectives. A Conditioning Factor is introduced to leverage these objectives. Our model takes further advantage of spectral partitioning by representing different frequency levels with disparate, more suitable <b>representations.</b> <b>Low</b> frequencies are represented with standardised Euclidean coordinates, and high frequencies with a normalised deformation <b>representation</b> <b>(DR).</b> This paper investigates applications of our proposed approach in mesh reconstruction, mesh interpolation, and multi-frequency editing. It is demonstrated that our method improves the overall quality of generated meshes on most datasets when considering both the $L_1$ norm and perceptual Dihedral Angle Mesh Error (DAME) metrics.</p></p class="citation"></blockquote><h3 id=729--149249-mm-point-multi-view-information-enhanced-multi-modal-self-supervised-3d-point-cloud-understanding-hai-tao-yu-et-al-2024>(7/29 | 149/249) MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding (Hai-Tao Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hai-Tao Yu, Mofei Song. (2024)<br><strong>MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding</strong><br><button class=copy-to-clipboard title="MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 48<br>Keywords: Contrastive Learning, Few-shot, Multi-modal, Representation Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10002v1.pdf filename=2402.10002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior <b>self-supervised</b> signals for 3D objects. In this paper, we propose a novel <b>self-supervised</b> point cloud <b>representation</b> <b>learning</b> method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the <b>Multi-modal</b> interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on <b>contrastive</b> <b>learning,</b> we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully designed transformation strategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point demonstrates state-of-the-art (SOTA) performance in various downstream tasks. For instance, it achieves a peak accuracy of 92.4% on the synthetic dataset ModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN, comparable to fully <b>supervised</b> methods. Additionally, we demonstrate its effectiveness in tasks such as <b>few-shot</b> classification, 3D part segmentation and 3D semantic segmentation.</p></p class="citation"></blockquote><h3 id=829--150249-textual-localization-decomposing-multi-concept-images-for-subject-driven-text-to-image-generation-junjie-shentu-et-al-2024>(8/29 | 150/249) Textual Localization: Decomposing Multi-concept Images for Subject-Driven Text-to-Image Generation (Junjie Shentu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjie Shentu, Matthew Watson, Noura Al Moubayed. (2024)<br><strong>Textual Localization: Decomposing Multi-concept Images for Subject-Driven Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Textual Localization: Decomposing Multi-concept Images for Subject-Driven Text-to-Image Generation" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Image2text, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09966v1.pdf filename=2402.09966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Subject-driven <b>text-to-image</b> diffusion models empower users to tailor the model to new concepts absent in the pre-training dataset using a few sample images. However, prevalent subject-driven models primarily rely on single-concept input images, facing challenges in specifying the target concept when dealing with multi-concept input images. To this end, we introduce a textual localized <b>text-to-image</b> model (Texual Localization) to handle multi-concept input images. During <b>fine-tuning,</b> our method incorporates a novel cross-attention guidance to decompose multiple concepts, establishing distinct connections between the visual representation of the target concept and the identifier token in the text <b>prompt.</b> Experimental results reveal that our method outperforms or performs comparably to the baseline models in terms of image fidelity and <b>image-text</b> alignment on multi-concept input images. In comparison to Custom Diffusion, our method with hard guidance achieves CLIP-I scores that are 7.04%, 8.13% higher and CLIP-T scores that are 2.22%, 5.85% higher in single-concept and multi-concept generation, respectively. Notably, our method generates cross-attention maps consistent with the target concept in the generated images, a capability absent in existing models.</p></p class="citation"></blockquote><h3 id=929--151249-social-reward-evaluating-and-enhancing-generative-ai-through-million-user-feedback-from-an-online-creative-community-arman-isajanyan-et-al-2024>(9/29 | 151/249) Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community (Arman Isajanyan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arman Isajanyan, Artur Shatveryan, David Kocharyan, Zhangyang Wang, Humphrey Shi. (2024)<br><strong>Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community</strong><br><button class=copy-to-clipboard title="Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Generative AI, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09872v1.pdf filename=2402.09872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social reward as a form of community recognition provides a strong source of motivation for users of online platforms to engage and contribute with content. The recent progress of text-conditioned image synthesis has ushered in a collaborative era where AI empowers users to craft original visual artworks seeking community validation. Nevertheless, assessing these models in the context of collective community preference introduces distinct challenges. Existing evaluation methods predominantly center on limited size user studies guided by image quality and <b>prompt</b> alignment. This work pioneers a paradigm shift, unveiling Social Reward - an innovative reward modeling framework that leverages implicit feedback from social network users engaged in creative editing of generated images. We embark on an extensive journey of dataset curation and refinement, drawing from Picsart: an online visual creation and editing platform, yielding a first million-user-scale dataset of implicit human preferences for user-generated visual art named Picsart Image-Social. Our analysis exposes the shortcomings of current metrics in modeling community creative preference of <b>text-to-image</b> models&rsquo; outputs, compelling us to introduce a novel predictive model explicitly tailored to address these limitations. Rigorous quantitative experiments and user study show that our Social Reward model aligns better with social popularity than existing metrics. Furthermore, we utilize Social Reward to <b>fine-tune</b> <b>text-to-image</b> models, yielding images that are more favored by not only Social Reward, but also other established metrics. These findings highlight the relevance and effectiveness of Social Reward in assessing community appreciation for AI-generated artworks, establishing a closer alignment with users&rsquo; creative goals: creating popular visual art. Codes can be accessed at <a href=https://github.com/Picsart-AI-Research/Social-Reward>https://github.com/Picsart-AI-Research/Social-Reward</a></p></p class="citation"></blockquote><h3 id=1029--152249-dreammatcher-appearance-matching-self-attention-for-semantically-consistent-text-to-image-personalization-jisu-nam-et-al-2024>(10/29 | 152/249) DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization (Jisu Nam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, Seunggyu Chang. (2024)<br><strong>DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization</strong><br><button class=copy-to-clipboard title="DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Text2image, Prompt, Self-Attention, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09812v1.pdf filename=2402.09812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The objective of <b>text-to-image</b> <b>(T2I)</b> personalization is to customize a diffusion model to a user-provided reference concept, generating diverse images of the concept aligned with the target <b>prompts.</b> Conventional methods representing the reference concepts using unique <b>text</b> <b>embeddings</b> often fail to accurately mimic the appearance of the reference. To address this, one solution may be explicitly conditioning the reference images into the target denoising process, known as key-value replacement. However, prior works are constrained to local editing since they disrupt the structure path of the pre-trained T2I model. To overcome this, we propose a novel plug-in method, called DreamMatcher, which reformulates T2I personalization as semantic matching. Specifically, DreamMatcher replaces the target values with reference values aligned by semantic matching, while leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures. We also introduce a semantic-consistent masking strategy to isolate the personalized concept from irrelevant regions introduced by the target <b>prompts.</b> Compatible with existing T2I models, DreamMatcher shows significant improvements in complex scenarios. Intensive analyses demonstrate the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=1129--153249-textron-weakly-supervised-multilingual-text-detection-through-data-programming-dhruv-kudale-et-al-2024>(11/29 | 153/249) TEXTRON: Weakly Supervised Multilingual Text Detection through Data Programming (Dhruv Kudale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhruv Kudale, Badri Vishal Kasuba, Venkatapathy Subramanian, Parag Chaudhuri, Ganesh Ramakrishnan. (2024)<br><strong>TEXTRON: Weakly Supervised Multilingual Text Detection through Data Programming</strong><br><button class=copy-to-clipboard title="TEXTRON: Weakly Supervised Multilingual Text Detection through Data Programming" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Low-Resource, Supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09811v1.pdf filename=2402.09811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several recent deep learning (DL) based techniques perform considerably well on image-based multilingual text detection. However, their performance relies heavily on the availability and quality of training data. There are numerous types of page-level document images consisting of information in several modalities, languages, fonts, and layouts. This makes text detection a challenging problem in the field of computer vision (CV), especially for <b>low-resource</b> or handwritten languages. Furthermore, there is a scarcity of word-level labeled data for text detection, especially for multilingual settings and Indian scripts that incorporate both printed and handwritten text. Conventionally, Indian script text detection requires training a DL model on plenty of labeled data, but to the best of our knowledge, no relevant datasets are available. Manual annotation of such data requires a lot of time, effort, and expertise. In order to solve this problem, we propose TEXTRON, a Data Programming-based approach, where users can plug various text detection methods into a <b>weak</b> <b>supervision-based</b> learning framework. One can view this approach to multilingual text detection as an ensemble of different CV-based techniques and DL approaches. TEXTRON can leverage the predictions of DL models pre-trained on a significant amount of language data in conjunction with CV-based methods to improve text detection in other languages. We demonstrate that TEXTRON can improve the detection performance for documents written in Indian languages, despite the absence of corresponding labeled data. Further, through extensive experimentation, we show improvement brought about by our approach over the current State-of-the-art (SOTA) models, especially for handwritten Devanagari text. Code and dataset has been made available at <a href=https://github.com/IITB-LEAP-OCR/TEXTRON>https://github.com/IITB-LEAP-OCR/TEXTRON</a></p></p class="citation"></blockquote><h3 id=1229--154249-foul-prediction-with-estimated-poses-from-soccer-broadcast-video-jiale-fang-et-al-2024>(12/29 | 154/249) Foul prediction with estimated poses from soccer broadcast video (Jiale Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiale Fang, Calvin Yeung, Keisuke Fujii. (2024)<br><strong>Foul prediction with estimated poses from soccer broadcast video</strong><br><button class=copy-to-clipboard title="Foul prediction with estimated poses from soccer broadcast video" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09650v1.pdf filename=2402.09650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in computer vision have made significant progress in tracking and pose estimation of sports players. However, there have been fewer studies on behavior prediction with pose estimation in sports, in particular, the prediction of soccer fouls is challenging because of the smaller image size of each player and of difficulty in the usage of e.g., the ball and pose information. In our research, we introduce an innovative deep learning approach for anticipating soccer fouls. This method integrates video data, bounding box positions, image details, and pose information by curating a novel soccer foul dataset. Our model utilizes a combination of <b>convolutional</b> and <b>recurrent</b> <b>neural</b> <b>networks</b> <b>(CNNs</b> and <b>RNNs)</b> to effectively merge information from these four modalities. The experimental results show that our full model outperformed the ablated models, and all of the <b>RNN</b> modules, bounding box position and image, and estimated pose were useful for the foul prediction. Our findings have important implications for a deeper understanding of foul play in soccer and provide a valuable reference for future research and practice in this area.</p></p class="citation"></blockquote><h3 id=1329--155249-exploiting-alpha-transparency-in-language-and-vision-based-ai-systems-david-noever-et-al-2024>(13/29 | 155/249) Exploiting Alpha Transparency In Language And Vision-Based AI Systems (David Noever et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Noever, Forrest McKee. (2024)<br><strong>Exploiting Alpha Transparency In Language And Vision-Based AI Systems</strong><br><button class=copy-to-clipboard title="Exploiting Alpha Transparency In Language And Vision-Based AI Systems" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 36<br>Keywords: Convolution, Convolutional Neural Network, Multi-modal, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09671v1.pdf filename=2402.09671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems. Our method uses this alpha layer as a clandestine channel invisible to human observers but fully actionable by AI image processors. The scope tested for the vulnerability spans representative vision systems from Apple, Microsoft, Google, Salesforce, Nvidia, and Facebook, highlighting the attack&rsquo;s potential breadth. This vulnerability challenges the security protocols of existing and fielded vision systems, from medical imaging to autonomous driving technologies. Our experiments demonstrate that the affected systems, which rely on <b>convolutional</b> <b>neural</b> <b>networks</b> or the latest <b>multimodal</b> language models, cannot quickly mitigate these vulnerabilities through simple patches or updates. Instead, they require retraining and architectural changes, indicating a persistent hole in <b>multimodal</b> technologies without some future adversarial hardening against such <b>vision-language</b> exploits.</p></p class="citation"></blockquote><h3 id=1429--156249-visirnet-deep-image-alignment-for-uav-taken-visible-and-infrared-image-pairs-sedat-ozer-et-al-2024>(14/29 | 156/249) VisIRNet: Deep Image Alignment for UAV-taken Visible and Infrared Image Pairs (Sedat Ozer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sedat Ozer, Alain P. Ndigande. (2024)<br><strong>VisIRNet: Deep Image Alignment for UAV-taken Visible and Infrared Image Pairs</strong><br><button class=copy-to-clipboard title="VisIRNet: Deep Image Alignment for UAV-taken Visible and Infrared Image Pairs" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09635v1.pdf filename=2402.09635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a deep learning based solution for <b>multi-modal</b> image alignment regarding UAV-taken images. Many recently proposed state-of-the-art alignment techniques rely on using Lucas-Kanade (LK) based solutions for a successful alignment. However, we show that we can achieve state of the art results without using LK-based methods. Our approach carefully utilizes a two-branch based <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> based on feature embedding blocks. We propose two variants of our approach, where in the first variant (ModelA), we directly predict the new coordinates of only the four corners of the image to be aligned; and in the second one (ModelB), we predict the homography matrix directly. Applying alignment on the image corners forces algorithm to match only those four corners as opposed to computing and matching many (key)points, since the latter may cause many outliers, yielding less accurate alignment. We test our proposed approach on four aerial datasets and obtain state of the art results, when compared to the existing recent deep LK-based architectures.</p></p class="citation"></blockquote><h3 id=1529--157249-any-shift-prompting-for-generalization-over-distributions-zehao-xiao-et-al-2024>(15/29 | 157/249) Any-Shift Prompting for Generalization over Distributions (Zehao Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehao Xiao, Jiayi Shen, Mohammad Mahdi Derakhshani, Shengcai Liao, Cees G. M. Snoek. (2024)<br><strong>Any-Shift Prompting for Generalization over Distributions</strong><br><button class=copy-to-clipboard title="Any-Shift Prompting for Generalization over Distributions" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Transformer, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10099v1.pdf filename=2402.10099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image-language models with <b>prompt</b> <b>learning</b> have shown remarkable advances in numerous downstream vision tasks. Nevertheless, conventional <b>prompt</b> <b>learning</b> methods overfit their training distribution and lose the generalization ability on test distributions. To improve generalization across various distribution shifts, we propose any-shift <b>prompting:</b> <b>a</b> general probabilistic inference framework that considers the relationship between training and test distributions during <b>prompt</b> <b>learning.</b> We explicitly connect training and test distributions in the latent space by constructing training and test <b>prompts</b> <b>in</b> a hierarchical architecture. Within this framework, the test <b>prompt</b> <b>exploits</b> the distribution relationships to guide the generalization of the CLIP image-language model from training to any test distribution. To effectively encode the distribution information and their relationships, we further introduce a <b>transformer</b> inference network with a pseudo-shift training mechanism. The network generates the tailored test <b>prompt</b> <b>with</b> both training and test information in a feedforward pass, avoiding extra training costs at test time. Extensive experiments on twenty-three datasets demonstrate the effectiveness of any-shift <b>prompting</b> <b>on</b> the generalization over various distribution shifts.</p></p class="citation"></blockquote><h3 id=1629--158249-pobevm-real-time-video-matting-via-progressively-optimize-the-target-body-and-edge-jianming-xian-2024>(16/29 | 158/249) POBEVM: Real-time Video Matting via Progressively Optimize the Target Body and Edge (Jianming Xian, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianming Xian. (2024)<br><strong>POBEVM: Real-time Video Matting via Progressively Optimize the Target Body and Edge</strong><br><button class=copy-to-clipboard title="POBEVM: Real-time Video Matting via Progressively Optimize the Target Body and Edge" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IR, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09731v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09731v1.pdf filename=2402.09731v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> based approaches have achieved great performance in video matting. Many of these methods can produce accurate alpha estimation for the target body but typically yield fuzzy or incorrect target edges. This is usually caused by the following reasons: 1) The current methods always treat the target body and edge indiscriminately; 2) Target body dominates the whole target with only a tiny proportion target edge. For the first problem, we propose a <b>CNN-based</b> module that separately optimizes the matting target body and edge (SOBE). And on this basis, we introduce a real-time, trimap-free video matting method via progressively optimizing the matting target body and edge (POBEVM) that is much lighter than previous approaches and achieves significant improvements in the predicted target edge. For the second problem, we propose an Edge-L1-Loss (ELL) function that enforces our network on the matting target edge. Experiments demonstrate our method outperforms prior trimap-free matting methods on both Distinctions-646 (D646) and VideoMatte240K(VM) dataset, especially in edge optimization.</p></p class="citation"></blockquote><h3 id=1729--159249-examining-pathological-bias-in-a-generative-adversarial-network-discriminator-a-case-study-on-a-stylegan3-model-alvin-grissom-ii-et-al-2024>(17/29 | 159/249) Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model (Alvin Grissom II et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alvin Grissom II, Ryan F. Lei, Jeova Farias Sales Rocha Neto, Bailey Lin, Ryan Trotter. (2024)<br><strong>Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model</strong><br><button class=copy-to-clipboard title="Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-CY, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09786v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09786v2.pdf filename=2402.09786v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>adversarial</b> <b>networks</b> generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular <b>GAN</b> network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator&rsquo;s bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.</p></p class="citation"></blockquote><h3 id=1829--160249-diffusion-model-with-cross-attention-as-an-inductive-bias-for-disentanglement-tao-yang-et-al-2024>(18/29 | 160/249) Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement (Tao Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Yang, Cuiling Lan, Yan Lu, Nanning zheng. (2024)<br><strong>Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement</strong><br><button class=copy-to-clipboard title="Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Representation Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09712v1.pdf filename=2402.09712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Disentangled <b>representation</b> <b>learning</b> strives to extract the intrinsic factors within observed data. Factorizing these <b>representations</b> <b>in</b> an <b>unsupervised</b> manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled <b>representations.</b> <b>We</b> propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the <b>benchmark</b> datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive ablation studies and visualization analysis, shedding light on the functioning of this model. This is the first work to reveal the potent disentanglement capability of diffusion models with cross-attention, requiring no complex designs. We anticipate that our findings will inspire more investigation on exploring diffusion for disentangled <b>representation</b> <b>learning</b> towards more sophisticated data analysis and understanding.</p></p class="citation"></blockquote><h3 id=1929--161249-nyctale-neuro-evidence-transformer-for-adaptive-and-personalized-lung-nodule-invasiveness-prediction-sadaf-khademi-et-al-2024>(19/29 | 161/249) NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung Nodule Invasiveness Prediction (Sadaf Khademi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sadaf Khademi, Anastasia Oikonomou, Konstantinos N. Plataniotis, Arash Mohammadi. (2024)<br><strong>NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung Nodule Invasiveness Prediction</strong><br><button class=copy-to-clipboard title="NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung Nodule Invasiveness Prediction" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-NE, cs.CV, eess-IV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10066v1.pdf filename=2402.10066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Drawing inspiration from the primate brain&rsquo;s intriguing evidence accumulation process, and guided by models from cognitive psychology and neuroscience, the paper introduces the NYCTALE framework, a neuro-inspired and evidence accumulation-based <b>Transformer</b> architecture. The proposed neuro-inspired NYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for lung cancer diagnosis. In nature, Nyctales are small owls known for their nocturnal behavior, hunting primarily during the darkness of night. The NYCTALE operates in a similarly vigilant manner, i.e., processing data in an evidence-based fashion and making predictions dynamically/adaptively. Distinct from conventional Computed Tomography (CT)-based Deep Learning (DL) models, the NYCTALE performs predictions only when sufficient amount of evidence is accumulated. In other words, instead of processing all or a pre-defined subset of CT slices, for each person, slices are provided one at a time. The NYCTALE framework then computes an evidence vector associated with contribution of each new CT image. A decision is made once the total accumulated evidence surpasses a specific threshold. Preliminary experimental analyses conducted using a challenging in-house dataset comprising 114 subjects. The results are noteworthy, suggesting that NYCTALE outperforms the <b>benchmark</b> accuracy even with approximately 60% less training data on this demanding and small dataset.</p></p class="citation"></blockquote><h3 id=2029--162249-seed-optimization-with-frozen-generator-for-superior-zero-shot-low-light-enhancement-yuxuan-gu-et-al-2024>(20/29 | 162/249) Seed Optimization with Frozen Generator for Superior Zero-shot Low-light Enhancement (Yuxuan Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Gu, Yi Jin, Ben Wang, Zhixiang Wei, Xiaoxiao Ma, Pengyang Ling, Haoxuan Wang, Huaian Chen, Enhong Chen. (2024)<br><strong>Seed Optimization with Frozen Generator for Superior Zero-shot Low-light Enhancement</strong><br><button class=copy-to-clipboard title="Seed Optimization with Frozen Generator for Superior Zero-shot Low-light Enhancement" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09694v1.pdf filename=2402.09694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we observe that the generators, which are pre-trained on massive natural images, inherently hold the promising potential for superior low-light image enhancement against varying scenarios.Specifically, we embed a pre-trained generator to Retinex model to produce reflectance maps with enhanced detail and vividness, thereby recovering features degraded by low-light conditions.Taking one step further, we introduce a novel optimization strategy, which backpropagates the gradients to the input seeds rather than the parameters of the low-light enhancement model, thus intactly retaining the generative knowledge learned from natural images and achieving faster convergence speed. Benefiting from the pre-trained knowledge and seed-optimization strategy, the low-light enhancement model can significantly regularize the realness and fidelity of the enhanced result, thus rapidly generating high-quality images without training on any low-light dataset. Extensive experiments on various <b>benchmarks</b> demonstrate the superiority of the proposed method over numerous state-of-the-art methods qualitatively and quantitatively.</p></p class="citation"></blockquote><h3 id=2129--163249-hi-gan-hierarchical-inpainting-gan-with-auxiliary-inputs-for-combined-rgb-and-depth-inpainting-ankan-dash-et-al-2024>(21/29 | 163/249) HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting (Ankan Dash et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankan Dash, Jingyi Gu, Guiling Wang. (2024)<br><strong>HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting</strong><br><button class=copy-to-clipboard title="HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10334v1.pdf filename=2402.10334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inpainting involves filling in missing pixels or areas in an image, a crucial technique employed in Mixed Reality environments for various applications, particularly in Diminished Reality (DR) where content is removed from a user&rsquo;s visual environment. Existing methods rely on digital replacement techniques which necessitate multiple cameras and incur high costs. AR devices and smartphones use ToF depth sensors to capture scene depth maps aligned with RGB images. Despite speed and affordability, ToF cameras create imperfect depth maps with missing pixels. To address the above challenges, we propose Hierarchical Inpainting <b>GAN</b> (HI-GAN), a novel approach comprising three <b>GANs</b> in a hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked edge and segmentation label images respectively, while CombinedRGBD-GAN combines their latent representation outputs and performs RGB and Depth inpainting. Edge images and particularly segmentation label images as auxiliary inputs significantly enhance inpainting performance by complementary context and hierarchical optimization. We believe we make the first attempt to incorporate label images into inpainting process.Unlike previous approaches requiring multiple sequential models and separate outputs, our work operates in an end-to-end manner, training all three models simultaneously and hierarchically. Specifically, EdgeGAN and LabelGAN are first optimized separately and further optimized inside CombinedRGBD-GAN to enhance inpainting quality. Experiments demonstrate that HI-GAN works seamlessly and achieves overall superior performance compared with existing approaches.</p></p class="citation"></blockquote><h3 id=2229--164249-mim-refiner-a-contrastive-learning-boost-from-intermediate-pre-trained-representations-benedikt-alkin-et-al-2024>(22/29 | 164/249) MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations (Benedikt Alkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benedikt Alkin, Lukas Miklautz, Sepp Hochreiter, Johannes Brandstetter. (2024)<br><strong>MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations</strong><br><button class=copy-to-clipboard title="MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10093v1.pdf filename=2402.10093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce MIM (Masked Image Modeling)-Refiner, a <b>contrastive</b> <b>learning</b> boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple <b>contrastive</b> <b>heads</b> that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters. The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger models that were trained on up to 2000x more data such as DINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page: <a href=https://ml-jku.github.io/MIM-Refiner>https://ml-jku.github.io/MIM-Refiner</a></p></p class="citation"></blockquote><h3 id=2329--165249-investigation-of-federated-learning-algorithms-for-retinal-optical-coherence-tomography-image-classification-with-statistical-heterogeneity-sanskar-amgain-et-al-2024>(23/29 | 165/249) Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity (Sanskar Amgain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanskar Amgain, Prashant Shrestha, Sophia Bano, Ignacio del Valle Torres, Michael Cunniffe, Victor Hernandez, Phil Beales, Binod Bhattarai. (2024)<br><strong>Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity</strong><br><button class=copy-to-clipboard title="Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-DC, cs.CV<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10035v1.pdf filename=2402.10035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: We apply <b>federated</b> <b>learning</b> to train an OCT image classifier simulating a realistic scenario with multiple clients and statistical heterogeneous data distribution where data in the clients lack samples of some categories entirely. Methods: We investigate the effectiveness of FedAvg and FedProx to train an OCT image classification model in a decentralized fashion, addressing privacy concerns associated with centralizing data. We partitioned a publicly available OCT dataset across multiple clients under IID and Non-IID settings and conducted local training on the subsets for each client. We evaluated two <b>federated</b> <b>learning</b> methods, FedAvg and FedProx for these settings. Results: Our experiments on the dataset suggest that under IID settings, both methods perform on par with training on a central data pool. However, the performance of both algorithms declines as we increase the statistical heterogeneity across the client data, while FedProx consistently performs better than FedAvg in the increased heterogeneity settings. Conclusion: Despite the effectiveness of <b>federated</b> <b>learning</b> in the utilization of private data across multiple medical institutions, the large number of clients and heterogeneous distribution of labels deteriorate the performance of both algorithms. Notably, FedProx appears to be more robust to the increased heterogeneity.</p></p class="citation"></blockquote><h3 id=2429--166249-beyond-kalman-filters-deep-learning-based-filters-for-improved-object-tracking-momir-adžemović-et-al-2024>(24/29 | 166/249) Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking (Momir Adžemović et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Momir Adžemović, Predrag Tadić, Andrija Petrović, Mladen Nikolić. (2024)<br><strong>Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking</strong><br><button class=copy-to-clipboard title="Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09865v1.pdf filename=2402.09865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation. However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns. To address these limitations, we propose two innovative data-driven filtering methods. Our first method employs a Bayesian filter with a trainable motion model to predict an object&rsquo;s future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy. Moreover, it dispenses with most domain-specific design choices characteristic of the KF. The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise. Additionally, we introduce a range of motion model architectures based on <b>Recurrent</b> <b>Neural</b> <b>Networks,</b> Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods. Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns &ndash; the use case our filters are best suited to. We also conduct noise robustness analysis of our filters with convincing positive results. We further propose a new cost function for associating observations with tracks. Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets.</p></p class="citation"></blockquote><h3 id=2529--167249-a-comprehensive-review-on-computer-vision-analysis-of-aerial-data-vivek-tetarwal-et-al-2024>(25/29 | 167/249) A Comprehensive Review on Computer Vision Analysis of Aerial Data (Vivek Tetarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vivek Tetarwal, Sandeep Kumar. (2024)<br><strong>A Comprehensive Review on Computer Vision Analysis of Aerial Data</strong><br><button class=copy-to-clipboard title="A Comprehensive Review on Computer Vision Analysis of Aerial Data" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IT, cs.CV, math-IT<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09781v1.pdf filename=2402.09781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the emergence of new technologies in the field of airborne platforms and imaging sensors, aerial data analysis is becoming very popular, capitalizing on its advantages over land data. This paper presents a comprehensive review of the computer vision tasks within the domain of aerial data analysis. While addressing fundamental aspects such as <b>object</b> <b>detection</b> and tracking, the primary focus is on pivotal tasks like change detection, <b>object</b> <b>segmentation,</b> and scene-level analysis. The paper provides the comparison of various hyper parameters employed across diverse architectures and tasks. A substantial section is dedicated to an in-depth discussion on libraries, their categorization, and their relevance to different domain expertise. The paper encompasses aerial datasets, the architectural nuances adopted, and the evaluation metrics associated with all the tasks in aerial data analysis. Applications of computer vision tasks in aerial data across different domains are explored, with case studies providing further insights. The paper thoroughly examines the challenges inherent in aerial data analysis, offering practical solutions. Additionally, unresolved issues of significance are identified, paving the way for future research directions in the field of aerial data analysis.</p></p class="citation"></blockquote><h3 id=2629--168249-evaluating-nerfs-for-3d-plant-geometry-reconstruction-in-field-conditions-muhammad-arbab-arshad-et-al-2024>(26/29 | 168/249) Evaluating NeRFs for 3D Plant Geometry Reconstruction in Field Conditions (Muhammad Arbab Arshad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Arbab Arshad, Talukder Jubery, James Afful, Anushrut Jignasu, Aditya Balu, Baskar Ganapathysubramanian, Soumik Sarkar, Adarsh Krishnamurthy. (2024)<br><strong>Evaluating NeRFs for 3D Plant Geometry Reconstruction in Field Conditions</strong><br><button class=copy-to-clipboard title="Evaluating NeRFs for 3D Plant Geometry Reconstruction in Field Conditions" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10344v1.pdf filename=2402.10344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We evaluate different Neural Radiance Fields (NeRFs) techniques for reconstructing (3D) plants in varied environments, from indoor settings to outdoor fields. Traditional techniques often struggle to capture the complex details of plants, which is crucial for botanical and agricultural understanding. We evaluate three scenarios with increasing complexity and compare the results with the point cloud obtained using LiDAR as ground truth data. In the most realistic field scenario, the NeRF models achieve a 74.65% F1 score with 30 minutes of training on the GPU, highlighting the efficiency and accuracy of NeRFs in challenging environments. These findings not only demonstrate the potential of NeRF in detailed and realistic 3D plant modeling but also suggest practical approaches for enhancing the speed and efficiency of the 3D reconstruction process.</p></p class="citation"></blockquote><h3 id=2729--169249-lester-rotoscope-animation-through-video-object-segmentation-and-tracking-ruben-tous-2024>(27/29 | 169/249) Lester: rotoscope animation through video object segmentation and tracking (Ruben Tous, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruben Tous. (2024)<br><strong>Lester: rotoscope animation through video object segmentation and tracking</strong><br><button class=copy-to-clipboard title="Lester: rotoscope animation through video object segmentation and tracking" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-MM, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09883v1.pdf filename=2402.09883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos. The method approaches the challenge mainly as an object segmentation and tracking problem. Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation. The <b>geometry</b> of the masks&rsquo; contours is simplified with the Douglas-Peucker algorithm. Finally, facial traits, pixelation and a basic shadow effect can be optionally added. The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds. The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer from temporal consistency problems and do not cope well with pixelated and schematic outputs. The method is also much most practical than techniques based on 3D human pose estimation, which require custom handcrafted 3D models and are very limited with respect to the type of scenes they can process.</p></p class="citation"></blockquote><h3 id=2829--170249-ges-generalized-exponential-splatting-for-efficient-radiance-field-rendering-abdullah-hamdi-et-al-2024>(28/29 | 170/249) GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering (Abdullah Hamdi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi. (2024)<br><strong>GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering</strong><br><button class=copy-to-clipboard title="GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10128v1.pdf filename=2402.10128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advancements in 3D Gaussian Splatting have significantly accelerated 3D reconstruction and generation. However, it may require a large number of Gaussians, which creates a substantial memory footprint. This paper introduces GES (Generalized Exponential Splatting), a novel representation that employs Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer particles to represent a scene and thus significantly outperforming Gaussian Splatting methods in efficiency with a plug-and-play replacement ability for Gaussian-based utilities. GES is validated theoretically and empirically in both principled 1D setup and realistic 3D scenes. It is shown to represent signals with sharp edges more accurately, which are typically challenging for Gaussians due to their inherent low-pass characteristics. Our empirical analysis demonstrates that GEF outperforms Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and parabolic signals), thereby reducing the need for extensive splitting operations that increase the memory footprint of Gaussian Splatting. With the aid of a frequency-modulated loss, GES achieves competitive performance in novel-view synthesis <b>benchmarks</b> while requiring less than half the memory storage of Gaussian Splatting and increasing the rendering speed by up to 39%. The code is available on the project website <a href=https://abdullahamdi.com/ges>https://abdullahamdi.com/ges</a> .</p></p class="citation"></blockquote><h3 id=2929--171249-visually-dehallucinative-instruction-generation-know-what-you-dont-know-sungguk-cha-et-al-2024>(29/29 | 171/249) Visually Dehallucinative Instruction Generation: Know What You Don&rsquo;t Know (Sungguk Cha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungguk Cha, Jusung Lee, Younghyun Lee, Cheoljong Yang. (2024)<br><strong>Visually Dehallucinative Instruction Generation: Know What You Don&rsquo;t Know</strong><br><button class=copy-to-clipboard title="Visually Dehallucinative Instruction Generation: Know What You Don't Know" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09717v1.pdf filename=2402.09717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>&ldquo;When did the emperor Napoleon invented iPhone?&rdquo; Such hallucination-inducing question is well known challenge in generative language modeling. In this study, we present an innovative concept of visual hallucination, referred to as &ldquo;I Know (IK)&rdquo; hallucination, to address scenarios where &ldquo;I Don&rsquo;t Know&rdquo; is the desired response. To effectively tackle this issue, we propose the VQAv2-IDK <b>benchmark,</b> the subset of VQAv2 comprising unanswerable image-question pairs as determined by human annotators. Stepping further, we present the visually dehallucinative instruction generation method for IK hallucination and introduce the IDK-Instructions visual instruction database. Our experiments show that current methods struggle with IK hallucination. Yet, our approach effectively reduces these hallucinations, proving its versatility across different frameworks and datasets.</p></p class="citation"></blockquote><h2 id=cscr-5>cs.CR (5)</h2><h3 id=15--172249-abusegpt-abuse-of-generative-ai-chatbots-to-create-smishing-campaigns-ashfak-md-shibli-et-al-2024>(1/5 | 172/249) AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns (Ashfak Md Shibli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashfak Md Shibli, Mir Mehedi A. Pritom, Maanak Gupta. (2024)<br><strong>AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns</strong><br><button class=copy-to-clipboard title="AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 70<br>Keywords: Generative AI, Bard, ChatGPT, Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09728v1.pdf filename=2402.09728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>SMS phishing, also known as &ldquo;smishing&rdquo;, is a growing threat that tricks users into disclosing private information or clicking into URLs with malicious content through fraudulent mobile text messages. In recent past, we have also observed a rapid advancement of conversational <b>generative</b> <b>AI</b> <b>chatbot</b> services (e.g., OpenAI&rsquo;s <b>ChatGPT,</b> Google&rsquo;s <b>BARD),</b> which are powered by pre-trained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> These AI <b>chatbots</b> certainly have a lot of utilities but it is not systematically understood how they can play a role in creating threats and attacks. In this paper, we propose AbuseGPT method to show how the existing <b>generative</b> <b>AI-based</b> <b>chatbot</b> services can be exploited by attackers in real world to create smishing texts and eventually lead to craftier smishing campaigns. To the best of our knowledge, there is no pre-existing work that evidently shows the impacts of these <b>generative</b> <b>text-based</b> models on creating SMS phishing. Thus, we believe this study is the first of its kind to shed light on this emerging cybersecurity threat. We have found strong empirical evidences to show that attackers can exploit ethical standards in the existing <b>generative</b> <b>AI-based</b> <b>chatbot</b> services by crafting <b>prompt</b> injection attacks to create newer smishing campaigns. We also discuss some future research directions and guidelines to protect the abuse of <b>generative</b> <b>AI-based</b> services and safeguard users from smishing attacks.</p></p class="citation"></blockquote><h3 id=25--173249-preserving-data-privacy-for-ml-driven-applications-in-open-radio-access-networks-pranshav-gajjar-et-al-2024>(2/5 | 173/249) Preserving Data Privacy for ML-driven Applications in Open Radio Access Networks (Pranshav Gajjar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pranshav Gajjar, Azuka Chiejina, Vijay K. Shah. (2024)<br><strong>Preserving Data Privacy for ML-driven Applications in Open Radio Access Networks</strong><br><button class=copy-to-clipboard title="Preserving Data Privacy for ML-driven Applications in Open Radio Access Networks" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs-NI, cs.CR<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09710v1.pdf filename=2402.09710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning offers a promising solution to improve spectrum access techniques by utilizing data-driven approaches to manage and share limited spectrum resources for emerging applications. For several of these applications, the sensitive wireless data (such as spectrograms) are stored in a shared database or multistakeholder cloud environment and are therefore prone to privacy leaks. This paper aims to address such privacy concerns by examining the representative case study of shared database scenarios in 5G Open Radio Access Network (O-RAN) networks where we have a shared database within the near-real-time (near-RT) RAN intelligent controller. We focus on securing the data that can be used by machine learning (ML) models for spectrum sharing and interference mitigation applications without compromising the model and network performances. The underlying idea is to leverage a (i) Shuffling-based learnable encryption technique to encrypt the data, following which, (ii) employ a custom <b>Vision</b> <b>transformer</b> (ViT) as the trained ML model that is capable of performing accurate inferences on such encrypted data. The paper offers a thorough analysis and comparisons with analogous <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNN)</b> as well as deeper architectures (such as ResNet-50) as baselines. Our experiments showcase that the proposed approach significantly outperforms the baseline <b>CNN</b> with an improvement of 24.5% and 23.9% for the percent accuracy and F1-Score respectively when operated on encrypted data. Though deeper ResNet-50 architecture is obtained as a slightly more accurate model, with an increase of 4.4%, the proposed approach boasts a reduction of parameters by 99.32%, and thus, offers a much-improved prediction time by nearly 60%.</p></p class="citation"></blockquote><h3 id=35--174249-on-the-domain-generalizability-of-rf-fingerprints-through-multifractal-dimension-representation-benjamin-johnson-et-al-2024>(3/5 | 174/249) On the Domain Generalizability of RF Fingerprints Through Multifractal Dimension Representation (Benjamin Johnson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Johnson, Bechir Hamdaoui. (2024)<br><strong>On the Domain Generalizability of RF Fingerprints Through Multifractal Dimension Representation</strong><br><button class=copy-to-clipboard title="On the Domain Generalizability of RF Fingerprints Through Multifractal Dimension Representation" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10044v1.pdf filename=2402.10044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>RF data-driven device fingerprinting through the use of deep learning has recently surfaced as a possible method for enabling secure device identification and authentication. Traditional approaches are commonly susceptible to the <b>domain</b> <b>adaptation</b> problem where a model trained on data collected under one <b>domain</b> <b>performs</b> badly when tested on data collected under a different <b>domain.</b> <b>Some</b> examples of a <b>domain</b> <b>change</b> include varying the location or environment of the device and varying the time or day of the data collection. In this work, we propose using multifractal analysis and the variance fractal dimension trajectory (VFDT) as a data representation input to the deep neural network to extract device fingerprints that are <b>domain</b> <b>generalizable.</b> We analyze the effectiveness of the proposed VFDT representation in detecting device-specific signatures from hardware-impaired IQ (in-phase and quadrature) signals, and we evaluate its robustness in real-world settings, using an experimental testbed of 30 WiFi-enabled Pycom devices. Our experimental results show that the proposed VFDT representation improves the scalability, robustness and generalizability of the deep learning models significantly compared to when using IQ data samples.</p></p class="citation"></blockquote><h3 id=45--175249-an-advanced-data-fabric-architecture-leveraging-homomorphic-encryption-and-federated-learning-sakib-anwar-rieyan-et-al-2024>(4/5 | 175/249) An advanced data fabric architecture leveraging homomorphic encryption and federated learning (Sakib Anwar Rieyan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sakib Anwar Rieyan, Md. Raisul Kabir News, A. B. M. Muntasir Rahman, Sadia Afrin Khan, Sultan Tasneem Jawad Zaarif, Md. Golam Rabiul Alam, Mohammad Mehedi Hassan, Michele Ianni, Giancarlo Fortino. (2024)<br><strong>An advanced data fabric architecture leveraging homomorphic encryption and federated learning</strong><br><button class=copy-to-clipboard title="An advanced data fabric architecture leveraging homomorphic encryption and federated learning" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-DB, cs.CR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09795v1.pdf filename=2402.09795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data fabric is an automated and AI-driven data fusion approach to accomplish data management unification without moving data to a centralized location for solving complex data problems. In a <b>Federated</b> <b>learning</b> architecture, the global model is trained based on the learned parameters of several local models that eliminate the necessity of moving data to a centralized repository for machine learning. This paper introduces a secure approach for medical image analysis using <b>federated</b> <b>learning</b> and partially homomorphic encryption within a distributed data fabric architecture. With this method, multiple parties can collaborate in training a machine-learning model without exchanging raw data but using the learned or fused features. The approach complies with laws and regulations such as HIPAA and GDPR, ensuring the privacy and security of the data. The study demonstrates the method&rsquo;s effectiveness through a case study on pituitary tumor classification, achieving a significant level of accuracy. However, the primary focus of the study is on the development and evaluation of <b>federated</b> <b>learning</b> and partially homomorphic encryption as tools for secure medical image analysis. The results highlight the potential of these techniques to be applied to other privacy-sensitive domains and contribute to the growing body of research on secure and privacy-preserving machine learning.</p></p class="citation"></blockquote><h3 id=55--176249-hoacs-homomorphic-obfuscation-assisted-concealing-of-secrets-to-thwart-trojan-attacks-in-cots-processor-tanvir-hossain-et-al-2024>(5/5 | 176/249) HOACS: Homomorphic Obfuscation Assisted Concealing of Secrets to Thwart Trojan Attacks in COTS Processor (Tanvir Hossain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanvir Hossain, Matthew Showers, Mahmudul Hasan, Tamzidul Hoque. (2024)<br><strong>HOACS: Homomorphic Obfuscation Assisted Concealing of Secrets to Thwart Trojan Attacks in COTS Processor</strong><br><button class=copy-to-clipboard title="HOACS: Homomorphic Obfuscation Assisted Concealing of Secrets to Thwart Trojan Attacks in COTS Processor" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Zero Trust<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09701v1.pdf filename=2402.09701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Commercial-off-the-shelf (COTS) components are often preferred over custom Integrated Circuits (ICs) to achieve reduced system development time and cost, easy adoption of new technologies, and replaceability. Unfortunately, the integration of COTS components introduces serious security concerns. None of the entities in the COTS IC supply chain are trusted from a consumer&rsquo;s perspective, leading to a <b>&lsquo;&lsquo;zero</b> <b>trust&rsquo;&rsquo;</b> threat model. Any of these entities could introduce hidden malicious circuits or hardware Trojans within the component, allowing an attacker in the field to extract secret information (e.g., cryptographic keys) or cause a functional failure. Existing solutions to counter hardware Trojans are inapplicable in such a <b>zero-trust</b> <b>scenario</b> as they assume either the design house or the foundry to be trusted and consider the design to be available for either analysis or modification. In this work, we have proposed a software-oriented countermeasure to ensure the confidentiality of secret assets against hardware Trojans that can be seamlessly integrated in existing COTS microprocessors. The proposed solution does not require any supply chain entity to be trusted and does not require analysis or modification of the IC design. To protect secret assets in an untrusted microprocessor, the proposed method leverages the concept of residue number coding (RNC) to transform the software functions operating on the asset to be fully homomorphic. We have implemented the proposed solution to protect the secret key within the Advanced Encryption Standard (AES) program and presented a detailed security analysis. We also have developed a plugin for the LLVM compiler toolchain that automatically integrates the solution in AES. Finally, we compare the execution time overhead of the operations in the RNC-based technique with comparable homomorphic solutions and demonstrate significant improvement.</p></p class="citation"></blockquote><h2 id=cssd-4>cs.SD (4)</h2><h3 id=14--177249-deepsrgm----sequence-classification-and-ranking-in-indian-classical-music-with-deep-learning-sathwik-tejaswi-madhusudhan-et-al-2024>(1/4 | 177/249) DeepSRGM &ndash; Sequence Classification and Ranking in Indian Classical Music with Deep Learning (Sathwik Tejaswi Madhusudhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sathwik Tejaswi Madhusudhan, Girish Chowdhary. (2024)<br><strong>DeepSRGM &ndash; Sequence Classification and Ranking in Indian Classical Music with Deep Learning</strong><br><button class=copy-to-clipboard title="DeepSRGM -- Sequence Classification and Ranking in Indian Classical Music with Deep Learning" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-IR, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 60<br>Keywords: Recommendation, LSTM, LSTM, LSTM, Recurrent Neural Network, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10168v1.pdf filename=2402.10168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music <b>information</b> <b>retrieval</b> task in ICM as it can aid numerous downstream applications ranging from music <b>recommendations</b> to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient pre possessing and learns temporal sequences in music data using <b>Long</b> <b>Short</b> <b>Term</b> <b>Memory</b> based <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(LSTM-RNN).</b> We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence ranking which aids us in retrieving melodic patterns from a given music data base that are closely related to the presented query sequence.</p></p class="citation"></blockquote><h3 id=24--178249-muchin-a-chinese-colloquial-description-benchmark-for-evaluating-language-models-in-the-field-of-music-zihao-wang-et-al-2024>(2/4 | 178/249) MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music (Zihao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Wang, Shuyu Li, Tao Zhang, Qi Wang, Pengfei Yu, Jinyang Luo, Yan Liu, Ming Xi, Kejun Zhang. (2024)<br><strong>MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music</strong><br><button class=copy-to-clipboard title="MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: 68Txx(Primary)14F05, 91Fxx(Secondary), I-2-7; J-5, cs-AI, cs-MM, cs-SD, cs.SD, eess-AS<br>Keyword Score: 49<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09871v1.pdf filename=2402.09871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapidly evolving <b>multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> urgently require new <b>benchmarks</b> to uniformly evaluate their performance on understanding and textually describing music. However, due to semantic gaps between Music <b>Information</b> <b>Retrieval</b> (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as <b>benchmarks.</b> To this end, we present MuChin, the first open-source music description <b>benchmark</b> in Chinese colloquial language, designed to evaluate the performance of <b>multimodal</b> <b>LLMs</b> in understanding and describing music. We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics. Utilizing this method, we built a dataset with multi-dimensional, high-precision music annotations, the Caichong Music Dataset (CaiMD), and carefully selected 1,000 high-quality entries to serve as the test set for MuChin. Based on MuChin, we analyzed the discrepancies between professionals and amateurs in terms of music description, and empirically demonstrated the effectiveness of annotated data for <b>fine-tuning</b> <b>LLMs.</b> Ultimately, we employed MuChin to evaluate existing music understanding models on their ability to provide colloquial descriptions of music. All data related to the <b>benchmark</b> and the code for scoring have been open-sourced.</p></p class="citation"></blockquote><h3 id=34--179249-a-cross-talk-robust-multichannel-vad-model-for-multiparty-agent-interactions-trained-using-synthetic-re-recordings-hyewon-han-et-al-2024>(3/4 | 179/249) A cross-talk robust multichannel VAD model for multiparty agent interactions trained using synthetic re-recordings (Hyewon Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyewon Han, Naveen Kumar. (2024)<br><strong>A cross-talk robust multichannel VAD model for multiparty agent interactions trained using synthetic re-recordings</strong><br><button class=copy-to-clipboard title="A cross-talk robust multichannel VAD model for multiparty agent interactions trained using synthetic re-recordings" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-HC, cs-SD, cs.SD, eess-AS<br>Keyword Score: 40<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Natural Language Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09797v1.pdf filename=2402.09797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose a novel cross-talk rejection framework for a multi-channel multi-talker setup for a live multiparty interactive show. Our far-field audio setup is required to be hands-free during live interaction and comprises four adjacent talkers with directional microphones in the same space. Such setups often introduce heavy cross-talk between channels, resulting in reduced <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> and <b>natural</b> <b>language</b> <b>understanding</b> (NLU) performance. To address this problem, we propose voice activity detection (VAD) model for all talkers using multichannel information, which is then used to filter audio for downstream tasks. We adopt a synthetic training data generation approach through playback and re-recording for such scenarios, simulating challenging <b>speech</b> <b>overlap</b> conditions. We train our models on this synthetic data and demonstrate that our approach outperforms single-channel VAD models and energy-based multi-channel VAD algorithm in various acoustic environments. In addition to VAD results, we also present multiparty <b>ASR</b> evaluation results to highlight the impact of using our VAD model for filtering audio in downstream tasks by significantly reducing the insertion error.</p></p class="citation"></blockquote><h3 id=44--180249-zero-shot-unsupervised-and-text-based-audio-editing-using-ddpm-inversion-hila-manor-et-al-2024>(4/4 | 180/249) Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion (Hila Manor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hila Manor, Tomer Michaeli. (2024)<br><strong>Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion</strong><br><button class=copy-to-clipboard title="Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10009v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10009v2.pdf filename=2402.10009v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Editing signals using large pre-trained models, in a <b>zero-shot</b> manner, has recently seen rapid advancements in the image domain. However, this wave has yet to reach the audio domain. In this paper, we explore two <b>zero-shot</b> editing techniques for audio signals, which use DDPM inversion on pre-trained diffusion models. The first, adopted from the image domain, allows text-based editing. The second, is a novel approach for discovering semantically meaningful editing directions without supervision. When applied to music signals, this method exposes a range of musically interesting modifications, from controlling the participation of specific instruments to improvisations on the melody. Samples and code can be found on our examples page in <a href=https://hilamanor.github.io/AudioEditing/>https://hilamanor.github.io/AudioEditing/</a> .</p></p class="citation"></blockquote><h2 id=physicschem-ph-1>physics.chem-ph (1)</h2><h3 id=11--181249-chemreasoner-heuristic-search-over-a-large-language-models-knowledge-space-using-quantum-chemical-feedback-henry-w-sprueill-et-al-2024>(1/1 | 181/249) ChemReasoner: Heuristic Search over a Large Language Model&rsquo;s Knowledge Space using Quantum-Chemical Feedback (Henry W. Sprueill et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Henry W. Sprueill, Carl Edwards, Khushbu Agarwal, Mariefel V. Olarte, Udishnu Sanyal, Conrad Johnston, Hongbin Liu, Heng Ji, Sutanay Choudhury. (2024)<br><strong>ChemReasoner: Heuristic Search over a Large Language Model&rsquo;s Knowledge Space using Quantum-Chemical Feedback</strong><br><button class=copy-to-clipboard title="ChemReasoner: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.chem-ph<br>Categories: cs-AI, cs-CE, cs-LG, physics-chem-ph, physics.chem-ph<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10980v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10980v2.pdf filename=2402.10980v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic <b>reasoning</b> with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of <b>large</b> <b>language</b> <b>model</b> <b>(LLM)-derived</b> hypotheses and atomistic <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)-derived</b> feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the <b>LLM&rsquo;s</b> knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automatically guide the exploration without human input, providing competitive performance against expert-enumerated chemical descriptor-based implementations. By integrating language-guided <b>reasoning</b> with computational chemistry feedback, our work pioneers AI-accelerated, trustworthy catalyst discovery.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--182249-approximate-message-passing-enhanced-graph-neural-network-for-otfs-data-detection-wenhao-zhuang-et-al-2024>(1/1 | 182/249) Approximate Message Passing-Enhanced Graph Neural Network for OTFS Data Detection (Wenhao Zhuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhao Zhuang, Yuyi Mao, Hengtao He, Lei Xie, Shenghui Song, Yao Ge, Zhi Ding. (2024)<br><strong>Approximate Message Passing-Enhanced Graph Neural Network for OTFS Data Detection</strong><br><button class=copy-to-clipboard title="Approximate Message Passing-Enhanced Graph Neural Network for OTFS Data Detection" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 53<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10071v1.pdf filename=2402.10071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Orthogonal time frequency space (OTFS) modulation has emerged as a promising solution to support high-mobility wireless communications, for which, cost-effective data detectors are critical. Although <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)-based</b> data detectors can achieve decent detection accuracy at reasonable computation cost, they fail to best harness prior information of transmitted data. To further minimize the data detection error of OTFS systems, this letter develops an AMP-GNN-based detector, leveraging the approximate message passing (AMP) algorithm to iteratively improve the symbol estimates of a <b>GNN.</b> Given the inter-Doppler interference (IDI) symbols incur substantial computational overhead to the constructed <b>GNN,</b> learning-based IDI approximation is implemented to sustain low detection complexity. <b>Simulation</b> results demonstrate a remarkable bit error rate (BER) performance achieved by the proposed AMP-GNN-based detector compared to existing baselines. Meanwhile, the proposed IDI approximation scheme avoids a large amount of computations with negligible BER degradation.</p></p class="citation"></blockquote><h2 id=csir-3>cs.IR (3)</h2><h3 id=13--183249-sequential-recommendation-on-temporal-proximities-with-contrastive-learning-and-self-attention-hansol-jung-et-al-2024>(1/3 | 183/249) Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention (Hansol Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hansol Jung, Hyunwoo Seo, Chiehyeon Lim. (2024)<br><strong>Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention</strong><br><button class=copy-to-clipboard title="Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 53<br>Keywords: Benchmarking, Contrastive Learning, Recommendation, Recommender System, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09784v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09784v2.pdf filename=2402.09784v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential <b>recommender</b> <b>systems</b> identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern <b>transformer-based</b> models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored. Notably, recent models often neglect similarities in users&rsquo; actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity. These models primarily adapt the <b>self-attention</b> mechanisms of the <b>transformer</b> to consider the temporal context in individual user actions. Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a month. To address these gaps, we propose a sequential <b>recommendation</b> model called TemProxRec, which includes <b>contrastive</b> <b>learning</b> and <b>self-attention</b> methods to consider temporal proximities both across and within user-item interactions. The proposed <b>contrastive</b> <b>learning</b> method learns representations of items selected in close temporal periods across different users to be close. Simultaneously, the proposed <b>self-attention</b> mechanism encodes temporal and positional contexts in a user sequence using both absolute and relative embeddings. This way, our TemProxRec accurately predicts the relevant items based on the user-item interactions within a specific timeframe. We validate this work through comprehensive experiments on TemProxRec, consistently outperforming existing models on <b>benchmark</b> datasets as well as showing the significance of considering the vertical and horizontal temporal proximities into sequential <b>recommendation.</b></p></p class="citation"></blockquote><h3 id=23--184249-llm-based-federated-recommendation-jujia-zhao-et-al-2024>(2/3 | 184/249) LLM-based Federated Recommendation (Jujia Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jujia Zhao, Wenjie Wang, Chen Xu, Zhaochun Ren, See-Kiong Ng, Tat-Seng Chua. (2024)<br><strong>LLM-based Federated Recommendation</strong><br><button class=copy-to-clipboard title="LLM-based Federated Recommendation" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 50<br>Keywords: Federated Learning, Fine-tuning, Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09959v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09959v2.pdf filename=2402.09959v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> with their advanced contextual understanding abilities, have demonstrated considerable potential in enhancing <b>recommendation</b> systems via <b>fine-tuning</b> methods. However, <b>fine-tuning</b> requires users&rsquo; behavior data, which poses considerable privacy risks due to the incorporation of sensitive user information. The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues. To mitigate these privacy issues, <b>Federated</b> <b>Learning</b> for <b>Recommendation</b> (Fed4Rec) has emerged as a promising approach. Nevertheless, applying Fed4Rec to <b>LLM-based</b> <b>recommendation</b> presents two main challenges: first, an increase in the imbalance of performance across clients, affecting the system&rsquo;s efficiency over time, and second, a high demand on clients&rsquo; computational and storage resources for local training and inference of <b>LLMs.</b> To address these challenges, we introduce a Privacy-Preserving <b>LLM-based</b> <b>Recommendation</b> (PPLR) framework. The PPLR framework employs two primary strategies. First, it implements a dynamic balance strategy, which involves the design of dynamic parameter aggregation and adjustment of learning speed for different clients during the training phase, to ensure relatively balanced performance across all clients. Second, PPLR adopts a flexible storage strategy, selectively retaining certain sensitive layers of the language model on the client side while offloading non-sensitive layers to the server. This approach aims to preserve user privacy while efficiently saving computational and storage resources. Experimental results demonstrate that PPLR not only achieves a balanced performance among clients but also enhances overall system performance in a manner that is both computationally and storage-efficient, while effectively protecting user privacy.</p></p class="citation"></blockquote><h3 id=33--185249-from-variability-to-stability-advancing-recsys-benchmarking-practices-valeriy-shevchenko-et-al-2024>(3/3 | 185/249) From Variability to Stability: Advancing RecSys Benchmarking Practices (Valeriy Shevchenko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valeriy Shevchenko, Nikita Belousov, Alexey Vasilev, Vladimir Zholobov, Artyom Sosedka, Natalia Semenova, Anna Volodkevich, Andrey Savchenko, Alexey Zaytsev. (2024)<br><strong>From Variability to Stability: Advancing RecSys Benchmarking Practices</strong><br><button class=copy-to-clipboard title="From Variability to Stability: Advancing RecSys Benchmarking Practices" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs-LG, cs.IR<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09766v1.pdf filename=2402.09766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving domain of <b>Recommender</b> <b>Systems</b> (RecSys), new algorithms frequently claim state-of-the-art performance based on evaluations over a limited set of arbitrarily selected datasets. However, this approach may fail to holistically reflect their effectiveness due to the significant impact of dataset characteristics on algorithm performance. Addressing this deficiency, this paper introduces a novel <b>benchmarking</b> methodology to facilitate a fair and robust comparison of RecSys algorithms, thereby advancing evaluation practices. By utilizing a diverse set of $30$ open datasets, including two introduced in this work, and evaluating $11$ collaborative filtering algorithms across $9$ metrics, we critically examine the influence of dataset characteristics on algorithm performance. We further investigate the feasibility of aggregating outcomes from multiple datasets into a unified ranking. Through rigorous experimental analysis, we validate the reliability of our methodology under the variability of datasets, offering a <b>benchmarking</b> strategy that balances quality and computational demands. This methodology enables a fair yet effective means of evaluating RecSys algorithms, providing valuable guidance for future research endeavors.</p></p class="citation"></blockquote><h2 id=csro-10>cs.RO (10)</h2><h3 id=110--186249-on-the-safety-concerns-of-deploying-llmsvlms-in-robotics-highlighting-the-risks-and-vulnerabilities-xiyang-wu-et-al-2024>(1/10 | 186/249) On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities (Xiyang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiyang Wu, Ruiqi Xian, Tianrui Guan, Jing Liang, Souradip Chakraborty, Fuxiao Liu, Brian Sadler, Dinesh Manocha, Amrit Singh Bedi. (2024)<br><strong>On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities</strong><br><button class=copy-to-clipboard title="On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Large Language Model, Large Language Model, Prompt, Vision-and-Language, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10340v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10340v2.pdf filename=2402.10340v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we highlight the critical issues of robustness and safety associated with integrating <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and <b>vision-language</b> models (VLMs) into robotics applications. Recent works have focused on using <b>LLMs</b> and VLMs to improve the performance of robotics tasks, such as manipulation, navigation, etc. However, such integration can introduce significant vulnerabilities, in terms of their susceptibility to <b>adversarial</b> <b>attacks</b> due to the language models, potentially leading to catastrophic consequences. By examining recent works at the interface of LLMs/VLMs and robotics, we show that it is easy to manipulate or misguide the robot&rsquo;s actions, leading to safety hazards. We define and provide examples of several plausible <b>adversarial</b> <b>attacks,</b> and conduct experiments on three prominent robot frameworks integrated with a language model, including KnowNo VIMA, and Instruct2Act, to assess their susceptibility to these attacks. Our empirical findings reveal a striking vulnerability of LLM/VLM-robot integrated systems: simple <b>adversarial</b> <b>attacks</b> can significantly undermine the effectiveness of LLM/VLM-robot integrated systems. Specifically, our data demonstrate an average performance deterioration of 21.2% under <b>prompt</b> attacks and a more alarming 30.2% under perception attacks. These results underscore the critical need for robust countermeasures to ensure the safe and reliable deployment of the advanced LLM/VLM-based robotic systems.</p></p class="citation"></blockquote><h3 id=210--187249-a-computationally-efficient-learning-based-model-predictive-control-for-multirotors-under-aerodynamic-disturbances-babak-akbari-et-al-2024>(2/10 | 187/249) A Computationally Efficient Learning-Based Model Predictive Control for Multirotors under Aerodynamic Disturbances (Babak Akbari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Babak Akbari, Melissa Greeff. (2024)<br><strong>A Computationally Efficient Learning-Based Model Predictive Control for Multirotors under Aerodynamic Disturbances</strong><br><button class=copy-to-clipboard title="A Computationally Efficient Learning-Based Model Predictive Control for Multirotors under Aerodynamic Disturbances" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10323v1.pdf filename=2402.10323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neglecting complex aerodynamic effects hinders high-speed yet high-precision multirotor autonomy. In this paper, we present a computationally efficient learning-based model predictive controller that simultaneously optimizes a trajectory that can be tracked within the physical limits (on thrust and orientation) of the multirotor system despite unknown aerodynamic forces and adapts the control input. To do this, we leverage the well-known differential flatness property of multirotors, which allows us to transform their nonlinear dynamics into a linear model. The main limitation of current flatness-based planning and control approaches is that they often neglect dynamic feasibility. This is because these constraints are nonlinear as a result of the mapping between the input, i.e., multirotor thrust, and the flat state. In our approach, we learn a novel representation of the drag forces by learning the mapping from the flat state to the multirotor thrust vector (in a world frame) as a <b>Gaussian</b> <b>Process</b> (GP). Our proposed approach leverages the properties of GPs to develop a convex optimal controller that can be iteratively solved as a second-order cone program (SOCP). In <b>simulation</b> experiments, our proposed approach outperforms related model predictive controllers that do not account for aerodynamic effects on trajectory feasibility, leading to a reduction of up to 55% in absolute tracking error.</p></p class="citation"></blockquote><h3 id=310--188249-trajectory-guidance-enhanced-remote-driving-of-highly-automated-vehicles-domagoj-majstorovic-et-al-2024>(3/10 | 188/249) Trajectory Guidance: Enhanced Remote Driving of highly-automated Vehicles (Domagoj Majstorovic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Domagoj Majstorovic, Simon Hoffmann, Frank Diermeyer. (2024)<br><strong>Trajectory Guidance: Enhanced Remote Driving of highly-automated Vehicles</strong><br><button class=copy-to-clipboard title="Trajectory Guidance: Enhanced Remote Driving of highly-automated Vehicles" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Human Intervention, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10014v1.pdf filename=2402.10014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the rapid technological progress, autonomous vehicles still face a wide range of complex driving situations that require <b>human</b> <b>intervention.</b> Teleoperation technology offers a versatile and effective way to address these challenges. The following work puts existing ideas into a modern context and introduces a novel technical implementation of the trajectory guidance teleoperation concept. The presented system was developed within a high-fidelity <b>simulation</b> environment and experimentally validated, demonstrating a realistic ride-hailing mission with prototype autonomous vehicles and onboard passengers. The results indicate that the proposed concept can be a viable alternative to the existing remote driving options, offering a promising way to enhance teleoperation technology and improve overall operation safety.</p></p class="citation"></blockquote><h3 id=410--189249-self-supervised-learning-of-visual-robot-localization-using-led-state-prediction-as-a-pretext-task-mirko-nava-et-al-2024>(4/10 | 189/249) Self-Supervised Learning of Visual Robot Localization Using LED State Prediction as a Pretext Task (Mirko Nava et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mirko Nava, Nicholas Carlotti, Luca Crupi, Daniele Palossi, Alessandro Giusti. (2024)<br><strong>Self-Supervised Learning of Visual Robot Localization Using LED State Prediction as a Pretext Task</strong><br><button class=copy-to-clipboard title="Self-Supervised Learning of Visual Robot Localization Using LED State Prediction as a Pretext Task" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09886v1.pdf filename=2402.09886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel <b>self-supervised</b> <b>approach</b> for learning to visually localize robots equipped with controllable LEDs. We rely on a few training samples labeled with position ground truth and many training samples in which only the LED state is known, whose collection is cheap. We show that using LED state prediction as a pretext task significantly helps to learn the visual localization end task. The resulting model does not require knowledge of LED states during inference. We instantiate the approach to visual relative localization of nano-quadrotors: experimental results show that using our pretext task significantly improves localization accuracy (from 68.3% to 76.2%) and outperforms alternative strategies, such as a <b>supervised</b> baseline, model pre-training, and an autoencoding pretext task. We deploy our model aboard a 27-g Crazyflie nano-drone, running at 21 fps, in a position-tracking task of a peer nano-drone. Our approach, relying on position labels for only 300 images, yields a mean tracking error of 4.2 cm versus 11.9 cm of a <b>supervised</b> baseline model trained without our pretext task. Videos and code of the proposed approach are available at <a href=https://github.com/idsia-robotics/leds-as-pretext>https://github.com/idsia-robotics/leds-as-pretext</a></p></p class="citation"></blockquote><h3 id=510--190249-sequential-manipulation-of-deformable-linear-object-networks-with-endpoint-pose-measurements-using-adaptive-model-predictive-control-tyler-toner-et-al-2024>(5/10 | 190/249) Sequential Manipulation of Deformable Linear Object Networks with Endpoint Pose Measurements using Adaptive Model Predictive Control (Tyler Toner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tyler Toner, Vahidreza Molazadeh, Miguel Saez, Dawn M. Tilbury, Kira Barton. (2024)<br><strong>Sequential Manipulation of Deformable Linear Object Networks with Endpoint Pose Measurements using Adaptive Model Predictive Control</strong><br><button class=copy-to-clipboard title="Sequential Manipulation of Deformable Linear Object Networks with Endpoint Pose Measurements using Adaptive Model Predictive Control" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10372v1.pdf filename=2402.10372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic manipulation of deformable linear objects (DLOs) is an active area of research, though emerging applications, like automotive wire harness installation, introduce constraints that have not been considered in prior work. Confined workspaces and limited visibility complicate prior assumptions of multi-robot manipulation and direct measurement of DLO configuration (state). This work focuses on single-arm manipulation of stiff DLOs (StDLOs) connected to form a DLO network (DLON), for which the measurements (output) are the endpoint poses of the DLON, which are subject to unknown dynamics during manipulation. To demonstrate feasibility of output-based control without state estimation, direct input-output dynamics are shown to exist by training neural network models on simulated trajectories. Output dynamics are then approximated with polynomials and found to contain well-known rigid body dynamics terms. A composite model consisting of a rigid body model and an online data-driven residual is developed, which predicts output dynamics more accurately than either model alone, and without prior experience with the system. An adaptive model predictive controller is developed with the composite model for DLON manipulation, which completes DLON installation tasks, both in <b>simulation</b> and with a physical automotive wire harness.</p></p class="citation"></blockquote><h3 id=610--191249-lasersam-zero-shot-change-detection-using-visual-segmentation-of-spinning-lidar-alexander-krawciw-et-al-2024>(6/10 | 191/249) LaserSAM: Zero-Shot Change Detection Using Visual Segmentation of Spinning LiDAR (Alexander Krawciw et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Krawciw, Sven Lilge, Timothy D. Barfoot. (2024)<br><strong>LaserSAM: Zero-Shot Change Detection Using Visual Segmentation of Spinning LiDAR</strong><br><button class=copy-to-clipboard title="LaserSAM: Zero-Shot Change Detection Using Visual Segmentation of Spinning LiDAR" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Foundation Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10321v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10321v1.pdf filename=2402.10321v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an approach for applying camera perception techniques to spinning LiDAR data. To improve the robustness of long-term change detection from a 3D LiDAR, range and intensity information are rendered into virtual perspectives using a pinhole camera model. Hue-saturation-value image encoding is used to colourize the images by range and near-IR intensity. The LiDAR&rsquo;s active scene illumination makes it invariant to ambient brightness, which enables night-to-day change detection without additional processing. Using the colourized, perspective range image allows existing <b>foundation</b> <b>models</b> to detect semantic regions. Specifically, the Segment Anything Model detects semantically similar regions in both a previously acquired map and live view from a path-repeating robot. By comparing the masks in both views, changes in the live scan are detected. Results indicate that the Segment Anything Model is capable of accurately capturing the shape of arbitrary changes introduced into scenes. The system achieves an object recall of 82.6% and a precision of 47.0%. Changes can be detected through day-to-night illumination variations reliably. After pixel-level masks are generated, the one-to-one correspondence with 3D points means that the 2D masks can be directly used to recover the 3D location of the changes. Eventually, the detected 3D changes can be avoided by treating them as obstacles in a local motion planner.</p></p class="citation"></blockquote><h3 id=710--192249-robotic-exploration-using-generalized-behavioral-entropy-aamodh-suresh-et-al-2024>(7/10 | 192/249) Robotic Exploration using Generalized Behavioral Entropy (Aamodh Suresh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aamodh Suresh, Carlos Nieto-Granda, Sonia Martinez. (2024)<br><strong>Robotic Exploration using Generalized Behavioral Entropy</strong><br><button class=copy-to-clipboard title="Robotic Exploration using Generalized Behavioral Entropy" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-IT, cs-RO, cs.RO, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10161v1.pdf filename=2402.10161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents and evaluates a novel strategy for robotic exploration that leverages human models of uncertainty perception. To do this, we introduce a measure of uncertainty that we term ``Behavioral entropy&rsquo;&rsquo;, which builds on Prelec&rsquo;s probability weighting from Behavioral Economics. We show that the new operator is an admissible generalized entropy, analyze its theoretical properties and compare it with other common formulations such as Shannon&rsquo;s and Renyi&rsquo;s. In particular, we discuss how the new formulation is more expressive in the sense of measures of sensitivity and perceptiveness to uncertainty introduced here. Then we use Behavioral entropy to define a new type of utility function that can guide a frontier-based environment exploration process. The approach&rsquo;s benefits are illustrated and compared in a Proof-of-Concept and ROS-unity <b>simulation</b> environment with a Clearpath Warthog robot. We show that the robot equipped with Behavioral entropy explores faster than Shannon and Renyi entropies.</p></p class="citation"></blockquote><h3 id=810--193249-universal-manipulation-interface-in-the-wild-robot-teaching-without-in-the-wild-robots-cheng-chi-et-al-2024>(8/10 | 193/249) Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots (Cheng Chi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, Shuran Song. (2024)<br><strong>Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots</strong><br><button class=copy-to-clipboard title="Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10329v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10329v2.pdf filename=2402.10329v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Universal Manipulation Interface (UMI) &ndash; a data collection and policy learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot manipulation capabilities, allowing <b>zero-shot</b> generalizable dynamic, bimanual, precise, and long-horizon behaviors, by only changing the training data for each task. We demonstrate UMI&rsquo;s versatility and efficacy with comprehensive real-world experiments, where policies learned via UMI <b>zero-shot</b> generalize to novel environments and objects when trained on diverse human demonstrations. UMI&rsquo;s hardware and software system is open-sourced at <a href=https://umi-gripper.github.io>https://umi-gripper.github.io</a>.</p></p class="citation"></blockquote><h3 id=910--194249-reg-nf-efficient-registration-of-implicit-surfaces-within-neural-fields-stephen-hausler-et-al-2024>(9/10 | 194/249) Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields (Stephen Hausler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephen Hausler, David Hall, Sutharsan Mahendren, Peyman Moghadam. (2024)<br><strong>Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields</strong><br><button class=copy-to-clipboard title="Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09722v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09722v1.pdf filename=2402.09722v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural fields, coordinate-based neural networks, have recently gained popularity for implicitly representing a scene. In contrast to classical methods that are based on explicit representations such as point clouds, neural fields provide a continuous scene representation able to represent 3D <b>geometry</b> and appearance in a way which is compact and ideal for robotics applications. However, limited prior methods have investigated registering multiple neural fields by directly utilising these continuous implicit representations. In this paper, we present Reg-NF, a neural fields-based registration that optimises for the relative 6-DoF transformation between two arbitrary neural fields, even if those two fields have different scale factors. Key components of Reg-NF include a bidirectional registration loss, multi-view surface sampling, and utilisation of volumetric signed distance functions (SDFs). We showcase our approach on a new neural field dataset for evaluating registration problems. We provide an exhaustive set of experiments and ablation studies to identify the performance of our approach, while also discussing limitations to provide future direction to the research community on open challenges in utilizing neural fields in unconstrained environments.</p></p class="citation"></blockquote><h3 id=1010--195249-towards-tight-convex-relaxations-for-contact-rich-manipulation-bernhard-p-graesdal-et-al-2024>(10/10 | 195/249) Towards Tight Convex Relaxations for Contact-Rich Manipulation (Bernhard P. Graesdal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bernhard P. Graesdal, Shao Y. C. Chia, Tobia Marcucci, Savva Morozov, Alexandre Amice, Pablo A. Parrilo, Russ Tedrake. (2024)<br><strong>Towards Tight Convex Relaxations for Contact-Rich Manipulation</strong><br><button class=copy-to-clipboard title="Towards Tight Convex Relaxations for Contact-Rich Manipulation" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10312v1.pdf filename=2402.10312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a method for global motion planning of robotic systems that interact with the environment through contacts. Our method directly handles the hybrid nature of such tasks using tools from convex optimization. We formulate the motion-planning problem as a shortest-path problem in a <b>graph</b> of convex sets, where a path in the <b>graph</b> corresponds to a contact sequence and a convex set models the quasi-static dynamics within a fixed contact mode. For each contact mode, we use semidefinite programming to relax the nonconvex dynamics that results from the simultaneous optimization of the object&rsquo;s pose, contact locations, and contact forces. The result is a tight convex relaxation of the overall planning problem, that can be efficiently solved and quickly rounded to find a feasible contact-rich trajectory. As a first application of this technique, we focus on the task of planar pushing. Exhaustive experiments show that our convex-optimization method generates plans that are consistently within a small percentage of the global optimum. We demonstrate the quality of these plans on a real robotic system.</p></p class="citation"></blockquote><h2 id=csne-3>cs.NE (3)</h2><h3 id=13--196249-hidden-traveling-waves-bind-working-memory-variables-in-recurrent-neural-networks-arjun-karuvally-et-al-2024>(1/3 | 196/249) Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks (Arjun Karuvally et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arjun Karuvally, Terrence J. Sejnowski, Hava T. Siegelmann. (2024)<br><strong>Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks</strong><br><button class=copy-to-clipboard title="Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 40<br>Keywords: Recurrent Neural Network, Recurrent Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10163v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10163v2.pdf filename=2402.10163v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traveling waves are a fundamental phenomenon in the brain, playing a crucial role in short-term information storage. In this study, we leverage the concept of traveling wave dynamics within a neural lattice to formulate a theoretical model of neural working memory, study its properties, and its real world implications in AI. The proposed model diverges from traditional approaches, which assume information storage in static, register-like locations updated by interference. Instead, the model stores data as waves that is updated by the wave&rsquo;s boundary conditions. We rigorously examine the model&rsquo;s capabilities in representing and learning state histories, which are vital for learning history-dependent dynamical systems. The findings reveal that the model reliably stores external information and enhances the learning process by addressing the diminishing gradient problem. To understand the model&rsquo;s real-world applicability, we explore two cases: linear boundary condition and non-linear, <b>self-attention-driven</b> boundary condition. The experiments reveal that the linear scenario is effectively learned by <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs)</b> through backpropagation when modeling history-dependent dynamical systems. Conversely, the non-linear scenario parallels the autoregressive loop of an attention-only <b>transformer.</b> Collectively, our findings suggest the broader relevance of traveling waves in AI and its potential in advancing neural network architectures.</p></p class="citation"></blockquote><h3 id=23--197249-system-level-impact-of-non-ideal-program-time-of-charge-trap-flash-ctf-on-deep-neural-network-s-shrivastava-et-al-2024>(2/3 | 197/249) System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on Deep Neural Network (S. Shrivastava et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. Shrivastava, A. Biswas, S. Chakrabarty, G. Dash, V. Saraswat, U. Ganguly. (2024)<br><strong>System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on Deep Neural Network</strong><br><button class=copy-to-clipboard title="System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on Deep Neural Network" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-ET, cs-NE, cs.NE, eess-IV<br>Keyword Score: 30<br>Keywords: MNIST, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09792v1.pdf filename=2402.09792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning of deep neural networks (DNN) using Resistive Processing Unit (RPU) architecture is energy-efficient as it utilizes dedicated neuromorphic hardware and stochastic computation of weight updates for in-memory computing. Charge Trap Flash (CTF) devices can implement RPU-based weight updates in DNNs. However, prior work has shown that the weight updates (V_T) in CTF-based RPU are impacted by the non-ideal program time of CTF. The non-ideal program time is affected by two factors of CTF. Firstly, the effects of the number of input pulses (N) or pulse width (pw), and secondly, the gap between successive update pulses (t_gap) used for the stochastic computation of weight updates. Therefore, the impact of this non-ideal program time must be studied for neural network training <b>simulations.</b> In this study, Firstly, we propose a pulse-train design compensation technique to reduce the total error caused by non-ideal program time of CTF and stochastic variance of a network. Secondly, we simulate RPU-based DNN with non-ideal program time of CTF on <b>MNIST</b> and Fashion-MNIST datasets. We find that for larger N (~1000), learning performance approaches the ideal (software-level) training level and, therefore, is not much impacted by the choice of t_gap used to implement RPU-based weight updates. However, for lower N (&lt;500), learning performance depends on T_gap of the pulses. Finally, we also performed an ablation study to isolate the causal factor of the improved learning performance. We conclude that the lower noise level in the weight updates is the most likely significant factor to improve the learning performance of DNN. Thus, our study attempts to compensate for the error caused by non-ideal program time and standardize the pulse length (N) and pulse gap (t_gap) specifications for CTF-based RPUs for accurate system-level on-chip training.</p></p class="citation"></blockquote><h3 id=33--198249-large-scale-benchmarking-of-metaphor-based-optimization-heuristics-diederick-vermetten-et-al-2024>(3/3 | 198/249) Large-scale Benchmarking of Metaphor-based Optimization Heuristics (Diederick Vermetten et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diederick Vermetten, Carola Doerr, Hao Wang, Anna V. Kononova, Thomas Bäck. (2024)<br><strong>Large-scale Benchmarking of Metaphor-based Optimization Heuristics</strong><br><button class=copy-to-clipboard title="Large-scale Benchmarking of Metaphor-based Optimization Heuristics" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09800v1.pdf filename=2402.09800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The number of proposed iterative optimization heuristics is growing steadily, and with this growth, there have been many points of discussion within the wider community. One particular criticism that is raised towards many new algorithms is their focus on metaphors used to present the method, rather than emphasizing their potential algorithmic contributions. Several studies into popular metaphor-based algorithms have highlighted these problems, even showcasing algorithms that are functionally equivalent to older existing methods. Unfortunately, this detailed approach is not scalable to the whole set of metaphor-based algorithms. Because of this, we investigate ways in which <b>benchmarking</b> can shed light on these algorithms. To this end, we run a set of 294 algorithm implementations on the BBOB function suite. We investigate how the choice of the budget, the performance measure, or other aspects of experimental design impact the comparison of these algorithms. Our results emphasize why <b>benchmarking</b> is a key step in expanding our understanding of the algorithm space, and what challenges still need to be overcome to fully gauge the potential improvements to the state-of-the-art hiding behind the metaphors.</p></p class="citation"></blockquote><h2 id=physicsapp-ph-1>physics.app-ph (1)</h2><h3 id=11--199249-deep-learning-for-the-design-of-non-hermitian-topolectrical-circuits-xi-chen-et-al-2024>(1/1 | 199/249) Deep learning for the design of non-Hermitian topolectrical circuits (Xi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xi Chen, Jinyang Sun, Xiumei Wang, Hengxuan Jiang, Dandan Zhu, Xingping Zhou. (2024)<br><strong>Deep learning for the design of non-Hermitian topolectrical circuits</strong><br><button class=copy-to-clipboard title="Deep learning for the design of non-Hermitian topolectrical circuits" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.app-ph<br>Categories: cs-LG, physics-app-ph, physics.app-ph<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09978v1.pdf filename=2402.09978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Non-Hermitian topological phases can produce some remarkable properties, compared with their Hermitian counterpart, such as the breakdown of conventional bulk-boundary correspondence and the non-Hermitian topological edge mode. Here, we introduce several algorithms with multi-layer perceptron (MLP), and <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> in the field of deep learning, to predict the winding of eigenvalues non-Hermitian Hamiltonians. Subsequently, we use the smallest module of the periodic circuit as one unit to construct high-dimensional circuit data features. Further, we use the Dense <b>Convolutional</b> <b>Network</b> <b>(DenseNet),</b> a type of <b>convolutional</b> <b>neural</b> <b>network</b> that utilizes dense connections between layers to design a non-Hermitian topolectrical Chern circuit, as the DenseNet algorithm is more suitable for processing high-dimensional data. Our results demonstrate the effectiveness of the deep learning network in capturing the global topological characteristics of a non-Hermitian system based on training data.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--200249-mixture-of-experts-for-network-optimization-a-large-language-model-enabled-approach-hongyang-du-et-al-2024>(1/1 | 200/249) Mixture of Experts for Network Optimization: A Large Language Model-enabled Approach (Hongyang Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyang Du, Guangyuan Liu, Yijing Lin, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim. (2024)<br><strong>Mixture of Experts for Network Optimization: A Large Language Model-enabled Approach</strong><br><button class=copy-to-clipboard title="Mixture of Experts for Network Optimization: A Large Language Model-enabled Approach" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09756v1.pdf filename=2402.09756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimizing various wireless user tasks poses a significant challenge for networking systems because of the expanding range of user requirements. Despite advancements in Deep <b>Reinforcement</b> <b>Learning</b> (DRL), the need for customized optimization tasks for individual users complicates developing and applying numerous DRL models, leading to substantial computation resource and energy consumption and can lead to inconsistent outcomes. To address this issue, we propose a novel approach utilizing a Mixture of Experts (MoE) framework, augmented with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> to analyze user objectives and constraints effectively, select specialized DRL experts, and weigh each decision from the participating experts. Specifically, we develop a gate network to oversee the expert models, allowing a collective of experts to tackle a wide array of new tasks. Furthermore, we innovatively substitute the traditional gate network with an <b>LLM,</b> leveraging its advanced <b>reasoning</b> capabilities to manage expert model selection for joint decisions. Our proposed method reduces the need to train new DRL models for each unique optimization problem, decreasing energy consumption and AI model implementation costs. The <b>LLM-enabled</b> MoE approach is validated through a general maze navigation task and a specific network service provider utility maximization task, demonstrating its effectiveness and practical applicability in optimizing complex networking systems.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--201249-protchatgpt-towards-understanding-proteins-with-large-language-models-chao-wang-et-al-2024>(1/1 | 201/249) ProtChatGPT: Towards Understanding Proteins with Large Language Models (Chao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Wang, Hehe Fan, Ruijie Quan, Yi Yang. (2024)<br><strong>ProtChatGPT: Towards Understanding Proteins with Large Language Models</strong><br><button class=copy-to-clipboard title="ProtChatGPT: Towards Understanding Proteins with Large Language Models" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-AI, cs-CE, cs.CE, q-bio-BM<br>Keyword Score: 40<br>Keywords: ChatGPT, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09649v1.pdf filename=2402.09649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging. Recent <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have made significant strides in comprehending task-specific knowledge, suggesting the potential for <b>ChatGPT-like</b> systems specialized in protein to facilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages. ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers. The system comprises protein encoders, a Protein-Language Pertaining <b>Transformer</b> (PLP-former), a projection adapter, and an <b>LLM.</b> The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the <b>LLM.</b> The <b>LLM</b> finally combines user questions with projected embeddings to generate informative answers. Experiments show that ProtChatGPT can produce promising responses to proteins and their corresponding questions. We hope that ProtChatGPT could form the basis for further exploration and application in protein research. Code and our pre-trained model will be publicly available.</p></p class="citation"></blockquote><h2 id=csse-2>cs.SE (2)</h2><h3 id=12--202249-codemind-a-framework-to-challenge-large-language-models-for-code-reasoning-changshu-liu-et-al-2024>(1/2 | 202/249) CodeMind: A Framework to Challenge Large Language Models for Code Reasoning (Changshu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changshu Liu, Shizhuo Dylan Zhang, Reyhaneh Jabbarvand. (2024)<br><strong>CodeMind: A Framework to Challenge Large Language Models for Code Reasoning</strong><br><button class=copy-to-clipboard title="CodeMind: A Framework to Challenge Large Language Models for Code Reasoning" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-PL, cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09664v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09664v2.pdf filename=2402.09664v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Solely relying on test passing to evaluate <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code <b>reasoning</b> abilities of <b>LLMs.</b> CodeMind currently supports three code <b>reasoning</b> tasks: Independent Execution <b>Reasoning</b> (IER), Dependent Execution <b>Reasoning</b> (DER), and Specification <b>Reasoning</b> (SR). The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which <b>LLMs</b> implement the specified expected behavior. Our extensive evaluation of nine <b>LLMs</b> across five <b>benchmarks</b> in two different programming languages using CodeMind shows that <b>LLMs</b> fairly understand control flow constructs and, in general, are capable of <b>reasoning</b> how inputs evolve to output, specifically for simple programs and the ones they can correctly synthesize. However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls. Furthermore, we observe that, while correlated, specification <b>reasoning</b> (essential for code synthesis) does not imply execution <b>reasoning</b> (essential for broader programming tasks such as testing and debugging): ranking <b>LLMs</b> based on test passing can be different compared to code <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=22--203249-ijtyper-an-iterative-type-inference-framework-for-java-by-integrating-constraint--and-statistically-based-methods-zhixiang-chen-et-al-2024>(2/2 | 203/249) iJTyper: An Iterative Type Inference Framework for Java by Integrating Constraint- and Statistically-based Methods (Zhixiang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixiang Chen, Anji Li, Neng Zhang, Jianguo Chen, Yuan Huang, Zibin Zheng. (2024)<br><strong>iJTyper: An Iterative Type Inference Framework for Java by Integrating Constraint- and Statistically-based Methods</strong><br><button class=copy-to-clipboard title="iJTyper: An Iterative Type Inference Framework for Java by Integrating Constraint- and Statistically-based Methods" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09995v1.pdf filename=2402.09995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inferring the types of API elements in incomplete code snippets (e.g., those on Q&amp;A forums) is a prepositive step required to work with the code snippets. Existing type inference methods can be mainly categorized as constraint-based or statistically-based. The former imposes higher requirements on code syntax and often suffers from low recall due to the syntactic limitation of code snippets. The latter relies on the statistical regularities learned from a training corpus and does not take full advantage of the type constraints in code snippets, which may lead to low precision. In this paper, we propose an iterative type inference framework for Java, called iJTyper, by integrating the strengths of both constraint- and statistically-based methods. For a code snippet, iJTyper first applies a constraint-based method and augments the code context with the inferred types of API elements. iJTyper then applies a statistically-based method to the augmented code snippet. The predicted candidate types of API elements are further used to improve the constraint-based method by reducing its pre-built knowledge base. iJTyper iteratively executes both methods and performs code context augmentation and knowledge base reduction until a termination condition is satisfied. Finally, the final inference results are obtained by combining the results of both methods. We evaluated iJTyper on two open-source datasets. Results show that 1) iJTyper achieves high average precision/recall of 97.31% and 92.52% on both datasets; 2) iJTyper significantly improves the recall of two state-of-the-art baselines, SnR and MLMTyper, by at least 7.31% and 27.44%, respectively; and 3) iJTyper improves the average precision/recall of the popular language model, <b>ChatGPT,</b> by 3.25% and 0.51% on both datasets.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=12--204249-stuck-at-faults-in-reram-neuromorphic-circuit-array-and-their-correction-through-machine-learning-vedant-sawal-et-al-2024>(1/2 | 204/249) Stuck-at Faults in ReRAM Neuromorphic Circuit Array and their Correction through Machine Learning (Vedant Sawal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vedant Sawal, Hiu Yung Wong. (2024)<br><strong>Stuck-at Faults in ReRAM Neuromorphic Circuit Array and their Correction through Machine Learning</strong><br><button class=copy-to-clipboard title="Stuck-at Faults in ReRAM Neuromorphic Circuit Array and their Correction through Machine Learning" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-LG, cs-NE, cs.AR<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10981v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10981v1.pdf filename=2402.10981v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the inference accuracy of the Resistive Random Access Memory (ReRAM) neuromorphic circuit due to stuck-at faults (stuck-on, stuck-off, and stuck at a certain resistive value). A <b>simulation</b> framework using Python is used to perform <b>supervised</b> machine learning (neural network with 3 hidden layers, 1 input layer, and 1 output layer) of handwritten digits and construct a corresponding fully analog neuromorphic circuit (4 synaptic arrays) simulated by Spectre. A generic 45nm Process Development Kit (PDK) was used. We study the difference in the inference accuracy degradation due to stuck-on and stuck-off defects. Various defect patterns are studied including circular, ring, row, column, and circular-complement defects. It is found that stuck-on and stuck-off defects have a similar effect on inference accuracy. However, it is also found that if there is a spatial defect variation across the columns, the inference accuracy may be degraded significantly. We also propose a machine learning (ML) strategy to recover the inference accuracy degradation due to stuck-at faults. The inference accuracy is improved from 48% to 85% in a defective neuromorphic circuit.</p></p class="citation"></blockquote><h3 id=22--205249-reusing-softmax-hardware-unit-for-gelu-computation-in-transformers-christodoulos-peltekis-et-al-2024>(2/2 | 205/249) Reusing Softmax Hardware Unit for GELU Computation in Transformers (Christodoulos Peltekis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christodoulos Peltekis, Kosmas Alexandridis, Giorgos Dimitrakopoulos. (2024)<br><strong>Reusing Softmax Hardware Unit for GELU Computation in Transformers</strong><br><button class=copy-to-clipboard title="Reusing Softmax Hardware Unit for GELU Computation in Transformers" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-LG, cs.AR<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10118v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10118v2.pdf filename=2402.10118v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have improved drastically the performance of natural language processing (NLP) and computer vision applications. The computation of <b>transformers</b> involves matrix multiplications and non-linear activation functions such as softmax and GELU (Gaussion Error Linear Unit) that are accelerated directly in hardware. Currently, function evaluation is done separately for each function and rarely allows for hardware reuse. To mitigate this problem, in this work, we map the computation of GELU to a softmax operator. In this way, the efficient hardware units designed already for softmax can be reused for computing GELU as well. Computation of GELU can enjoy the inherent vectorized nature of softmax and produce in parallel multiple GELU outcomes. Experimental results show that computing GELU via a pre-existing and incrementally modified softmax hardware unit (a) does not reduce the accuracy of representative NLP applications and (b) allows the reduction of the overall hardware area and power by 6.1% and 11.9%, respectively, on average.</p></p class="citation"></blockquote><h2 id=eessiv-7>eess.IV (7)</h2><h3 id=17--206249-current-and-future-roles-of-artificial-intelligence-in-retinopathy-of-prematurity-ali-jafarizadeh-et-al-2024>(1/7 | 206/249) Current and future roles of artificial intelligence in retinopathy of prematurity (Ali Jafarizadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Jafarizadeh, Shadi Farabi Maleki, Parnia Pouya, Navid Sobhi, Mirsaeed Abdollahi, Siamak Pedrammehr, Chee Peng Lim, Houshyar Asadi, Roohallah Alizadehsani, Ru-San Tan, Sheikh Mohammad Shariful Islam, U. Rajendra Acharya. (2024)<br><strong>Current and future roles of artificial intelligence in retinopathy of prematurity</strong><br><button class=copy-to-clipboard title="Current and future roles of artificial intelligence in retinopathy of prematurity" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: J-3-2; J-3-3, cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09975v1.pdf filename=2402.09975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retinopathy of prematurity (ROP) is a severe condition affecting premature infants, leading to abnormal retinal blood vessel growth, retinal detachment, and potential blindness. While semi-automated systems have been used in the past to diagnose ROP-related plus disease by quantifying retinal vessel features, traditional machine learning (ML) models face challenges like accuracy and overfitting. Recent advancements in deep learning (DL), especially <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs),</b> have significantly improved ROP detection and classification. The i-ROP deep learning (i-ROP-DL) system also shows promise in detecting plus disease, offering reliable ROP diagnosis potential. This research comprehensively examines the contemporary progress and challenges associated with using retinal imaging and artificial intelligence (AI) to detect ROP, offering valuable insights that can guide further investigation in this domain. Based on 89 original studies in this field (out of 1487 studies that were comprehensively reviewed), we concluded that traditional methods for ROP diagnosis suffer from subjectivity and manual analysis, leading to inconsistent clinical decisions. AI holds great promise for improving ROP management. This review explores AI&rsquo;s potential in ROP detection, classification, diagnosis, and prognosis.</p></p class="citation"></blockquote><h3 id=27--207249-towards-precision-cardiovascular-analysis-in-zebrafish-the-zacaf-paradigm-amir-mohammad-naderi-et-al-2024>(2/7 | 207/249) Towards Precision Cardiovascular Analysis in Zebrafish: The ZACAF Paradigm (Amir Mohammad Naderi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Mohammad Naderi, Jennifer G. Casey, Mao-Hsiang Huang, Rachelle Victorio, David Y. Chiang, Calum MacRae, Hung Cao, Vandana A. Gupta. (2024)<br><strong>Towards Precision Cardiovascular Analysis in Zebrafish: The ZACAF Paradigm</strong><br><button class=copy-to-clipboard title="Towards Precision Cardiovascular Analysis in Zebrafish: The ZACAF Paradigm" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09658v1.pdf filename=2402.09658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantifying cardiovascular parameters like ejection fraction in zebrafish as a host of biological investigations has been extensively studied. Since current manual monitoring techniques are time-consuming and fallible, several image processing frameworks have been proposed to automate the process. Most of these works rely on <b>supervised</b> deep-learning architectures. However, <b>supervised</b> methods tend to be overfitted on their training dataset. This means that applying the same framework to new <b>data</b> <b>with</b> different imaging setups and mutant types can severely decrease performance. We have developed a Zebrafish Automatic Cardiovascular Assessment Framework (ZACAF) to quantify the cardiac function in zebrafish. In this work, we further applied <b>data</b> <b>augmentation,</b> <b>Transfer</b> <b>Learning</b> (TL), and Test Time Augmentation (TTA) to ZACAF to improve the performance for the quantification of cardiovascular function quantification in zebrafish. This strategy can be integrated with the available frameworks to aid other researchers. We demonstrate that using TL, even with a constrained dataset, the model can be refined to accommodate a novel microscope setup, encompassing diverse mutant types and accommodating various video recording protocols. Additionally, as users engage in successive rounds of TL, the model is anticipated to undergo substantial enhancements in both generalizability and accuracy. Finally, we applied this approach to assess the cardiovascular function in nrap mutant zebrafish, a model of cardiomyopathy.</p></p class="citation"></blockquote><h3 id=37--208249-spatiotemporal-disentanglement-of-arteriovenous-malformations-in-digital-subtraction-angiography-kathleen-baur-et-al-2024>(3/7 | 208/249) Spatiotemporal Disentanglement of Arteriovenous Malformations in Digital Subtraction Angiography (Kathleen Baur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kathleen Baur, Xin Xiong, Erickson Torio, Rose Du, Parikshit Juvekar, Reuben Dorent, Alexandra Golby, Sarah Frisken, Nazim Haouchine. (2024)<br><strong>Spatiotemporal Disentanglement of Arteriovenous Malformations in Digital Subtraction Angiography</strong><br><button class=copy-to-clipboard title="Spatiotemporal Disentanglement of Arteriovenous Malformations in Digital Subtraction Angiography" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09636v1.pdf filename=2402.09636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although Digital Subtraction Angiography (DSA) is the most important imaging for visualizing cerebrovascular anatomy, its interpretation by clinicians remains difficult. This is particularly true when treating arteriovenous malformations (AVMs), where entangled vasculature connecting arteries and veins needs to be carefully identified.The presented method aims to enhance DSA image series by highlighting critical information via automatic classification of vessels using a combination of two learning models: An <b>unsupervised</b> machine learning method based on Independent Component Analysis that decomposes the phases of flow and a <b>convolutional</b> <b>neural</b> <b>network</b> that automatically delineates the vessels in image space. The proposed method was tested on clinical DSA images series and demonstrated efficient differentiation between arteries and veins that provides a viable solution to enhance visualizations for clinical use.</p></p class="citation"></blockquote><h3 id=47--209249-less-is-more-ensemble-learning-for-retinal-disease-recognition-under-limited-resources-jiahao-wang-et-al-2024>(4/7 | 209/249) Less is more: Ensemble Learning for Retinal Disease Recognition Under Limited Resources (Jiahao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Wang, Hong Peng, Shengchao Chen, Sufen Ren. (2024)<br><strong>Less is more: Ensemble Learning for Retinal Disease Recognition Under Limited Resources</strong><br><button class=copy-to-clipboard title="Less is more: Ensemble Learning for Retinal Disease Recognition Under Limited Resources" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Low-Resource, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09747v1.pdf filename=2402.09747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retinal optical coherence tomography (OCT) images provide crucial insights into the health of the posterior ocular segment. Therefore, the advancement of automated image analysis methods is imperative to equip clinicians and researchers with quantitative data, thereby facilitating informed decision-making. The application of deep learning (DL)-based approaches has gained extensive traction for executing these analysis tasks, demonstrating remarkable performance compared to labor-intensive manual analyses. However, the acquisition of Retinal OCT images often presents challenges <b>stemming</b> from privacy concerns and the resource-intensive labeling procedures, which contradicts the prevailing notion that DL models necessitate substantial data volumes for achieving superior performance. Moreover, limitations in available computational resources constrain the progress of high-performance medical artificial intelligence, particularly in less developed regions and countries. This paper introduces a novel ensemble learning mechanism designed for recognizing retinal diseases under limited resources (e.g., data, computation). The mechanism leverages insights from multiple pre-trained models, facilitating the transfer and adaptation of their knowledge to Retinal OCT images. This approach establishes a robust model even when confronted with limited labeled data, eliminating the need for an extensive array of parameters, as required in learning from scratch. Comprehensive experimentation on real-world datasets demonstrates that the proposed approach can achieve superior performance in recognizing Retinal OCT images, even when dealing with exceedingly restricted labeled datasets. Furthermore, this method obviates the necessity of learning extensive-scale parameters, making it well-suited for deployment in <b>low-resource</b> scenarios.</p></p class="citation"></blockquote><h3 id=57--210249-robust-semi-automatic-vessel-tracing-in-the-human-retinal-image-by-an-instance-segmentation-neural-network-siyi-chen-et-al-2024>(5/7 | 210/249) Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network (Siyi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyi Chen, Amir H. Kashani, Ji Yi. (2024)<br><strong>Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network</strong><br><button class=copy-to-clipboard title="Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Morphological Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10055v1.pdf filename=2402.10055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The morphology and hierarchy of the vascular systems are essential for perfusion in supporting metabolism. In human retina, one of the most energy-demanding organs, retinal circulation nourishes the entire inner retina by an intricate vasculature emerging and remerging at the optic nerve head (ONH). Thus, tracing the vascular branching from ONH through the vascular tree can illustrate vascular hierarchy and allow detailed <b>morphological</b> <b>quantification,</b> and yet remains a challenging task. Here, we presented a novel approach for a robust semi-automatic vessel tracing algorithm on human fundus images by an instance segmentation neural network (InSegNN). Distinct from semantic segmentation, InSegNN separates and labels different vascular trees individually and therefore enable tracing each tree throughout its branching. We have built-in three strategies to improve robustness and accuracy with temporal learning, spatial multi-sampling, and dynamic probability map. We achieved 83% specificity, and 50% improvement in Symmetric Best Dice (SBD) compared to literature, and outperformed baseline U-net. We have demonstrated tracing individual vessel trees from fundus images, and simultaneously retain the vessel hierarchy information. InSegNN paves a way for any subsequent <b>morphological</b> <b>analysis</b> of vascular morphology in relation to retinal diseases.</p></p class="citation"></blockquote><h3 id=67--211249-hybrid-cnn-bi-lstm-neural-network-for-hyperspectral-image-classification-alok-ranjan-sahoo-et-al-2024>(6/7 | 211/249) Hybrid CNN Bi-LSTM neural network for Hyperspectral image classification (Alok Ranjan Sahoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alok Ranjan Sahoo, Pavan Chakraborty. (2024)<br><strong>Hybrid CNN Bi-LSTM neural network for Hyperspectral image classification</strong><br><button class=copy-to-clipboard title="Hybrid CNN Bi-LSTM neural network for Hyperspectral image classification" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10026v1.pdf filename=2402.10026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyper spectral images have drawn the attention of the researchers for its complexity to classify. It has nonlinear relation between the materials and the spectral information provided by the HSI image. Deep learning methods have shown superiority in learning this nonlinearity in comparison to traditional machine learning methods. Use of 3-D <b>CNN</b> along with 2-D <b>CNN</b> have shown great success for learning spatial and spectral features. However, it uses comparatively large number of parameters. Moreover, it is not effective to learn inter layer information. Hence, this paper proposes a neural network combining 3-D <b>CNN,</b> 2-D <b>CNN</b> and Bi-LSTM. The performance of this model has been tested on Indian Pines(IP) University of Pavia(PU) and Salinas Scene(SA) data sets. The results are compared with the state of-the-art deep learning-based models. This model performed better in all three datasets. It could achieve 99.83, 99.98 and 100 percent accuracy using only 30 percent trainable parameters of the state-of-art model in IP, PU and SA datasets respectively.</p></p class="citation"></blockquote><h3 id=77--212249-tiaviz-a-browser-based-visualization-tool-for-computational-pathology-models-mark-eastwood-et-al-2024>(7/7 | 212/249) TIAViz: A Browser-based Visualization Tool for Computational Pathology Models (Mark Eastwood et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mark Eastwood, John Pocock, Mostafa Jahanifar, Adam Shephard, Skiros Habib, Ethar Alzaid, Abdullah Alsalemi, Jan Lukas Robertus, Nasir Rajpoot, Shan Raza, Fayyaz Minhas. (2024)<br><strong>TIAViz: A Browser-based Visualization Tool for Computational Pathology Models</strong><br><button class=copy-to-clipboard title="TIAViz: A Browser-based Visualization Tool for Computational Pathology Models" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-HC, cs-LG, eess-IV, eess.IV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09990v1.pdf filename=2402.09990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digital pathology has gained significant traction in modern healthcare systems. This shift from optical microscopes to digital imagery brings with it the potential for improved diagnosis, efficiency, and the integration of AI tools into the pathologists workflow. A critical aspect of this is visualization. Throughout the development of a machine learning (ML) model in digital pathology, it is crucial to have flexible, openly available tools to visualize models, from their outputs and predictions to the underlying annotations and images used to train or test a model. We introduce TIAViz, a Python-based visualization tool built into TIAToolbox which allows flexible, interactive, fully zoomable overlay of a wide variety of information onto whole slide images, including <b>graphs,</b> heatmaps, segmentations, annotations and other WSIs. The UI is browser-based, allowing use either locally, on a remote machine, or on a server to provide publicly available demos. This tool is open source and is made available at: <a href=https://github.com/TissueImageAnalytics/tiatoolbox>https://github.com/TissueImageAnalytics/tiatoolbox</a> and via pip installation (pip install tiatoolbox) and conda as part of TIAToolbox.</p></p class="citation"></blockquote><h2 id=csit-3>cs.IT (3)</h2><h3 id=13--213249-digital-versus-analog-transmissions-for-federated-learning-over-wireless-networks-jiacheng-yao-et-al-2024>(1/3 | 213/249) Digital versus Analog Transmissions for Federated Learning over Wireless Networks (Jiacheng Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacheng Yao, Wei Xu, Zhaohui Yang, Xiaohu You, Mehdi Bennis, H. Vincent Poor. (2024)<br><strong>Digital versus Analog Transmissions for Federated Learning over Wireless Networks</strong><br><button class=copy-to-clipboard title="Digital versus Analog Transmissions for Federated Learning over Wireless Networks" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs-NI, cs.IT, math-IT<br>Keyword Score: 30<br>Keywords: Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09657v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09657v1.pdf filename=2402.09657v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we quantitatively compare these two effective communication schemes, i.e., digital and analog ones, for wireless <b>federated</b> <b>learning</b> (FL) over resource-constrained networks, highlighting their essential differences as well as their respective application scenarios. We first examine both digital and analog transmission methods, together with a unified and fair comparison scheme under practical constraints. A universal convergence analysis under various imperfections is established for FL performance evaluation in wireless networks. These analytical results reveal that the fundamental difference between the two paradigms lies in whether communication and computation are jointly designed or not. The digital schemes decouple the communication design from specific FL tasks, making it difficult to support simultaneous uplink transmission of massive devices with limited bandwidth. In contrast, the analog communication allows over-the-air computation (AirComp), thus achieving efficient spectrum utilization. However, computation-oriented analog transmission reduces power efficiency, and its performance is sensitive to computational errors. Finally, numerical <b>simulations</b> are conducted to verify these theoretical observations.</p></p class="citation"></blockquote><h3 id=23--214249-two-timescale-design-for-active-star-ris-aided-massive-mimo-systems-anastasios-papazafeiropoulos-et-al-2024>(2/3 | 214/249) Two-Timescale Design for Active STAR-RIS Aided Massive MIMO Systems (Anastasios Papazafeiropoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anastasios Papazafeiropoulos, Hanxiao Ge, Pandelis Kourtessis, Tharmalingam Ratnarajah, Symeon Chatzinotas, Symeon Papavassiliou. (2024)<br><strong>Two-Timescale Design for Active STAR-RIS Aided Massive MIMO Systems</strong><br><button class=copy-to-clipboard title="Two-Timescale Design for Active STAR-RIS Aided Massive MIMO Systems" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09896v1.pdf filename=2402.09896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Simultaneously transmitting and reflecting \textcolor{black}{reconfigurable intelligent surface} (STAR-RIS) is a promising implementation of RIS-assisted systems that enables full-space coverage. However, STAR-RIS as well as conventional RIS suffer from the double-fading effect. Thus, in this paper, we propose the marriage of active RIS and STAR-RIS, denoted as ASTARS for massive multiple-input multiple-output (mMIMO) systems, and we focus on the energy splitting (ES) and mode switching (MS) protocols. Compared to prior literature, we consider the impact of correlated fading, and we rely our analysis on the two timescale protocol, being dependent on statistical channel state information (CSI). On this ground, we propose a channel estimation method for ASTARS with reduced overhead that accounts for its architecture. Next, we derive a \textcolor{black}{closed-form expression} for the achievable sum-rate for both types of users in the transmission and reflection regions in a unified approach with significant practical advantages such as reduced complexity and overhead, which result in a lower number of required iterations for convergence compared to an alternating optimization (AO) approach. Notably, we maximize simultaneously the amplitudes, the phase shifts, and the active amplifying coefficients of the ASTARS by applying the projected gradient ascent method (PGAM). Remarkably, the proposed optimization can be executed at every several coherence intervals that reduces the processing burden considerably. <b>Simulations</b> corroborate the analytical results, provide insight into the effects of fundamental variables on the sum achievable SE, and present the superiority of 16 ASTARS compared to passive STAR-RIS for a practical number of surface elements.</p></p class="citation"></blockquote><h3 id=33--215249-infonet-neural-estimation-of-mutual-information-without-test-time-optimization-zhengyang-hu-et-al-2024>(3/3 | 215/249) InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization (Zhengyang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyang Hu, Song Kang, Qunsong Zeng, Kaibin Huang, Yanchao Yang. (2024)<br><strong>InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization</strong><br><button class=copy-to-clipboard title="InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10158v1.pdf filename=2402.10158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating <b>mutual</b> <b>correlations</b> between random variables or data streams is essential for intelligent behavior and decision-making. As a fundamental quantity for measuring statistical relationships, <b>mutual</b> <b>information</b> has been extensively studied and utilized for its generality and equitability. However, existing methods often lack the efficiency needed for real-time applications, such as test-time optimization of a neural network, or the differentiability required for end-to-end learning, like histograms. We introduce a neural network called InfoNet, which directly outputs <b>mutual</b> <b>information</b> estimations of data streams by leveraging the attention mechanism and the computational efficiency of deep learning infrastructures. By maximizing a dual formulation of <b>mutual</b> <b>information</b> through large-scale simulated training, our approach circumvents time-consuming test-time optimization and offers generalization ability. We evaluate the effectiveness and generalization of our proposed <b>mutual</b> <b>information</b> estimation scheme on various families of distributions and applications. Our results demonstrate that InfoNet and its training process provide a graceful efficiency-accuracy trade-off and order-preserving properties. We will make the code and models available as a comprehensive toolbox to facilitate studies in different fields requiring real-time <b>mutual</b> <b>information</b> estimation.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--216249-reeb-complements-for-exploring-inclusions-between-isosurfaces-from-two-scalar-fields-akito-fujii-et-al-2024>(1/1 | 216/249) Reeb Complements for Exploring Inclusions Between Isosurfaces From Two Scalar Fields (Akito Fujii et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akito Fujii, Osamu Saeki, Daisuke Sakurai. (2024)<br><strong>Reeb Complements for Exploring Inclusions Between Isosurfaces From Two Scalar Fields</strong><br><button class=copy-to-clipboard title="Reeb Complements for Exploring Inclusions Between Isosurfaces From Two Scalar Fields" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09768v1.pdf filename=2402.09768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article proposes to integrate two Reeb <b>graphs</b> with the information of their isosurfaces&rsquo; inclusion relation. As computing power evolves, there arise numerical data that have small-scale physics inside larger ones &ndash; for example, small clouds in a <b>simulation</b> can be contained inside an atmospheric layer, which is further contained in an enormous hurricane. Extracting such inclusions between isosurfaces is a challenge for isosurfacing: the user would have to explore the vast combinations of isosurfaces $(f_1^{-1}(l_1), f_2^{-1}(l_2))$ from scalar fields $f_i: M \to \mathbb{R}$, $i = 1, 2$, where $M$ is a domain manifold and $f_i$ are physical quantities, to find inclusion of one isosurface within another. For this, we propose the \textit{Reeb complement}, a topological space that integrates two Reeb <b>graphs</b> with the inclusion relation. The Reeb complement has a natural partition that classifies equivalent containment of isosurfaces. This is a handy characteristic to let the Reeb complement serve as an overview of the inclusion relationship in the data. We also propose level-of-detail control of the inclusions through simplification of the Reeb complement.</p></p class="citation"></blockquote><h2 id=q-biogn-2>q-bio.GN (2)</h2><h3 id=12--217249-toward-a-team-of-ai-made-scientists-for-scientific-discovery-from-gene-expression-data-haoyang-liu-et-al-2024>(1/2 | 217/249) Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data (Haoyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyang Liu, Yijiang Li, Jinglin Jian, Yuxuan Cheng, Jianrong Lu, Shuyi Guo, Jinglei Zhu, Mianchen Zhang, Miantong Zhang, Haohan Wang. (2024)<br><strong>Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data</strong><br><button class=copy-to-clipboard title="Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-AI, cs-LG, q-bio-GN, q-bio.GN<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12391v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12391v2.pdf filename=2402.12391v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM).</b> These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a <b>benchmark</b> dataset to assess TAIS&rsquo;s effectiveness in gene identification, demonstrating our system&rsquo;s potential to significantly enhance the efficiency and scope of scientific exploration. Our findings represent a solid step towards automating scientific discovery through <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=22--218249-data-smoothing-filling-method-based-on-scrna-seq-data-zero-value-identification-linfeng-jiang-et-al-2024>(2/2 | 218/249) Data Smoothing Filling Method based on ScRNA-Seq Data Zero-Value Identification (Linfeng Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linfeng Jiang, Yuan Zhu. (2024)<br><strong>Data Smoothing Filling Method based on ScRNA-Seq Data Zero-Value Identification</strong><br><button class=copy-to-clipboard title="Data Smoothing Filling Method based on ScRNA-Seq Data Zero-Value Identification" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-IT, math-IT, q-bio-GN, q-bio.GN<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09755v1.pdf filename=2402.09755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single-cell RNA sequencing (scRNA-seq) determines RNA expression at single-cell resolution. It provides a powerful tool for studying immunity, regulation, and other life activities of cells. However, due to the limitations of the sequencing technique, the scRNA-seq data are represented with sparsity, whichcontains missing gene values, i.e., zero values, called dropout. Therefore, it is necessary to impute missing values before analyzing scRNA-seq data. However, existing imputation computation methods often only focus on the identification of technical zeros or imputing all zeros based on cell similarity. This study proposes a new method (SFAG) to reconstruct the gene expression relationship matrix by usinggraph regularization technology to preserve the high-dimensional manifold information of the data, andto mine the relationship between genes and cells in the data, and then uses a method of averaging the <b>clustering</b> results to fill in the identified technical zeros. Experimental results show that SFAGcan helpimprove downstream analysis and reconstruct cell trajectory</p></p class="citation"></blockquote><h2 id=mathoc-3>math.OC (3)</h2><h3 id=13--219249-mathematical-opportunities-in-digital-twins-math-dt-harbir-antil-2024>(1/3 | 219/249) Mathematical Opportunities in Digital Twins (MATH-DT) (Harbir Antil, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harbir Antil. (2024)<br><strong>Mathematical Opportunities in Digital Twins (MATH-DT)</strong><br><button class=copy-to-clipboard title="Mathematical Opportunities in Digital Twins (MATH-DT)" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-NA, math-NA, math-OC, math.OC, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10326v1.pdf filename=2402.10326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The report describes the discussions from the Workshop on Mathematical Opportunities in Digital Twins (MATH-DT) from December 11-13, 2023, George Mason University. It illustrates that foundational Mathematical advances are required for Digital Twins (DTs) that are different from traditional approaches. A traditional model, in biology, physics, engineering or medicine, starts with a generic physical law (e.g., equations) and is often a simplification of reality. A DT starts with a specific ecosystem, object or person (e.g., personalized care) representing reality, requiring multi -scale, -physics modeling and coupling. Thus, these processes begin at opposite ends of the <b>simulation</b> and modeling pipeline, requiring different reliability criteria and uncertainty assessments. Additionally, unlike existing approaches, a DT assists humans to make decisions for the physical system, which (via sensors) in turn feeds data into the DT, and operates for the life of the physical system. While some of the foundational mathematical research can be done without a specific application context, one must also keep specific applications in mind for DTs. E.g., modeling a bridge or a biological system (a patient), or a socio-technical system (a city) is very different. The models range from differential equations (deterministic/uncertain) in engineering, to stochastic in biology, including agent-based. These are multi-scale hybrid models or large scale (multi-objective) optimization problems under uncertainty. There are no universal models or approaches. For e.g., Kalman filters for forecasting might work in engineering, but can fail in biomedical domain. Ad hoc studies, with limited systematic work, have shown that AI/ML methods can fail for simple engineering systems and can work well for biomedical problems. A list of `Mathematical Opportunities and Challenges&rsquo; concludes the report.</p></p class="citation"></blockquote><h3 id=23--220249-a-system-dynamic-based-simulation-and-bayesian-optimization-for-inventory-management-sarit-maitra-2024>(2/3 | 220/249) A System-Dynamic Based Simulation and Bayesian Optimization for Inventory Management (Sarit Maitra, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarit Maitra. (2024)<br><strong>A System-Dynamic Based Simulation and Bayesian Optimization for Inventory Management</strong><br><button class=copy-to-clipboard title="A System-Dynamic Based Simulation and Bayesian Optimization for Inventory Management" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-CC, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10975v1.pdf filename=2402.10975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inventory management is a fundamental challenge in supply chain management. The challenge is compounded when the associated products have unpredictable demands. This study proposes an innovative optimization approach combining system-dynamic Monte-Carlo <b>simulation</b> and Bayesian optimization. The proposed algorithm is tested with a real-life, unpredictable demand dataset to find the optimal stock to meet the business objective. The findings show a considerable improvement in inventory policy. This information is helpful for supply chain analytics decision-making, which increases productivity and profitability. This study further adds sensitivity analysis, considering the variation in demand and expected output in profit percentage. This paper makes a substantial contribution by presenting a simple yet robust approach to addressing the fundamental difficulty of inventory management in a dynamic business environment.</p></p class="citation"></blockquote><h3 id=33--221249-an-accelerated-distributed-stochastic-gradient-method-with-momentum-kun-huang-et-al-2024>(3/3 | 221/249) An Accelerated Distributed Stochastic Gradient Method with Momentum (Kun Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Huang, Shi Pu, Angelia Nedić. (2024)<br><strong>An Accelerated Distributed Stochastic Gradient Method with Momentum</strong><br><button class=copy-to-clipboard title="An Accelerated Distributed Stochastic Gradient Method with Momentum" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-DC, cs-MA, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09714v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09714v2.pdf filename=2402.09714v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce an accelerated distributed <b>stochastic</b> <b>gradient</b> <b>method</b> with momentum for solving the distributed optimization problem, where a group of $n$ agents collaboratively minimize the average of the local objective functions over a connected network. The method, termed ``Distributed <b>Stochastic</b> <b>Momentum</b> <b>Tracking</b> (DSMT)&rsquo;&rsquo;, is a single-loop algorithm that utilizes the momentum tracking technique as well as the Loopless Chebyshev Acceleration (LCA) method. We show that DSMT can asymptotically achieve comparable convergence rates as centralized <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> method under a general variance condition regarding the <b>stochastic</b> <b>gradients.</b> <b>Moreover,</b> the number of iterations (transient times) required for DSMT to achieve such rates behaves as $\mathcal{O}(n^{5/3}/(1-\lambda))$ for minimizing general smooth objective functions, and $\mathcal{O}(\sqrt{n/(1-\lambda)})$ under the Polyak-{\L}ojasiewicz (PL) condition. Here, the term $1-\lambda$ denotes the spectral gap of the mixing matrix related to the underlying network topology. Notably, the obtained results do not rely on multiple inter-node communications or <b>stochastic</b> <b>gradient</b> <b>accumulation</b> per iteration, and the transient times are the shortest under the setting to the best of our knowledge.</p></p class="citation"></blockquote><h2 id=cshc-6>cs.HC (6)</h2><h3 id=16--222249-lave-llm-powered-agent-assistance-and-language-augmentation-for-video-editing-bryan-wang-et-al-2024>(1/6 | 222/249) LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing (Bryan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bryan Wang, Yuliang Li, Zhaoyang Lv, Haijun Xia, Yan Xu, Raj Sodhi. (2024)<br><strong>LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing</strong><br><button class=copy-to-clipboard title="LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-HC, cs-MM, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10294v1.pdf filename=2402.10294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video creation has become increasingly popular, yet the expertise and effort required for editing often pose barriers to beginners. In this paper, we explore the integration of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> into the video editing workflow to reduce these barriers. Our design vision is embodied in LAVE, a novel system that provides <b>LLM-powered</b> agent assistance and language-augmented editing features. LAVE automatically generates language descriptions for the user&rsquo;s footage, serving as the foundation for enabling the <b>LLM</b> to process videos and assist in editing tasks. When the user provides editing objectives, the agent plans and executes relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through either the agent or direct UI manipulation, providing flexibility and enabling manual refinement of agent actions. Our user study, which included eight participants ranging from novices to proficient editors, demonstrated LAVE&rsquo;s effectiveness. The results also shed light on user perceptions of the proposed <b>LLM-assisted</b> editing paradigm and its impact on users&rsquo; creativity and sense of co-creation. Based on these findings, we propose design implications to inform the future development of agent-assisted content editing.</p></p class="citation"></blockquote><h3 id=26--223249-not-just-novelty-a-longitudinal-study-on-utility-and-customization-of-ai-workflows-tao-long-et-al-2024>(2/6 | 223/249) Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows (Tao Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Long, Katy Ilonka Gero, Lydia B. Chilton. (2024)<br><strong>Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows</strong><br><button class=copy-to-clipboard title="Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-CY, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Generative AI, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09894v1.pdf filename=2402.09894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> brings novel and impressive abilities to help people in everyday tasks. There are many AI workflows that solve real and complex problems by chaining AI outputs together with human interaction. Although there is an undeniable lure of AI, it&rsquo;s uncertain how useful <b>generative</b> <b>AI</b> workflows are after the novelty wears off. Additionally, tools built with <b>generative</b> <b>AI</b> have the potential to be personalized and adapted quickly and easily, but do users take advantage of the potential to customize? We conducted a three-week longitudinal study with 12 users to understand the familiarization and customization of <b>generative</b> <b>AI</b> tools for science communication. Our study revealed that the familiarization phase lasts for 4.3 sessions, where users explore the capabilities of the workflow and which aspects they find useful. After familiarization, the perceived utility of the system is rated higher than before, indicating that the perceived utility of AI is not just a novelty effect. The increase in benefits mainly comes from end-users&rsquo; ability to customize <b>prompts,</b> and thus appropriate the system to their own needs. This points to a future where <b>generative</b> <b>AI</b> systems can allow us to design for appropriation.</p></p class="citation"></blockquote><h3 id=36--224249-exploring-the-potential-of-large-language-models-in-artistic-creation-collaboration-and-reflection-on-creative-programming-anqi-wang-et-al-2024>(3/6 | 224/249) Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming (Anqi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anqi Wang, Zhizhuo Yin, Yulu Hu, Yuanyuan Mao, Pan Hui. (2024)<br><strong>Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming</strong><br><button class=copy-to-clipboard title="Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: J-5, cs-AI, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09750v1.pdf filename=2402.09750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the potential of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has been widely used in assisting programming. However, current research does not explore the artist potential of <b>LLMs</b> in creative coding within artist and AI collaboration. Our work probes the reflection type of artists in the creation process with such collaboration. We compare two common collaboration approaches: invoking the entire program and multiple subtasks. Our findings exhibit artists&rsquo; different stimulated reflections in two different methods. Our finding also shows the correlation of reflection type with user performance, user satisfaction, and subjective experience in two collaborations through conducting two methods, including experimental data and qualitative interviews. In this sense, our work reveals the artistic potential of <b>LLM</b> in creative coding. Meanwhile, we provide a critical lens of human-AI collaboration from the artists&rsquo; perspective and expound design suggestions for future work of AI-assisted creative tasks.</p></p class="citation"></blockquote><h3 id=46--225249-geobotsvr-a-robotics-learning-game-for-beginners-with-hands-on-learning-simulation-syed-t-mubarrat-2024>(4/6 | 225/249) GeoBotsVR: A Robotics Learning Game for Beginners with Hands-on Learning Simulation (Syed T. Mubarrat, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Syed T. Mubarrat. (2024)<br><strong>GeoBotsVR: A Robotics Learning Game for Beginners with Hands-on Learning Simulation</strong><br><button class=copy-to-clipboard title="GeoBotsVR: A Robotics Learning Game for Beginners with Hands-on Learning Simulation" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-RO, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09662v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09662v3.pdf filename=2402.09662v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article introduces GeoBotsVR, an easily accessible virtual reality game that combines elements of puzzle-solving with robotics learning and aims to cultivate interest and motivation in robotics, programming, and electronics among individuals with limited experience in these domains. The game allows players to build and customize a two-wheeled mobile robot using various robotic components and use their robot to solve various procedurally-generated puzzles in a diverse range of environments. An innovative aspect is the inclusion of a repair feature, requiring players to address randomly generated electronics and programming issues with their robot through hands-on manipulation. GeoBotsVR is designed to be immersive, replayable, and practical application-based, offering an enjoyable and accessible tool for beginners to acquaint themselves with robotics. The game simulates a hands-on learning experience and does not require prior technical knowledge, making it a potentially valuable resource for beginners to get an engaging introduction to the field of robotics.</p></p class="citation"></blockquote><h3 id=56--226249-user-privacy-harms-and-risks-in-conversational-ai-a-proposed-framework-ece-gumusel-et-al-2024>(5/6 | 226/249) User Privacy Harms and Risks in Conversational AI: A Proposed Framework (Ece Gumusel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ece Gumusel, Kyrie Zhixuan Zhou, Madelyn Rose Sanfilippo. (2024)<br><strong>User Privacy Harms and Risks in Conversational AI: A Proposed Framework</strong><br><button class=copy-to-clipboard title="User Privacy Harms and Risks in Conversational AI: A Proposed Framework" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CY, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09716v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09716v1.pdf filename=2402.09716v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a unique framework that applies and extends Solove (2006)&rsquo;s taxonomy to address privacy concerns in interactions with text-based AI <b>chatbots.</b> As <b>chatbot</b> prevalence grows, concerns about user privacy have heightened. While existing literature highlights design elements compromising privacy, a comprehensive framework is lacking. Through semi-structured interviews with 13 participants interacting with two AI <b>chatbots,</b> this study identifies 9 privacy harms and 9 privacy risks in text-based interactions. Using a grounded theory approach for interview and chatlog analysis, the framework examines privacy implications at various interaction stages. The aim is to offer developers, policymakers, and researchers a tool for responsible and secure implementation of conversational AI, filling the existing gap in addressing privacy issues associated with text-based AI <b>chatbots.</b></p></p class="citation"></blockquote><h3 id=66--227249-a-framework-for-gait-based-user-demography-estimation-using-inertial-sensors-chinmay-prakash-swami-2024>(6/6 | 227/249) A Framework For Gait-Based User Demography Estimation Using Inertial Sensors (Chinmay Prakash Swami, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chinmay Prakash Swami. (2024)<br><strong>A Framework For Gait-Based User Demography Estimation Using Inertial Sensors</strong><br><button class=copy-to-clipboard title="A Framework For Gait-Based User Demography Estimation Using Inertial Sensors" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-LG, cs.HC, eess-SP<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09761v1.pdf filename=2402.09761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human gait has been shown to provide crucial motion cues for various applications. Recognizing patterns in human gait has been widely adopted in various application areas such as security, virtual reality gaming, medical rehabilitation, and ailment identification. Furthermore, wearable inertial sensors have been widely used for not only recording gait but also to predict users&rsquo; demography. Machine Learning techniques such as deep learning, combined with inertial sensor signals, have shown promising results in recognizing patterns in human gait and estimate users&rsquo; demography. However, the <b>black-box</b> <b>nature</b> of such deep learning models hinders the researchers from uncovering the reasons behind the model&rsquo;s predictions. Therefore, we propose leveraging deep learning and Layer-Wise Relevance Propagation (LRP) to identify the important variables that play a vital role in identifying the users&rsquo; demography such as age and gender. To assess the efficacy of this approach we train a deep neural network model on a large sensor-based gait dataset consisting of 745 subjects to identify users&rsquo; age and gender. Using LRP we identify the variables relevant for characterizing the gait patterns. Thus, we enable interpretation of non-linear ML models which are experts in identifying the users&rsquo; demography based on inertial signals. We believe this approach can not only provide clinicians information about the gait parameters relevant to age and gender but also can be expanded to analyze and diagnose gait disorders.</p></p class="citation"></blockquote><h2 id=cond-matdis-nn-1>cond-mat.dis-nn (1)</h2><h3 id=11--228249-random-features-and-polynomial-rules-fabián-aguirre-lópez-et-al-2024>(1/1 | 228/249) Random features and polynomial rules (Fabián Aguirre-López et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabián Aguirre-López, Silvio Franz, Mauro Pastore. (2024)<br><strong>Random features and polynomial rules</strong><br><button class=copy-to-clipboard title="Random features and polynomial rules" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.dis-nn<br>Categories: cond-mat-dis-nn, cond-mat.dis-nn, cs-LG<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10164v1.pdf filename=2402.10164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Random features models play a distinguished role in the theory of deep learning, describing the behavior of neural networks close to their infinite-width limit. In this work, we present a thorough analysis of the generalization performance of random features models for generic <b>supervised</b> <b>learning</b> problems with Gaussian data. Our approach, built with tools from the statistical mechanics of disordered systems, maps the random features model to an equivalent polynomial model, and allows us to plot average generalization curves as functions of the two main control parameters of the problem: the number of random features $N$ and the size $P$ of the training set, both assumed to scale as powers in the input dimension $D$. Our results extend the case of proportional scaling between $N$, $P$ and $D$. They are in accordance with rigorous bounds known for certain particular learning tasks and are in quantitative agreement with numerical experiments performed over many order of magnitudes of $N$ and $P$. We find good agreement also far from the asymptotic limits where $D\to \infty$ and at least one between $P/D^K$, $N/D^L$ remains finite.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--229249-identifying-and-modelling-cognitive-biases-in-mobility-choices-chloe-conrad-et-al-2024>(1/1 | 229/249) Identifying and modelling cognitive biases in mobility choices (Chloe Conrad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chloe Conrad, Carole Adam. (2024)<br><strong>Identifying and modelling cognitive biases in mobility choices</strong><br><button class=copy-to-clipboard title="Identifying and modelling cognitive biases in mobility choices" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: K-4-2, cs-AI, cs-CY, cs-MA, cs.CY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09921v1.pdf filename=2402.09921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This report presents results from an M1 internship dedicated to agent-based modelling and <b>simulation</b> of daily mobility choices. This <b>simulation</b> is intended to be realistic enough to serve as a basis for a serious game about the mobility transition. In order to ensure this level of realism, we conducted a survey to measure if real mobility choices are made rationally, or how biased they are. Results analysed here show that various biases could play a role in decisions. We then propose an implementation in a GAMA agent-based <b>simulation.</b></p></p class="citation"></blockquote><h2 id=q-fincp-1>q-fin.CP (1)</h2><h3 id=11--230249-alpha-gpt-20-human-in-the-loop-ai-for-quantitative-investment-hang-yuan-et-al-2024>(1/1 | 230/249) Alpha-GPT 2.0: Human-in-the-Loop AI for Quantitative Investment (Hang Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Yuan, Saizhuo Wang, Jian Guo. (2024)<br><strong>Alpha-GPT 2.0: Human-in-the-Loop AI for Quantitative Investment</strong><br><button class=copy-to-clipboard title="Alpha-GPT 2.0: Human-in-the-Loop AI for Quantitative Investment" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.CP<br>Categories: cs-AI, q-fin-CP, q-fin.CP<br>Keyword Score: 20<br>Keywords: human-in-the-loop, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09746v1.pdf filename=2402.09746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, we introduced a new paradigm for alpha mining in the realm of quantitative investment, developing a new interactive alpha mining system framework, Alpha-GPT. This system is centered on iterative Human-AI interaction based on <b>large</b> <b>language</b> <b>models,</b> introducing a <b>Human-in-the-Loop</b> approach to alpha discovery. In this paper, we present the next-generation Alpha-GPT 2.0 \footnote{Draft. Work in progress}, a quantitative investment framework that further encompasses crucial modeling and analysis phases in quantitative investment. This framework emphasizes the iterative, interactive research between humans and AI, embodying a <b>Human-in-the-Loop</b> strategy throughout the entire quantitative investment pipeline. By assimilating the insights of human researchers into the systematic alpha research process, we effectively leverage the <b>Human-in-the-Loop</b> approach, enhancing the efficiency and precision of quantitative investment research.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--231249-dpbalance-efficient-and-fair-privacy-budget-scheduling-for-federated-learning-as-a-service-yu-liu-et-al-2024>(1/1 | 231/249) DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated Learning as a Service (Yu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Liu, Zibo Wang, Yifei Zhu, Chen Chen. (2024)<br><strong>DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated Learning as a Service</strong><br><button class=copy-to-clipboard title="DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated Learning as a Service" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-CR, cs-DC, cs-LG, cs.DC<br>Keyword Score: 20<br>Keywords: Fairness, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09715v1.pdf filename=2402.09715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) has emerged as a prevalent distributed machine learning scheme that enables collaborative model training without aggregating raw data. Cloud service providers further embrace <b>Federated</b> <b>Learning</b> as a Service (FLaaS), allowing data analysts to execute their FL training pipelines over differentially-protected data. Due to the intrinsic properties of differential privacy, the enforced privacy level on data blocks can be viewed as a privacy budget that requires careful scheduling to cater to diverse training pipelines. Existing privacy budget scheduling studies prioritize either efficiency or <b>fairness</b> individually. In this paper, we propose DPBalance, a novel privacy budget scheduling mechanism that jointly optimizes both efficiency and <b>fairness.</b> We first develop a comprehensive utility function incorporating data analyst-level dominant shares and FL-specific performance metrics. A sequential allocation mechanism is then designed using the Lagrange multiplier method and effective greedy heuristics. We theoretically prove that DPBalance satisfies Pareto Efficiency, Sharing Incentive, Envy-Freeness, and Weak Strategy Proofness. We also theoretically prove the existence of a <b>fairness-efficiency</b> tradeoff in privacy budgeting. Extensive experiments demonstrate that DPBalance outperforms state-of-the-art solutions, achieving an average efficiency improvement of $1.44\times \sim 3.49 \times$, and an average <b>fairness</b> improvement of $1.37\times \sim 24.32 \times$.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--232249-zastosowanie-grafów-i-sieci-w-systemach-rekomendacji-michał-malinowski-2024>(1/2 | 232/249) Zastosowanie grafów i sieci w systemach rekomendacji (Michał Malinowski, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michał Malinowski. (2024)<br><strong>Zastosowanie grafów i sieci w systemach rekomendacji</strong><br><button class=copy-to-clipboard title="Zastosowanie grafów i sieci w systemach rekomendacji" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 13<br>Keywords: Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10250v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10250v1.pdf filename=2402.10250v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The chapter aims to explore the application of <b>graph</b> theory and networks in the <b>recommendation</b> domain, encompassing the mathematical models that form the foundation for the algorithms and <b>recommendation</b> systems developed based on them. The initial section of the chapter provides a concise overview of the <b>recommendation</b> field, with a particular focus on the types of <b>recommendation</b> solutions and the mathematical description of the problem. Subsequently, the chapter delves into the models and techniques for utilizing <b>graphs</b> and networks, along with illustrative examples of algorithms constructed on their basis.</p></p class="citation"></blockquote><h3 id=22--233249-modeling-the-impact-of-timeline-algorithms-on-opinion-dynamics-using-low-rank-updates-tianyi-zhou-et-al-2024>(2/2 | 233/249) Modeling the Impact of Timeline Algorithms on Opinion Dynamics Using Low-rank Updates (Tianyi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyi Zhou, Stefan Neumann, Kiran Garimella, Aristides Gionis. (2024)<br><strong>Modeling the Impact of Timeline Algorithms on Opinion Dynamics Using Low-rank Updates</strong><br><button class=copy-to-clipboard title="Modeling the Impact of Timeline Algorithms on Opinion Dynamics Using Low-rank Updates" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10053v1.pdf filename=2402.10053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Timeline algorithms are key parts of online social networks, but during recent years they have been blamed for increasing polarization and disagreement in our society. Opinion-dynamics models have been used to study a variety of phenomena in online social networks, but an open question remains on how these models can be augmented to take into account the fine-grained impact of user-level timeline algorithms. We make progress on this question by providing a way to model the impact of timeline algorithms on opinion dynamics. Specifically, we show how the popular Friedkin&ndash;Johnsen opinion-formation model can be augmented based on aggregate information, extracted from timeline data. We use our model to study the problem of minimizing the polarization and disagreement; we assume that we are allowed to make small changes to the users&rsquo; timeline compositions by strengthening some topics of discussion and penalizing some others. We present a gradient descent-based algorithm for this problem, and show that under realistic parameter settings, our algorithm computes a $(1+\varepsilon)$-approximate solution in time $\tilde{O}(m\sqrt{n} \lg(1/\varepsilon))$, where $m$ is the number of edges in the <b>graph</b> and $n$ is the number of vertices. We also present an algorithm that provably computes an $\varepsilon$-approximation of our model in near-linear time. We evaluate our method on real-world data and show that it effectively reduces the polarization and disagreement in the network. Finally, we release an anonymized <b>graph</b> dataset with ground-truth opinions and more than 27,000 nodes (the previously largest publicly available dataset contains less than 550 nodes).</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--234249-radio-astronomical-image-reconstruction-with-conditional-denoising-diffusion-model-mariia-drozdova-et-al-2024>(1/1 | 234/249) Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model (Mariia Drozdova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mariia Drozdova, Vitaliy Kinakh, Omkar Bait, Olga Taran, Erica Lastufka, Miroslava Dessauges-Zavadsky, Taras Holotyak, Daniel Schaerer, Slava Voloshynovskiy. (2024)<br><strong>Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model</strong><br><button class=copy-to-clipboard title="Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph.IM, cs-AI, cs-CV<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10204v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10204v2.pdf filename=2402.10204v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstructing sky models from dirty radio images for accurate source localization and flux estimation is crucial for studying galaxy evolution at high redshift, especially in deep fields using instruments like the Atacama Large Millimetre Array (ALMA). With new projects like the Square Kilometre Array (SKA), there&rsquo;s a growing need for better source extraction methods. Current techniques, such as CLEAN and PyBDSF, often fail to detect faint sources, highlighting the need for more accurate methods. This study proposes using stochastic neural networks to rebuild sky models directly from dirty images. This method can pinpoint radio sources and measure their fluxes with related uncertainties, marking a potential improvement in radio source characterization. We tested this approach on 10164 images simulated with the CASA tool simalma, based on ALMA&rsquo;s Cycle 5.3 antenna setup. We applied conditional Denoising Diffusion <b>Probabilistic</b> <b>Models</b> (DDPMs) for sky models reconstruction, then used Photutils to determine source coordinates and fluxes, assessing the model&rsquo;s performance across different water vapor levels. Our method showed excellence in source localization, achieving more than 90% completeness at a signal-to-noise ratio (SNR) as low as 2. It also surpassed PyBDSF in flux estimation, accurately identifying fluxes for 96% of sources in the test set, a significant improvement over CLEAN+ PyBDSF&rsquo;s 57%. Conditional DDPMs is a powerful tool for image-to-image translation, yielding accurate and robust characterisation of radio sources, and outperforming existing methodologies. While this study underscores its significant potential for applications in radio astronomy, we also acknowledge certain limitations that accompany its usage, suggesting directions for further refinement and research.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--235249-brant-2-foundation-model-for-brain-signals-zhizhang-yuan-et-al-2024>(1/1 | 235/249) Brant-2: Foundation Model for Brain Signals (Zhizhang Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhizhang Yuan, Daoze Zhang, Junru Chen, Geifei Gu, Yang Yang. (2024)<br><strong>Brant-2: Foundation Model for Brain Signals</strong><br><button class=copy-to-clipboard title="Brant-2: Foundation Model for Brain Signals" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-AI, cs-LG, eess-SP, q-bio-NC, q-bio.NC<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10251v1.pdf filename=2402.10251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundational</b> <b>models</b> benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest <b>foundation</b> <b>model</b> in brain signals, Brant-2. Compared to Brant, a <b>foundation</b> <b>model</b> designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component&rsquo;s effectiveness, and showcase our model&rsquo;s ability to maintain performance in scenarios with scarce labels. The source code and pre-trained weights are available at: <a href=https://anonymous.4open.science/r/Brant-2-5843>https://anonymous.4open.science/r/Brant-2-5843</a>.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--236249-modeling-methodology-for-the-accurate-and-prompt-prediction-of-symptomatic-events-in-chronic-diseases-josué-pagán-et-al-2024>(1/1 | 236/249) Modeling methodology for the accurate and prompt prediction of symptomatic events in chronic diseases (Josué Pagán et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Josué Pagán, José L. Risco-Martín, José M. Moya, José L. Ayala. (2024)<br><strong>Modeling methodology for the accurate and prompt prediction of symptomatic events in chronic diseases</strong><br><button class=copy-to-clipboard title="Modeling methodology for the accurate and prompt prediction of symptomatic events in chronic diseases" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10972v1.pdf filename=2402.10972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prediction of symptomatic crises in chronic diseases allows to take decisions before the symptoms occur, such as the intake of drugs to avoid the symptoms or the activation of medical alarms. The prediction horizon is in this case an important parameter in order to fulfill the pharmacokinetics of medications, or the time response of medical services. This paper presents a study about the prediction limits of a chronic disease with symptomatic crises: the migraine. For that purpose, this work develops a methodology to build predictive migraine models and to improve these predictions beyond the limits of the initial models. The maximum prediction horizon is analyzed, and its dependency on the selected features is studied. A strategy for model selection is proposed to tackle the trade off between conservative but robust predictive models, with respect to less accurate predictions with higher horizons. The obtained results show a prediction horizon close to 40 minutes, which is in the time range of the drug pharmacokinetics. Experiments have been performed in a realistic scenario where input data have been acquired in an ambulatory clinical study by the deployment of a non-intrusive Wireless Body Sensor Network. Our results provide an effective methodology for the selection of the future horizon in the development of prediction algorithms for diseases experiencing symptomatic crises.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--237249-efficient-φ-regret-minimization-with-low-degree-swap-deviations-in-extensive-form-games-brian-hu-zhang-et-al-2024>(1/1 | 237/249) Efficient $Φ$-Regret Minimization with Low-Degree Swap Deviations in Extensive-Form Games (Brian Hu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brian Hu Zhang, Ioannis Anagnostides, Gabriele Farina, Tuomas Sandholm. (2024)<br><strong>Efficient $Φ$-Regret Minimization with Low-Degree Swap Deviations in Extensive-Form Games</strong><br><button class=copy-to-clipboard title="Efficient $Φ$-Regret Minimization with Low-Degree Swap Deviations in Extensive-Form Games" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09670v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09670v2.pdf filename=2402.09670v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent breakthrough results by Dagan, Daskalakis, Fishelson and Golowich [2023] and Peng and Rubinstein [2023] established an efficient algorithm attaining at most $\epsilon$ swap regret over extensive-form strategy spaces of dimension $N$ in $N^{\tilde O(1/\epsilon)}$ rounds. On the other extreme, Farina and Pipis [2023] developed an efficient algorithm for minimizing the weaker notion of linear-swap regret in $\mathsf{poly}(N)/\epsilon^2$ rounds. In this paper, we take a step toward bridging the gap between those two results. We introduce the set of $k$-mediator deviations, which generalize the untimed communication deviations recently introduced by Zhang, Farina and Sandholm [2024] to the case of having multiple mediators. We develop parameterized algorithms for minimizing the regret with respect to this set of deviations in $N^{O(k)}/\epsilon^2$ rounds. This closes the gap in the sense that $k=1$ recovers linear swap regret, while $k=N$ recovers swap regret. Moreover, by relating $k$-mediator deviations to low-degree polynomials, we show that regret minimization against degree-$k$ polynomial swap deviations is achievable in $N^{O(kd)^3}/\epsilon^2$ rounds, where $d$ is the depth of the game, assuming constant branching factor. For a fixed degree $k$, this is polynomial for Bayesian games and quasipolynomial more broadly when $d = \mathsf{polylog} N$ &ndash; the usual balancedness assumption on the game tree.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=12--238249-benchmarking-the-operation-of-quantum-heuristics-and-ising-machines-scoring-parameter-setting-strategies-on-optimization-applications-david-e-bernal-neira-et-al-2024>(1/2 | 238/249) Benchmarking the Operation of Quantum Heuristics and Ising Machines: Scoring Parameter Setting Strategies on Optimization Applications (David E. Bernal Neira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David E. Bernal Neira, Robin Brown, Pratik Sathe, Filip Wudarski, Marco Pavone, Eleanor G. Rieffel, Davide Venturelli. (2024)<br><strong>Benchmarking the Operation of Quantum Heuristics and Ising Machines: Scoring Parameter Setting Strategies on Optimization Applications</strong><br><button class=copy-to-clipboard title="Benchmarking the Operation of Quantum Heuristics and Ising Machines: Scoring Parameter Setting Strategies on Optimization Applications" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-ET, quant-ph, quant-ph, stat-CO, stat-ME<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10255v1.pdf filename=2402.10255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We discuss guidelines for evaluating the performance of parameterized stochastic solvers for optimization problems, with particular attention to systems that employ novel hardware, such as digital quantum processors running variational algorithms, analog processors performing quantum annealing, or coherent Ising Machines. We illustrate through an example a <b>benchmarking</b> procedure grounded in the statistical analysis of the expectation of a given performance metric measured in a test environment. In particular, we discuss the necessity and cost of setting parameters that affect the algorithm&rsquo;s performance. The optimal value of these parameters could vary significantly between instances of the same target problem. We present an open-source software package that facilitates the design, evaluation, and visualization of practical parameter tuning strategies for complex use of the heterogeneous components of the solver. We examine in detail an example using parallel tempering and a simulator of a photonic Coherent Ising Machine computing and display the scoring of an illustrative baseline family of parameter-setting strategies that feature an exploration-exploitation trade-off.</p></p class="citation"></blockquote><h3 id=22--239249-linear-depth-qft-over-ibm-heavy-hex-architecture-xiangyu-gao-et-al-2024>(2/2 | 239/249) Linear Depth QFT over IBM Heavy-hex Architecture (Xiangyu Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyu Gao, Yuwei Jin, Minghao Guo, Henry Chen, Eddy Z. Zhang. (2024)<br><strong>Linear Depth QFT over IBM Heavy-hex Architecture</strong><br><button class=copy-to-clipboard title="Linear Depth QFT over IBM Heavy-hex Architecture" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AR, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09705v1.pdf filename=2402.09705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compiling a given quantum algorithm into a target hardware architecture is a challenging optimization problem. The compiler must take into consideration the coupling <b>graph</b> of physical qubits and the gate operation dependencies. The existing noise in hardware architectures requires the compilation to use as few running cycles as possible. Existing approaches include using SAT solver or heuristics to complete the mapping but these may cause the issue of either long compilation time (e.g., timeout after hours) or suboptimal compilation results in terms of running cycles (e.g., exponentially increasing number of total cycles). In this paper, we propose an efficient mapping approach for Quantum Fourier Transformation (QFT) circuits over the existing IBM heavy-hex architecture. Such proposal first of all turns the architecture into a structure consisting of a straight line with dangling qubits, and then do the mapping over this generated structure recursively. The calculation shows that there is a linear depth upper bound for the time complexity of these structures and for a special case where there is 1 dangling qubit in every 5 qubits, the time complexity is 5N+O(1). All these results are better than state of the art methods.</p></p class="citation"></blockquote><h2 id=cssc-1>cs.SC (1)</h2><h3 id=11--240249-fast-interpolation-and-multiplication-of-unbalanced-polynomials-pascal-giorgi-et-al-2024>(1/1 | 240/249) Fast interpolation and multiplication of unbalanced polynomials (Pascal Giorgi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pascal Giorgi, Bruno Grenet, Armelle Perret du Cray, Daniel S. Roche. (2024)<br><strong>Fast interpolation and multiplication of unbalanced polynomials</strong><br><button class=copy-to-clipboard title="Fast interpolation and multiplication of unbalanced polynomials" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SC<br>Categories: cs-CC, cs-SC, cs.SC<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10139v1.pdf filename=2402.10139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the classical problems of interpolating a polynomial given a <b>black</b> <b>box</b> for evaluation, and of multiplying two polynomials, in the setting where the bit-lengths of the coefficients may vary widely, so-called unbalanced polynomials. Writing s for the total bit-length and D for the degree, our new algorithms have expected running time $\tilde{O}(s \log D)$, whereas previous methods for (resp.) dense or sparse arithmetic have at least $\tilde{O}(sD)$ or $\tilde{O}(s^2)$ bit complexity.</p></p class="citation"></blockquote><h2 id=csds-4>cs.DS (4)</h2><h3 id=14--241249-non-adaptive-bellman-ford-yens-improvement-is-optimal-jialu-hu-et-al-2024>(1/4 | 241/249) Non-adaptive Bellman-Ford: Yen&rsquo;s improvement is optimal (Jialu Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialu Hu, László Kozma. (2024)<br><strong>Non-adaptive Bellman-Ford: Yen&rsquo;s improvement is optimal</strong><br><button class=copy-to-clipboard title="Non-adaptive Bellman-Ford: Yen's improvement is optimal" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10343v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10343v1.pdf filename=2402.10343v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Bellman-Ford algorithm for single-source shortest paths repeatedly updates tentative distances in an operation called relaxing an edge. In several important applications a non-adaptive (oblivious) implementation is preferred, which means fixing the entire sequence of relaxations upfront, independent of the edge-weights. In a dense <b>graph</b> on $n$ vertices, the algorithm in its standard form performs $(1 + o(1))n^3$ relaxations. An improvement by Yen from 1970 reduces the number of relaxations by a factor of two. We show that no further constant-factor improvements are possible, and every non-adaptive deterministic algorithm based on relaxations must perform $(\frac{1}{2} - o(1))n^3$ steps. This improves an earlier lower bound of Eppstein of $(\frac{1}{6} - o(1))n^3$. Given that a non-adaptive randomized variant of Bellman-Ford with at most $(\frac{1}{3} + o(1))n^3$ relaxations (with high probability) is known, our result implies a strict separation between deterministic and randomized strategies, answering an open question of Eppstein.</p></p class="citation"></blockquote><h3 id=24--242249-correlation-clustering-with-vertex-splitting-matthias-bentert-et-al-2024>(2/4 | 242/249) Correlation Clustering with Vertex Splitting (Matthias Bentert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthias Bentert, Alex Crane, Pål Grønås Drange, Felix Reidl, Blair D. Sullivan. (2024)<br><strong>Correlation Clustering with Vertex Splitting</strong><br><button class=copy-to-clipboard title="Correlation Clustering with Vertex Splitting" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: 05C85, F-2-2; I-5-3, cs-DS, cs-SI, cs.DS<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10335v1.pdf filename=2402.10335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore Cluster Editing and its generalization Correlation <b>Clustering</b> with a new operation called permissive vertex splitting which addresses finding overlapping clusters in the face of uncertain information. We determine that both problems are NP-hard, yet they exhibit significant differences in parameterized complexity and approximability. For Cluster Editing with Permissive Vertex Splitting, we show a polynomial kernel when parameterized by the solution size and develop a polynomial-time algorithm with approximation factor 7. In the case of Correlation <b>Clustering,</b> we establish para-NP-hardness when parameterized by solution size and demonstrate that computing an $n^{1-\epsilon}$-approximation is NP-hard for any constant $\epsilon > 0$. Additionally, we extend the established link between Correlation <b>Clustering</b> and Multicut to the setting with permissive vertex splitting.</p></p class="citation"></blockquote><h3 id=34--243249-parameterized-vertex-integrity-revisited-tesshu-hanaka-et-al-2024>(3/4 | 243/249) Parameterized Vertex Integrity Revisited (Tesshu Hanaka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tesshu Hanaka, Michael Lampis, Manolis Vasilakis, Kanae Yoshiwatari. (2024)<br><strong>Parameterized Vertex Integrity Revisited</strong><br><button class=copy-to-clipboard title="Parameterized Vertex Integrity Revisited" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CC, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09971v1.pdf filename=2402.09971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vertex integrity is a <b>graph</b> parameter that measures the connectivity of a <b>graph.</b> Informally, its meaning is that a <b>graph</b> has small vertex integrity if it has a small separator whose removal disconnects the <b>graph</b> into connected components which are themselves also small. <b>Graphs</b> with low vertex integrity are extremely structured; this renders many hard problems tractable and has recently attracted interest in this notion from the parameterized complexity community. In this paper we revisit the NP-complete problem of computing the vertex integrity of a given <b>graph</b> from the point of view of structural parameterizations. We present a number of new results, which also answer some recently posed open questions from the literature. Specifically: We show that unweighted vertex integrity is W[1]-hard parameterized by treedepth; we show that the problem remains W[1]-hard if we parameterize by feedback edge set size (via a reduction from a Bin Packing variant which may be of independent interest); and complementing this we show that the problem is FPT by max-leaf number. Furthermore, for weighted vertex integrity, we show that the problem admits a single-exponential FPT algorithm parameterized by vertex cover or by modular width, the latter result improving upon a previous algorithm which required weights to be polynomially bounded.</p></p class="citation"></blockquote><h3 id=44--244249-parameterized-algorithms-for-steiner-forest-in-bounded-width-graphs-andreas-emil-feldmann-et-al-2024>(4/4 | 244/249) Parameterized Algorithms for Steiner Forest in Bounded Width Graphs (Andreas Emil Feldmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Emil Feldmann, Michael Lampis. (2024)<br><strong>Parameterized Algorithms for Steiner Forest in Bounded Width Graphs</strong><br><button class=copy-to-clipboard title="Parameterized Algorithms for Steiner Forest in Bounded Width Graphs" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09835v1.pdf filename=2402.09835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we reassess the parameterized complexity and approximability of the well-studied Steiner Forest problem in several <b>graph</b> classes of bounded width. The problem takes an edge-weighted <b>graph</b> and pairs of vertices as input, and the aim is to find a minimum cost subgraph in which each given vertex pair lies in the same connected component. It is known that this problem is APX-hard in general, and NP-hard on <b>graphs</b> of treewidth 3, treedepth 4, and feedback vertex set size 2. However, Bateni, Hajiaghayi and Marx [JACM, 2011] gave an approximation scheme with a runtime of $n^{O(\frac{k^2}{\varepsilon})}$ on <b>graphs</b> of treewidth $k$. Our main result is a much faster efficient parameterized approximation scheme (EPAS) with a runtime of $2^{O(\frac{k^2}{\varepsilon} \log \frac{k^2}{\varepsilon})} \cdot n^{O(1)}$. If $k$ instead is the vertex cover number of the input <b>graph,</b> we show how to compute the optimum solution in $2^{O(k \log k)} \cdot n^{O(1)}$ time, and we also prove that this runtime dependence on $k$ is asymptotically best possible, under ETH. Furthermore, if $k$ is the size of a feedback edge set, then we obtain a faster $2^{O(k)} \cdot n^{O(1)}$ time algorithm, which again cannot be improved under ETH.</p></p class="citation"></blockquote><h2 id=mathna-1>math.NA (1)</h2><h3 id=11--245249-a-new-type-of-simplified-inverse-lax-wendroff-boundary-treatment-i-hyperbolic-conservation-laws-shihao-liu-et-al-2024>(1/1 | 245/249) A new type of simplified inverse Lax-Wendroff boundary treatment I: hyperbolic conservation laws (Shihao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shihao Liu, Tingting Li, Ziqiang Cheng, Yan Jiang, Chi-Wang Shu, Mengping Zhang. (2024)<br><strong>A new type of simplified inverse Lax-Wendroff boundary treatment I: hyperbolic conservation laws</strong><br><button class=copy-to-clipboard title="A new type of simplified inverse Lax-Wendroff boundary treatment I: hyperbolic conservation laws" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10152v1.pdf filename=2402.10152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we design a new kind of high order inverse Lax-Wendroff (ILW) boundary treatment for solving hyperbolic conservation laws with finite difference method on a Cartesian mesh. This new ILW method decomposes the construction of ghost point values near inflow boundary into two steps: interpolation and extrapolation. At first, we impose values of some artificial auxiliary points through a polynomial interpolating the interior points near the boundary. Then, we will construct a Hermite extrapolation based on those auxiliary point values and the spatial derivatives at boundary obtained via the ILW procedure. This polynomial will give us the approximation to the ghost point value. By an appropriate selection of those artificial auxiliary points, high-order accuracy and stable results can be achieved. Moreover, theoretical analysis indicates that comparing with the original ILW method, especially for higher order accuracy, the new proposed one would require fewer terms using the relatively complicated ILW procedure and thus improve computational efficiency on the premise of maintaining accuracy and stability. We perform numerical experiments on several <b>benchmarks,</b> including one- and two-dimensional scalar equations and systems. The robustness and efficiency of the proposed scheme is numerically verified.</p></p class="citation"></blockquote><h2 id=physicsao-ph-1>physics.ao-ph (1)</h2><h3 id=11--246249-a-deep-learning-approach-to-radar-based-qpe-ting-shuo-yo-et-al-2024>(1/1 | 246/249) A Deep Learning Approach to Radar-based QPE (Ting-Shuo Yo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ting-Shuo Yo, Shih-Hao Su, Jung-Lien Chu, Chiao-Wei Chang, Hung-Chi Kuo. (2024)<br><strong>A Deep Learning Approach to Radar-based QPE</strong><br><button class=copy-to-clipboard title="A Deep Learning Approach to Radar-based QPE" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.ao-ph<br>Categories: cs-LG, eess-SP, physics-ao-ph, physics.ao-ph<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09846v1.pdf filename=2402.09846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we propose a volume-to-point framework for quantitative precipitation estimation (QPE) based on the Quantitative Precipitation Estimation and Segregation Using Multiple Sensor (QPESUMS) Mosaic Radar data set. With a data volume consisting of the time series of gridded radar reflectivities over the Taiwan area, we used machine learning algorithms to establish a statistical model for QPE in weather stations. The model extracts spatial and temporal features from the input data volume and then associates these features with the location-specific precipitations. In contrast to QPE methods based on the Z-R relation, we leverage the machine learning algorithms to automatically detect the evolution and movement of weather systems and associate these patterns to a location with specific topographic attributes. Specifically, we evaluated this framework with the hourly precipitation data of 45 weather stations in Taipei during 2013-2016. In comparison to the operational QPE scheme used by the Central Weather Bureau, the volume-to-point framework performed comparably well in general cases and excelled in detecting heavy-rainfall events. By using the current results as the reference <b>benchmark,</b> the proposed method can integrate the heterogeneous data sources and potentially improve the forecast in extreme precipitation scenarios.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--247249-coevolution-of-relationship-and-interaction-in-cooperative-dynamical-multiplex-networks-xiaojin-xiong-et-al-2024>(1/1 | 247/249) Coevolution of relationship and interaction in cooperative dynamical multiplex networks (Xiaojin Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaojin Xiong, Ziyan Zeng, Minyu Feng, Attila Szolnoki. (2024)<br><strong>Coevolution of relationship and interaction in cooperative dynamical multiplex networks</strong><br><button class=copy-to-clipboard title="Coevolution of relationship and interaction in cooperative dynamical multiplex networks" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cond-mat-stat-mech, cs-GT, nlin-PS, physics-soc-ph, physics.soc-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09804v1.pdf filename=2402.09804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While actors in a population can interact with anyone else freely, social relations significantly influence our inclination towards particular individuals. The consequence of such interactions, however, may also form the intensity of our relations established earlier. These dynamical processes are captured via a coevolutionary model staged in multiplex networks with two distinct layers. In a so-called relationship layer the weights of edges among players may change in time as a consequence of games played in the alternative interaction layer. As an reasonable assumption, bilateral cooperation confirms while mutual defection weakens these weight factors. Importantly, the fitness of a player, which basically determines the success of a strategy imitation, depends not only on the payoff collected from interactions, but also on the individual relationship index calculated from the mentioned weight factors of related edges. Within the framework of weak prisoner&rsquo;s dilemma situation we explore the potential outcomes of the mentioned coevolutionary process where we assume different topologies for relationship layer. We find that higher average degree of the relationship <b>graph</b> is more beneficial to maintain cooperation in regular <b>graphs,</b> but the randomness of links could be a decisive factor in harsh situations. Surprisingly, a stronger coupling between relationship index and fitness discourage the evolution of cooperation by weakening the direct consequence of a strategy change. To complete our study we also monitor how the distribution of relationship index vary and detect a strong relation between its polarization and the general cooperation level.</p></p class="citation"></blockquote><h2 id=eesssy-1>eess.SY (1)</h2><h3 id=11--248249-quickest-detection-of-false-data-injection-attack-in-distributed-process-tracking-saqib-abbas-baba-et-al-2024>(1/1 | 248/249) Quickest Detection of False Data Injection Attack in Distributed Process Tracking (Saqib Abbas Baba et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saqib Abbas Baba, Arpan Chattopadhyay. (2024)<br><strong>Quickest Detection of False Data Injection Attack in Distributed Process Tracking</strong><br><button class=copy-to-clipboard title="Quickest Detection of False Data Injection Attack in Distributed Process Tracking" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SP, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09743v1.pdf filename=2402.09743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of detecting false data injection (FDI) attacks in a distributed network without a fusion center, represented by a connected <b>graph</b> among multiple agent nodes. Each agent node is equipped with a sensor, and uses a Kalman consensus information filter (KCIF) to track a discrete time global process with linear dynamics and additive Gaussian noise. The state estimate of the global process at any sensor is computed from the local observation history and the information received by that agent node from its neighbors. At an unknown time, an attacker starts altering the local observation of one agent node. In the Bayesian setting where there is a known prior distribution of the attack beginning instant, we formulate a Bayesian quickest change detection (QCD) problem for FDI detection in order to minimize the mean detection delay subject to a false alarm probability constraint. While it is well-known that the optimal Bayesian QCD rule involves checking the Shriyaev&rsquo;s statistic against a threshold, we demonstrate how to compute the Shriyaev&rsquo;s statistic at each node in a recursive fashion given our non-i.i.d. observations. Next, we consider non-Bayesian QCD where the attack begins at an arbitrary and unknown time, and the detector seeks to minimize the worst case detection delay subject to a constraint on the mean time to false alarm and probability of misidentification. We use the multiple hypothesis sequential probability ratio test for attack detection and identification at each sensor. For unknown attack strategy, we use the window-limited generalized likelihood ratio (WL-GLR) algorithm to solve the QCD problem. Numerical results demonstrate the performances and trade-offs of the proposed algorithms.</p></p class="citation"></blockquote><h2 id=csos-1>cs.OS (1)</h2><h3 id=11--249249-a-system-level-dynamic-binary-translator-using-automatically-learned-translation-rules-jinhu-jiang-et-al-2024>(1/1 | 249/249) A System-Level Dynamic Binary Translator using Automatically-Learned Translation Rules (Jinhu Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhu Jiang, Chaoyi Liang, Rongchao Dong, Zhaohui Yang, Zhongjun Zhou, Wenwen Wang, Pen-Chung Yew, Weihua Zhang. (2024)<br><strong>A System-Level Dynamic Binary Translator using Automatically-Learned Translation Rules</strong><br><button class=copy-to-clipboard title="A System-Level Dynamic Binary Translator using Automatically-Learned Translation Rules" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.OS<br>Categories: cs-OS, cs-PF, cs.OS<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09688v1.pdf filename=2402.09688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>System-level emulators have been used extensively for system design, debugging and evaluation. They work by providing a system-level virtual machine to support a guest operating system (OS) running on a platform with the same or different native OS that uses the same or different instruction-set architecture. For such system-level emulation, dynamic binary translation (DBT) is one of the core technologies. A recently proposed learning-based DBT approach has shown a significantly improved performance with a higher quality of translated code using automatically learned translation rules. However, it has only been applied to user-level emulation, and not yet to system-level emulation. In this paper, we explore the feasibility of applying this approach to improve system-level emulation, and use QEMU to build a prototype. &mldr; To achieve better performance, we leverage several optimizations that include coordination overhead reduction to reduce the overhead of each coordination, and coordination elimination and code scheduling to reduce the coordination frequency. Experimental results show that it can achieve an average of 1.36X speedup over QEMU 6.1 with negligible coordination overhead in the system emulation mode using SPEC CINT2006 as application <b>benchmarks</b> and 1.15X on real-world applications.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.16</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.18</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-49>cs.CL (49)</a><ul><li><a href=#149--1249-prompt-based-bias-calibration-for-better-zerofew-shot-learning-of-language-models-kang-he-et-al-2024>(1/49 | 1/249) Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models (Kang He et al., 2024)</a></li><li><a href=#249--2249-self-augmented-in-context-learning-for-unsupervised-word-translation-yaoyiran-li-et-al-2024>(2/49 | 2/249) Self-Augmented In-Context Learning for Unsupervised Word Translation (Yaoyiran Li et al., 2024)</a></li><li><a href=#349--3249-unlocking-structure-measuring-introducing-pdd-an-automatic-metric-for-positional-discourse-coherence-yinhong-liu-et-al-2024>(3/49 | 3/249) Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence (Yinhong Liu et al., 2024)</a></li><li><a href=#449--4249-rs-dpo-a-hybrid-rejection-sampling-and-direct-preference-optimization-method-for-alignment-of-large-language-models-saeed-khaki-et-al-2024>(4/49 | 4/249) RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models (Saeed Khaki et al., 2024)</a></li><li><a href=#549--5249-pal-proxy-guided-black-box-attack-on-large-language-models-chawin-sitawarin-et-al-2024>(5/49 | 5/249) PAL: Proxy-Guided Black-Box Attack on Large Language Models (Chawin Sitawarin et al., 2024)</a></li><li><a href=#649--6249-answer-is-all-you-need-instruction-following-text-embedding-via-answering-the-question-letian-peng-et-al-2024>(6/49 | 6/249) Answer is All You Need: Instruction-following Text Embedding via Answering the Question (Letian Peng et al., 2024)</a></li><li><a href=#749--7249-selective-reflection-tuning-student-selected-data-recycling-for-llm-instruction-tuning-ming-li-et-al-2024>(7/49 | 7/249) Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning (Ming Li et al., 2024)</a></li><li><a href=#849--8249-model-compression-and-efficient-inference-for-large-language-models-a-survey-wenxiao-wang-et-al-2024>(8/49 | 8/249) Model Compression and Efficient Inference for Large Language Models: A Survey (Wenxiao Wang et al., 2024)</a></li><li><a href=#949--9249-entaile-introducing-textual-entailment-in-commonsense-knowledge-graph-completion-ying-su-et-al-2024>(9/49 | 9/249) EntailE: Introducing Textual Entailment in Commonsense Knowledge Graph Completion (Ying Su et al., 2024)</a></li><li><a href=#1049--10249-lapdoc-layout-aware-prompting-for-documents-marcel-lamott-et-al-2024>(10/49 | 10/249) LAPDoc: Layout-Aware Prompting for Documents (Marcel Lamott et al., 2024)</a></li><li><a href=#1149--11249-biomistral-a-collection-of-open-source-pretrained-large-language-models-for-medical-domains-yanis-labrak-et-al-2024>(11/49 | 11/249) BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains (Yanis Labrak et al., 2024)</a></li><li><a href=#1249--12249-openmathinstruct-1-a-18-million-math-instruction-tuning-dataset-shubham-toshniwal-et-al-2024>(12/49 | 12/249) OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset (Shubham Toshniwal et al., 2024)</a></li><li><a href=#1349--13249-quantized-embedding-vectors-for-controllable-diffusion-language-models-cheng-kang-et-al-2024>(13/49 | 13/249) Quantized Embedding Vectors for Controllable Diffusion Language Models (Cheng Kang et al., 2024)</a></li><li><a href=#1449--14249-crafting-a-good-prompt-or-providing-exemplary-dialogues-a-study-of-in-context-learning-for-persona-based-dialogue-generation-jiashu-pu-et-al-2024>(14/49 | 14/249) Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation (Jiashu Pu et al., 2024)</a></li><li><a href=#1549--15249-nuteprune-efficient-progressive-pruning-with-numerous-teachers-for-large-language-models-shengrui-li-et-al-2024>(15/49 | 15/249) NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models (Shengrui Li et al., 2024)</a></li><li><a href=#1649--16249-chain-of-thought-reasoning-without-prompting-xuezhi-wang-et-al-2024>(16/49 | 16/249) Chain-of-Thought Reasoning Without Prompting (Xuezhi Wang et al., 2024)</a></li><li><a href=#1749--17249-enhancing-large-language-models-with-pseudo--and-multisource--knowledge-graphs-for-open-ended-question-answering-jiaxiang-liu-et-al-2024>(17/49 | 17/249) Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering (Jiaxiang Liu et al., 2024)</a></li><li><a href=#1849--18249-generative-representational-instruction-tuning-niklas-muennighoff-et-al-2024>(18/49 | 18/249) Generative Representational Instruction Tuning (Niklas Muennighoff et al., 2024)</a></li><li><a href=#1949--19249-uncertainty-decomposition-and-quantification-for-in-context-learning-of-large-language-models-chen-ling-et-al-2024>(19/49 | 19/249) Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models (Chen Ling et al., 2024)</a></li><li><a href=#2049--20249-grounding-language-model-with-chunking-free-in-context-retrieval-hongjin-qian-et-al-2024>(20/49 | 20/249) Grounding Language Model with Chunking-Free In-Context Retrieval (Hongjin Qian et al., 2024)</a></li><li><a href=#2149--21249-an-analysis-of-language-frequency-and-error-correction-for-esperanto-junhong-liang-2024>(21/49 | 21/249) An Analysis of Language Frequency and Error Correction for Esperanto (Junhong Liang, 2024)</a></li><li><a href=#2249--22249-unmemorization-in-large-language-models-via-self-distillation-and-deliberate-imagination-yijiang-river-dong-et-al-2024>(22/49 | 22/249) Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination (Yijiang River Dong et al., 2024)</a></li><li><a href=#2349--23249-improving-non-autoregressive-machine-translation-with-error-exposure-and-consistency-regularization-xinran-chen-et-al-2024>(23/49 | 23/249) Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization (Xinran Chen et al., 2024)</a></li><li><a href=#2449--24249-both-matter-enhancing-the-emotional-intelligence-of-large-language-models-without-compromising-the-general-intelligence-weixiang-zhao-et-al-2024>(24/49 | 24/249) Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence (Weixiang Zhao et al., 2024)</a></li><li><a href=#2549--25249-towards-safer-large-language-models-through-machine-unlearning-zheyuan-liu-et-al-2024>(25/49 | 25/249) Towards Safer Large Language Models through Machine Unlearning (Zheyuan Liu et al., 2024)</a></li><li><a href=#2649--26249-efficient-language-adaptive-pre-training-extending-state-of-the-art-large-language-models-for-polish-szymon-ruciński-2024>(26/49 | 26/249) Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish (Szymon Ruciński, 2024)</a></li><li><a href=#2749--27249-qurating-selecting-high-quality-data-for-training-language-models-alexander-wettig-et-al-2024>(27/49 | 27/249) QuRating: Selecting High-Quality Data for Training Language Models (Alexander Wettig et al., 2024)</a></li><li><a href=#2849--28249-align-before-attend-aligning-visual-and-textual-features-for-multimodal-hateful-content-detection-eftekhar-hossain-et-al-2024>(28/49 | 28/249) Align before Attend: Aligning Visual and Textual Features for Multimodal Hateful Content Detection (Eftekhar Hossain et al., 2024)</a></li><li><a href=#2949--29249-sportsmetrics-blending-text-and-numerical-data-to-understand-information-fusion-in-llms-yebowen-hu-et-al-2024>(29/49 | 29/249) SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs (Yebowen Hu et al., 2024)</a></li><li><a href=#3049--30249-tdag-a-multi-agent-framework-based-on-dynamic-task-decomposition-and-agent-generation-yaoxiang-wang-et-al-2024>(30/49 | 30/249) TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation (Yaoxiang Wang et al., 2024)</a></li><li><a href=#3149--31249-ai-hospital-interactive-evaluation-and-collaboration-of-llms-as-intern-doctors-for-clinical-diagnosis-zhihao-fan-et-al-2024>(31/49 | 31/249) AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis (Zhihao Fan et al., 2024)</a></li><li><a href=#3249--32249-a-trembling-house-of-cards-mapping-adversarial-attacks-against-language-agents-lingbo-mo-et-al-2024>(32/49 | 32/249) A Trembling House of Cards? Mapping Adversarial Attacks against Language Agents (Lingbo Mo et al., 2024)</a></li><li><a href=#3349--33249-knowledge-infused-llm-powered-conversational-health-agent-a-case-study-for-diabetes-patients-mahyar-abbasian-et-al-2024>(33/49 | 33/249) Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients (Mahyar Abbasian et al., 2024)</a></li><li><a href=#3449--34249-controllm-crafting-diverse-personalities-for-language-models-yixuan-weng-et-al-2024>(34/49 | 34/249) ControlLM: Crafting Diverse Personalities for Language Models (Yixuan Weng et al., 2024)</a></li><li><a href=#3549--35249-case-study-testing-model-capabilities-in-some-reasoning-tasks-min-zhang-et-al-2024>(35/49 | 35/249) Case Study: Testing Model Capabilities in Some Reasoning Tasks (Min Zhang et al., 2024)</a></li><li><a href=#3649--36249-camouflage-is-all-you-need-evaluating-and-enhancing-language-model-robustness-against-camouflage-adversarial-attacks-álvaro-huertas-garcía-et-al-2024>(36/49 | 36/249) Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks (Álvaro Huertas-García et al., 2024)</a></li><li><a href=#3749--37249-do-llms-know-about-hallucination-an-empirical-investigation-of-llms-hidden-states-hanyu-duan-et-al-2024>(37/49 | 37/249) Do LLMs Know about Hallucination? An Empirical Investigation of LLM&rsquo;s Hidden States (Hanyu Duan et al., 2024)</a></li><li><a href=#3849--38249-a-human-inspired-reading-agent-with-gist-memory-of-very-long-contexts-kuang-huei-lee-et-al-2024>(38/49 | 38/249) A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts (Kuang-Huei Lee et al., 2024)</a></li><li><a href=#3949--39249-efuf-efficient-fine-grained-unlearning-framework-for-mitigating-hallucinations-in-multimodal-large-language-models-shangyu-xing-et-al-2024>(39/49 | 39/249) EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models (Shangyu Xing et al., 2024)</a></li><li><a href=#4049--40249-toad-task-oriented-automatic-dialogs-with-diverse-response-styles-yinhong-liu-et-al-2024>(40/49 | 40/249) TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles (Yinhong Liu et al., 2024)</a></li><li><a href=#4149--41249-a-dataset-of-open-domain-question-answering-with-multiple-span-answers-zhiyi-luo-et-al-2024>(41/49 | 41/249) A Dataset of Open-Domain Question Answering with Multiple-Span Answers (Zhiyi Luo et al., 2024)</a></li><li><a href=#4249--42249-data-engineering-for-scaling-language-models-to-128k-context-yao-fu-et-al-2024>(42/49 | 42/249) Data Engineering for Scaling Language Models to 128K Context (Yao Fu et al., 2024)</a></li><li><a href=#4349--43249-multi-word-tokenization-for-sequence-compression-leonidas-gee-et-al-2024>(43/49 | 43/249) Multi-Word Tokenization for Sequence Compression (Leonidas Gee et al., 2024)</a></li><li><a href=#4449--44249-de-cop-detecting-copyrighted-content-in-language-models-training-data-andré-v-duarte-et-al-2024>(44/49 | 44/249) DE-COP: Detecting Copyrighted Content in Language Models Training Data (André V. Duarte et al., 2024)</a></li><li><a href=#4549--45249-how-to-discern-important-urgent-news-oleg-vasilyev-et-al-2024>(45/49 | 45/249) How to Discern Important Urgent News? (Oleg Vasilyev et al., 2024)</a></li><li><a href=#4649--46249-fast-vocabulary-transfer-for-language-model-compression-leonidas-gee-et-al-2024>(46/49 | 46/249) Fast Vocabulary Transfer for Language Model Compression (Leonidas Gee et al., 2024)</a></li><li><a href=#4749--47249-paying-attention-to-deflections-mining-pragmatic-nuances-for-whataboutism-detection-in-online-discourse-khiem-phi-et-al-2024>(47/49 | 47/249) Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse (Khiem Phi et al., 2024)</a></li><li><a href=#4849--48249-knowledge-of-pretrained-language-models-on-surface-information-of-tokens-tatsuya-hiraoka-et-al-2024>(48/49 | 48/249) Knowledge of Pretrained Language Models on Surface Information of Tokens (Tatsuya Hiraoka et al., 2024)</a></li><li><a href=#4949--49249-buster-a-business-transaction-entity-recognition-dataset-andrea-zugarini-et-al-2024>(49/49 | 49/249) BUSTER: a &lsquo;BUSiness Transaction Entity Recognition&rsquo; dataset (Andrea Zugarini et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--50249-best-arm-identification-for-prompt-learning-under-a-limited-budget-chengshuai-shi-et-al-2024>(1/3 | 50/249) Best Arm Identification for Prompt Learning under a Limited Budget (Chengshuai Shi et al., 2024)</a></li><li><a href=#23--51249-thompson-sampling-in-partially-observable-contextual-bandits-hongju-park-et-al-2024>(2/3 | 51/249) Thompson Sampling in Partially Observable Contextual Bandits (Hongju Park et al., 2024)</a></li><li><a href=#33--52249-nonlinear-spiked-covariance-matrices-and-signal-propagation-in-deep-neural-networks-zhichao-wang-et-al-2024>(3/3 | 52/249) Nonlinear spiked covariance matrices and signal propagation in deep neural networks (Zhichao Wang et al., 2024)</a></li></ul></li><li><a href=#cslg-66>cs.LG (66)</a><ul><li><a href=#166--53249-self-play-fine-tuning-of-diffusion-models-for-text-to-image-generation-huizhuo-yuan-et-al-2024>(1/66 | 53/249) Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation (Huizhuo Yuan et al., 2024)</a></li><li><a href=#266--54249-rewards-in-context-multi-objective-alignment-of-foundation-models-with-dynamic-preference-adjustment-rui-yang-et-al-2024>(2/66 | 54/249) Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment (Rui Yang et al., 2024)</a></li><li><a href=#366--55249-bitdelta-your-fine-tune-may-only-be-worth-one-bit-james-liu-et-al-2024>(3/66 | 55/249) BitDelta: Your Fine-Tune May Only Be Worth One Bit (James Liu et al., 2024)</a></li><li><a href=#466--56249-a-strongreject-for-empty-jailbreaks-alexandra-souly-et-al-2024>(4/66 | 56/249) A StrongREJECT for Empty Jailbreaks (Alexandra Souly et al., 2024)</a></li><li><a href=#566--57249-class-balanced-and-reinforced-active-learning-on-graphs-chengcheng-yu-et-al-2024>(5/66 | 57/249) Class-Balanced and Reinforced Active Learning on Graphs (Chengcheng Yu et al., 2024)</a></li><li><a href=#666--58249-covidhealth-a-benchmark-twitter-dataset-and-machine-learning-based-web-application-for-classifying-covid-19-discussions-mahathir-mohammad-bishal-et-al-2024>(6/66 | 58/249) COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web Application for Classifying COVID-19 Discussions (Mahathir Mohammad Bishal et al., 2024)</a></li><li><a href=#766--59249-multi-fidelity-methods-for-optimization-a-survey-ke-li-et-al-2024>(7/66 | 59/249) Multi-Fidelity Methods for Optimization: A Survey (Ke Li et al., 2024)</a></li><li><a href=#866--60249-can-we-soft-prompt-llms-for-graph-learning-tasks-zheyuan-liu-et-al-2024>(8/66 | 60/249) Can we soft prompt LLMs for graph learning tasks? (Zheyuan Liu et al., 2024)</a></li><li><a href=#966--61249-bridging-associative-memory-and-probabilistic-modeling-rylan-schaeffer-et-al-2024>(9/66 | 61/249) Bridging Associative Memory and Probabilistic Modeling (Rylan Schaeffer et al., 2024)</a></li><li><a href=#1066--62249-rethinking-information-structures-in-rlhf-reward-generalization-from-a-graph-theory-perspective-tianyi-qiu-et-al-2024>(10/66 | 62/249) Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective (Tianyi Qiu et al., 2024)</a></li><li><a href=#1166--63249-all-in-one-and-one-for-all-a-simple-yet-effective-method-towards-cross-domain-graph-pretraining-haihong-zhao-et-al-2024>(11/66 | 63/249) All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining (Haihong Zhao et al., 2024)</a></li><li><a href=#1266--64249-non-orthogonal-age-optimal-information-dissemination-in-vehicular-networks-a-meta-multi-objective-reinforcement-learning-approach-a-a-habob-et-al-2024>(12/66 | 64/249) Non-orthogonal Age-Optimal Information Dissemination in Vehicular Networks: A Meta Multi-Objective Reinforcement Learning Approach (A. A. Habob et al., 2024)</a></li><li><a href=#1366--65249-generative-ai-and-process-systems-engineering-the-next-frontier-benjamin-decardi-nelson-et-al-2024>(13/66 | 65/249) Generative AI and Process Systems Engineering: The Next Frontier (Benjamin Decardi-Nelson et al., 2024)</a></li><li><a href=#1466--66249-node-duplication-improves-cold-start-link-prediction-zhichun-guo-et-al-2024>(14/66 | 66/249) Node Duplication Improves Cold-start Link Prediction (Zhichun Guo et al., 2024)</a></li><li><a href=#1566--67249-quick-quantization-aware-interleaving-and-conflict-free-kernel-for-efficient-llm-inference-taesu-kim-et-al-2024>(15/66 | 67/249) QUICK: Quantization-aware Interleaving and Conflict-free Kernel for efficient LLM inference (Taesu Kim et al., 2024)</a></li><li><a href=#1666--68249-how-to-train-data-efficient-llms-noveen-sachdeva-et-al-2024>(16/66 | 68/249) How to Train Data-Efficient LLMs (Noveen Sachdeva et al., 2024)</a></li><li><a href=#1766--69249-large-language-models-for-forecasting-and-anomaly-detection-a-systematic-literature-review-jing-su-et-al-2024>(17/66 | 69/249) Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review (Jing Su et al., 2024)</a></li><li><a href=#1866--70249-smart-information-exchange-for-unsupervised-federated-learning-via-reinforcement-learning-seohyun-lee-et-al-2024>(18/66 | 70/249) Smart Information Exchange for Unsupervised Federated Learning via Reinforcement Learning (Seohyun Lee et al., 2024)</a></li><li><a href=#1966--71249-multi-excitation-projective-simulation-with-a-many-body-physics-inspired-inductive-bias-philip-a-lemaitre-et-al-2024>(19/66 | 71/249) Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias (Philip A. LeMaitre et al., 2024)</a></li><li><a href=#2066--72249-f-micl-understanding-and-generalizing-infonce-based-contrastive-learning-yiwei-lu-et-al-2024>(20/66 | 72/249) $f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning (Yiwei Lu et al., 2024)</a></li><li><a href=#2166--73249-utilizing-gans-for-fraud-detection-model-training-with-synthetic-transaction-data-mengran-zhu-et-al-2024>(21/66 | 73/249) Utilizing GANs for Fraud Detection: Model Training with Synthetic Transaction Data (Mengran Zhu et al., 2024)</a></li><li><a href=#2266--74249-revisiting-experience-replayable-conditions-taisuke-kobayashi-2024>(22/66 | 74/249) Revisiting Experience Replayable Conditions (Taisuke Kobayashi, 2024)</a></li><li><a href=#2366--75249-exploration-driven-policy-optimization-in-rlhf-theoretical-insights-on-efficient-data-utilization-yihan-du-et-al-2024>(23/66 | 75/249) Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization (Yihan Du et al., 2024)</a></li><li><a href=#2466--76249-hierarchical-state-space-models-for-continuous-sequence-to-sequence-modeling-raunaq-bhirangi-et-al-2024>(24/66 | 76/249) Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling (Raunaq Bhirangi et al., 2024)</a></li><li><a href=#2566--77249-recovering-the-pre-fine-tuning-weights-of-generative-models-eliahu-horwitz-et-al-2024>(25/66 | 77/249) Recovering the Pre-Fine-Tuning Weights of Generative Models (Eliahu Horwitz et al., 2024)</a></li><li><a href=#2666--78249-fedanchor-enhancing-federated-semi-supervised-learning-with-label-contrastive-loss-for-unlabeled-clients-xinchi-qiu-et-al-2024>(26/66 | 78/249) FedAnchor: Enhancing Federated Semi-Supervised Learning with Label Contrastive Loss for Unlabeled Clients (Xinchi Qiu et al., 2024)</a></li><li><a href=#2766--79249-revisiting-recurrent-reinforcement-learning-with-memory-monoids-steven-morad-et-al-2024>(27/66 | 79/249) Revisiting Recurrent Reinforcement Learning with Memory Monoids (Steven Morad et al., 2024)</a></li><li><a href=#2866--80249-performative-reinforcement-learning-in-gradually-shifting-environments-ben-rank-et-al-2024>(28/66 | 80/249) Performative Reinforcement Learning in Gradually Shifting Environments (Ben Rank et al., 2024)</a></li><li><a href=#2966--81249-language-models-with-conformal-factuality-guarantees-christopher-mohri-et-al-2024>(29/66 | 81/249) Language Models with Conformal Factuality Guarantees (Christopher Mohri et al., 2024)</a></li><li><a href=#3066--82249-reward-poisoning-attack-against-offline-reinforcement-learning-yinglun-xu-et-al-2024>(30/66 | 82/249) Reward Poisoning Attack Against Offline Reinforcement Learning (Yinglun Xu et al., 2024)</a></li><li><a href=#3166--83249-ising-on-the-graph-task-specific-graph-subsampling-via-the-ising-model-maria-bånkestad-et-al-2024>(31/66 | 83/249) Ising on the Graph: Task-specific Graph Subsampling via the Ising Model (Maria Bånkestad et al., 2024)</a></li><li><a href=#3266--84249-learnability-is-a-compact-property-julian-asilis-et-al-2024>(32/66 | 84/249) Learnability is a Compact Property (Julian Asilis et al., 2024)</a></li><li><a href=#3366--85249-what-to-do-when-your-discrete-optimization-is-the-size-of-a-neural-network-hugo-silva-et-al-2024>(33/66 | 85/249) What to Do When Your Discrete Optimization Is the Size of a Neural Network? (Hugo Silva et al., 2024)</a></li><li><a href=#3466--86249-interpretable-generative-adversarial-imitation-learning-wenliang-liu-et-al-2024>(34/66 | 86/249) Interpretable Generative Adversarial Imitation Learning (Wenliang Liu et al., 2024)</a></li><li><a href=#3566--87249-discrete-probabilistic-inference-as-control-in-multi-path-environments-tristan-deleu-et-al-2024>(35/66 | 87/249) Discrete Probabilistic Inference as Control in Multi-path Environments (Tristan Deleu et al., 2024)</a></li><li><a href=#3666--88249-an-evaluation-of-real-time-adaptive-sampling-change-point-detection-algorithm-using-kcusum-vijayalakshmi-saravanan-et-al-2024>(36/66 | 88/249) An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM (Vijayalakshmi Saravanan et al., 2024)</a></li><li><a href=#3766--89249-susfl-energy-aware-federated-learning-based-monitoring-for-sustainable-smart-farms-dian-chen-et-al-2024>(37/66 | 89/249) SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms (Dian Chen et al., 2024)</a></li><li><a href=#3866--90249-tracking-changing-probabilities-via-dynamic-learners-omid-madani-2024>(38/66 | 90/249) Tracking Changing Probabilities via Dynamic Learners (Omid Madani, 2024)</a></li><li><a href=#3966--91249-balancing-the-causal-effects-in-class-incremental-learning-junhao-zheng-et-al-2024>(39/66 | 91/249) Balancing the Causal Effects in Class-Incremental Learning (Junhao Zheng et al., 2024)</a></li><li><a href=#4066--92249-explaining-probabilistic-models-with-distributional-values-luca-franceschi-et-al-2024>(40/66 | 92/249) Explaining Probabilistic Models with Distributional Values (Luca Franceschi et al., 2024)</a></li><li><a href=#4166--93249-a-data-driven-supervised-machine-learning-approach-to-estimating-global-ambient-air-pollution-concentrations-with-associated-prediction-intervals-liam-j-berrisford-et-al-2024>(41/66 | 93/249) A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals (Liam J Berrisford et al., 2024)</a></li><li><a href=#4266--94249-tinycl-an-efficient-hardware-architecture-for-continual-learning-on-autonomous-systems-eugenio-ressa-et-al-2024>(42/66 | 94/249) TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems (Eugenio Ressa et al., 2024)</a></li><li><a href=#4366--95249-dform-diffeomorphic-vector-field-alignment-for-assessing-dynamics-across-learned-models-ruiqi-chen-et-al-2024>(43/66 | 95/249) DFORM: Diffeomorphic vector field alignment for assessing dynamics across learned models (Ruiqi Chen et al., 2024)</a></li><li><a href=#4466--96249-benchmarking-federated-strategies-in-peer-to-peer-federated-learning-for-biomedical-data-jose-l-salmeron-et-al-2024>(44/66 | 96/249) Benchmarking federated strategies in Peer-to-Peer Federated learning for biomedical data (Jose L. Salmeron et al., 2024)</a></li><li><a href=#4566--97249-recommendations-for-baselines-and-benchmarking-approximate-gaussian-processes-sebastian-w-ober-et-al-2024>(45/66 | 97/249) Recommendations for Baselines and Benchmarking Approximate Gaussian Processes (Sebastian W. Ober et al., 2024)</a></li><li><a href=#4666--98249-large-scale-constrained-clustering-with-reinforcement-learning-benedikt-schesch-et-al-2024>(46/66 | 98/249) Large Scale Constrained Clustering With Reinforcement Learning (Benedikt Schesch et al., 2024)</a></li><li><a href=#4766--99249-is-continual-learning-ready-for-real-world-challenges-theodora-kontogianni-et-al-2024>(47/66 | 99/249) Is Continual Learning Ready for Real-world Challenges? (Theodora Kontogianni et al., 2024)</a></li><li><a href=#4866--100249-risk-sensitive-soft-actor-critic-for-robust-deep-reinforcement-learning-under-distribution-shifts-tobias-enders-et-al-2024>(48/66 | 100/249) Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under Distribution Shifts (Tobias Enders et al., 2024)</a></li><li><a href=#4966--101249-fedlion-faster-adaptive-federated-optimization-with-fewer-communication-zhiwei-tang-et-al-2024>(49/66 | 101/249) FedLion: Faster Adaptive Federated Optimization with Fewer Communication (Zhiwei Tang et al., 2024)</a></li><li><a href=#5066--102249-hypermagnet-a-magnetic-laplacian-based-hypergraph-neural-network-tatyana-benko-et-al-2024>(50/66 | 102/249) HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network (Tatyana Benko et al., 2024)</a></li><li><a href=#5166--103249-backdoor-attack-against-one-class-sequential-anomaly-detection-models-he-cheng-et-al-2024>(51/66 | 103/249) Backdoor Attack against One-Class Sequential Anomaly Detection Models (He Cheng et al., 2024)</a></li><li><a href=#5266--104249-information-capacity-regret-bounds-for-bandits-with-mediator-feedback-khaled-eldowa-et-al-2024>(52/66 | 104/249) Information Capacity Regret Bounds for Bandits with Mediator Feedback (Khaled Eldowa et al., 2024)</a></li><li><a href=#5366--105249-unlocking-the-potential-of-transformers-in-time-series-forecasting-with-sharpness-aware-minimization-and-channel-wise-attention-romain-ilbert-et-al-2024>(53/66 | 105/249) Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention (Romain Ilbert et al., 2024)</a></li><li><a href=#5466--106249-self-consistent-validation-for-machine-learning-electronic-structure-gengyuan-hu-et-al-2024>(54/66 | 106/249) Self-consistent Validation for Machine Learning Electronic Structure (Gengyuan Hu et al., 2024)</a></li><li><a href=#5566--107249-a-chaotic-maps-based-privacy-preserving-distributed-deep-learning-for-incomplete-and-non-iid-datasets-irina-arévalo-et-al-2024>(55/66 | 107/249) A chaotic maps-based privacy-preserving distributed deep learning for incomplete and Non-IID datasets (Irina Arévalo et al., 2024)</a></li><li><a href=#5666--108249-adaptive-federated-learning-in-heterogeneous-wireless-networks-with-independent-sampling-jiaxiang-geng-et-al-2024>(56/66 | 108/249) Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling (Jiaxiang Geng et al., 2024)</a></li><li><a href=#5766--109249-fedrdf-a-robust-and-dynamic-aggregation-function-against-poisoning-attacks-in-federated-learning-enrique-mármol-campos-et-al-2024>(57/66 | 109/249) FedRDF: A Robust and Dynamic Aggregation Function against Poisoning Attacks in Federated Learning (Enrique Mármol Campos et al., 2024)</a></li><li><a href=#5866--110249-diffusion-models-meet-contextual-bandits-with-large-action-spaces-imad-aouali-2024>(58/66 | 110/249) Diffusion Models Meet Contextual Bandits with Large Action Spaces (Imad Aouali, 2024)</a></li><li><a href=#5966--111249-accelerating-parallel-sampling-of-diffusion-models-zhiwei-tang-et-al-2024>(59/66 | 111/249) Accelerating Parallel Sampling of Diffusion Models (Zhiwei Tang et al., 2024)</a></li><li><a href=#6066--112249-why-are-sensitive-functions-hard-for-transformers-michael-hahn-et-al-2024>(60/66 | 112/249) Why are Sensitive Functions Hard for Transformers? (Michael Hahn et al., 2024)</a></li><li><a href=#6166--113249-enhancing-courier-scheduling-in-crowdsourced-last-mile-delivery-through-dynamic-shift-extensions-a-deep-reinforcement-learning-approach-zead-saleh-et-al-2024>(61/66 | 113/249) Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through Dynamic Shift Extensions: A Deep Reinforcement Learning Approach (Zead Saleh et al., 2024)</a></li><li><a href=#6266--114249-predictors-from-causal-features-do-not-generalize-better-to-new-domains-vivian-y-nastl-et-al-2024>(62/66 | 114/249) Predictors from causal features do not generalize better to new domains (Vivian Y. Nastl et al., 2024)</a></li><li><a href=#6366--115249-mimic-minimally-modified-counterfactuals-in-the-representation-space-shashwat-singh-et-al-2024>(63/66 | 115/249) MiMiC: Minimally Modified Counterfactuals in the Representation Space (Shashwat Singh et al., 2024)</a></li><li><a href=#6466--116249-privacy-attacks-in-decentralized-learning-abdellah-el-mrini-et-al-2024>(64/66 | 116/249) Privacy Attacks in Decentralized Learning (Abdellah El Mrini et al., 2024)</a></li><li><a href=#6566--117249-explaining-kernel-clustering-via-decision-trees-maximilian-fleissner-et-al-2024>(65/66 | 117/249) Explaining Kernel Clustering via Decision Trees (Maximilian Fleissner et al., 2024)</a></li><li><a href=#6666--118249-mc-dbn-a-deep-belief-network-based-model-for-modality-completion-zihong-luo-et-al-2024>(66/66 | 118/249) MC-DBN: A Deep Belief Network-Based Model for Modality Completion (Zihong Luo et al., 2024)</a></li></ul></li><li><a href=#csai-24>cs.AI (24)</a><ul><li><a href=#124--119249-fine-tuning-large-language-model-llm-artificial-intelligence-chatbots-in-ophthalmology-and-llm-based-evaluation-using-gpt-4-ting-fang-tan-et-al-2024>(1/24 | 119/249) Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4 (Ting Fang Tan et al., 2024)</a></li><li><a href=#224--120249-beyond-imitation-generating-human-mobility-from-context-aware-reasoning-with-large-language-models-chenyang-shao-et-al-2024>(2/24 | 120/249) Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models (Chenyang Shao et al., 2024)</a></li><li><a href=#324--121249-the-butterfly-effect-of-model-editing-few-edits-can-trigger-large-language-models-collapse-wanli-yang-et-al-2024>(3/24 | 121/249) The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse (Wanli Yang et al., 2024)</a></li><li><a href=#424--122249-generative-ai-in-the-construction-industry-a-state-of-the-art-analysis-ridwan-taiwo-et-al-2024>(4/24 | 122/249) Generative AI in the Construction Industry: A State-of-the-art Analysis (Ridwan Taiwo et al., 2024)</a></li><li><a href=#524--123249-user-modeling-and-user-profiling-a-comprehensive-survey-erasmo-purificato-et-al-2024>(5/24 | 123/249) User Modeling and User Profiling: A Comprehensive Survey (Erasmo Purificato et al., 2024)</a></li><li><a href=#624--124249-geoeval-benchmark-for-evaluating-llms-and-multi-modal-models-on-geometry-problem-solving-jiaxin-zhang-et-al-2024>(6/24 | 124/249) GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving (Jiaxin Zhang et al., 2024)</a></li><li><a href=#724--125249-inadequacies-of-large-language-model-benchmarks-in-the-era-of-generative-artificial-intelligence-timothy-r-mcintosh-et-al-2024>(7/24 | 125/249) Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence (Timothy R. McIntosh et al., 2024)</a></li><li><a href=#824--126249-loraretriever-input-aware-lora-retrieval-and-composition-for-mixed-tasks-in-the-wild-ziyu-zhao-et-al-2024>(8/24 | 126/249) LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild (Ziyu Zhao et al., 2024)</a></li><li><a href=#924--127249-aligning-crowd-feedback-via-distributional-preference-reward-modeling-dexun-li-et-al-2024>(9/24 | 127/249) Aligning Crowd Feedback via Distributional Preference Reward Modeling (Dexun Li et al., 2024)</a></li><li><a href=#1024--128249-federated-prompt-based-decision-transformer-for-customized-vr-services-in-mobile-edge-computing-system-tailin-zhou-et-al-2024>(10/24 | 128/249) Federated Prompt-based Decision Transformer for Customized VR Services in Mobile Edge Computing System (Tailin Zhou et al., 2024)</a></li><li><a href=#1124--129249-gpt-4s-assessment-of-its-performance-in-a-usmle-based-case-study-uttam-dhakal-et-al-2024>(11/24 | 129/249) GPT-4&rsquo;s assessment of its performance in a USMLE-based case study (Uttam Dhakal et al., 2024)</a></li><li><a href=#1224--130249-optimus-scalable-optimization-modeling-with-milp-solvers-and-large-language-models-ali-ahmaditeshnizi-et-al-2024>(12/24 | 130/249) OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models (Ali AhmadiTeshnizi et al., 2024)</a></li><li><a href=#1324--131249-zero-shot-reasoning-personalized-content-generation-without-the-cold-start-problem-davor-hafnar-et-al-2024>(13/24 | 131/249) Zero-Shot Reasoning: Personalized Content Generation Without the Cold Start Problem (Davor Hafnar et al., 2024)</a></li><li><a href=#1424--132249-jack-of-all-trades-master-of-some-a-multi-purpose-transformer-agent-quentin-gallouédec-et-al-2024>(14/24 | 132/249) Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent (Quentin Gallouédec et al., 2024)</a></li><li><a href=#1524--133249-swissnyf-tool-grounded-llm-agents-for-black-box-setting-somnath-sendhil-kumar-et-al-2024>(15/24 | 133/249) SwissNYF: Tool Grounded LLM Agents for Black Box Setting (Somnath Sendhil Kumar et al., 2024)</a></li><li><a href=#1624--134249-representation-learning-using-a-single-forward-pass-aditya-somasundaram-et-al-2024>(16/24 | 134/249) Representation Learning Using a Single Forward Pass (Aditya Somasundaram et al., 2024)</a></li><li><a href=#1724--135249-clifford-group-equivariant-simplicial-message-passing-networks-cong-liu-et-al-2024>(17/24 | 135/249) Clifford Group Equivariant Simplicial Message Passing Networks (Cong Liu et al., 2024)</a></li><li><a href=#1824--136249-generating-visual-stimuli-from-eeg-recordings-using-transformer-encoder-based-eeg-encoder-and-gan-rahul-mishra-et-al-2024>(18/24 | 136/249) Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN (Rahul Mishra et al., 2024)</a></li><li><a href=#1924--137249-towards-reducing-diagnostic-errors-with-interpretable-risk-prediction-denis-jered-mcinerney-et-al-2024>(19/24 | 137/249) Towards Reducing Diagnostic Errors with Interpretable Risk Prediction (Denis Jered McInerney et al., 2024)</a></li><li><a href=#2024--138249-reinforcement-learning-for-solving-stochastic-vehicle-routing-problem-with-time-windows-zangir-iklassov-et-al-2024>(20/24 | 138/249) Reinforcement Learning for Solving Stochastic Vehicle Routing Problem with Time Windows (Zangir Iklassov et al., 2024)</a></li><li><a href=#2124--139249-a-privacy-preserving-distributed-and-cooperative-fcm-based-learning-approach-for-cancer-research-jose-l-salmeron-et-al-2024>(21/24 | 139/249) A privacy-preserving, distributed and cooperative FCM-based learning approach for Cancer Research (Jose L. Salmeron et al., 2024)</a></li><li><a href=#2224--140249-agents-need-not-know-their-purpose-paulo-garcia-2024>(22/24 | 140/249) Agents Need Not Know Their Purpose (Paulo Garcia, 2024)</a></li><li><a href=#2324--141249-road-graph-generator-mapping-roads-at-construction-sites-from-gps-data-katarzyna-michałowska-et-al-2024>(23/24 | 141/249) Road Graph Generator: Mapping roads at construction sites from GPS data (Katarzyna Michałowska et al., 2024)</a></li><li><a href=#2424--142249-on-computing-plans-with-uniform-action-costs-alberto-pozanco-et-al-2024>(24/24 | 142/249) On Computing Plans with Uniform Action Costs (Alberto Pozanco et al., 2024)</a></li></ul></li><li><a href=#cscv-29>cs.CV (29)</a><ul><li><a href=#129--143249-vigeo-an-assessment-of-vision-gnns-in-earth-observation-luca-colomba-et-al-2024>(1/29 | 143/249) ViGEO: an Assessment of Vision GNNs in Earth Observation (Luca Colomba et al., 2024)</a></li><li><a href=#229--144249-llms-as-bridges-reformulating-grounded-multimodal-named-entity-recognition-jinyuan-li-et-al-2024>(2/29 | 144/249) LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition (Jinyuan Li et al., 2024)</a></li><li><a href=#329--145249-data-augmentation-and-transfer-learning-approaches-applied-to-facial-expressions-recognition-enrico-randellini-et-al-2024>(3/29 | 145/249) Data Augmentation and Transfer Learning Approaches Applied to Facial Expressions Recognition (Enrico Randellini et al., 2024)</a></li><li><a href=#429--146249-prompt-based-personalized-federated-learning-for-medical-visual-question-answering-he-zhu-et-al-2024>(4/29 | 146/249) Prompt-based Personalized Federated Learning for Medical Visual Question Answering (He Zhu et al., 2024)</a></li><li><a href=#529--147249-mind-the-modality-gap-towards-a-remote-sensing-vision-language-model-via-cross-modal-alignment-angelos-zavras-et-al-2024>(5/29 | 147/249) Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment (Angelos Zavras et al., 2024)</a></li><li><a href=#629--148249-deep-spectral-meshes-multi-frequency-facial-mesh-processing-with-graph-neural-networks-robert-kosk-et-al-2024>(6/29 | 148/249) Deep Spectral Meshes: Multi-Frequency Facial Mesh Processing with Graph Neural Networks (Robert Kosk et al., 2024)</a></li><li><a href=#729--149249-mm-point-multi-view-information-enhanced-multi-modal-self-supervised-3d-point-cloud-understanding-hai-tao-yu-et-al-2024>(7/29 | 149/249) MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding (Hai-Tao Yu et al., 2024)</a></li><li><a href=#829--150249-textual-localization-decomposing-multi-concept-images-for-subject-driven-text-to-image-generation-junjie-shentu-et-al-2024>(8/29 | 150/249) Textual Localization: Decomposing Multi-concept Images for Subject-Driven Text-to-Image Generation (Junjie Shentu et al., 2024)</a></li><li><a href=#929--151249-social-reward-evaluating-and-enhancing-generative-ai-through-million-user-feedback-from-an-online-creative-community-arman-isajanyan-et-al-2024>(9/29 | 151/249) Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community (Arman Isajanyan et al., 2024)</a></li><li><a href=#1029--152249-dreammatcher-appearance-matching-self-attention-for-semantically-consistent-text-to-image-personalization-jisu-nam-et-al-2024>(10/29 | 152/249) DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization (Jisu Nam et al., 2024)</a></li><li><a href=#1129--153249-textron-weakly-supervised-multilingual-text-detection-through-data-programming-dhruv-kudale-et-al-2024>(11/29 | 153/249) TEXTRON: Weakly Supervised Multilingual Text Detection through Data Programming (Dhruv Kudale et al., 2024)</a></li><li><a href=#1229--154249-foul-prediction-with-estimated-poses-from-soccer-broadcast-video-jiale-fang-et-al-2024>(12/29 | 154/249) Foul prediction with estimated poses from soccer broadcast video (Jiale Fang et al., 2024)</a></li><li><a href=#1329--155249-exploiting-alpha-transparency-in-language-and-vision-based-ai-systems-david-noever-et-al-2024>(13/29 | 155/249) Exploiting Alpha Transparency In Language And Vision-Based AI Systems (David Noever et al., 2024)</a></li><li><a href=#1429--156249-visirnet-deep-image-alignment-for-uav-taken-visible-and-infrared-image-pairs-sedat-ozer-et-al-2024>(14/29 | 156/249) VisIRNet: Deep Image Alignment for UAV-taken Visible and Infrared Image Pairs (Sedat Ozer et al., 2024)</a></li><li><a href=#1529--157249-any-shift-prompting-for-generalization-over-distributions-zehao-xiao-et-al-2024>(15/29 | 157/249) Any-Shift Prompting for Generalization over Distributions (Zehao Xiao et al., 2024)</a></li><li><a href=#1629--158249-pobevm-real-time-video-matting-via-progressively-optimize-the-target-body-and-edge-jianming-xian-2024>(16/29 | 158/249) POBEVM: Real-time Video Matting via Progressively Optimize the Target Body and Edge (Jianming Xian, 2024)</a></li><li><a href=#1729--159249-examining-pathological-bias-in-a-generative-adversarial-network-discriminator-a-case-study-on-a-stylegan3-model-alvin-grissom-ii-et-al-2024>(17/29 | 159/249) Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model (Alvin Grissom II et al., 2024)</a></li><li><a href=#1829--160249-diffusion-model-with-cross-attention-as-an-inductive-bias-for-disentanglement-tao-yang-et-al-2024>(18/29 | 160/249) Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement (Tao Yang et al., 2024)</a></li><li><a href=#1929--161249-nyctale-neuro-evidence-transformer-for-adaptive-and-personalized-lung-nodule-invasiveness-prediction-sadaf-khademi-et-al-2024>(19/29 | 161/249) NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung Nodule Invasiveness Prediction (Sadaf Khademi et al., 2024)</a></li><li><a href=#2029--162249-seed-optimization-with-frozen-generator-for-superior-zero-shot-low-light-enhancement-yuxuan-gu-et-al-2024>(20/29 | 162/249) Seed Optimization with Frozen Generator for Superior Zero-shot Low-light Enhancement (Yuxuan Gu et al., 2024)</a></li><li><a href=#2129--163249-hi-gan-hierarchical-inpainting-gan-with-auxiliary-inputs-for-combined-rgb-and-depth-inpainting-ankan-dash-et-al-2024>(21/29 | 163/249) HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting (Ankan Dash et al., 2024)</a></li><li><a href=#2229--164249-mim-refiner-a-contrastive-learning-boost-from-intermediate-pre-trained-representations-benedikt-alkin-et-al-2024>(22/29 | 164/249) MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations (Benedikt Alkin et al., 2024)</a></li><li><a href=#2329--165249-investigation-of-federated-learning-algorithms-for-retinal-optical-coherence-tomography-image-classification-with-statistical-heterogeneity-sanskar-amgain-et-al-2024>(23/29 | 165/249) Investigation of Federated Learning Algorithms for Retinal Optical Coherence Tomography Image Classification with Statistical Heterogeneity (Sanskar Amgain et al., 2024)</a></li><li><a href=#2429--166249-beyond-kalman-filters-deep-learning-based-filters-for-improved-object-tracking-momir-adžemović-et-al-2024>(24/29 | 166/249) Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking (Momir Adžemović et al., 2024)</a></li><li><a href=#2529--167249-a-comprehensive-review-on-computer-vision-analysis-of-aerial-data-vivek-tetarwal-et-al-2024>(25/29 | 167/249) A Comprehensive Review on Computer Vision Analysis of Aerial Data (Vivek Tetarwal et al., 2024)</a></li><li><a href=#2629--168249-evaluating-nerfs-for-3d-plant-geometry-reconstruction-in-field-conditions-muhammad-arbab-arshad-et-al-2024>(26/29 | 168/249) Evaluating NeRFs for 3D Plant Geometry Reconstruction in Field Conditions (Muhammad Arbab Arshad et al., 2024)</a></li><li><a href=#2729--169249-lester-rotoscope-animation-through-video-object-segmentation-and-tracking-ruben-tous-2024>(27/29 | 169/249) Lester: rotoscope animation through video object segmentation and tracking (Ruben Tous, 2024)</a></li><li><a href=#2829--170249-ges-generalized-exponential-splatting-for-efficient-radiance-field-rendering-abdullah-hamdi-et-al-2024>(28/29 | 170/249) GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering (Abdullah Hamdi et al., 2024)</a></li><li><a href=#2929--171249-visually-dehallucinative-instruction-generation-know-what-you-dont-know-sungguk-cha-et-al-2024>(29/29 | 171/249) Visually Dehallucinative Instruction Generation: Know What You Don&rsquo;t Know (Sungguk Cha et al., 2024)</a></li></ul></li><li><a href=#cscr-5>cs.CR (5)</a><ul><li><a href=#15--172249-abusegpt-abuse-of-generative-ai-chatbots-to-create-smishing-campaigns-ashfak-md-shibli-et-al-2024>(1/5 | 172/249) AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns (Ashfak Md Shibli et al., 2024)</a></li><li><a href=#25--173249-preserving-data-privacy-for-ml-driven-applications-in-open-radio-access-networks-pranshav-gajjar-et-al-2024>(2/5 | 173/249) Preserving Data Privacy for ML-driven Applications in Open Radio Access Networks (Pranshav Gajjar et al., 2024)</a></li><li><a href=#35--174249-on-the-domain-generalizability-of-rf-fingerprints-through-multifractal-dimension-representation-benjamin-johnson-et-al-2024>(3/5 | 174/249) On the Domain Generalizability of RF Fingerprints Through Multifractal Dimension Representation (Benjamin Johnson et al., 2024)</a></li><li><a href=#45--175249-an-advanced-data-fabric-architecture-leveraging-homomorphic-encryption-and-federated-learning-sakib-anwar-rieyan-et-al-2024>(4/5 | 175/249) An advanced data fabric architecture leveraging homomorphic encryption and federated learning (Sakib Anwar Rieyan et al., 2024)</a></li><li><a href=#55--176249-hoacs-homomorphic-obfuscation-assisted-concealing-of-secrets-to-thwart-trojan-attacks-in-cots-processor-tanvir-hossain-et-al-2024>(5/5 | 176/249) HOACS: Homomorphic Obfuscation Assisted Concealing of Secrets to Thwart Trojan Attacks in COTS Processor (Tanvir Hossain et al., 2024)</a></li></ul></li><li><a href=#cssd-4>cs.SD (4)</a><ul><li><a href=#14--177249-deepsrgm----sequence-classification-and-ranking-in-indian-classical-music-with-deep-learning-sathwik-tejaswi-madhusudhan-et-al-2024>(1/4 | 177/249) DeepSRGM &ndash; Sequence Classification and Ranking in Indian Classical Music with Deep Learning (Sathwik Tejaswi Madhusudhan et al., 2024)</a></li><li><a href=#24--178249-muchin-a-chinese-colloquial-description-benchmark-for-evaluating-language-models-in-the-field-of-music-zihao-wang-et-al-2024>(2/4 | 178/249) MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music (Zihao Wang et al., 2024)</a></li><li><a href=#34--179249-a-cross-talk-robust-multichannel-vad-model-for-multiparty-agent-interactions-trained-using-synthetic-re-recordings-hyewon-han-et-al-2024>(3/4 | 179/249) A cross-talk robust multichannel VAD model for multiparty agent interactions trained using synthetic re-recordings (Hyewon Han et al., 2024)</a></li><li><a href=#44--180249-zero-shot-unsupervised-and-text-based-audio-editing-using-ddpm-inversion-hila-manor-et-al-2024>(4/4 | 180/249) Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion (Hila Manor et al., 2024)</a></li></ul></li><li><a href=#physicschem-ph-1>physics.chem-ph (1)</a><ul><li><a href=#11--181249-chemreasoner-heuristic-search-over-a-large-language-models-knowledge-space-using-quantum-chemical-feedback-henry-w-sprueill-et-al-2024>(1/1 | 181/249) ChemReasoner: Heuristic Search over a Large Language Model&rsquo;s Knowledge Space using Quantum-Chemical Feedback (Henry W. Sprueill et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--182249-approximate-message-passing-enhanced-graph-neural-network-for-otfs-data-detection-wenhao-zhuang-et-al-2024>(1/1 | 182/249) Approximate Message Passing-Enhanced Graph Neural Network for OTFS Data Detection (Wenhao Zhuang et al., 2024)</a></li></ul></li><li><a href=#csir-3>cs.IR (3)</a><ul><li><a href=#13--183249-sequential-recommendation-on-temporal-proximities-with-contrastive-learning-and-self-attention-hansol-jung-et-al-2024>(1/3 | 183/249) Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention (Hansol Jung et al., 2024)</a></li><li><a href=#23--184249-llm-based-federated-recommendation-jujia-zhao-et-al-2024>(2/3 | 184/249) LLM-based Federated Recommendation (Jujia Zhao et al., 2024)</a></li><li><a href=#33--185249-from-variability-to-stability-advancing-recsys-benchmarking-practices-valeriy-shevchenko-et-al-2024>(3/3 | 185/249) From Variability to Stability: Advancing RecSys Benchmarking Practices (Valeriy Shevchenko et al., 2024)</a></li></ul></li><li><a href=#csro-10>cs.RO (10)</a><ul><li><a href=#110--186249-on-the-safety-concerns-of-deploying-llmsvlms-in-robotics-highlighting-the-risks-and-vulnerabilities-xiyang-wu-et-al-2024>(1/10 | 186/249) On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities (Xiyang Wu et al., 2024)</a></li><li><a href=#210--187249-a-computationally-efficient-learning-based-model-predictive-control-for-multirotors-under-aerodynamic-disturbances-babak-akbari-et-al-2024>(2/10 | 187/249) A Computationally Efficient Learning-Based Model Predictive Control for Multirotors under Aerodynamic Disturbances (Babak Akbari et al., 2024)</a></li><li><a href=#310--188249-trajectory-guidance-enhanced-remote-driving-of-highly-automated-vehicles-domagoj-majstorovic-et-al-2024>(3/10 | 188/249) Trajectory Guidance: Enhanced Remote Driving of highly-automated Vehicles (Domagoj Majstorovic et al., 2024)</a></li><li><a href=#410--189249-self-supervised-learning-of-visual-robot-localization-using-led-state-prediction-as-a-pretext-task-mirko-nava-et-al-2024>(4/10 | 189/249) Self-Supervised Learning of Visual Robot Localization Using LED State Prediction as a Pretext Task (Mirko Nava et al., 2024)</a></li><li><a href=#510--190249-sequential-manipulation-of-deformable-linear-object-networks-with-endpoint-pose-measurements-using-adaptive-model-predictive-control-tyler-toner-et-al-2024>(5/10 | 190/249) Sequential Manipulation of Deformable Linear Object Networks with Endpoint Pose Measurements using Adaptive Model Predictive Control (Tyler Toner et al., 2024)</a></li><li><a href=#610--191249-lasersam-zero-shot-change-detection-using-visual-segmentation-of-spinning-lidar-alexander-krawciw-et-al-2024>(6/10 | 191/249) LaserSAM: Zero-Shot Change Detection Using Visual Segmentation of Spinning LiDAR (Alexander Krawciw et al., 2024)</a></li><li><a href=#710--192249-robotic-exploration-using-generalized-behavioral-entropy-aamodh-suresh-et-al-2024>(7/10 | 192/249) Robotic Exploration using Generalized Behavioral Entropy (Aamodh Suresh et al., 2024)</a></li><li><a href=#810--193249-universal-manipulation-interface-in-the-wild-robot-teaching-without-in-the-wild-robots-cheng-chi-et-al-2024>(8/10 | 193/249) Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots (Cheng Chi et al., 2024)</a></li><li><a href=#910--194249-reg-nf-efficient-registration-of-implicit-surfaces-within-neural-fields-stephen-hausler-et-al-2024>(9/10 | 194/249) Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields (Stephen Hausler et al., 2024)</a></li><li><a href=#1010--195249-towards-tight-convex-relaxations-for-contact-rich-manipulation-bernhard-p-graesdal-et-al-2024>(10/10 | 195/249) Towards Tight Convex Relaxations for Contact-Rich Manipulation (Bernhard P. Graesdal et al., 2024)</a></li></ul></li><li><a href=#csne-3>cs.NE (3)</a><ul><li><a href=#13--196249-hidden-traveling-waves-bind-working-memory-variables-in-recurrent-neural-networks-arjun-karuvally-et-al-2024>(1/3 | 196/249) Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural Networks (Arjun Karuvally et al., 2024)</a></li><li><a href=#23--197249-system-level-impact-of-non-ideal-program-time-of-charge-trap-flash-ctf-on-deep-neural-network-s-shrivastava-et-al-2024>(2/3 | 197/249) System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF) on Deep Neural Network (S. Shrivastava et al., 2024)</a></li><li><a href=#33--198249-large-scale-benchmarking-of-metaphor-based-optimization-heuristics-diederick-vermetten-et-al-2024>(3/3 | 198/249) Large-scale Benchmarking of Metaphor-based Optimization Heuristics (Diederick Vermetten et al., 2024)</a></li></ul></li><li><a href=#physicsapp-ph-1>physics.app-ph (1)</a><ul><li><a href=#11--199249-deep-learning-for-the-design-of-non-hermitian-topolectrical-circuits-xi-chen-et-al-2024>(1/1 | 199/249) Deep learning for the design of non-Hermitian topolectrical circuits (Xi Chen et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--200249-mixture-of-experts-for-network-optimization-a-large-language-model-enabled-approach-hongyang-du-et-al-2024>(1/1 | 200/249) Mixture of Experts for Network Optimization: A Large Language Model-enabled Approach (Hongyang Du et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--201249-protchatgpt-towards-understanding-proteins-with-large-language-models-chao-wang-et-al-2024>(1/1 | 201/249) ProtChatGPT: Towards Understanding Proteins with Large Language Models (Chao Wang et al., 2024)</a></li></ul></li><li><a href=#csse-2>cs.SE (2)</a><ul><li><a href=#12--202249-codemind-a-framework-to-challenge-large-language-models-for-code-reasoning-changshu-liu-et-al-2024>(1/2 | 202/249) CodeMind: A Framework to Challenge Large Language Models for Code Reasoning (Changshu Liu et al., 2024)</a></li><li><a href=#22--203249-ijtyper-an-iterative-type-inference-framework-for-java-by-integrating-constraint--and-statistically-based-methods-zhixiang-chen-et-al-2024>(2/2 | 203/249) iJTyper: An Iterative Type Inference Framework for Java by Integrating Constraint- and Statistically-based Methods (Zhixiang Chen et al., 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#12--204249-stuck-at-faults-in-reram-neuromorphic-circuit-array-and-their-correction-through-machine-learning-vedant-sawal-et-al-2024>(1/2 | 204/249) Stuck-at Faults in ReRAM Neuromorphic Circuit Array and their Correction through Machine Learning (Vedant Sawal et al., 2024)</a></li><li><a href=#22--205249-reusing-softmax-hardware-unit-for-gelu-computation-in-transformers-christodoulos-peltekis-et-al-2024>(2/2 | 205/249) Reusing Softmax Hardware Unit for GELU Computation in Transformers (Christodoulos Peltekis et al., 2024)</a></li></ul></li><li><a href=#eessiv-7>eess.IV (7)</a><ul><li><a href=#17--206249-current-and-future-roles-of-artificial-intelligence-in-retinopathy-of-prematurity-ali-jafarizadeh-et-al-2024>(1/7 | 206/249) Current and future roles of artificial intelligence in retinopathy of prematurity (Ali Jafarizadeh et al., 2024)</a></li><li><a href=#27--207249-towards-precision-cardiovascular-analysis-in-zebrafish-the-zacaf-paradigm-amir-mohammad-naderi-et-al-2024>(2/7 | 207/249) Towards Precision Cardiovascular Analysis in Zebrafish: The ZACAF Paradigm (Amir Mohammad Naderi et al., 2024)</a></li><li><a href=#37--208249-spatiotemporal-disentanglement-of-arteriovenous-malformations-in-digital-subtraction-angiography-kathleen-baur-et-al-2024>(3/7 | 208/249) Spatiotemporal Disentanglement of Arteriovenous Malformations in Digital Subtraction Angiography (Kathleen Baur et al., 2024)</a></li><li><a href=#47--209249-less-is-more-ensemble-learning-for-retinal-disease-recognition-under-limited-resources-jiahao-wang-et-al-2024>(4/7 | 209/249) Less is more: Ensemble Learning for Retinal Disease Recognition Under Limited Resources (Jiahao Wang et al., 2024)</a></li><li><a href=#57--210249-robust-semi-automatic-vessel-tracing-in-the-human-retinal-image-by-an-instance-segmentation-neural-network-siyi-chen-et-al-2024>(5/7 | 210/249) Robust semi-automatic vessel tracing in the human retinal image by an instance segmentation neural network (Siyi Chen et al., 2024)</a></li><li><a href=#67--211249-hybrid-cnn-bi-lstm-neural-network-for-hyperspectral-image-classification-alok-ranjan-sahoo-et-al-2024>(6/7 | 211/249) Hybrid CNN Bi-LSTM neural network for Hyperspectral image classification (Alok Ranjan Sahoo et al., 2024)</a></li><li><a href=#77--212249-tiaviz-a-browser-based-visualization-tool-for-computational-pathology-models-mark-eastwood-et-al-2024>(7/7 | 212/249) TIAViz: A Browser-based Visualization Tool for Computational Pathology Models (Mark Eastwood et al., 2024)</a></li></ul></li><li><a href=#csit-3>cs.IT (3)</a><ul><li><a href=#13--213249-digital-versus-analog-transmissions-for-federated-learning-over-wireless-networks-jiacheng-yao-et-al-2024>(1/3 | 213/249) Digital versus Analog Transmissions for Federated Learning over Wireless Networks (Jiacheng Yao et al., 2024)</a></li><li><a href=#23--214249-two-timescale-design-for-active-star-ris-aided-massive-mimo-systems-anastasios-papazafeiropoulos-et-al-2024>(2/3 | 214/249) Two-Timescale Design for Active STAR-RIS Aided Massive MIMO Systems (Anastasios Papazafeiropoulos et al., 2024)</a></li><li><a href=#33--215249-infonet-neural-estimation-of-mutual-information-without-test-time-optimization-zhengyang-hu-et-al-2024>(3/3 | 215/249) InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization (Zhengyang Hu et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--216249-reeb-complements-for-exploring-inclusions-between-isosurfaces-from-two-scalar-fields-akito-fujii-et-al-2024>(1/1 | 216/249) Reeb Complements for Exploring Inclusions Between Isosurfaces From Two Scalar Fields (Akito Fujii et al., 2024)</a></li></ul></li><li><a href=#q-biogn-2>q-bio.GN (2)</a><ul><li><a href=#12--217249-toward-a-team-of-ai-made-scientists-for-scientific-discovery-from-gene-expression-data-haoyang-liu-et-al-2024>(1/2 | 217/249) Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data (Haoyang Liu et al., 2024)</a></li><li><a href=#22--218249-data-smoothing-filling-method-based-on-scrna-seq-data-zero-value-identification-linfeng-jiang-et-al-2024>(2/2 | 218/249) Data Smoothing Filling Method based on ScRNA-Seq Data Zero-Value Identification (Linfeng Jiang et al., 2024)</a></li></ul></li><li><a href=#mathoc-3>math.OC (3)</a><ul><li><a href=#13--219249-mathematical-opportunities-in-digital-twins-math-dt-harbir-antil-2024>(1/3 | 219/249) Mathematical Opportunities in Digital Twins (MATH-DT) (Harbir Antil, 2024)</a></li><li><a href=#23--220249-a-system-dynamic-based-simulation-and-bayesian-optimization-for-inventory-management-sarit-maitra-2024>(2/3 | 220/249) A System-Dynamic Based Simulation and Bayesian Optimization for Inventory Management (Sarit Maitra, 2024)</a></li><li><a href=#33--221249-an-accelerated-distributed-stochastic-gradient-method-with-momentum-kun-huang-et-al-2024>(3/3 | 221/249) An Accelerated Distributed Stochastic Gradient Method with Momentum (Kun Huang et al., 2024)</a></li></ul></li><li><a href=#cshc-6>cs.HC (6)</a><ul><li><a href=#16--222249-lave-llm-powered-agent-assistance-and-language-augmentation-for-video-editing-bryan-wang-et-al-2024>(1/6 | 222/249) LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing (Bryan Wang et al., 2024)</a></li><li><a href=#26--223249-not-just-novelty-a-longitudinal-study-on-utility-and-customization-of-ai-workflows-tao-long-et-al-2024>(2/6 | 223/249) Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows (Tao Long et al., 2024)</a></li><li><a href=#36--224249-exploring-the-potential-of-large-language-models-in-artistic-creation-collaboration-and-reflection-on-creative-programming-anqi-wang-et-al-2024>(3/6 | 224/249) Exploring the Potential of Large Language Models in Artistic Creation: Collaboration and Reflection on Creative Programming (Anqi Wang et al., 2024)</a></li><li><a href=#46--225249-geobotsvr-a-robotics-learning-game-for-beginners-with-hands-on-learning-simulation-syed-t-mubarrat-2024>(4/6 | 225/249) GeoBotsVR: A Robotics Learning Game for Beginners with Hands-on Learning Simulation (Syed T. Mubarrat, 2024)</a></li><li><a href=#56--226249-user-privacy-harms-and-risks-in-conversational-ai-a-proposed-framework-ece-gumusel-et-al-2024>(5/6 | 226/249) User Privacy Harms and Risks in Conversational AI: A Proposed Framework (Ece Gumusel et al., 2024)</a></li><li><a href=#66--227249-a-framework-for-gait-based-user-demography-estimation-using-inertial-sensors-chinmay-prakash-swami-2024>(6/6 | 227/249) A Framework For Gait-Based User Demography Estimation Using Inertial Sensors (Chinmay Prakash Swami, 2024)</a></li></ul></li><li><a href=#cond-matdis-nn-1>cond-mat.dis-nn (1)</a><ul><li><a href=#11--228249-random-features-and-polynomial-rules-fabián-aguirre-lópez-et-al-2024>(1/1 | 228/249) Random features and polynomial rules (Fabián Aguirre-López et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--229249-identifying-and-modelling-cognitive-biases-in-mobility-choices-chloe-conrad-et-al-2024>(1/1 | 229/249) Identifying and modelling cognitive biases in mobility choices (Chloe Conrad et al., 2024)</a></li></ul></li><li><a href=#q-fincp-1>q-fin.CP (1)</a><ul><li><a href=#11--230249-alpha-gpt-20-human-in-the-loop-ai-for-quantitative-investment-hang-yuan-et-al-2024>(1/1 | 230/249) Alpha-GPT 2.0: Human-in-the-Loop AI for Quantitative Investment (Hang Yuan et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--231249-dpbalance-efficient-and-fair-privacy-budget-scheduling-for-federated-learning-as-a-service-yu-liu-et-al-2024>(1/1 | 231/249) DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated Learning as a Service (Yu Liu et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--232249-zastosowanie-grafów-i-sieci-w-systemach-rekomendacji-michał-malinowski-2024>(1/2 | 232/249) Zastosowanie grafów i sieci w systemach rekomendacji (Michał Malinowski, 2024)</a></li><li><a href=#22--233249-modeling-the-impact-of-timeline-algorithms-on-opinion-dynamics-using-low-rank-updates-tianyi-zhou-et-al-2024>(2/2 | 233/249) Modeling the Impact of Timeline Algorithms on Opinion Dynamics Using Low-rank Updates (Tianyi Zhou et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--234249-radio-astronomical-image-reconstruction-with-conditional-denoising-diffusion-model-mariia-drozdova-et-al-2024>(1/1 | 234/249) Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model (Mariia Drozdova et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--235249-brant-2-foundation-model-for-brain-signals-zhizhang-yuan-et-al-2024>(1/1 | 235/249) Brant-2: Foundation Model for Brain Signals (Zhizhang Yuan et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--236249-modeling-methodology-for-the-accurate-and-prompt-prediction-of-symptomatic-events-in-chronic-diseases-josué-pagán-et-al-2024>(1/1 | 236/249) Modeling methodology for the accurate and prompt prediction of symptomatic events in chronic diseases (Josué Pagán et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--237249-efficient-φ-regret-minimization-with-low-degree-swap-deviations-in-extensive-form-games-brian-hu-zhang-et-al-2024>(1/1 | 237/249) Efficient $Φ$-Regret Minimization with Low-Degree Swap Deviations in Extensive-Form Games (Brian Hu Zhang et al., 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#12--238249-benchmarking-the-operation-of-quantum-heuristics-and-ising-machines-scoring-parameter-setting-strategies-on-optimization-applications-david-e-bernal-neira-et-al-2024>(1/2 | 238/249) Benchmarking the Operation of Quantum Heuristics and Ising Machines: Scoring Parameter Setting Strategies on Optimization Applications (David E. Bernal Neira et al., 2024)</a></li><li><a href=#22--239249-linear-depth-qft-over-ibm-heavy-hex-architecture-xiangyu-gao-et-al-2024>(2/2 | 239/249) Linear Depth QFT over IBM Heavy-hex Architecture (Xiangyu Gao et al., 2024)</a></li></ul></li><li><a href=#cssc-1>cs.SC (1)</a><ul><li><a href=#11--240249-fast-interpolation-and-multiplication-of-unbalanced-polynomials-pascal-giorgi-et-al-2024>(1/1 | 240/249) Fast interpolation and multiplication of unbalanced polynomials (Pascal Giorgi et al., 2024)</a></li></ul></li><li><a href=#csds-4>cs.DS (4)</a><ul><li><a href=#14--241249-non-adaptive-bellman-ford-yens-improvement-is-optimal-jialu-hu-et-al-2024>(1/4 | 241/249) Non-adaptive Bellman-Ford: Yen&rsquo;s improvement is optimal (Jialu Hu et al., 2024)</a></li><li><a href=#24--242249-correlation-clustering-with-vertex-splitting-matthias-bentert-et-al-2024>(2/4 | 242/249) Correlation Clustering with Vertex Splitting (Matthias Bentert et al., 2024)</a></li><li><a href=#34--243249-parameterized-vertex-integrity-revisited-tesshu-hanaka-et-al-2024>(3/4 | 243/249) Parameterized Vertex Integrity Revisited (Tesshu Hanaka et al., 2024)</a></li><li><a href=#44--244249-parameterized-algorithms-for-steiner-forest-in-bounded-width-graphs-andreas-emil-feldmann-et-al-2024>(4/4 | 244/249) Parameterized Algorithms for Steiner Forest in Bounded Width Graphs (Andreas Emil Feldmann et al., 2024)</a></li></ul></li><li><a href=#mathna-1>math.NA (1)</a><ul><li><a href=#11--245249-a-new-type-of-simplified-inverse-lax-wendroff-boundary-treatment-i-hyperbolic-conservation-laws-shihao-liu-et-al-2024>(1/1 | 245/249) A new type of simplified inverse Lax-Wendroff boundary treatment I: hyperbolic conservation laws (Shihao Liu et al., 2024)</a></li></ul></li><li><a href=#physicsao-ph-1>physics.ao-ph (1)</a><ul><li><a href=#11--246249-a-deep-learning-approach-to-radar-based-qpe-ting-shuo-yo-et-al-2024>(1/1 | 246/249) A Deep Learning Approach to Radar-based QPE (Ting-Shuo Yo et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--247249-coevolution-of-relationship-and-interaction-in-cooperative-dynamical-multiplex-networks-xiaojin-xiong-et-al-2024>(1/1 | 247/249) Coevolution of relationship and interaction in cooperative dynamical multiplex networks (Xiaojin Xiong et al., 2024)</a></li></ul></li><li><a href=#eesssy-1>eess.SY (1)</a><ul><li><a href=#11--248249-quickest-detection-of-false-data-injection-attack-in-distributed-process-tracking-saqib-abbas-baba-et-al-2024>(1/1 | 248/249) Quickest Detection of False Data Injection Attack in Distributed Process Tracking (Saqib Abbas Baba et al., 2024)</a></li></ul></li><li><a href=#csos-1>cs.OS (1)</a><ul><li><a href=#11--249249-a-system-level-dynamic-binary-translator-using-automatically-learned-translation-rules-jinhu-jiang-et-al-2024>(1/1 | 249/249) A System-Level Dynamic Binary Translator using Automatically-Learned Translation Rules (Jinhu Jiang et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>