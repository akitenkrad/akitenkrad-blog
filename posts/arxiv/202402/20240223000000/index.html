<!doctype html><html><head><title>arXiv @ 2024.02.23</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.23"><meta property="og:description" content="Primary Categories astro-ph.EP (1) astro-ph.IM (1) cs.AI (8) cs.AR (2) cs.CC (1) cs.CG (1) cs.CL (94) cs.CR (7) cs.CV (41) cs.CY (1) cs.DC (2) cs.DM (1) cs.DS (5) cs.GT (1) cs.HC (3) cs.IR (13) cs.IT (5) cs.LG (62) cs.LO (1) cs.MA (1) cs.MS (1) cs.NE (3) cs.RO (13) cs.SD (3) cs.SE (6) cs.SI (2) eess.IV (3) eess.SP (1) eess.SY (6) math.CO (2) math.NA (4) math.OC (2) physics.geo-ph (1) q-bio.NC (1) q-bio."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240223000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-23T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-23T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.23"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240223000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Friday, Feb 23, 2024</p></div><div class=title><h1>arXiv @ 2024.02.23</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#astro-phep-1>astro-ph.EP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csai-8>cs.AI (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#cscl-94>cs.CL (94)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#cscr-7>cs.CR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#cscv-41>cs.CV (41)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csds-5>cs.DS (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csir-13>cs.IR (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csit-5>cs.IT (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#cslg-62>cs.LG (62)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csms-1>cs.MS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csne-3>cs.NE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csro-13>cs.RO (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#cssd-3>cs.SD (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#csse-6>cs.SE (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#eessiv-3>eess.IV (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#eesssy-6>eess.SY (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#mathco-2>math.CO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#mathna-4>math.NA (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#physicsgeo-ph-1>physics.geo-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#statap-1>stat.AP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/#statml-4>stat.ML (4)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.IR</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Attack</td><td>1</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Aspect-based Sentiment Analysis</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Automatic Evaluation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Benchmarking</td><td>29</td><td>8</td><td>1</td><td>7</td><td></td></tr><tr><td>Black Box</td><td>2</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>3</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Claude</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td>3</td><td></td><td>2</td><td></td></tr><tr><td>Code Generation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>3</td><td>1</td><td>1</td><td></td></tr><tr><td>Convolution</td><td></td><td>6</td><td></td><td>3</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>6</td><td></td><td>4</td><td></td></tr><tr><td>Coreference Resolution</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Counter-factual</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Dialogue System</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>6</td><td></td><td>3</td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Edge Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Essay Scoring</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Face Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Few-shot</td><td>4</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>19</td><td>4</td><td></td><td>7</td><td>1</td></tr><tr><td>Foundation Model</td><td>1</td><td></td><td></td><td>2</td><td>1</td></tr><tr><td>GLUE</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT</td><td>18</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>GPT-2</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>15</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Generative Adversarial Network</td><td>1</td><td>2</td><td></td><td>6</td><td></td></tr><tr><td>Geometry</td><td>1</td><td>2</td><td></td><td>1</td><td>1</td></tr><tr><td>Graph</td><td>4</td><td></td><td>6</td><td>11</td><td>3</td></tr><tr><td>Graph Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Contrastive Learning</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>4</td><td>11</td><td></td></tr><tr><td>Hate Speech Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td>1</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Image2text</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td>15</td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>Information Retrieval</td><td></td><td></td><td>5</td><td></td><td></td></tr><tr><td>Instruction Following</td><td>3</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>4</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Intent Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>6</td><td>5</td><td></td><td>7</td><td></td></tr><tr><td>Knowledge Graph</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>LLaMA</td><td>6</td><td></td><td></td><td>2</td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td></td><td>4</td><td></td></tr><tr><td>Language Generation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>126</td><td>5</td><td>7</td><td>24</td><td>1</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Low-Resource</td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td>2</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Mistral</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Model Compression</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td>17</td><td>8</td><td></td><td>2</td><td>6</td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Mutual Information</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Named Entity Recognition</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>3</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Open-Domain Dialogue</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Open-Domain Question Answering</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Perplexity</td><td>2</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Pre-trained Language Model</td><td>6</td><td></td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Prompt</td><td>24</td><td>4</td><td></td><td>4</td><td>1</td></tr><tr><td>Pruning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Quantization</td><td>1</td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>Question Answering</td><td>11</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>12</td><td>3</td><td>1</td><td>5</td><td>1</td></tr><tr><td>Recommendation</td><td>3</td><td>1</td><td>6</td><td>1</td><td></td></tr><tr><td>Recommender System</td><td></td><td></td><td>5</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>4</td><td>1</td><td></td><td>5</td><td>3</td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Retrieval Augmentation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Rouge-L</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Self-Distillation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td>2</td><td>2</td><td></td><td>4</td><td>1</td></tr><tr><td>Self-supervised Pre-training</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Sentiment Analysis</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td></td><td>2</td><td>2</td></tr><tr><td>Simulator</td><td></td><td></td><td></td><td>2</td><td>2</td></tr><tr><td>Stemming</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Style Transfer</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>5</td><td>4</td><td></td><td>3</td><td></td></tr><tr><td>T5</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td>2</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Text Generation</td><td>4</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Text2image</td><td></td><td>4</td><td></td><td></td><td>2</td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Transformer</td><td>5</td><td>6</td><td></td><td>8</td><td></td></tr><tr><td>Unsupervised Learning</td><td>2</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>2</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>6</td><td>5</td><td>1</td><td>4</td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td>1</td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cslg-62>cs.LG (62)</h2><h3 id=162--1306-unigraph-learning-a-cross-domain-graph-foundation-model-from-natural-language-yufei-he-et-al-2024>(1/62 | 1/306) UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language (Yufei He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufei He, Bryan Hooi. (2024)<br><strong>UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language</strong><br><button class=copy-to-clipboard title="UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 148<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Few-shot, Foundation Model, Representation Learning, Self-supervised Learning, Supervised Learning, Zero-shot, ChatGPT, GPT, GPT-4, In-context Learning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13630v1.pdf filename=2402.13630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> like <b>ChatGPT</b> and <b>GPT-4</b> have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to <b>graph</b> <b>learning,</b> <b>a</b> stark contrast emerges. <b>Graph</b> <b>learning</b> <b>has</b> predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of <b>graph</b> <b>structures,</b> <b>along</b> with the different feature and label spaces specific to <b>graph</b> <b>data.</b> <b>In</b> this paper, we present our UniGraph framework, designed to train a <b>graph</b> <b>foundation</b> <b>model</b> capable of generalizing to unseen <b>graphs</b> <b>and</b> <b>tasks</b> across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed <b>Graphs</b> <b>(TAGs)</b> <b>for</b> unifying node <b>representations.</b> <b>We</b> propose a cascaded architecture of Language Models (LMs) and <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> as backbone networks with a <b>self-supervised</b> training objective based on Masked <b>Graph</b> <b>Modeling</b> <b>(MGM).</b> We introduce <b>graph</b> <b>instruction</b> <b>tuning</b> using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to enable <b>zero-shot</b> prediction ability. Our comprehensive experiments across various <b>graph</b> <b>learning</b> <b>tasks</b> and domains demonstrate the model&rsquo;s effectiveness in <b>self-supervised</b> <b>representation</b> <b>learning</b> on unseen <b>graphs,</b> <b>few-shot</b> <b>in-context</b> transfer, and <b>zero-shot</b> transfer, even surpassing or matching the performance of <b>GNNs</b> that have undergone <b>supervised</b> training on target datasets.</p></p class="citation"></blockquote><h3 id=262--2306-an-explainable-transformer-based-model-for-phishing-email-detection-a-large-language-model-approach-mohammad-amaz-uddin-et-al-2024>(2/62 | 2/306) An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach (Mohammad Amaz Uddin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Amaz Uddin, Iqbal H. Sarker. (2024)<br><strong>An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach</strong><br><button class=copy-to-clipboard title="An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 80<br>Keywords: Explainable AI, Fine-tuning, Transformer, Text Classification, Large Language Model, Large Language Model, Masked Language Model, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13871v1.pdf filename=2402.13871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>Masked</b> <b>Language</b> <b>Models</b> <b>(MLMs)</b> possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, <b>fine-tuned</b> <b>transformer-based</b> DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well. Finally, we demonstrate our <b>fine-tuned</b> model using <b>Explainable-AI</b> <b>(XAI)</b> techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and <b>Transformer</b> Interpret to explain how our model makes predictions in the context of <b>text</b> <b>classification</b> for phishing emails.</p></p class="citation"></blockquote><h3 id=362--3306-aptq-attention-aware-post-training-mixed-precision-quantization-for-large-language-models-ziyi-guan-et-al-2024>(3/62 | 3/306) APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models (Ziyi Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, Hao Yu. (2024)<br><strong>APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models</strong><br><button class=copy-to-clipboard title="APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Quantization, Quantization, Zero-shot, LLaMA, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14866v1.pdf filename=2402.14866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision <b>Quantization)</b> for <b>LLMs,</b> which considers not only the second-order information of each layer&rsquo;s weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision <b>quantization,</b> ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous <b>quantization</b> methods, achieving an average of 4 bit width a 5.22 <b>perplexity</b> nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art <b>zero-shot</b> accuracy of 68.24% and 70.48% at an average bitwidth of 3.8 in <b>LLaMa-7B</b> and <b>LLaMa-13B,</b> respectively, demonstrating its effectiveness to produce high-quality <b>quantized</b> <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=462--4306-fingpt-hpc-efficient-pretraining-and-finetuning-large-language-models-for-financial-applications-with-high-performance-computing-xiao-yang-liu-et-al-2024>(4/62 | 4/306) FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing (Xiao-Yang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao-Yang Liu, Jie Zhang, Guoxuan Wang, Weiqing Tong, Anwar Walid. (2024)<br><strong>FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing</strong><br><button class=copy-to-clipboard title="FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-DC, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Model Compression, Quantization, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13533v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13533v1.pdf filename=2402.13533v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of <b>LLMs&rsquo;</b> parameters come from the linear layers of the <b>transformer</b> structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the <b>model</b> <b>size.</b> To pretrain and <b>finetune</b> <b>LLMs</b> efficiently, there are three major challenges to address: 1) reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3) improving GPU utilization when using distributed training. Prior methods, such as LoRA and QLoRA, utilized low-rank matrices and <b>quantization</b> to reduce the number of trainable parameters and <b>model</b> <b>size,</b> respectively. However, the resulting <b>model</b> <b>still</b> consumes a <b>large</b> <b>amount</b> <b>of</b> GPU memory. In this paper, we present high-performance GPU-based methods that exploit low-rank structures to pretrain and <b>finetune</b> <b>LLMs</b> for financial applications. We replace one conventional linear layer of the <b>transformer</b> structure with two narrower linear layers, which allows us to reduce the number of parameters by several orders of magnitude. By quantizing the parameters into low precision (8-bit and 4-bit), the memory consumption of the resulting <b>model</b> <b>is</b> further reduced. Compared with existing <b>LLMs,</b> our methods achieve a speedup of 1.3X and a <b>model</b> <b>compression</b> ratio of 2.64X for pretaining without accuracy drop. For <b>finetuning,</b> our methods achieve an average accuracy increase of 6.3% and 24.0% in general tasks and financial tasks, respectively, and GPU memory consumption ratio of 6.3X. The sizes of our <b>models</b> <b>are</b> smaller than 0.59 GB, allowing inference on a smartphone.</p></p class="citation"></blockquote><h3 id=562--5306-stencil-submodular-mutual-information-based-weak-supervision-for-cold-start-active-learning-nathan-beck-et-al-2024>(5/62 | 5/306) STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning (Nathan Beck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathan Beck, Adithya Iyer, Rishabh Iyer. (2024)<br><strong>STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning</strong><br><button class=copy-to-clipboard title="STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Active Learning, Fine-tuning, Mutual Information, Supervised Learning, Text Classification, Large Language Model, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13468v1.pdf filename=2402.13468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>supervised</b> <b>fine-tuning</b> of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in <b>large</b> <b>language</b> <b>models.</b> <b>Active</b> <b>learning,</b> which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of <b>active</b> <b>learning</b> selection before improving rare classes. We present STENCIL, which utilizes a set of <b>text</b> <b>exemplars</b> and the recently proposed submodular <b>mutual</b> <b>information</b> to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10%-24%$ and rare-class F-1 score by $17%-40%$ on multiple <b>text</b> <b>classification</b> datasets over common <b>active</b> <b>learning</b> methods within the class-imbalanced cold-start setting.</p></p class="citation"></blockquote><h3 id=662--6306-deep-generative-models-for-offline-policy-learning-tutorial-survey-and-perspectives-on-future-directions-jiayu-chen-et-al-2024>(6/62 | 6/306) Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions (Jiayu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayu Chen, Bhargav Ganguly, Yang Xu, Yongsheng Mei, Tian Lan, Vaneet Aggarwal. (2024)<br><strong>Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions</strong><br><button class=copy-to-clipboard title="Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Diffusion Model, Generative Adversarial Network, Knowledge Distillation, Offline Reinforcement Learning, Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13777v4 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13777v4.pdf filename=2402.13777v4.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>generative</b> <b>models</b> <b>(DGMs)</b> have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from <b>offline</b> <b>data.</b> <b>Similarly,</b> data-driven decision-making and robotic control also necessitate learning a generator function from the <b>offline</b> <b>data</b> <b>to</b> serve as the strategy or policy. In this case, applying deep <b>generative</b> <b>models</b> <b>in</b> <b>offline</b> <b>policy</b> <b>learning</b> exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep <b>generative</b> <b>models</b> <b>for</b> <b>offline</b> <b>policy</b> <b>learning.</b> In particular, we cover five mainstream deep <b>generative</b> <b>models,</b> <b>including</b> Variational Auto-Encoders, <b>Generative</b> <b>Adversarial</b> <b>Networks,</b> Normalizing Flows, <b>Transformers,</b> and <b>Diffusion</b> <b>Models,</b> and their applications in both <b>offline</b> <b>reinforcement</b> <b>learning</b> <b>(offline</b> <b>RL)</b> <b>and</b> imitation learning (IL). <b>Offline</b> <b>RL</b> <b>and</b> IL are two main branches of <b>offline</b> <b>policy</b> <b>learning</b> and are widely-adopted techniques for sequential decision-making. Specifically, for each type of DGM-based <b>offline</b> <b>policy</b> <b>learning,</b> we <b>distill</b> its fundamental scheme, categorize related works based on the usage of the DGM, and sort out the development process of algorithms in that field. Subsequent to the main content, we provide in-depth discussions on deep <b>generative</b> <b>models</b> <b>and</b> <b>offline</b> <b>policy</b> <b>learning</b> as a summary, based on which we present our perspectives on future research directions. This work offers a hands-on reference for the research progress in deep <b>generative</b> <b>models</b> <b>for</b> <b>offline</b> <b>policy</b> <b>learning,</b> and aims to inspire improved DGM-based <b>offline</b> <b>RL</b> <b>or</b> IL algorithms. For convenience, we maintain a paper list on <a href=https://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning>https://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning</a>.</p></p class="citation"></blockquote><h3 id=762--7306-deisam-segment-anything-with-deictic-prompting-hikaru-shindo-et-al-2024>(7/62 | 7/306) DeiSAM: Segment Anything with Deictic Prompting (Hikaru Shindo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hikaru Shindo, Manuel Brack, Gopika Sudhakaran, Devendra Singh Dhami, Patrick Schramowski, Kristian Kersting. (2024)<br><strong>DeiSAM: Segment Anything with Deictic Prompting</strong><br><button class=copy-to-clipboard title="DeiSAM: Segment Anything with Deictic Prompting" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Graph, Zero-shot, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14123v1.pdf filename=2402.14123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large-scale,</b> <b>pre-trained</b> <b>neural</b> networks have demonstrated strong capabilities in various tasks, including <b>zero-shot</b> image segmentation. To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as &ldquo;The object that is on the desk and behind the cup.&rdquo;. However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of <b>reasoning</b> capabilities in complex scenarios. To remedy this issue, we propose DeiSAM &ndash; a combination of <b>large</b> <b>pre-trained</b> <b>neural</b> networks with differentiable logic reasoners &ndash; for deictic promptable segmentation. Given a complex, textual segmentation description, DeiSAM leverages <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to generate first-order logic rules and performs differentiable forward <b>reasoning</b> on generated scene <b>graphs.</b> Subsequently, DeiSAM segments objects by matching them to the logically inferred image regions. As part of our evaluation, we propose the Deictic Visual Genome (DeiVG) dataset, containing paired visual input and complex, deictic textual <b>prompts.</b> Our empirical results demonstrate that DeiSAM is a substantial improvement over purely data-driven baselines for deictic promptable segmentation.</p></p class="citation"></blockquote><h3 id=862--8306-wisdom-of-committee-distilling-from-foundation-model-to-specialized-application-model-zichang-liu-et-al-2024>(8/62 | 8/306) Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model (Zichang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali Shrivastava, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe Zhao. (2024)<br><strong>Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model</strong><br><button class=copy-to-clipboard title="Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Foundation Model, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14035v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14035v2.pdf filename=2402.14035v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>foundation</b> <b>models</b> have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the <b>knowledge</b> <b>in</b> <b>foundation</b> <b>models</b> into specialized application models, which are generally more efficient for serving. Techniques from <b>knowledge</b> <b>distillation</b> may be applied here, where the application model learns to mimic the <b>foundation</b> <b>model.</b> However, specialized application models and <b>foundation</b> <b>models</b> have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for <b>distillation</b> methods. In this work, we propose creating a teaching committee comprising both <b>foundation</b> <b>model</b> teachers and complementary teachers. Complementary teachers possess model characteristics akin to the student&rsquo;s, aiming to bridge the gap between the <b>foundation</b> <b>model</b> and specialized application models for a smoother <b>knowledge</b> <b>transfer.</b> Further, to accommodate the dissimilarity among the teachers in the committee, we introduce DiverseDistill, which allows the student to understand the expertise of each teacher and extract task <b>knowledge.</b> <b>Our</b> evaluations demonstrate that adding complementary teachers enhances student performance. Finally, DiverseDistill consistently outperforms baseline <b>distillation</b> methods, regardless of the teacher choices, resulting in significantly improved student performance.</p></p class="citation"></blockquote><h3 id=962--9306-from-self-attention-to-markov-models-unveiling-the-dynamics-of-generative-transformers-m-emrullah-ildiz-et-al-2024>(9/62 | 9/306) From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers (M. Emrullah Ildiz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M. Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, Samet Oymak. (2024)<br><strong>From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers</strong><br><button class=copy-to-clipboard title="From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Transformer, Text Generation, Large Language Model, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13512v1.pdf filename=2402.13512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern language models rely on the <b>transformer</b> architecture and attention mechanism to perform language understanding and <b>text</b> <b>generation.</b> In this work, we study learning a 1-layer <b>self-attention</b> model from a set of <b>prompts</b> and associated output data sampled from the model. We first establish a precise mapping between the <b>self-attention</b> mechanism and Markov models: Inputting a <b>prompt</b> to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the <b>prompt</b> distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from an initial <b>prompt.</b> We characterize an intriguing winner-takes-all phenomenon where the generative process implemented by <b>self-attention</b> collapses into sampling a limited subset of tokens due to its non-mixing nature. This provides a mathematical explanation to the tendency of modern <b>LLMs</b> to generate repetitive <b>text.</b> <b>In</b> summary, the equivalence to CCMC provides a simple but powerful framework to study <b>self-attention</b> and its properties.</p></p class="citation"></blockquote><h3 id=1062--10306-pqa-zero-shot-protein-question-answering-for-free-form-scientific-enquiry-with-large-language-models-eli-m-carrami-et-al-2024>(10/62 | 10/306) PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models (Eli M Carrami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eli M Carrami, Sahand Sharifzadeh. (2024)<br><strong>PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models</strong><br><button class=copy-to-clipboard title="PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Benchmarking, Multi-modal, Zero-shot, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13653v1.pdf filename=2402.13653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the novel task of <b>zero-shot</b> Protein <b>Question</b> <b>Answering</b> (PQA) for free-form scientific enquiry. Given a previously unseen protein sequence and a natural language <b>question,</b> <b>the</b> task is to deliver a scientifically accurate answer. This task not only supports future biological research, but could also provide a test bed for assessing the scientific precision of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We contribute the first specialized dataset for PQA model training, containing 257K protein sequences annotated with 1.97M scientific <b>question-answer</b> <b>pairs.</b> Additionally, we propose and study several novel biologically relevant <b>benchmarks</b> for scientific PQA. Employing two robust <b>multi-modal</b> architectures, we establish an initial state-of-the-art performance for PQA and reveal key performance factors through ablation studies. Our comprehensive PQA framework, named Pika, including dataset, code, model checkpoints, and a user-friendly demo, is openly accessible on github.com/EMCarrami/Pika, promoting wider research and application in the field.</p></p class="citation"></blockquote><h3 id=1162--11306-packd-pattern-clustered-knowledge-distillation-for-compressing-memory-access-prediction-models-neelesh-gupta-et-al-2024>(11/62 | 11/306) PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models (Neelesh Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neelesh Gupta, Pengmiao Zhang, Rajgopal Kannan, Viktor Prasanna. (2024)<br><strong>PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models</strong><br><button class=copy-to-clipboard title="PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Graph, Clustering, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13441v1.pdf filename=2402.13441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks (DNNs) have proven to be effective models for accurate Memory Access Prediction (MAP), a critical task in mitigating memory latency through data prefetching. However, existing DNN-based MAP models suffer from the challenges such as significant physical storage space and poor inference latency, primarily due to their large number of parameters. These limitations render them impractical for deployment in real-world scenarios. In this paper, we propose PaCKD, a Pattern-Clustered <b>Knowledge</b> <b>Distillation</b> approach to compress MAP models while maintaining the prediction performance. The PaCKD approach encompasses three steps: <b>clustering</b> memory access sequences into distinct partitions involving similar patterns, training large pattern-specific teacher models for memory access prediction for each partition, and training a single lightweight student model by <b>distilling</b> the <b>knowledge</b> <b>from</b> the trained pattern-specific teachers. We evaluate our approach on <b>LSTM,</b> MLP-Mixer, and ResNet models, as they exhibit diverse structures and are widely used for image classification tasks in order to test their effectiveness in four widely used <b>graph</b> applications. Compared to the teacher models with 5.406M parameters and an F1-score of 0.4626, our student models achieve a 552$\times$ model size compression while maintaining an F1-score of 0.4538 (with a 1.92% performance drop). Our approach yields an 8.70% higher result compared to student models trained with standard <b>knowledge</b> <b>distillation</b> and an 8.88% higher result compared to student models trained without any form of <b>knowledge</b> <b>distillation.</b></p></p class="citation"></blockquote><h3 id=1262--12306-a-simple-and-yet-fairly-effective-defense-for-graph-neural-networks-sofiane-ennadir-et-al-2024>(12/62 | 12/306) A Simple and Yet Fairly Effective Defense for Graph Neural Networks (Sofiane Ennadir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sofiane Ennadir, Yassine Abbahaddou, Johannes F. Lutzeyer, Michalis Vazirgiannis, Henrik Boström. (2024)<br><strong>A Simple and Yet Fairly Effective Defense for Graph Neural Networks</strong><br><button class=copy-to-clipboard title="A Simple and Yet Fairly Effective Defense for Graph Neural Networks" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph Convolutional Network, Node Classification, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13987v1.pdf filename=2402.13987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have emerged as the dominant approach for machine learning on <b>graph-structured</b> <b>data.</b> <b>However,</b> concerns have arisen regarding the vulnerability of <b>GNNs</b> to small adversarial perturbations. Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model&rsquo;s performance on clean <b>graphs.</b> <b>To</b> <b>address</b> these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model&rsquo;s architecture. We establish a theoretical connection between noise injection and the enhancement of <b>GNN</b> robustness, highlighting the effectiveness of our approach. We further conduct extensive empirical evaluations on the <b>node</b> <b>classification</b> task to validate our theoretical findings, focusing on two popular <b>GNNs:</b> the <b>GCN</b> and GIN. The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while minimizing added time complexity. The NoisyGNN approach is model-agnostic, allowing it to be integrated with different <b>GNN</b> architectures. Successful combinations of our NoisyGNN approach with existing defense techniques demonstrate even further improved adversarial defense results. Our code is publicly available at: <a href=https://github.com/Sennadir/NoisyGNN>https://github.com/Sennadir/NoisyGNN</a>.</p></p class="citation"></blockquote><h3 id=1362--13306-inductive-graph-alignment-prompt-bridging-the-gap-between-graph-pre-training-and-inductive-fine-tuning-from-spectral-perspective-yuchen-yan-et-al-2024>(13/62 | 13/306) Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective (Yuchen Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Yan, Peiyan Zhang, Zheng Fang, Qingqing Long. (2024)<br><strong>Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective</strong><br><button class=copy-to-clipboard title="Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: E-2, cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph Classification, Graph, Graph Neural Network, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13556v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13556v1.pdf filename=2402.13556v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>&ldquo;Graph</b> <b>pre-training</b> and <b>fine-tuning&rdquo;</b> paradigm has significantly improved <b>Graph</b> <b>Neural</b> Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and <b>fine-tuning</b> stages, the model performance is still limited. Inspired by <b>prompt</b> <b>fine-tuning</b> in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in <b>graph</b> <b>domain.</b> But existing methods simply reformulate the form of <b>fine-tuning</b> tasks to the pre-training ones. With the premise that the pre-training <b>graphs</b> <b>are</b> compatible with the <b>fine-tuning</b> ones, these methods typically operate in transductive setting. In order to generalize <b>graph</b> <b>pre-training</b> to inductive scenario where the <b>fine-tuning</b> <b>graphs</b> <b>might</b> significantly differ from pre-training ones, we propose a novel <b>graph</b> <b>prompt</b> based method called Inductive <b>Graph</b> <b>Alignment</b> Prompt(IGAP). Firstly, we unify the mainstream <b>graph</b> <b>pre-training</b> frameworks and analyze the essence of <b>graph</b> <b>pre-training</b> from <b>graph</b> <b>spectral</b> theory. Then we identify the two sources of the data gap in inductive setting: (i) <b>graph</b> <b>signal</b> gap and (ii) <b>graph</b> <b>structure</b> gap. Based on the insight of <b>graph</b> <b>pre-training,</b> we propose to bridge the <b>graph</b> <b>signal</b> gap and the <b>graph</b> <b>structure</b> gap with learnable <b>prompts</b> in the spectral space. A theoretical analysis ensures the effectiveness of our method. At last, we conduct extensive experiments among nodes classification and <b>graph</b> <b>classification</b> tasks under the transductive, semi-inductive and inductive settings. The results demonstrate that our proposed method can successfully bridge the data gap under different settings.</p></p class="citation"></blockquote><h3 id=1462--14306-learning-to-poison-large-language-models-during-instruction-tuning-yao-qiang-et-al-2024>(14/62 | 14/306) Learning to Poison Large Language Models During Instruction Tuning (Yao Qiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu. (2024)<br><strong>Learning to Poison Large Language Models During Instruction Tuning</strong><br><button class=copy-to-clipboard title="Learning to Poison Large Language Models During Instruction Tuning" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13459v1.pdf filename=2402.13459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has marked significant achievements in language processing and <b>reasoning</b> capabilities. Despite their advancements, <b>LLMs</b> face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in <b>LLMs</b> by designing a new data poisoning attack tailored to exploit the <b>instruction</b> <b>tuning</b> process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various <b>LLMs</b> and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1% of 4,000 <b>instruction</b> <b>tuning</b> samples leads to a Performance Drop Rate (PDR) of around 80%. Our work highlights the need for stronger defenses against data poisoning attack, offering insights into safeguarding <b>LLMs</b> against these more sophisticated attacks. The source code can be found on this GitHub repository: <a href=https://github.com/RookieZxy/GBTL/blob/main/README.md>https://github.com/RookieZxy/GBTL/blob/main/README.md</a>.</p></p class="citation"></blockquote><h3 id=1562--15306-contextual-molecule-representation-learning-from-chemical-reaction-knowledge-han-tang-et-al-2024>(15/62 | 15/306) Contextual Molecule Representation Learning from Chemical Reaction Knowledge (Han Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Tang, Shikun Feng, Bicheng Lin, Yuyan Ni, JIngjing Liu, Wei-Ying Ma, Yanyan Lan. (2024)<br><strong>Contextual Molecule Representation Learning from Chemical Reaction Knowledge</strong><br><button class=copy-to-clipboard title="Contextual Molecule Representation Learning from Chemical Reaction Knowledge" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 38<br>Keywords: Benchmarking, Fine-tuning, Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13779v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13779v1.pdf filename=2402.13779v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>self-supervised</b> <b>learning</b> has emerged as a powerful tool to harness abundant unlabelled data for <b>representation</b> <b>learning</b> and has been broadly adopted in diverse areas. However, when applied to molecular <b>representation</b> <b>learning</b> (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm. To tackle this challenge, we introduce REMO, a <b>self-supervised</b> <b>learning</b> framework that takes advantage of well-defined atom-combination rules in common chemistry. Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature. We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO offers a novel solution to MRL by exploiting the underlying shared patterns in chemical reactions as \textit{context} for pre-training, which effectively infers meaningful <b>representations</b> <b>of</b> common chemistry knowledge. Such contextual <b>representations</b> <b>can</b> then be utilized to support diverse downstream molecular tasks with minimum <b>finetuning,</b> such as affinity prediction and drug-drug interaction prediction. Extensive experimental results on MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type classification show that across all tested downstream tasks, REMO outperforms the standard baseline of single-molecule masked modeling used in current MRL. Remarkably, REMO is the pioneering deep learning model surpassing fingerprint-based methods in activity cliff <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1662--16306-ai-powered-predictions-for-electricity-load-in-prosumer-communities-aleksei-kychkin-et-al-2024>(16/62 | 16/306) AI-Powered Predictions for Electricity Load in Prosumer Communities (Aleksei Kychkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleksei Kychkin, Georgios C. Chasparis. (2024)<br><strong>AI-Powered Predictions for Electricity Load in Prosumer Communities</strong><br><button class=copy-to-clipboard title="AI-Powered Predictions for Electricity Load in Prosumer Communities" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 35<br>Keywords: Black Box, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13752v1.pdf filename=2402.13752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The flexibility in electricity consumption and production in communities of residential buildings, including those with renewable energy sources and energy storage (a.k.a., prosumers), can effectively be utilized through the advancement of short-term demand response mechanisms. It is known that flexibility can further be increased if demand response is performed at the level of communities of prosumers, since aggregated groups can better coordinate electricity consumption. However, the effectiveness of such short-term optimization is highly dependent on the accuracy of electricity load forecasts both for each building as well as for the whole community. Structural variations in the electricity load profile can be associated with different exogenous factors, such as weather conditions, calendar information and day of the week, as well as user behavior. In this paper, we review a wide range of electricity load forecasting techniques, that can provide significant assistance in optimizing load consumption in prosumer communities. We present and test artificial intelligence (AI) powered short-term load forecasting methodologies that operate with <b>black-box</b> <b>time</b> series models, such as Facebook&rsquo;s Prophet and <b>Long</b> <b>Short-term</b> <b>Memory</b> <b>(LSTM)</b> models; season-based SARIMA and smoothing Holt-Winters models; and empirical regression-based models that utilize domain knowledge. The integration of weather forecasts into data-driven time series forecasts is also tested. Results show that the combination of persistent and regression terms (adapted to the load forecasting task) achieves the best forecast accuracy.</p></p class="citation"></blockquote><h3 id=1762--17306-misalignment-learning-and-ranking-harnessing-users-limited-attention-arpit-agarwal-et-al-2024>(17/62 | 17/306) Misalignment, Learning, and Ranking: Harnessing Users Limited Attention (Arpit Agarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arpit Agarwal, Rad Niazadeh, Prathamesh Patil. (2024)<br><strong>Misalignment, Learning, and Ranking: Harnessing Users Limited Attention</strong><br><button class=copy-to-clipboard title="Misalignment, Learning, and Ranking: Harnessing Users Limited Attention" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Bandit Algorithm, Bandit Algorithm, Benchmarking, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14013v1.pdf filename=2402.14013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In digital health and EdTech, <b>recommendation</b> systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform&rsquo;s long-term payoffs. This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs. Our paper tackles this issue by utilizing users&rsquo; limited attention spans. We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time. Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item). We study the design of online <b>bandit</b> <b>algorithms</b> that obtain vanishing regret against hindsight optimal <b>benchmarks.</b> We first consider adversarial window sizes and stochastic iid payoffs. We design an active-elimination-based algorithm that achieves an optimal instance-dependent regret bound of $O(\log(T))$, by showing matching regret upper and lower bounds. The key idea is using the combinatorial structure of the problem to either obtain a large payoff from each item or to explore by getting a sample from that item. This method systematically narrows down the item choices to enhance learning efficiency and payoff. Second, we consider adversarial payoffs and stochastic iid window sizes. We start from the full-information problem of finding the permutation that maximizes the expected payoff. By a novel combinatorial argument, we characterize the polytope of admissible item selection probabilities by a permutation and show it has a polynomial-size representation. Using this representation, we show how standard algorithms for adversarial online linear optimization in the space of admissible probabilities can be used to obtain a polynomial-time algorithm with $O(\sqrt{T})$ regret.</p></p class="citation"></blockquote><h3 id=1862--18306-stability-aware-training-of-neural-network-interatomic-potentials-with-differentiable-boltzmann-estimators-sanjeev-raja-et-al-2024>(18/62 | 18/306) Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators (Sanjeev Raja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanjeev Raja, Ishan Amin, Fabian Pedregosa, Aditi S. Krishnapriyan. (2024)<br><strong>Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators</strong><br><button class=copy-to-clipboard title="Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-dis-nn, cond-mat-mtrl-sci, cs-LG, cs.LG, physics-chem-ph, physics-comp-ph<br>Keyword Score: 33<br>Keywords: Multi-modal, Simulation, Simulator, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13984v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13984v1.pdf filename=2402.13984v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) <b>simulations.</b> However, they can produce unstable <b>simulations</b> which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a <b>multi-modal</b> training procedure which combines conventional <b>supervised</b> training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs. StABlE Training iteratively runs MD <b>simulations</b> to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local instabilities. We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, along with using three modern NNIP architectures. In all three cases, StABlE-trained models achieve significant improvements in <b>simulation</b> stability and recovery of structural and dynamic observables. In some cases, StABlE-trained models outperform conventional models trained on datasets 50 times larger. As a general framework applicable across NNIP architectures and systems, StABlE Training is a powerful tool for training stable and accurate NNIPs, particularly in the absence of large reference datasets.</p></p class="citation"></blockquote><h3 id=1962--19306-attackgnn-red-teaming-gnns-in-hardware-security-using-reinforcement-learning-vasudev-gohil-et-al-2024>(19/62 | 19/306) AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning (Vasudev Gohil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vasudev Gohil, Satwik Patnaik, Dileep Kalathil, Jeyavijayan Rajendran. (2024)<br><strong>AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning</strong><br><button class=copy-to-clipboard title="AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13946v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13946v2.pdf filename=2402.13946v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)-based</b> techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits. In this work, we propose AttackGNN, the first red-team attack on <b>GNN-based</b> techniques in hardware security. To this end, we devise a novel <b>reinforcement</b> <b>learning</b> (RL) agent that generates adversarial examples, i.e., circuits, against the <b>GNN-based</b> techniques. We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent. We target five <b>GNN-based</b> techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation. Through our approach, we craft circuits that fool all <b>GNNs</b> considered in this work. For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the <b>GNN-based</b> defense into classifying our crafted circuits as not pirated. For attacking HT localization <b>GNN,</b> our attack generates HT-infested circuits that fool the defense on all tested circuits. We obtain a similar 100% success rate against <b>GNNs</b> for all classes of problems.</p></p class="citation"></blockquote><h3 id=2062--20306-reasoning-algorithmically-in-graph-neural-networks-danilo-numeroso-2024>(20/62 | 20/306) Reasoning Algorithmically in Graph Neural Networks (Danilo Numeroso, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danilo Numeroso. (2024)<br><strong>Reasoning Algorithmically in Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Reasoning Algorithmically in Graph Neural Networks" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Edge Classification, Graph, Graph Neural Network, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13744v1.pdf filename=2402.13744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of artificial intelligence systems with advanced <b>reasoning</b> capabilities represents a persistent and long-standing research question. Traditionally, the primary strategy to address this challenge involved the adoption of symbolic approaches, where knowledge was explicitly represented by means of symbols and explicitly programmed rules. However, with the advent of machine learning, there has been a paradigm shift towards systems that can autonomously learn from data, requiring minimal human guidance. In light of this shift, in latest years, there has been increasing interest and efforts at endowing neural networks with the ability to reason, bridging the gap between data-driven learning and logical <b>reasoning.</b> Within this context, Neural Algorithmic <b>Reasoning</b> (NAR) stands out as a promising research field, aiming to integrate the structured and rule-based <b>reasoning</b> of algorithms with the adaptive learning capabilities of neural networks, typically by tasking neural models to mimic classical algorithms. In this dissertation, we provide theoretical and practical contributions to this area of research. We explore the connections between neural networks and tropical algebra, deriving powerful architectures that are aligned with algorithm execution. Furthermore, we discuss and show the ability of such neural reasoners to learn and manipulate complex algorithmic and combinatorial optimization concepts, such as the principle of strong duality. Finally, in our empirical efforts, we validate the real-world utility of NAR networks across different practical scenarios. This includes tasks as diverse as planning problems, large-scale <b>edge</b> <b>classification</b> tasks and the learning of polynomial-time approximate algorithms for NP-hard combinatorial problems. Through this exploration, we aim to showcase the potential integrating algorithmic <b>reasoning</b> in machine learning models.</p></p class="citation"></blockquote><h3 id=2162--21306-linear-transformers-are-versatile-in-context-learners-max-vladymyrov-et-al-2024>(21/62 | 21/306) Linear Transformers are Versatile In-Context Learners (Max Vladymyrov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Max Vladymyrov, Johannes von Oswald, Mark Sandler, Rong Ge. (2024)<br><strong>Linear Transformers are Versatile In-Context Learners</strong><br><button class=copy-to-clipboard title="Linear Transformers are Versatile In-Context Learners" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Transformer, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14180v1.pdf filename=2402.14180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research has demonstrated that <b>transformers,</b> particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided <b>in-context</b> <b>during</b> their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that any linear <b>transformer</b> maintains an implicit linear model and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear <b>transformers</b> in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear <b>transformers</b> discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We reverse-engineer this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our findings show that even linear <b>transformers</b> possess the surprising ability to discover sophisticated optimization strategies.</p></p class="citation"></blockquote><h3 id=2262--22306-recursive-speculative-decoding-accelerating-llm-inference-via-sampling-without-replacement-wonseok-jeon-et-al-2024>(22/62 | 22/306) Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement (Wonseok Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, Christopher Lott. (2024)<br><strong>Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement</strong><br><button class=copy-to-clipboard title="Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14160v1.pdf filename=2402.14160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speculative decoding is an inference-acceleration method for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> where a small language model generates a draft-token sequence which is further verified by the target <b>LLM</b> in parallel. Recent works have advanced this method by establishing a draft-token tree, achieving superior performance over a single-sequence speculative decoding. However, those works independently generate tokens at each level of the tree, not leveraging the tree&rsquo;s entire diversifiability. Besides, their empirical superiority has been shown for fixed length of sequences, implicitly granting more computational resource to <b>LLM</b> for the tree-based methods. None of the existing works has conducted empirical studies with fixed target computational budgets despite its importance to resource-bounded devices. We present Recursive Speculative Decoding (RSD), a novel tree-based method that samples draft tokens without replacement and maximizes the diversity of the tree. During RSD&rsquo;s drafting, the tree is built by either Gumbel-Top-$k$ trick that draws tokens without replacement in parallel or Stochastic Beam Search that samples sequences without replacement while early-truncating unlikely draft sequences and reducing the computational cost of <b>LLM.</b> We empirically evaluate RSD with <b>Llama</b> 2 and OPT models, showing that RSD outperforms the baseline methods, consistently for fixed draft sequence length and in most cases for fixed computational budgets at <b>LLM.</b></p></p class="citation"></blockquote><h3 id=2362--23306-neuroflux-memory-efficient-cnn-training-using-adaptive-local-learning-dhananjay-saikumar-et-al-2024>(23/62 | 23/306) NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning (Dhananjay Saikumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhananjay Saikumar, Blesson Varghese. (2024)<br><strong>NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning</strong><br><button class=copy-to-clipboard title="NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14139v1.pdf filename=2402.14139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient on-device <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> training in resource-constrained mobile and edge environments is an open challenge. Backpropagation is the standard approach adopted, but it is GPU memory intensive due to its strong inter-layer dependencies that demand intermediate activations across the entire <b>CNN</b> model to be retained in GPU memory. This necessitates smaller batch sizes to make training possible within the available GPU memory budget, but in turn, results in a substantially high and impractical training time. We introduce NeuroFlux, a novel <b>CNN</b> training system tailored for memory-constrained scenarios. We develop two novel opportunities: firstly, adaptive auxiliary networks that employ a variable number of filters to reduce GPU memory usage, and secondly, block-specific adaptive batch sizes, which not only cater to the GPU memory constraints but also accelerate the training process. NeuroFlux segments the <b>CNNs</b> into blocks based on GPU memory usage and further attaches an auxiliary network to each layer in these blocks. This disrupts the typical layer dependencies under a new training paradigm - &lsquo;adaptive local learning&rsquo;. Moreover, NeuroFlux adeptly caches intermediate activations, eliminating redundant forward passes over previously trained blocks, further accelerating the training process. The results are twofold when compared to Backpropagation: on various hardware platforms, NeuroFlux demonstrates training speed-ups of 2.3$\times$ to 6.1$\times$ under stringent GPU memory budgets, and NeuroFlux generates streamlined models that have 10.9$\times$ to 29.4$\times$ fewer parameters without sacrificing accuracy.</p></p class="citation"></blockquote><h3 id=2462--24306-coercing-llms-to-do-and-reveal-almost-anything-jonas-geiping-et-al-2024>(24/62 | 24/306) Coercing LLMs to do and reveal (almost) anything (Jonas Geiping et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, Tom Goldstein. (2024)<br><strong>Coercing LLMs to do and reveal (almost) anything</strong><br><button class=copy-to-clipboard title="Coercing LLMs to do and reveal (almost) anything" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14020v1.pdf filename=2402.14020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It has recently been shown that <b>adversarial</b> <b>attacks</b> on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can &ldquo;jailbreak&rdquo; the model into making harmful statements. In this work, we argue that the spectrum of <b>adversarial</b> <b>attacks</b> on <b>LLMs</b> is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction. We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training <b>LLMs</b> with coding capabilities, as well as the continued existence of strange &ldquo;glitch&rdquo; tokens in common <b>LLM</b> vocabularies that should be removed for security reasons.</p></p class="citation"></blockquote><h3 id=2562--25306-do-efficient-transformers-really-save-computation-kai-yang-et-al-2024>(25/62 | 25/306) Do Efficient Transformers Really Save Computation? (Kai Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Yang, Jan Ackermann, Zhenyu He, Guhao Feng, Bohang Zhang, Yunzhen Feng, Qiwei Ye, Di He, Liwei Wang. (2024)<br><strong>Do Efficient Transformers Really Save Computation?</strong><br><button class=copy-to-clipboard title="Do Efficient Transformers Really Save Computation?" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Transformer, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13934v1.pdf filename=2402.13934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>transformer-based</b> language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard <b>Transformer</b> has become very valuable. While many efficient <b>Transformers</b> and <b>Transformer</b> alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard <b>Transformer.</b> This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient <b>Transformers,</b> specifically the Sparse <b>Transformer</b> and the Linear <b>Transformer.</b> We focus on their <b>reasoning</b> capability as exhibited by Chain-of-Thought (CoT) <b>prompts</b> and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard <b>Transformer.</b> We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient <b>Transformers&rsquo;</b> practical strengths and weaknesses.</p></p class="citation"></blockquote><h3 id=2662--26306-non-asymptotic-convergence-of-discrete-time-diffusion-models-new-approach-and-improved-rate-yuchen-liang-et-al-2024>(26/62 | 26/306) Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate (Yuchen Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Liang, Peizhong Ju, Yingbin Liang, Ness Shroff. (2024)<br><strong>Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate</strong><br><button class=copy-to-clipboard title="Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP, stat-ML<br>Keyword Score: 30<br>Keywords: Diffusion Model, Continuous Time, Continuous Time, Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13901v1.pdf filename=2402.13901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The denoising <b>diffusion</b> <b>model</b> emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for <b>continuous-time</b> <b>diffusion</b> <b>models,</b> and has been obtained for <b>discrete-time</b> <b>diffusion</b> <b>models</b> only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under <b>discrete-time</b> <b>diffusion</b> <b>models</b> and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further propose a novel accelerated sampler and show that it improves the convergence rates of the corresponding regular sampler by orders of magnitude with respect to all system parameters. For distributions with bounded support, our result improves the dimensional dependence of the previous convergence rate by orders of magnitude. Our study features a novel analysis technique that constructs tilting factor representation of the convergence error and exploits Tweedie&rsquo;s formula for handling Taylor expansion power terms.</p></p class="citation"></blockquote><h3 id=2762--27306-protect-and-extend----using-gans-for-synthetic-data-generation-of-time-series-medical-records-navid-ashrafi-et-al-2024>(27/62 | 27/306) Protect and Extend &ndash; Using GANs for Synthetic Data Generation of Time-Series Medical Records (Navid Ashrafi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Navid Ashrafi, Vera Schmitt, Robert P. Spang, Sebastian Möller, Jan-Niklas Voigt-Antons. (2024)<br><strong>Protect and Extend &ndash; Using GANs for Synthetic Data Generation of Time-Series Medical Records</strong><br><button class=copy-to-clipboard title="Protect and Extend -- Using GANs for Synthetic Data Generation of Time-Series Medical Records" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14042v1.pdf filename=2402.14042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Preservation of private user data is of paramount importance for high Quality of Experience (QoE) and acceptability, particularly with services treating sensitive data, such as IT-based health services. Whereas anonymization techniques were shown to be prone to data re-identification, synthetic data generation has gradually replaced anonymization since it is relatively less time and resource-consuming and more robust to data leakage. <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> have been used for generating synthetic datasets, especially <b>GAN</b> frameworks adhering to the <b>differential</b> <b>privacy</b> phenomena. This research compares state-of-the-art <b>GAN-based</b> models for synthetic data generation to generate time-series synthetic medical records of dementia patients which can be distributed without privacy concerns. Predictive modeling, autocorrelation, and distribution analysis are used to assess the Quality of Generating (QoG) of the generated data. The privacy preservation of the respective models is assessed by applying membership inference attacks to determine potential data leakage risks. Our experiments indicate the superiority of the privacy-preserving <b>GAN</b> (PPGAN) model over other models regarding privacy preservation while maintaining an acceptable level of QoG. The presented results can support better data protection for medical use cases in the future.</p></p class="citation"></blockquote><h3 id=2862--28306-e2usd-efficient-yet-effective-unsupervised-state-detection-for-multivariate-time-series-zhichen-lai-et-al-2024>(28/62 | 28/306) E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series (Zhichen Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhichen Lai, Huan Li, Dalin Zhang, Yan Zhao, Weizhu Qian, Christian S. Jensen. (2024)<br><strong>E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series</strong><br><button class=copy-to-clipboard title="E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DB, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Unsupervised Learning, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14041v1.pdf filename=2402.14041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose E2USD that enables efficient-yet-accurate <b>unsupervised</b> <b>MTS</b> state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input <b>MTSs</b> at low computational overhead. Additionally, we propose a False Negative Cancellation <b>Contrastive</b> <b>Learning</b> method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at <a href=https://github.com/AI4CTS/E2Usd>https://github.com/AI4CTS/E2Usd</a>.</p></p class="citation"></blockquote><h3 id=2962--29306-propd-dynamic-token-tree-pruning-and-generation-for-llm-parallel-decoding-shuzhang-zhong-et-al-2024>(29/62 | 29/306) ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding (Shuzhang Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, Ru Huang. (2024)<br><strong>ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding</strong><br><button class=copy-to-clipboard title="ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Pruning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13485v1.pdf filename=2402.13485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in generative <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have significantly boosted the performance in natural language processing tasks. However, their efficiency is hampered by the inherent limitations in autoregressive token generation. While parallel decoding with token tree verification, e.g., Medusa, has been proposed to improve decoding parallelism and efficiency, it often struggles with maintaining contextual relationships due to its independent token prediction approach and incurs significant verification overhead, especially with <b>large</b> <b>tree</b> <b>sizes</b> and batch processing. In this paper, we propose ProPD, an efficient <b>LLM</b> parallel decoding framework based on dynamic token tree <b>pruning</b> and generation. ProPD features an advanced early <b>pruning</b> mechanism to efficiently eliminate unpromising token sequences to improve verification efficiency. Additionally, it introduces a dynamic token tree generation algorithm to balance the computation and parallelism of the verification phase in real-time and maximize the overall efficiency across different batch sizes, sequence lengths, and tasks, etc. We verify ProPD across a diverse set of datasets, <b>LLMs,</b> and batch sizes and demonstrate ProPD consistently outperforms existing decoding algorithms by 1.1-3.2x.</p></p class="citation"></blockquote><h3 id=3062--30306-overcoming-saturation-in-density-ratio-estimation-by-iterated-regularization-lukas-gruber-et-al-2024>(30/62 | 30/306) Overcoming Saturation in Density Ratio Estimation by Iterated Regularization (Lukas Gruber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Gruber, Markus Holzleitner, Johannes Lehner, Sepp Hochreiter, Werner Zellinger. (2024)<br><strong>Overcoming Saturation in Density Ratio Estimation by Iterated Regularization</strong><br><button class=copy-to-clipboard title="Overcoming Saturation in Density Ratio Estimation by Iterated Regularization" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 23<br>Keywords: Benchmarking, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13891v1.pdf filename=2402.13891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics. In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems. To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates. Our methods outperform its non-iteratively regularized versions on <b>benchmarks</b> for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep <b>unsupervised</b> <b>domain</b> <b>adaptation</b> models.</p></p class="citation"></blockquote><h3 id=3162--31306-dslr-diversity-enhancement-and-structure-learning-for-rehearsal-based-graph-continual-learning-seungyoon-choi-et-al-2024>(31/62 | 31/306) DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning (Seungyoon Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungyoon Choi, Wonjoong Kim, Sungwon Kim, Yeonjun In, Sein Kim, Chanyoung Park. (2024)<br><strong>DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning</strong><br><button class=copy-to-clipboard title="DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Contrastive Learning, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13711v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13711v3.pdf filename=2402.13711v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the replay buffer in rehearsal-based approaches for <b>graph</b> <b>continual</b> <b>learning</b> <b>(GCL)</b> methods. Existing rehearsal-based <b>GCL</b> methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a <b>GCL</b> model named DSLR, specifically, we devise a coverage-based diversity (CD) approach to consider both the class representativeness and the diversity within each class of the replayed nodes. Moreover, we adopt <b>graph</b> structure learning (GSL) to ensure that the replayed nodes are connected to truly informative neighbors. Extensive experimental results demonstrate the effectiveness and efficiency of DSLR. Our source code is available at <a href=https://github.com/seungyoon-Choi/DSLR_official>https://github.com/seungyoon-Choi/DSLR_official</a>.</p></p class="citation"></blockquote><h3 id=3262--32306-simpro-a-simple-probabilistic-framework-towards-realistic-long-tailed-semi-supervised-learning-chaoqun-du-et-al-2024>(32/62 | 32/306) SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning (Chaoqun Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoqun Du, Yizeng Han, Gao Huang. (2024)<br><strong>SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning</strong><br><button class=copy-to-clipboard title="SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Probabilistic Model, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13505v1.pdf filename=2402.13505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>semi-supervised</b> <b>learning</b> have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a <b>probabilistic</b> <b>model,</b> innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization phase, leading to the formulation of a Bayes classifier. The Bayes classifier, in turn, enhances the quality of pseudo-labels in the expectation phase. Remarkably, the SimPro framework not only comes with theoretical guarantees but also is straightforward to implement. Moreover, we introduce two novel class distributions broadening the scope of the evaluation. Our method showcases consistent state-of-the-art performance across diverse <b>benchmarks</b> and data distribution scenarios. Our code is available at <a href=https://github.com/LeapLabTHU/SimPro>https://github.com/LeapLabTHU/SimPro</a>.</p></p class="citation"></blockquote><h3 id=3362--33306-intriguing-properties-of-modern-gans-roy-friedman-et-al-2024>(33/62 | 33/306) Intriguing Properties of Modern GANs (Roy Friedman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roy Friedman, Yair Weiss. (2024)<br><strong>Intriguing Properties of Modern GANs</strong><br><button class=copy-to-clipboard title="Intriguing Properties of Modern GANs" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14098v1.pdf filename=2402.14098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>GANs</b> achieve remarkable performance in terms of generating realistic and diverse samples. This has led many to believe that ``GANs capture the training data manifold&rsquo;&rsquo;. In this work we show that this interpretation is wrong. We empirically show that the manifold learned by modern <b>GANs</b> does not fit the training distribution: specifically the manifold does not pass through the training examples and passes closer to <b>out-of-distribution</b> images than to in-distribution images. We also investigate the distribution over images implied by the prior over the latent codes and study whether modern <b>GANs</b> learn a density that approximates the training distribution. Surprisingly, we find that the learned density is very far from the data distribution and that <b>GANs</b> tend to assign higher density to <b>out-of-distribution</b> images. Finally, we demonstrate that the set of images used to train modern <b>GANs</b> are often not part of the typical set described by the <b>GANs&rsquo;</b> distribution.</p></p class="citation"></blockquote><h3 id=3462--34306-generative-adversarial-models-for-extreme-downscaling-of-climate-datasets-guiye-li-et-al-2024>(34/62 | 34/306) Generative Adversarial Models for Extreme Downscaling of Climate Datasets (Guiye Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guiye Li, Guofeng Cao. (2024)<br><strong>Generative Adversarial Models for Extreme Downscaling of Climate Datasets</strong><br><button class=copy-to-clipboard title="Generative Adversarial Models for Extreme Downscaling of Climate Datasets" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, physics-ao-ph<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14049v1.pdf filename=2402.14049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the challenges of climate change requires accurate and high-resolution mapping of climate and weather variables. However, many existing climate datasets, such as the gridded outputs of the state-of-the-art numerical climate models (e.g., general circulation models), are only available at very coarse spatial resolutions due to the model complexity and extremely high computational demand. Deep-learning-based methods, particularly <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> and their variants, have proved effective for refining natural images, and have shown great promise in improving scientific datasets. In this paper, we describe a conditional <b>GAN-based</b> geospatial downscaling method for extreme downscaling of gridded climate datasets. Compared to most existing methods, the method can generate high-resolution accurate climate datasets from very low-resolution inputs. More importantly, the method explicitly considers the uncertainty inherent to the downscaling process that tends to be ignored in existing methods. Given an input, the method can produce a multitude of plausible high-resolution samples instead of one single deterministic result. These samples allow for an empirical exploration and inferences of model uncertainty and robustness. With a case study of gridded climate datasets (wind velocity and solar irradiance), we demonstrate the performances of the framework in downscaling tasks with very high scaling factors (up to $64\times$) and highlight the advantages of the framework with a comprehensive comparison with commonly used downscaling methods, including area-to-point (ATP) kriging, deep image prior (DIP), enhanced deep super-resolution network (EDSR), enhanced super-resolution <b>generative</b> <b>adversarial</b> <b>networks</b> (ESRGAN), and physics-informed resolution-enhancing <b>GAN</b> (PhIRE <b>GAN).</b></p></p class="citation"></blockquote><h3 id=3562--35306-bias-correction-of-wind-power-forecasts-with-scada-data-and-continuous-learning-stefan-jonas-et-al-2024>(35/62 | 35/306) Bias correction of wind power forecasts with SCADA data and continuous learning (Stefan Jonas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Jonas, Kevin Winter, Bernhard Brodbeck, Angela Meyer. (2024)<br><strong>Bias correction of wind power forecasts with SCADA data and continuous learning</strong><br><button class=copy-to-clipboard title="Bias correction of wind power forecasts with SCADA data and continuous learning" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13916v1.pdf filename=2402.13916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wind energy plays a critical role in the transition towards renewable energy sources. However, the uncertainty and variability of wind can impede its full potential and the necessary growth of wind power capacity. To mitigate these challenges, wind power forecasting methods are employed for applications in power management, energy trading, or maintenance scheduling. In this work, we present, evaluate, and compare four machine learning-based wind power forecasting models. Our models correct and improve 48-hour forecasts extracted from a numerical weather prediction (NWP) model. The models are evaluated on datasets from a wind park comprising 65 wind turbines. The best improvement in forecasting error and mean bias was achieved by a <b>convolutional</b> <b>neural</b> <b>network,</b> reducing the average NRMSE down to 22%, coupled with a significant reduction in mean bias, compared to a NRMSE of 35% from the strongly biased baseline model using uncorrected NWP forecasts. Our findings further indicate that changes to neural network architectures play a minor role in affecting the forecasting performance, and that future research should rather investigate changes in the model pipeline. Moreover, we introduce a continuous learning strategy, which is shown to achieve the highest forecasting performance improvements when new data is made available.</p></p class="citation"></blockquote><h3 id=3662--36306-simple-and-effective-transfer-learning-for-neuro-symbolic-integration-alessandro-daniele-et-al-2024>(36/62 | 36/306) Simple and Effective Transfer Learning for Neuro-Symbolic Integration (Alessandro Daniele et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Daniele, Tommaso Campari, Sagar Malhotra, Luciano Serafini. (2024)<br><strong>Simple and Effective Transfer Learning for Neuro-Symbolic Integration</strong><br><button class=copy-to-clipboard title="Simple and Effective Transfer Learning for Neuro-Symbolic Integration" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transfer Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14047v1.pdf filename=2402.14047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning (DL) techniques have achieved remarkable successes in recent years. However, their ability to generalize and execute <b>reasoning</b> tasks remains a challenge. A potential solution to this issue is Neuro-Symbolic Integration (NeSy), where neural approaches are combined with symbolic <b>reasoning.</b> Most of these methods exploit a neural network to map perceptions to symbols and a logical reasoner to predict the output of the downstream task. These methods exhibit superior generalization capacity compared to fully neural architectures. However, they suffer from several issues, including slow convergence, learning difficulties with complex perception tasks, and convergence to local minima. This paper proposes a simple yet effective method to ameliorate these problems. The key idea involves pretraining a neural model on the downstream task. Then, a NeSy model is trained on the same task via <b>transfer</b> <b>learning,</b> where the weights of the perceptual part are injected from the pretrained network. The key observation of our work is that the neural network fails to generalize only at the level of the symbolic part while being perfectly capable of learning the mapping from perceptions to symbols. We have tested our training strategy on various SOTA NeSy methods and datasets, demonstrating consistent improvements in the aforementioned problems.</p></p class="citation"></blockquote><h3 id=3762--37306-average-gradient-outer-product-as-a-mechanism-for-deep-neural-collapse-daniel-beaglehole-et-al-2024>(37/62 | 37/306) Average gradient outer product as a mechanism for deep neural collapse (Daniel Beaglehole et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Beaglehole, Peter Súkeník, Marco Mondelli, Mikhail Belkin. (2024)<br><strong>Average gradient outer product as a mechanism for deep neural collapse</strong><br><button class=copy-to-clipboard title="Average gradient outer product as a mechanism for deep neural collapse" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13728v1.pdf filename=2402.13728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs). Though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood. In this work, we provide substantial evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP). This takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model. We proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in DNNs. As shown in recent work, this singular structure is highly correlated with that of the AGOP. We then establish experimentally and theoretically that AGOP induces neural collapse in a randomly initialized neural network. In particular, we demonstrate that Deep Recursive Feature Machines, a method originally introduced as an abstraction for AGOP feature learning in <b>convolutional</b> <b>neural</b> <b>networks,</b> exhibits DNC.</p></p class="citation"></blockquote><h3 id=3862--38306-sparse-and-structured-hopfield-networks-saul-santos-et-al-2024>(38/62 | 38/306) Sparse and Structured Hopfield Networks (Saul Santos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saul Santos, Vlad Niculae, Daniel McNamee, Andre F. T. Martins. (2024)<br><strong>Sparse and Structured Hopfield Networks</strong><br><button class=copy-to-clipboard title="Sparse and Structured Hopfield Networks" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Multiple Instance Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13725v1.pdf filename=2402.13725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern Hopfield networks have enjoyed recent interest due to their connection to attention in <b>transformers.</b> Our paper provides a unified framework for sparse Hopfield networks by establishing a link with Fenchel-Young losses. The result is a new family of Hopfield-Fenchel-Young energies whose update rules are end-to-end differentiable sparse transformations. We reveal a connection between loss margins, sparsity, and exact memory retrieval. We further extend this framework to structured Hopfield networks via the SparseMAP transformation, which can retrieve pattern associations instead of a single pattern. Experiments on <b>multiple</b> <b>instance</b> <b>learning</b> and text rationalization demonstrate the usefulness of our approach.</p></p class="citation"></blockquote><h3 id=3962--39306-spot-check-equivalence-an-interpretable-metric-for-information-elicitation-mechanisms-shengwei-xu-et-al-2024>(39/62 | 39/306) Spot Check Equivalence: an Interpretable Metric for Information Elicitation Mechanisms (Shengwei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengwei Xu, Yichi Zhang, Paul Resnick, Grant Schoenebeck. (2024)<br><strong>Spot Check Equivalence: an Interpretable Metric for Information Elicitation Mechanisms</strong><br><button class=copy-to-clipboard title="Spot Check Equivalence: an Interpretable Metric for Information Elicitation Mechanisms" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-GT, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13567v1.pdf filename=2402.13567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Because high-quality data is like oxygen for AI systems, effectively eliciting information from crowdsourcing workers has become a first-order problem for developing high-performance machine learning algorithms. Two prevalent paradigms, spot-checking and peer prediction, enable the design of mechanisms to evaluate and incentivize high-quality data from human labelers. So far, at least three metrics have been proposed to compare the performances of these techniques [33, 8, 3]. However, different metrics lead to divergent and even contradictory results in various contexts. In this paper, we harmonize these divergent stories, showing that two of these metrics are actually the same within certain contexts and explain the divergence of the third. Moreover, we unify these different contexts by introducing \textit{Spot Check Equivalence}, which offers an interpretable metric for the effectiveness of a peer prediction mechanism. Finally, we present two approaches to compute spot check equivalence in various contexts, where <b>simulation</b> results verify the effectiveness of our proposed metric.</p></p class="citation"></blockquote><h3 id=4062--40306-diffplf-a-conditional-diffusion-model-for-probabilistic-forecasting-of-ev-charging-load-siyang-li-et-al-2024>(40/62 | 40/306) DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of EV Charging Load (Siyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyang Li, Hui Xiong, Yize Chen. (2024)<br><strong>DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of EV Charging Load</strong><br><button class=copy-to-clipboard title="DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of EV Charging Load" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13548v1.pdf filename=2402.13548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the vast electric vehicle (EV) penetration to distribution grid, charging load forecasting is essential to promote charging station operation and demand-side management.However, the stochastic charging behaviors and associated exogenous factors render future charging load patterns quite volatile and hard to predict. Accordingly, we devise a novel <b>Diffusion</b> <b>model</b> termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can explicitly approximate the predictive load distribution conditioned on historical data and related covariates. Specifically, we leverage a denoising <b>diffusion</b> <b>model,</b> which can progressively convert the Gaussian prior to real time-series data by learning a reversal of the <b>diffusion</b> <b>process.</b> Besides, we couple such <b>diffusion</b> <b>model</b> with a cross-attention-based conditioning mechanism to execute conditional generation for possible charging demand profiles. We also propose a task-informed <b>fine-tuning</b> technique to better adapt DiffPLF to the probabilistic time-series forecasting task and acquire more accurate and reliable predicted intervals. Finally, we conduct multiple experiments to validate the superiority of DiffPLF to predict complex temporal patterns of erratic charging load and carry out controllable generation based on certain covariate. Results demonstrate that we can attain a notable rise of 39.58% and 49.87% on MAE and CRPS respectively compared to the conventional method.</p></p class="citation"></blockquote><h3 id=4162--41306-prosparse-introducing-and-enhancing-intrinsic-activation-sparsity-within-large-language-models-chenyang-song-et-al-2024>(41/62 | 41/306) ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models (Chenyang Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun. (2024)<br><strong>ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models</strong><br><button class=copy-to-clipboard title="ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-7, cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13516v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13516v2.pdf filename=2402.13516v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help <b>LLMs</b> achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named &ldquo;ProSparse&rdquo; to push <b>LLMs</b> for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of <b>LLMs</b> with ReLU, ProSparse adopts progressive sparsity regularization with a factor smoothly increasing along sine curves in multiple stages. This can enhance activation sparsity and alleviate performance degradation by avoiding radical shifts in activation distribution. With ProSparse, we obtain high sparsity of 89.32% and 88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable performance to their original Swish-activated versions. Our inference acceleration experiments further demonstrate the practical acceleration brought by higher activation sparsity.</p></p class="citation"></blockquote><h3 id=4262--42306-stealthy-adversarial-attacks-on-stochastic-multi-armed-bandits-zhiwei-wang-et-al-2024>(42/62 | 42/306) Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits (Zhiwei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiwei Wang, Huazheng Wang, Hongning Wang. (2024)<br><strong>Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits</strong><br><button class=copy-to-clipboard title="Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13487v1.pdf filename=2402.13487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>attacks</b> against stochastic multi-armed <b>bandit</b> (MAB) algorithms have been extensively studied in the literature. In this work, we focus on reward poisoning attacks and find most existing attacks can be easily detected by our proposed detection method based on the test of homogeneity, due to their aggressive nature in reward manipulations. This motivates us to study the notion of stealthy attack against stochastic MABs and investigate the resulting attackability. Our analysis shows that against two popularly employed MAB algorithms, UCB1 and $\epsilon$-greedy, the success of a stealthy attack depends on the environmental conditions and the realized reward of the arm pulled in the first round. We also analyze the situation for general MAB algorithms equipped with our attack detection method and find that it is possible to have a stealthy attack that almost always succeeds. This brings new insights into the security risks of MAB algorithms.</p></p class="citation"></blockquote><h3 id=4362--43306-vn-network-embedding-newly-emerging-entities-with-virtual-neighbors-yongquan-he-et-al-2024>(43/62 | 43/306) VN Network: Embedding Newly Emerging Entities with Virtual Neighbors (Yongquan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongquan He, Zihan Wang, Peng Zhang, Zhaopeng Tu, Zhaochun Ren. (2024)<br><strong>VN Network: Embedding Newly Emerging Entities with Virtual Neighbors</strong><br><button class=copy-to-clipboard title="VN Network: Embedding Newly Emerging Entities with Virtual Neighbors" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-4; I-2-6, cs-AI, cs-LG, cs.LG<br>Keyword Score: 18<br>Keywords: Graph, Graph Neural Network, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14033v1.pdf filename=2402.14033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Embedding entities and relations into continuous vector spaces has attracted a surge of interest in recent years. Most embedding methods assume that all test entities are available during training, which makes it time-consuming to retrain embeddings for newly emerging entities. To address this issue, recent works apply the <b>graph</b> <b>neural</b> <b>network</b> on the existing neighbors of the unseen entities. In this paper, we propose a novel framework, namely Virtual Neighbor (VN) network, to address three key challenges. Firstly, to reduce the neighbor sparsity problem, we introduce the concept of the virtual neighbors inferred by rules. And we assign soft labels to these neighbors by solving a rule-constrained problem, rather than simply regarding them as unquestionably true. Secondly, many existing methods only use one-hop or two-hop neighbors for aggregation and ignore the distant information that may be helpful. Instead, we identify both logic and symmetric path rules to capture complex patterns. Finally, instead of one-time injection of rules, we employ an iterative learning scheme between the embedding method and virtual neighbor prediction to capture the interactions within. Experimental results on two <b>knowledge</b> <b>graph</b> <b>completion</b> <b>tasks</b> demonstrate that our VN network significantly outperforms state-of-the-art baselines. Furthermore, results on Subject/Object-R show that our proposed VN network is highly robust to the neighbor sparsity problem.</p></p class="citation"></blockquote><h3 id=4462--44306-hettree-heterogeneous-tree-graph-neural-network-mingyu-guan-et-al-2024>(44/62 | 44/306) HetTree: Heterogeneous Tree Graph Neural Network (Mingyu Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyu Guan, Jack W. Stokes, Qinlong Luo, Fuchen Liu, Purvanshi Mehta, Elnaz Nouri, Taesoo Kim. (2024)<br><strong>HetTree: Heterogeneous Tree Graph Neural Network</strong><br><button class=copy-to-clipboard title="HetTree: Heterogeneous Tree Graph Neural Network" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 16<br>Keywords: Graph, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13496v1.pdf filename=2402.13496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent past has seen an increasing interest in Heterogeneous <b>Graph</b> <b>Neural</b> <b>Networks</b> (HGNNs) since many real-world <b>graphs</b> <b>are</b> <b>heterogeneous</b> in nature, from citation <b>graphs</b> <b>to</b> <b>email</b> <b>graphs.</b> <b>However,</b> <b>existing</b> methods ignore a tree hierarchy among metapaths, which is naturally constituted by different node types and relation types. In this paper, we present HetTree, a novel heterogeneous tree <b>graph</b> <b>neural</b> <b>network</b> that models both the <b>graph</b> <b>structure</b> <b>and</b> heterogeneous aspects in a scalable and effective manner. Specifically, HetTree builds a semantic tree data structure to capture the hierarchy among metapaths. Existing tree encoding techniques aggregate children nodes by weighting the contribution of children nodes based on similarity to the parent node. However, we find that this tree encoding fails to capture the entire parent-children hierarchy by only considering the parent node. Hence, HetTree uses a novel subtree attention mechanism to emphasize metapaths that are more helpful in encoding parent-children relationships. Moreover, instead of separating feature learning from label learning or treating features and labels equally by projecting them to the same latent space, HetTree proposes to match them carefully based on corresponding metapaths, which provides more accurate and richer information between node features and labels. Our evaluation of HetTree on a variety of real-world datasets demonstrates that it outperforms all existing baselines on open <b>benchmarks</b> and efficiently scales to large real-world <b>graphs</b> <b>with</b> <b>millions</b> of nodes and edges.</p></p class="citation"></blockquote><h3 id=4562--45306-opening-the-black-box-a-systematic-review-on-explainable-ai-in-remote-sensing-adrian-höhl-et-al-2024>(45/62 | 45/306) Opening the Black-Box: A Systematic Review on Explainable AI in Remote Sensing (Adrian Höhl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian Höhl, Ivica Obadic, Miguel Ángel Fernández Torres, Hiba Najjar, Dario Oliveira, Zeynep Akata, Andreas Dengel, Xiao Xiang Zhu. (2024)<br><strong>Opening the Black-Box: A Systematic Review on Explainable AI in Remote Sensing</strong><br><button class=copy-to-clipboard title="Opening the Black-Box: A Systematic Review on Explainable AI in Remote Sensing" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Black Box, Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13791v1.pdf filename=2402.13791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>black-box</b> <b>machine</b> learning approaches have become a dominant modeling paradigm for knowledge extraction in Remote Sensing. Despite the potential benefits of uncovering the inner workings of these models with <b>explainable</b> <b>AI,</b> a comprehensive overview summarizing the used <b>explainable</b> <b>AI</b> methods and their objectives, findings, and challenges in Remote Sensing applications is still missing. In this paper, we address this issue by performing a systematic review to identify the key trends of how <b>explainable</b> <b>AI</b> is used in Remote Sensing and shed light on novel <b>explainable</b> <b>AI</b> approaches and emerging directions that tackle specific Remote Sensing challenges. We also reveal the common patterns of explanation interpretation, discuss the extracted scientific insights in Remote Sensing, and reflect on the approaches used for <b>explainable</b> <b>AI</b> methods evaluation. Our review provides a complete summary of the state-of-the-art in the field. Further, we give a detailed outlook on the challenges and promising research directions, representing a basis for novel methodological development and a useful starting point for new researchers in the field of <b>explainable</b> <b>AI</b> in Remote Sensing.</p></p class="citation"></blockquote><h3 id=4662--46306-accuracy-preserving-calibration-via-statistical-modeling-on-probability-simplex-yasushi-esaki-et-al-2024>(46/62 | 46/306) Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex (Yasushi Esaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasushi Esaki, Akihiro Nakamura, Keisuke Kawano, Ryoko Tokuhisa, Takuro Kutsuna. (2024)<br><strong>Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex</strong><br><button class=copy-to-clipboard title="Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Benchmarking, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13765v1.pdf filename=2402.13765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classification models based on deep neural networks (DNNs) must be calibrated to measure the reliability of predictions. Some recent calibration methods have employed a <b>probabilistic</b> <b>model</b> on the probability simplex. However, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy. We propose an accuracy-preserving calibration method using the Concrete distribution as the <b>probabilistic</b> <b>model</b> on the probability simplex. We theoretically prove that a DNN model trained on cross-entropy loss has optimality as the parameter of the Concrete distribution. We also propose an efficient method that synthetically generates samples for training <b>probabilistic</b> <b>models</b> on the probability simplex. We demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=4762--47306-cloudnine-analyzing-meteorological-observation-impact-on-weather-prediction-using-explainable-graph-neural-networks-hyeon-ju-jeon-et-al-2024>(47/62 | 47/306) CloudNine: Analyzing Meteorological Observation Impact on Weather Prediction Using Explainable Graph Neural Networks (Hyeon-Ju Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyeon-Ju Jeon, Jeon-Ho Kang, In-Hyuk Kwon, O-Joun Lee. (2024)<br><strong>CloudNine: Analyzing Meteorological Observation Impact on Weather Prediction Using Explainable Graph Neural Networks</strong><br><button class=copy-to-clipboard title="CloudNine: Analyzing Meteorological Observation Impact on Weather Prediction Using Explainable Graph Neural Networks" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, physics-ao-ph<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14861v1.pdf filename=2402.14861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The impact of meteorological observations on weather forecasting varies with sensor type, location, time, and other environmental factors. Thus, quantitative analysis of observation impacts is crucial for effective and efficient development of weather forecasting systems. However, the existing impact analysis methods are difficult to be widely applied due to their high dependencies on specific forecasting systems. Also, they cannot provide observation impacts at multiple spatio-temporal scales, only global impacts of observation types. To address these issues, we present a novel system called ``CloudNine,&rsquo;&rsquo; which allows analysis of individual observations&rsquo; impacts on specific predictions based on explainable <b>graph</b> <b>neural</b> <b>networks</b> (XGNNs). Combining an XGNN-based atmospheric state estimation model with a numerical weather prediction model, we provide a web application to search for observations in the 3D space of the Earth system and to visualize the impact of individual observations on predictions in specific spatial regions and time periods.</p></p class="citation"></blockquote><h3 id=4862--48306-corrective-machine-unlearning-shashwat-goel-et-al-2024>(48/62 | 48/306) Corrective Machine Unlearning (Shashwat Goel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal. (2024)<br><strong>Corrective Machine Unlearning</strong><br><button class=copy-to-clipboard title="Corrective Machine Unlearning" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14015v1.pdf filename=2402.14015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>Learning</b> models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged. We formalize &ldquo;Corrective <b>Machine</b> <b>Unlearning&rdquo;</b> as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning. However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting. We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training.</p></p class="citation"></blockquote><h3 id=4962--49306-fedadmm-insa-an-inexact-and-self-adaptive-admm-for-federated-learning-yongcun-song-et-al-2024>(49/62 | 49/306) FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning (Yongcun Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongcun Song, Ziqi Wang, Enrique Zuazua. (2024)<br><strong>FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning</strong><br><button class=copy-to-clipboard title="FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DC, cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13989v1.pdf filename=2402.13989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients&rsquo; local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle effect. The convergence of the resulting inexact ADMM is proved under the assumption of strongly convex loss functions. Additionally, we present a self-adaptive scheme that dynamically adjusts each client&rsquo;s penalty parameter, enhancing algorithm robustness by mitigating the need for empirical penalty parameter choices for each client. Extensive numerical experiments on both synthetic and real-world datasets are conducted. As validated by some numerical tests, our proposed algorithm can reduce the clients&rsquo; local computational load significantly and also accelerate the learning process compared to the vanilla FedADMM.</p></p class="citation"></blockquote><h3 id=5062--50306-enhancing-reinforcement-learning-agents-with-local-guides-paul-daoudi-et-al-2024>(50/62 | 50/306) Enhancing Reinforcement Learning Agents with Local Guides (Paul Daoudi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Daoudi, Bogdan Robu, Christophe Prieur, Ludovic Dos Santos, Merwan Barlier. (2024)<br><strong>Enhancing Reinforcement Learning Agents with Local Guides</strong><br><button class=copy-to-clipboard title="Enhancing Reinforcement Learning Agents with Local Guides" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13930v1.pdf filename=2402.13930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of integrating local guide policies into a <b>Reinforcement</b> <b>Learning</b> agent. For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure. This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions. We evaluated our method on a set of classical <b>Reinforcement</b> <b>Learning</b> problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences. In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based <b>Reinforcement</b> <b>Learning</b> algorithm, especially in its first learning stages.</p></p class="citation"></blockquote><h3 id=5162--51306-polynet-learning-diverse-solution-strategies-for-neural-combinatorial-optimization-andré-hottung-et-al-2024>(51/62 | 51/306) PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization (André Hottung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>André Hottung, Mridul Mahajan, Kevin Tierney. (2024)<br><strong>PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization</strong><br><button class=copy-to-clipboard title="PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14048v1.pdf filename=2402.14048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning-based</b> methods for constructing solutions to combinatorial optimization problems are rapidly approaching the performance of human-designed algorithms. To further narrow the gap, learning-based approaches must efficiently explore the solution space during the search process. Recent approaches artificially increase exploration by enforcing diverse solution generation through handcrafted rules, however, these rules can impair solution quality and are difficult to design for more complex problems. In this paper, we introduce PolyNet, an approach for improving exploration of the solution space by learning complementary solution strategies. In contrast to other works, PolyNet uses only a single-decoder and a training schema that does not enforce diverse solution generation through handcrafted rules. We evaluate PolyNet on four combinatorial optimization problems and observe that the implicit diversity mechanism allows PolyNet to find better solutions than approaches the explicitly enforce diverse solution generation.</p></p class="citation"></blockquote><h3 id=5262--52306-dealing-with-unbounded-gradients-in-stochastic-saddle-point-optimization-gergely-neu-et-al-2024>(52/62 | 52/306) Dealing with unbounded gradients in stochastic saddle-point optimization (Gergely Neu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gergely Neu, Nneka Okolo. (2024)<br><strong>Dealing with unbounded gradients in stochastic saddle-point optimization</strong><br><button class=copy-to-clipboard title="Dealing with unbounded gradients in stochastic saddle-point optimization" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13903v1.pdf filename=2402.13903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). Besides providing a set of general results, we also apply our algorithm to a specific problem in <b>reinforcement</b> <b>learning,</b> where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span.</p></p class="citation"></blockquote><h3 id=5362--53306-generative-probabilistic-time-series-forecasting-and-applications-in-grid-operations-xinyi-wang-et-al-2024>(53/62 | 53/306) Generative Probabilistic Time Series Forecasting and Applications in Grid Operations (Xinyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyi Wang, Lang Tong, Qing Zhao. (2024)<br><strong>Generative Probabilistic Time Series Forecasting and Applications in Grid Operations</strong><br><button class=copy-to-clipboard title="Generative Probabilistic Time Series Forecasting and Applications in Grid Operations" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP, stat-AP<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13870v1.pdf filename=2402.13870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations. Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations. Inspired by Wiener and Kallianpur&rsquo;s innovation representation, we propose a weak innovation <b>autoencoder</b> architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series. We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation <b>autoencoder</b> a canonical architecture for generative probabilistic forecasting. The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating superior performance across multiple forecasting measures over leading probabilistic and point forecasting techniques.</p></p class="citation"></blockquote><h3 id=5462--54306-replicable-learning-of-large-margin-halfspaces-alkis-kalavasis-et-al-2024>(54/62 | 54/306) Replicable Learning of Large-Margin Halfspaces (Alkis Kalavasis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alkis Kalavasis, Amin Karbasi, Kasper Green Larsen, Grigoris Velegkas, Felix Zhou. (2024)<br><strong>Replicable Learning of Large-Margin Halfspaces</strong><br><button class=copy-to-clipboard title="Replicable Learning of Large-Margin Halfspaces" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13857v1.pdf filename=2402.13857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We provide efficient replicable algorithms for the problem of learning large-margin halfspaces. Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al. [2022] with respect to all the relevant parameters. Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\epsilon$. We also design an <b>SGD-based</b> replicable algorithm that, in some parameters&rsquo; regimes, achieves better sample and time complexity than our first algorithm. Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to obtain a replicable algorithm for large-margin halfspaces with improved sample complexity with respect to the margin parameter $\tau$, but running time doubly exponential in $1/\tau^2$ and worse sample complexity dependence on $\epsilon$ than one of our previous algorithms. We then design an improved algorithm with better sample complexity than all three of our previous algorithms and running time exponential in $1/\tau^{2}$.</p></p class="citation"></blockquote><h3 id=5562--55306-performance-improvement-bounds-for-lipschitz-configurable-markov-decision-processes-alberto-maria-metelli-2024>(55/62 | 55/306) Performance Improvement Bounds for Lipschitz Configurable Markov Decision Processes (Alberto Maria Metelli, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Maria Metelli. (2024)<br><strong>Performance Improvement Bounds for Lipschitz Configurable Markov Decision Processes</strong><br><button class=copy-to-clipboard title="Performance Improvement Bounds for Lipschitz Configurable Markov Decision Processes" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13821v1.pdf filename=2402.13821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Configurable Markov Decision Processes (Conf-MDPs) have recently been introduced as an extension of the traditional Markov Decision Processes <b>(MDPs)</b> to model the real-world scenarios in which there is the possibility to intervene in the environment in order to configure some of its parameters. In this paper, we focus on a particular subclass of Conf-MDP that satisfies regularity conditions, namely Lipschitz continuity. We start by providing a bound on the Wasserstein distance between $\gamma$-discounted stationary distributions induced by changing policy and configuration. This result generalizes the already existing bounds both for Conf-MDPs and traditional <b>MDPs.</b> Then, we derive a novel performance improvement lower bound.</p></p class="citation"></blockquote><h3 id=5662--56306-fld-fourier-latent-dynamics-for-structured-motion-representation-and-learning-chenhao-li-et-al-2024>(56/62 | 56/306) FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning (Chenhao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenhao Li, Elijah Stanger-Jones, Steve Heim, Sangbae Kim. (2024)<br><strong>FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning</strong><br><button class=copy-to-clipboard title="FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs-SY, cs.LG, eess-SP, eess-SY<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13820v1.pdf filename=2402.13820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a <b>self-supervised,</b> structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms. The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training. With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed. By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms.</p></p class="citation"></blockquote><h3 id=5762--57306-voice-driven-mortality-prediction-in-hospitalized-heart-failure-patients-a-machine-learning-approach-enhanced-with-diagnostic-biomarkers-nihat-ahmadli-et-al-2024>(57/62 | 57/306) Voice-Driven Mortality Prediction in Hospitalized Heart Failure Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers (Nihat Ahmadli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nihat Ahmadli, Mehmet Ali Sarsil, Berk Mizrak, Kurtulus Karauzum, Ata Shaker, Erol Tulumen, Didar Mirzamidinov, Dilek Ural, Onur Ergen. (2024)<br><strong>Voice-Driven Mortality Prediction in Hospitalized Heart Failure Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers</strong><br><button class=copy-to-clipboard title="Voice-Driven Mortality Prediction in Hospitalized Heart Failure Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SD, cs.LG, eess-AS<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13812v1.pdf filename=2402.13812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing heart failure (HF) as a prevalent global health concern poses difficulties in implementing innovative approaches for enhanced patient care. Predicting mortality rates in HF patients, in particular, is difficult yet critical, necessitating individualized care, proactive management, and enabling educated decision-making to enhance outcomes. Recently, the significance of voice biomarkers coupled with Machine Learning (ML) has surged, demonstrating remarkable efficacy, particularly in predicting heart failure. The synergy of voice analysis and ML algorithms provides a non-invasive and easily accessible means to evaluate patients&rsquo; health. However, there is a lack of voice biomarkers for predicting mortality rates among heart failure patients with standardized speech protocols. Here, we demonstrate a powerful and effective ML model for predicting mortality rates in hospitalized HF patients through the utilization of voice biomarkers. By seamlessly integrating voice biomarkers into routine patient monitoring, this strategy has the potential to improve patient outcomes, optimize resource allocation, and advance patient-centered HF management. In this study, a Machine Learning system, specifically a <b>logistic</b> <b>regression</b> model, is trained to predict patients&rsquo; 5-year mortality rates using their speech as input. The model performs admirably and consistently, as demonstrated by cross-validation and statistical approaches (p-value &lt; 0.001). Furthermore, integrating NT-proBNP, a diagnostic biomarker in HF, improves the model&rsquo;s predictive accuracy substantially.</p></p class="citation"></blockquote><h3 id=5862--58306-the-expected-loss-of-preconditioned-langevin-dynamics-reveals-the-hessian-rank-amitay-bar-et-al-2024>(58/62 | 58/306) The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank (Amitay Bar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amitay Bar, Rotem Mulayoff, Tomer Michaeli, Ronen Talmon. (2024)<br><strong>The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank</strong><br><button class=copy-to-clipboard title="The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13810v1.pdf filename=2402.13810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Langevin dynamics (LD) is widely used for sampling from distributions and for optimization. In this work, we derive a closed-form expression for the expected loss of preconditioned LD near stationary points of the objective function. We use the fact that at the vicinity of such points, LD reduces to an Ornstein-Uhlenbeck process, which is amenable to convenient mathematical treatment. Our analysis reveals that when the preconditioning matrix satisfies a particular relation with respect to the noise covariance, LD&rsquo;s expected loss becomes proportional to the rank of the objective&rsquo;s Hessian. We illustrate the applicability of this result in the context of neural networks, where the Hessian rank has been shown to capture the complexity of the predictor function but is usually computationally hard to probe. Finally, we use our analysis to compare <b>SGD-like</b> and Adam-like preconditioners and identify the regimes under which each of them leads to a lower expected loss.</p></p class="citation"></blockquote><h3 id=5962--59306-on-the-expressive-power-of-a-variant-of-the-looped-transformer-yihang-gao-et-al-2024>(59/62 | 59/306) On the Expressive Power of a Variant of the Looped Transformer (Yihang Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihang Gao, Chuanyang Zheng, Enze Xie, Han Shi, Tianyang Hu, Yu Li, Michael K. Ng, Zhenguo Li, Zhaoqiang Liu. (2024)<br><strong>On the Expressive Power of a Variant of the Looped Transformer</strong><br><button class=copy-to-clipboard title="On the Expressive Power of a Variant of the Looped Transformer" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13572v1.pdf filename=2402.13572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Besides natural language processing, <b>transformers</b> exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard <b>transformers</b> are capable of performing some algorithms. To empower <b>transformers</b> with algorithmic capabilities and motivated by the recently proposed looped <b>transformer</b> (Yang et al., 2024; Giannou et al., 2023), we design a novel <b>transformer</b> block, dubbed Algorithm <b>Transformer</b> (abbreviated as AlgoFormer). Compared with the standard <b>transformer</b> and vanilla looped <b>transformer,</b> the proposed AlgoFormer can achieve significantly higher expressiveness in algorithm representation when using the same number of parameters. In particular, inspired by the structure of human-designed learning algorithms, our <b>transformer</b> block consists of a pre-transformer that is responsible for task pre-processing, a looped <b>transformer</b> for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing. We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical and empirical results are presented to show that the designed <b>transformer</b> has the potential to be smarter than human-designed algorithms. Experimental results demonstrate the empirical superiority of the proposed <b>transformer</b> in that it outperforms the standard <b>transformer</b> and vanilla looped <b>transformer</b> in some challenging tasks.</p></p class="citation"></blockquote><h3 id=6062--60306-theoretical-analysis-of-submodular-information-measures-for-targeted-data-subset-selection-nathan-beck-et-al-2024>(60/62 | 60/306) Theoretical Analysis of Submodular Information Measures for Targeted Data Subset Selection (Nathan Beck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathan Beck, Truong Pham, Rishabh Iyer. (2024)<br><strong>Theoretical Analysis of Submodular Information Measures for Targeted Data Subset Selection</strong><br><button class=copy-to-clipboard title="Theoretical Analysis of Submodular Information Measures for Targeted Data Subset Selection" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13454v1.pdf filename=2402.13454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With increasing volume of data being used across machine learning tasks, the capability to target specific subsets of data becomes more important. To aid in this capability, the recently proposed Submodular <b>Mutual</b> <b>Information</b> (SMI) has been effectively applied across numerous tasks in literature to perform targeted subset selection with the aid of a exemplar query set. However, all such works are deficient in providing theoretical guarantees for SMI in terms of its sensitivity to a subset&rsquo;s relevance and coverage of the targeted data. For the first time, we provide such guarantees by deriving similarity-based bounds on quantities related to relevance and coverage of the targeted data. With these bounds, we show that the SMI functions, which have empirically shown success in multiple applications, are theoretically sound in achieving good query relevance and query coverage.</p></p class="citation"></blockquote><h3 id=6162--61306-geometry-informed-neural-networks-arturs-berzins-et-al-2024>(61/62 | 61/306) Geometry-Informed Neural Networks (Arturs Berzins et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arturs Berzins, Andreas Radler, Sebastian Sanokowski, Sepp Hochreiter, Johannes Brandstetter. (2024)<br><strong>Geometry-Informed Neural Networks</strong><br><button class=copy-to-clipboard title="Geometry-Informed Neural Networks" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14009v1.pdf filename=2402.14009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the concept of <b>geometry-informed</b> neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.</p></p class="citation"></blockquote><h3 id=6262--62306-improving-building-temperature-forecasting-a-data-driven-approach-with-system-scenario-clustering-dafang-zhao-et-al-2024>(62/62 | 62/306) Improving Building Temperature Forecasting: A Data-driven Approach with System Scenario Clustering (Dafang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dafang Zhao, Zheng Chen, Zhengmao Li, Xiaolei Yuan, Ittetsu Taniguchi. (2024)<br><strong>Improving Building Temperature Forecasting: A Data-driven Approach with System Scenario Clustering</strong><br><button class=copy-to-clipboard title="Improving Building Temperature Forecasting: A Data-driven Approach with System Scenario Clustering" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13628v1.pdf filename=2402.13628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Heat, Ventilation and Air Conditioning (HVAC) systems play a critical role in maintaining a comfortable thermal environment and cost approximately 40% of primary energy usage in the building sector. For smart energy management in buildings, usage patterns and their resulting profiles allow the improvement of control systems with prediction capabilities. However, for large-scale HVAC system management, it is difficult to construct a detailed model for each subsystem. In this paper, a new data-driven room temperature prediction model is proposed based on the k-means <b>clustering</b> method. The proposed data-driven temperature prediction approach extracts the system operation feature through historical data analysis and further simplifies the system-level model to improve generalization and computational efficiency. We evaluate the proposed approach in the real world. The results demonstrated that our approach can significantly reduce modeling time without reducing prediction accuracy.</p></p class="citation"></blockquote><h2 id=cscl-94>cs.CL (94)</h2><h3 id=194--63306-flame-self-supervised-low-resource-taxonomy-expansion-using-large-language-models-sahil-mishra-et-al-2024>(1/94 | 63/306) FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large Language Models (Sahil Mishra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahil Mishra, Ujjwal Sudev, Tanmoy Chakraborty. (2024)<br><strong>FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large Language Models</strong><br><button class=copy-to-clipboard title="FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large Language Models" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SI, cs.CL<br>Keyword Score: 113<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Low-Resource, Recommendation, Reinforcement Learning, Self-supervised Learning, Supervised Learning, Stemming, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13623v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13623v1.pdf filename=2402.13623v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Taxonomies represent an arborescence hierarchical structure that establishes relationships among entities to convey knowledge within a specific domain. Each edge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find utility in various real-world applications, such as e-commerce search engines and <b>recommendation</b> systems. Consequently, there arises a necessity to enhance these taxonomies over time. However, manually curating taxonomies with neoteric data presents challenges due to limitations in available human resources and the exponential growth of data. Therefore, it becomes imperative to develop automatic taxonomy expansion methods. Traditional <b>supervised</b> taxonomy expansion approaches encounter difficulties <b>stemming</b> from limited resources, primarily due to the small size of existing taxonomies. This scarcity of training data often leads to overfitting. In this paper, we propose FLAME, a novel approach for taxonomy expansion in <b>low-resource</b> environments by harnessing the capabilities of <b>large</b> <b>language</b> <b>models</b> that are trained on extensive real-world knowledge. <b>LLMs</b> help compensate for the scarcity of domain-specific knowledge. Specifically, FLAME leverages <b>prompting</b> in <b>few-shot</b> settings to extract the inherent knowledge within the <b>LLMs,</b> ascertaining the hypernym entities within the taxonomy. Furthermore, it employs <b>reinforcement</b> <b>learning</b> to <b>fine-tune</b> the <b>large</b> <b>language</b> <b>models,</b> resulting in more accurate predictions. Experiments on three real-world <b>benchmark</b> datasets demonstrate the effectiveness of FLAME in real-world scenarios, achieving a remarkable improvement of 18.5% in accuracy and 12.3% in Wu & Palmer metric over eight baselines. Furthermore, we elucidate the strengths and weaknesses of FLAME through an extensive case study, error analysis and ablation studies on the <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=294--64306-unsupervised-text-style-transfer-via-llms-and-attention-masking-with-multi-way-interactions-lei-pan-et-al-2024>(2/94 | 64/306) Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions (Lei Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Pan, Yunshi Lan, Yang Li, Weining Qian. (2024)<br><strong>Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions</strong><br><button class=copy-to-clipboard title="Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Knowledge Distillation, Knowledge Distillation, Supervised Learning, Unsupervised Learning, Style Transfer, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13647v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13647v1.pdf filename=2402.13647v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> Text <b>Style</b> <b>Transfer</b> (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another <b>style</b> <b>without</b> changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; <b>knowledge</b> <b>distillation</b> from <b>LLMs</b> to attention masking model; <b>in-context</b> <b>learning</b> with constructed parallel examples. We empirically show these multi-way interactions can improve the baselines in certain perspective of <b>style</b> <b>strength,</b> content preservation and text fluency. Experiments also demonstrate that simply conducting <b>prompting</b> followed by attention masking-based revision can consistently surpass the other systems, including <b>supervised</b> text <b>style</b> <b>transfer</b> systems. On Yelp-clean and Amazon-clean datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute percentages respectively, and achieves new SOTA results.</p></p class="citation"></blockquote><h3 id=394--65306-cognitive-visual-language-mapper-advancing-multimodal-comprehension-with-enhanced-visual-knowledge-alignment-yunxin-li-et-al-2024>(3/94 | 65/306) Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment (Yunxin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunxin Li, Xinyu Chen, Baotian Hu, Haoyuan Shi, Min Zhang. (2024)<br><strong>Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment</strong><br><button class=copy-to-clipboard title="Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 89<br>Keywords: Benchmarking, Knowledge Distillation, Multi-modal, Multi-modal, Image2text, Question Answering, Visual Question Answering, Visual Question Answering, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13561v1.pdf filename=2402.13561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating and Rethinking the current landscape of <b>Large</b> <b>Multimodal</b> <b>Models</b> (LMMs), we observe that widely-used <b>visual-language</b> <b>projection</b> <b>approaches</b> (e.g., Q-former or MLP) focus on the alignment of <b>image-text</b> descriptions yet ignore the <b>visual</b> <b>knowledge-dimension</b> <b>alignment,</b> i.e., connecting <b>visuals</b> <b>to</b> <b>their</b> relevant knowledge. <b>Visual</b> <b>knowledge</b> <b>plays</b> a significant role in analyzing, inferring, and interpreting information from <b>visuals,</b> <b>helping</b> <b>improve</b> the accuracy of answers to knowledge-based <b>visual</b> <b>questions.</b> <b>In</b> this paper, we mainly explore improving LMMs with <b>visual-language</b> <b>knowledge</b> <b>alignment,</b> especially aimed at challenging knowledge-based <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA).</b> To this end, we present a Cognitive <b>Visual-Language</b> <b>Mapper</b> <b>(CVLM),</b> which contains a pretrained <b>Visual</b> <b>Knowledge</b> <b>Aligner</b> (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the <b>multimodal</b> <b>instruction</b> <b>tuning</b> stage. Specifically, we design the VKA based on the interaction between a small language model and a <b>visual</b> <b>encoder,</b> <b>training</b> it on collected image-knowledge pairs to achieve <b>visual</b> <b>knowledge</b> <b>acquisition</b> and projection. FKA is employed to <b>distill</b> the fine-grained <b>visual</b> <b>knowledge</b> <b>of</b> an image and inject it into <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> We conduct extensive experiments on knowledge-based <b>VQA</b> <b>benchmarks</b> and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based <b>VQA</b> (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.</p></p class="citation"></blockquote><h3 id=494--66306-a-multimodal-in-context-tuning-approach-for-e-commerce-product-description-generation-yunxin-li-et-al-2024>(4/94 | 66/306) A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation (Yunxin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunxin Li, Baotian Hu, Wenhan Luo, Lin Ma, Yuxin Ding, Min Zhang. (2024)<br><strong>A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation</strong><br><button class=copy-to-clipboard title="A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 86<br>Keywords: Multi-modal, Multi-modal, Language Generation, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt, Rouge, Rouge-L<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13587v1.pdf filename=2402.13587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a <b>language</b> <b>model-based</b> decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on <b>large-scale</b> <b>samples</b> <b>makes</b> models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective <b>Multimodal</b> <b>In-Context</b> <b>Tuning</b> approach, named ModICT, which introduces a similar product sample as the reference and utilizes the <b>in-context</b> <b>learning</b> capability of <b>language</b> <b>models</b> to produce the description. During training, we keep the visual encoder and <b>language</b> <b>model</b> frozen, focusing on optimizing the modules responsible for creating <b>multimodal</b> <b>in-context</b> <b>references</b> and dynamic <b>prompts.</b> This approach preserves the <b>language</b> <b>generation</b> prowess of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> facilitating a substantial increase in description diversity. To assess the effectiveness of ModICT across various <b>language</b> <b>model</b> scales and types, we collect data from three distinct product categories within the E-commerce domain. Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on <b>Rouge-L)</b> and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods. Our findings underscore the potential of ModICT as a valuable tool for enhancing automatic generation of product descriptions in a wide range of applications.</p></p class="citation"></blockquote><h3 id=594--67306-fanoutqa-multi-hop-multi-document-question-answering-for-large-language-models-andrew-zhu-et-al-2024>(5/94 | 67/306) FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models (Andrew Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Zhu, Alyssa Hwang, Liam Dugan, Chris Callison-Burch. (2024)<br><strong>FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models</strong><br><button class=copy-to-clipboard title="FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Claude, GPT, GPT-4, LLaMA, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14116v1.pdf filename=2402.14116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One type of <b>question</b> <b>that</b> is commonly found in day-to-day scenarios is ``fan-out&rsquo;&rsquo; <b>questions,</b> <b>complex</b> multi-hop, multi-document <b>reasoning</b> <b>questions</b> <b>that</b> require finding information about a <b>large</b> <b>number</b> <b>of</b> entities. However, there exist few resources to evaluate this type of <b>question-answering</b> <b>capability</b> among <b>large</b> <b>language</b> <b>models.</b> To evaluate complex <b>reasoning</b> in <b>LLMs</b> more fully, we present FanOutQA, a high-quality dataset of fan-out <b>question-answer</b> <b>pairs</b> and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three <b>benchmark</b> settings across our dataset and <b>benchmark</b> 7 <b>LLMs,</b> including <b>GPT-4,</b> <b>LLaMA</b> 2, <b>Claude-2.1,</b> and Mixtral-8x7B, finding that contemporary models still have room to improve <b>reasoning</b> over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at <a href=https://fanoutqa.com>https://fanoutqa.com</a></p></p class="citation"></blockquote><h3 id=694--68306-synfac-edit-synthetic-imitation-edit-feedback-for-factual-alignment-in-clinical-summarization-prakamya-mishra-et-al-2024>(6/94 | 68/306) SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization (Prakamya Mishra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prakamya Mishra, Zonghai Yao, Parth Vashisht, Feiyun Ouyang, Beining Wang, Vidhi Dhaval Mody, Hong Yu. (2024)<br><strong>SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization</strong><br><button class=copy-to-clipboard title="SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13919v1.pdf filename=2402.13919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> such as <b>GPT</b> and <b>Llama</b> have demonstrated significant achievements in <b>summarization</b> tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes <b>GPT-3.5</b> and <b>GPT-4</b> to generate high-quality feedback aimed at enhancing factual consistency in clinical note <b>summarization.</b> Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite <b>GPT&rsquo;s</b> proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or <b>LLMs</b> generation quality. This work leverages <b>GPT&rsquo;s</b> advanced capabilities in clinical NLP to offer expert-level edit feedback. Through the use of two distinct alignment algorithms (DPO and SALT) based on <b>GPT</b> edit feedback, our goal is to reduce hallucinations and align closely with medical facts, endeavoring to narrow the divide between AI-generated content and factual accuracy. This highlights the substantial potential of <b>GPT</b> edits in enhancing the alignment of clinical factuality.</p></p class="citation"></blockquote><h3 id=794--69306-kuaiji-the-first-chinese-accounting-large-language-model-jiayuan-luo-et-al-2024>(7/94 | 69/306) Kuaiji: the First Chinese Accounting Large Language Model (Jiayuan Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayuan Luo, Songhua Yang, Xiaoling Qiu, Panyu Chen, Yufei Nai, Wenxuan Zeng, Wentao Zhang, Xinke Jiang. (2024)<br><strong>Kuaiji: the First Chinese Accounting Large Language Model</strong><br><button class=copy-to-clipboard title="Kuaiji: the First Chinese Accounting Large Language Model" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Supervised Learning, ChatGPT, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13866v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13866v2.pdf filename=2402.13866v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>ChatGPT</b> and <b>GPT-4</b> have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting <b>Large</b> <b>Language</b> <b>Model.</b> Kuaiji is meticulously <b>fine-tuned</b> using the Baichuan framework, which encompasses continuous pre-training and <b>supervised</b> <b>fine-tuning</b> processes. Supported by CAtAcctQA, a dataset containing <b>large</b> <b>genuine</b> <b>accountant-client</b> dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting <b>LLM,</b> and the validation of its efficacy through real-world accounting scenarios.</p></p class="citation"></blockquote><h3 id=894--70306-arl2-aligning-retrievers-for-black-box-large-language-models-via-self-guided-adaptive-relevance-labeling-lingxi-zhang-et-al-2024>(8/94 | 70/306) ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling (Lingxi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingxi Zhang, Yue Yu, Kuan Wang, Chao Zhang. (2024)<br><strong>ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling</strong><br><button class=copy-to-clipboard title="ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 75<br>Keywords: Black Box, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Transfer Learning, Zero-shot, Massive Multitask Language Understanding (MMLU), Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13542v1.pdf filename=2402.13542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-augmented</b> <b>generation</b> <b>enhances</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> by incorporating relevant information from external knowledge sources. This enables <b>LLMs</b> to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with <b>LLMs</b> due to their separate training processes and the <b>black-box</b> <b>nature</b> of <b>LLMs.</b> To address this challenge, we propose ARL2, a retriever learning technique that harnesses <b>LLMs</b> as labelers. ARL2 leverages <b>LLMs</b> to annotate and score relevant evidence, enabling learning the retriever from robust <b>LLM</b> supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on <b>MMLU</b> compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust <b>transfer</b> <b>learning</b> capabilities and strong <b>zero-shot</b> generalization abilities. Our code will be published at \url{https://github.com/zhanglingxi-cs/ARL2}.</p></p class="citation"></blockquote><h3 id=994--71306-distillation-contrastive-decoding-improving-llms-reasoning-with-contrastive-decoding-and-distillation-phuc-phan-et-al-2024>(9/94 | 71/306) Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation (Phuc Phan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phuc Phan, Hieu Tran, Long Phan. (2024)<br><strong>Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation</strong><br><button class=copy-to-clipboard title="Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Knowledge Distillation, Quantization, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14874v1.pdf filename=2402.14874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a straightforward approach called <b>Distillation</b> Contrastive Decoding (DCD) to enhance the <b>reasoning</b> capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive <b>Chain-of-thought</b> <b>Prompting</b> and advanced <b>distillation</b> techniques, including Dropout and <b>Quantization.</b> This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive <b>prompts</b> with <b>distillation,</b> DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances <b>LLM</b> performance across a range of <b>reasoning</b> <b>benchmarks,</b> surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.</p></p class="citation"></blockquote><h3 id=1094--72306-self-distillation-bridges-distribution-gap-in-language-model-fine-tuning-zhaorui-yang-et-al-2024>(10/94 | 72/306) Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning (Zhaorui Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, Haozhe Feng, Minfeng Zhu, Wei Chen. (2024)<br><strong>Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning</strong><br><button class=copy-to-clipboard title="Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Knowledge Distillation, Self-Distillation, LLaMA, Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13669v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13669v1.pdf filename=2402.13669v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The surge in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has revolutionized natural language processing, but <b>fine-tuning</b> them for specific tasks often encounters challenges in balancing performance and preserving general <b>instruction-following</b> <b>abilities.</b> In this paper, we posit that the distribution gap between task datasets and the <b>LLMs</b> serves as the primary underlying cause. To address the problem, we introduce <b>Self-Distillation</b> <b>Fine-Tuning</b> (SDFT), a novel approach that bridges the distribution gap by guiding <b>fine-tuning</b> with a <b>distilled</b> dataset generated by the model itself to match its original distribution. Experimental results on the <b>Llama-2-chat</b> model across various <b>benchmarks</b> demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla <b>fine-tuning.</b> Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of <b>LLMs.</b> Our code is available at \url{https://github.com/sail-sg/sdft}.</p></p class="citation"></blockquote><h3 id=1194--73306-making-reasoning-matter-measuring-and-improving-faithfulness-of-chain-of-thought-reasoning-debjit-paul-et-al-2024>(11/94 | 73/306) Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning (Debjit Paul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings. (2024)<br><strong>Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning</strong><br><button class=copy-to-clipboard title="Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Counter-factual, Fine-tuning, Out-of-distribution, Supervised Learning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13950v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13950v2.pdf filename=2402.13950v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model&rsquo;s final answer is faithful to the stated <b>reasoning</b> steps. In this paper, we perform a causal mediation analysis on twelve <b>LLMs</b> to examine how intermediate <b>reasoning</b> steps generated by the <b>LLM</b> influence the final outcome and find that <b>LLMs</b> do not reliably use their intermediate <b>reasoning</b> steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct <b>reasoning</b> steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct <b>reasoning</b> steps using an implicit causal reward function and a <b>reasoning</b> module that learns to faithfully reason over these intermediate inferences using a <b>counterfactual</b> and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the <b>reasoning</b> LM, yielding higher performance on <b>out-of-distribution</b> test sets. Finally, we find that FRODO&rsquo;s rationales are more faithful to its final answer predictions than standard <b>supervised</b> <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=1294--74306-activerag-revealing-the-treasures-of-knowledge-via-active-learning-zhipeng-xu-et-al-2024>(12/94 | 74/306) ActiveRAG: Revealing the Treasures of Knowledge via Active Learning (Zhipeng Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, Ge Yu. (2024)<br><strong>ActiveRAG: Revealing the Treasures of Knowledge via Active Learning</strong><br><button class=copy-to-clipboard title="ActiveRAG: Revealing the Treasures of Knowledge via Active Learning" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Active Learning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13547v1.pdf filename=2402.13547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> has introduced a new paradigm for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> aiding in the resolution of knowledge-intensive tasks. However, current <b>RAG</b> models position <b>LLMs</b> as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative <b>RAG</b> framework that shifts from passive knowledge acquisition to an <b>active</b> <b>learning</b> mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of <b>LLMs.</b> Our experimental results demonstrate that ActiveRAG surpasses previous <b>RAG</b> models, achieving a 5% improvement on <b>question-answering</b> <b>datasets.</b> All data and codes are available at <a href=https://github.com/OpenMatch/ActiveRAG>https://github.com/OpenMatch/ActiveRAG</a>.</p></p class="citation"></blockquote><h3 id=1394--75306-gradsafe-detecting-unsafe-prompts-for-llms-via-safety-critical-gradient-analysis-yueqi-xie-et-al-2024>(13/94 | 75/306) GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis (Yueqi Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yueqi Xie, Minghong Fang, Renjie Pi, Neil Gong. (2024)<br><strong>GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis</strong><br><button class=copy-to-clipboard title="GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Zero-shot, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13494v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13494v1.pdf filename=2402.13494v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> face threats from unsafe <b>prompts.</b> Existing methods for detecting unsafe <b>prompts</b> are primarily online moderation APIs or <b>finetuned</b> <b>LLMs.</b> These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe <b>prompts</b> by scrutinizing the gradients of safety-critical parameters in <b>LLMs.</b> Our methodology is grounded in a pivotal observation: the gradients of an <b>LLM&rsquo;s</b> loss for unsafe <b>prompts</b> paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe <b>prompts</b> lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from <b>prompts</b> (paired with compliance responses) to accurately detect unsafe <b>prompts.</b> We show that GradSafe, applied to <b>Llama-2</b> without further training, outperforms <b>Llama</b> Guard, despite its extensive <b>finetuning</b> with a <b>large</b> <b>dataset,</b> <b>in</b> detecting unsafe <b>prompts.</b> This superior performance is consistent across both <b>zero-shot</b> and adaptation scenarios, as evidenced by our evaluations on the ToxicChat and XSTest. The source code is available at <a href=https://github.com/xyq7/GradSafe>https://github.com/xyq7/GradSafe</a>.</p></p class="citation"></blockquote><h3 id=1494--76306-pca-bench-evaluating-multimodal-large-language-models-in-perception-cognition-action-chain-liang-chen-et-al-2024>(14/94 | 76/306) PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain (Liang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, Baobao Chang. (2024)<br><strong>PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain</strong><br><button class=copy-to-clipboard title="PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 69<br>Keywords: Automatic Evaluation, Benchmarking, Multi-modal, Multi-modal, GPT, GPT-4, Reasoning, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15527v1.pdf filename=2402.15527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present PCA-Bench, a <b>multimodal</b> decision-making <b>benchmark</b> for evaluating the integrated capabilities of <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs). Departing from previous <b>benchmarks</b> focusing on simplistic tasks and individual model capability, PCA-Bench introduces three complex scenarios: autonomous driving, domestic robotics, and open-world games. Given task <b>instructions</b> <b>and</b> diverse contexts, the model is required to seamlessly integrate multiple capabilities of Perception, Cognition, and Action in a <b>reasoning</b> chain to make accurate decisions. Moreover, PCA-Bench features error localization capabilities, scrutinizing model inaccuracies in areas such as perception, knowledge, or <b>reasoning.</b> This enhances the reliability of deploying MLLMs. To balance accuracy and efficiency in evaluation, we propose PCA-Eval, an <b>automatic</b> <b>evaluation</b> protocol, and assess 10 prevalent MLLMs. The results reveal significant performance disparities between open-source models and powerful proprietary models like <b>GPT-4</b> Vision. To address this, we introduce Embodied-Instruction-Evolution (EIE), an <b>automatic</b> <b>framework</b> for synthesizing <b>instruction</b> <b>tuning</b> examples in <b>multimodal</b> embodied environments. EIE generates 7,510 training examples in PCA-Bench and enhances the performance of open-source MLLMs, occasionally surpassing <b>GPT-4</b> Vision (+3% in decision accuracy), thereby validating the effectiveness of EIE. Our findings suggest that robust MLLMs like <b>GPT4-Vision</b> show promise for decision-making in embodied agents, opening new avenues for MLLM research.</p></p class="citation"></blockquote><h3 id=1594--77306-is-llm-as-a-judge-robust-investigating-universal-adversarial-attacks-on-zero-shot-llm-assessment-vyas-raina-et-al-2024>(15/94 | 77/306) Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment (Vyas Raina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vyas Raina, Adian Liusie, Mark Gales. (2024)<br><strong>Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment</strong><br><button class=copy-to-clipboard title="Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 66<br>Keywords: Benchmarking, Benchmarking, Zero-shot, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14016v1.pdf filename=2402.14016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are powerful <b>zero-shot</b> assessors and are increasingly used in real-world situations such as for written exams or <b>benchmarking</b> systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the <b>adversarial</b> <b>robustness</b> of assessment <b>LLMs,</b> where we search for short universal phrases that when appended to texts can deceive <b>LLMs</b> to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both <b>LLM-scoring</b> and pairwise <b>LLM-comparative</b> assessment are vulnerable to simple concatenation attacks, where in particular <b>LLM-scoring</b> is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferable and phrases learned on smaller open-source <b>LLMs</b> can be applied to larger closed-source models, such as <b>GPT3.5.</b> This highlights the pervasive nature of the <b>adversarial</b> <b>vulnerabilities</b> across different judge-LLM sizes, families and methods. Our findings raise significant concerns on the reliability of <b>LLMs-as-a-judge</b> methods, and underscore the importance of addressing vulnerabilities in <b>LLM</b> assessment methods before deployment in high-stakes real-world scenarios.</p></p class="citation"></blockquote><h3 id=1694--78306-investigating-multilingual-instruction-tuning-do-polyglot-models-demand-for-multilingual-instructions-alexander-arno-weber-et-al-2024>(16/94 | 78/306) Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions? (Alexander Arno Weber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Arno Weber, Klaudia Thellmann, Jan Ebert, Nicolas Flores-Herr, Jens Lehmann, Michael Fromm, Mehdi Ali. (2024)<br><strong>Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?</strong><br><button class=copy-to-clipboard title="Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-4, Instruction Following, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13703v1.pdf filename=2402.13703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The adaption of multilingual pre-trained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn <b>instruction-tuning</b> <b>benchmarks</b> across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and <b>instruction</b> <b>dataset</b> size on a mid-sized, multilingual <b>LLM</b> by <b>instruction-tuning</b> <b>it</b> on parallel <b>instruction-tuning</b> <b>datasets.</b> Our results demonstrate that <b>instruction-tuning</b> <b>on</b> parallel instead of monolingual corpora benefits cross-lingual <b>instruction</b> <b>following</b> capabilities by up to 4.6%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring <b>large-scale</b> <b>instruction-tuning</b> <b>datasets.</b> Finally, we conduct a human annotation study to understand the alignment between human-based and <b>GPT-4-based</b> evaluation within multilingual chat scenarios.</p></p class="citation"></blockquote><h3 id=1794--79306-refutebench-evaluating-refuting-instruction-following-for-large-language-models-jianhao-yan-et-al-2024>(17/94 | 79/306) RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models (Jianhao Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhao Yan, Yun Luo, Yue Zhang. (2024)<br><strong>RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models</strong><br><button class=copy-to-clipboard title="RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Instruction Following, Neural Machine Translation, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13463v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13463v2.pdf filename=2402.13463v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The application scope of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is increasingly expanding. In practical use, users might provide feedback based on the model&rsquo;s output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users&rsquo; refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive <b>benchmark,</b> RefuteBench, covering tasks such as <b>question</b> <b>answering,</b> <b>machine</b> <b>translation,</b> and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting <b>instructions</b> <b>and</b> whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous <b>LLMs</b> and find that <b>LLMs</b> are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the length of the conversation increases, models gradually forget the user&rsquo;s stated feedback and roll back to their own responses. We further propose a recall-and-repeat <b>prompts</b> as a simple and effective way to enhance the model&rsquo;s responsiveness to feedback.</p></p class="citation"></blockquote><h3 id=1894--80306-camelot-towards-large-language-models-with-training-free-consolidated-associative-memory-zexue-he-et-al-2024>(18/94 | 80/306) CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory (Zexue He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, Rogerio Feris. (2024)<br><strong>CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory</strong><br><button class=copy-to-clipboard title="CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Transformer, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13449v1.pdf filename=2402.13449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> struggle to handle long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new <b>LLM.</b> In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based <b>LLM</b> without re-training, enabling it to handle arbitrarily long input sequences. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base <b>LLM</b> can achieve significant (up to 29.7% on Arxiv) <b>perplexity</b> reduction in long-context modeling compared to other baselines evaluated on standard <b>benchmarks.</b> This architecture, which we call CAMELoT (Consolidated Associative Memory Enhanced Long <b>Transformer),</b> demonstrates superior performance even with a tiny context window of 128 tokens, and also enables improved <b>in-context</b> <b>learning</b> with a much larger set of demonstrations.</p></p class="citation"></blockquote><h3 id=1994--81306-lexc-gen-generating-data-for-extremely-low-resource-languages-with-large-language-models-and-bilingual-lexicons-zheng-xin-yong-et-al-2024>(19/94 | 81/306) LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons (Zheng-Xin Yong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach. (2024)<br><strong>LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons</strong><br><button class=copy-to-clipboard title="LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: High-Resource, Low-Resource, GPT-4, Sentiment Analysis, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14086v1.pdf filename=2402.14086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data scarcity in <b>low-resource</b> languages can be addressed with word-to-word translations from labeled task data in <b>high-resource</b> languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates <b>low-resource-language</b> classification task data at scale. Specifically, LexC-Gen first uses <b>high-resource-language</b> words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into <b>low-resource</b> languages with bilingual lexicons via word translation. Across 17 extremely <b>low-resource</b> languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on <b>sentiment</b> <b>analysis</b> and topic classification tasks respectively. We show that conditioning on bilingual lexicons is the key component of LexC-Gen. LexC-Gen is also practical &ndash; it only needs a single GPU to generate data at scale. It works well with open-access <b>LLMs,</b> and its cost is one-fifth of the cost of <b>GPT4-based</b> multilingual data generation.</p></p class="citation"></blockquote><h3 id=2094--82306-textttse2-textitsequential-example-textitselection-for-in-context-learning-haoyu-liu-et-al-2024>(20/94 | 82/306) $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning (Haoyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Liu, Jianfeng Liu, Shaohan Huang, Yuefeng Zhan, Hao Sun, Weiwei Deng, Furu Wei, Qi Zhang. (2024)<br><strong>$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning</strong><br><button class=copy-to-clipboard title="$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13874v1.pdf filename=2402.13874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The remarkable capability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for <b>in-context</b> <b>learning</b> <b>(ICL)</b> needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for <b>ICL,</b> predominantly following the &ldquo;select then organize&rdquo; paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\textit{se}$quential $\textit{se}$lection problem and introduce $\texttt{Se}^2$, a sequential-aware method that leverages the <b>LLM&rsquo;s</b> feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of <b>ICL</b> <b>prompts.</b> Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42% relative improvement over random selection. Further in-depth analysis show the effectiveness of proposed strategies, highlighting $\texttt{Se}^2$&rsquo;s exceptional stability and adaptability across various scenarios. Our code will be released to facilitate future research.</p></p class="citation"></blockquote><h3 id=2194--83306-unlocking-instructive-in-context-learning-with-tabular-prompting-for-relational-triple-extraction-guozheng-li-et-al-2024>(21/94 | 83/306) Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction (Guozheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guozheng Li, Wenjun Ke, Peng Wang, Zijie Xu, Ke Ji, Jiajun Liu, Ziyu Shang, Qiqing Luo. (2024)<br><strong>Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction</strong><br><button class=copy-to-clipboard title="Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13741v1.pdf filename=2402.13741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>in-context</b> <b>learning</b> <b>(ICL)</b> for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective <b>prompts</b> and (2) how to select proper demonstrations. Existing methods, however, fail to address these challenges appropriately. On the one hand, they usually recast RTE task to text-to-text <b>prompting</b> formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection. These issues are blocking improved performance in <b>ICL</b> for RTE, thus we aim to tackle <b>prompt</b> designing and sample selection challenges simultaneously. To this end, we devise a tabular <b>prompting</b> for RTE (\textsc{TableIE}) which frames RTE task into a table generation task to incorporate explicit structured information into <b>ICL,</b> facilitating conversion of outputs to RTE structures. Then we propose instructive <b>in-context</b> <b>learning</b> (I$^2$CL) which only selects and annotates a few samples considering internal triple semantics in massive unlabeled samples.</p></p class="citation"></blockquote><h3 id=2294--84306-kinit-at-semeval-2024-task-8-fine-tuned-llms-for-multilingual-machine-generated-text-detection-michal-spiegel-et-al-2024>(22/94 | 84/306) KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection (Michal Spiegel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michal Spiegel, Dominik Macko. (2024)<br><strong>KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection</strong><br><button class=copy-to-clipboard title="KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 55<br>Keywords: Black Box, Fine-tuning, Fine-tuning, Text Classification, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13671v1.pdf filename=2402.13671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual <b>black-box</b> <b>machine-generated</b> <b>text</b> <b>detection.</b> Such a detection is important for preventing a potential misuse of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> the newest of which are very capable in generating multilingual human-like <b>texts.</b> <b>We</b> have coped with this task in multiple ways, utilizing language identification and parameter-efficient <b>fine-tuning</b> of smaller <b>LLMs</b> for <b>text</b> <b>classification.</b> We have further used the per-language classification-threshold calibration to uniquely combine <b>fine-tuned</b> models predictions with statistical detection metrics to improve generalization of the system detection performance. Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner.</p></p class="citation"></blockquote><h3 id=2394--85306-hallucinations-or-attention-misdirection-the-path-to-strategic-value-extraction-in-business-using-large-language-models-aline-ioste-2024>(23/94 | 85/306) Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models (Aline Ioste, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aline Ioste. (2024)<br><strong>Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models</strong><br><button class=copy-to-clipboard title="Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT, Transformer, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14002v1.pdf filename=2402.14002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> with <b>transformer</b> architecture have revolutionized the domain of <b>text</b> <b>generation,</b> setting unprecedented <b>benchmarks.</b> Despite their impressive capabilities, <b>LLMs</b> have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations. This term, however, has often been misapplied to any results deviating from the instructor&rsquo;s expectations, which this paper defines as attention misdirection rather than true hallucinations. Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models. This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by <b>GPT</b> in response to a real business challenge. It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models. This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2494--86306-towards-building-multilingual-language-model-for-medicine-pengcheng-qiu-et-al-2024>(24/94 | 86/306) Towards Building Multilingual Language Model for Medicine (Pengcheng Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, Weidi Xie. (2024)<br><strong>Towards Building Multilingual Language Model for Medicine</strong><br><button class=copy-to-clipboard title="Towards Building Multilingual Language Model for Medicine" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT, GPT-4, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13963v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13963v2.pdf filename=2402.13963v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general <b>LLMs.</b> second, to monitor the development of multilingual <b>LLMs</b> in medicine, we propose a new multilingual medical multi-choice <b>question-answering</b> <b>benchmark</b> with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on our <b>benchmark,</b> along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling <b>GPT-4</b> on MMedBench. We will make the resources publicly available, including code, model weights, and datasets.</p></p class="citation"></blockquote><h3 id=2594--87306-omgeval-an-open-multilingual-generative-evaluation-benchmark-for-large-language-models-yang-liu-et-al-2024>(25/94 | 87/306) OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models (Yang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Liu, Meng Xu, Shuo Wang, Liner Yang, Haoyu Wang, Zhenghao Liu, Cunliang Kong, Yun Chen, Yang Liu, Maosong Sun, Erhong Yang. (2024)<br><strong>OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models</strong><br><button class=copy-to-clipboard title="OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT, GPT-4, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13524v1.pdf filename=2402.13524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> should generally benefit individuals from various cultural backgrounds around the world. However, most recent advanced generative evaluation <b>benchmarks</b> tailed for <b>LLMs</b> mainly focus on English. To this end, we introduce OMGEval, the first Open-source Multilingual Generative test set that can assess the capability of <b>LLMs</b> in different languages. For each language, OMGEval provides 804 open-ended questions, covering a wide range of important capabilities of <b>LLMs,</b> such as general knowledge, logical <b>reasoning,</b> and so on. Each question is rigorously verified by human annotators. Notably, to sufficiently reflect the compatibility of <b>LLMs</b> in different cultural backgrounds, we perform localization for each non-English language. Specifically, the current version of OMGEval includes 5 languages (i.e., Zh, Ru, Fr, Es, Ar). Following AlpacaEval, we employ <b>GPT-4</b> as the adjudicator to automatically score different model outputs, which is shown closely related to human evaluation. We evaluate several representative multilingual <b>LLMs</b> on the proposed OMGEval, which we believe will provide a valuable reference for the community to further understand and improve the multilingual capability of <b>LLMs.</b> OMGEval is available at <a href=https://github.com/blcuicall/OMGEval>https://github.com/blcuicall/OMGEval</a>.</p></p class="citation"></blockquote><h3 id=2694--88306-mm-soc-benchmarking-multimodal-large-language-models-in-social-media-platforms-yiqiao-jin-et-al-2024>(26/94 | 88/306) MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms (Yiqiao Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, Srijan Kumar. (2024)<br><strong>MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms</strong><br><button class=copy-to-clipboard title="MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-CY, cs.CL<br>Keyword Score: 52<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Zero-shot, Hate Speech Detection, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14154v1.pdf filename=2402.14154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social media platforms are hubs for <b>multimodal</b> information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation. This paper introduces MM-Soc, a comprehensive <b>benchmark</b> designed to evaluate MLLMs&rsquo; understanding of <b>multimodal</b> social media content. MM-Soc compiles prominent <b>multimodal</b> datasets and incorporates a novel <b>large-scale</b> <b>YouTube</b> <b>tagging</b> dataset, targeting a range of tasks from misinformation detection, <b>hate</b> <b>speech</b> <b>detection,</b> and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need for advancements in models&rsquo; social understanding capabilities. Our analysis reveals that, in a <b>zero-shot</b> setting, various types of MLLMs generally exhibit difficulties in handling social media tasks. However, MLLMs demonstrate performance improvements post <b>fine-tuning,</b> suggesting potential pathways for improvement.</p></p class="citation"></blockquote><h3 id=2794--89306-bba-bi-modal-behavioral-alignment-for-reasoning-with-large-vision-language-models-xueliang-zhao-et-al-2024>(27/94 | 89/306) BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models (Xueliang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xueliang Zhao, Xinting Huang, Tingchen Fu, Qintong Li, Shansan Gong, Lemao Liu, Wei Bi, Lingpeng Kong. (2024)<br><strong>BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 51<br>Keywords: Geometry, Multi-modal, Multi-modal, GPT, Reasoning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13577v1.pdf filename=2402.13577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>reasoning</b> stands as a pivotal capability for large <b>vision-language</b> models (LVLMs). The integration with Domain-Specific Languages (DSL), offering precise visual representations, equips these models with the opportunity to execute more accurate <b>reasoning</b> in complex and professional domains. However, the vanilla Chain-of-Thought (CoT) <b>prompting</b> method faces challenges in effectively leveraging the unique strengths of visual and DSL representations, primarily due to their differing <b>reasoning</b> mechanisms. Additionally, it often falls short in addressing critical steps in multi-step <b>reasoning</b> tasks. To mitigate these challenges, we introduce the \underline{B}i-Modal \underline{B}ehavioral \underline{A}lignment (BBA) <b>prompting</b> method, designed to maximize the potential of DSL in augmenting complex <b>multi-modal</b> <b>reasoning</b> tasks. This method initiates by guiding LVLMs to create separate <b>reasoning</b> chains for visual and DSL representations. Subsequently, it aligns these chains by addressing any inconsistencies, thus achieving a cohesive integration of behaviors from different modalities. Our experiments demonstrate that BBA substantially improves the performance of <b>GPT-4V(ision)</b> on <b>geometry</b> problem solving ($28.34% \to 34.22%$), chess positional advantage prediction ($42.08% \to 46.99%$) and molecular property prediction ($77.47% \to 83.52%$).</p></p class="citation"></blockquote><h3 id=2894--90306-driving-generative-agents-with-their-personality-lawrence-j-klinkert-et-al-2024>(28/94 | 90/306) Driving Generative Agents With Their Personality (Lawrence J. Klinkert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lawrence J. Klinkert, Stephanie Buongiorno, Corey Clark. (2024)<br><strong>Driving Generative Agents With Their Personality</strong><br><button class=copy-to-clipboard title="Driving Generative Agents With Their Personality" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14879v1.pdf filename=2402.14879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research explores the potential of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to utilize psychometric values, specifically personality information, within the context of video game character development. Affective Computing (AC) systems quantify a Non-Player character&rsquo;s (NPC) psyche, and an <b>LLM</b> can take advantage of the system&rsquo;s information by using the values for <b>prompt</b> generation. The research shows an <b>LLM</b> can consistently represent a given personality profile, thereby enhancing the human-like characteristics of game characters. Repurposing a human examination, the International Personality Item Pool (IPIP) questionnaire, to evaluate an <b>LLM</b> shows that the model can accurately generate content concerning the personality provided. Results show that the improvement of <b>LLM,</b> such as the latest <b>GPT-4</b> model, can consistently utilize and interpret a personality to represent behavior.</p></p class="citation"></blockquote><h3 id=2994--91306-whats-in-a-name-auditing-large-language-models-for-race-and-gender-bias-amit-haim-et-al-2024>(29/94 | 91/306) What&rsquo;s in a Name? Auditing Large Language Models for Race and Gender Bias (Amit Haim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Haim, Alejandro Salinas, Julian Nyarko. (2024)<br><strong>What&rsquo;s in a Name? Auditing Large Language Models for Race and Gender Bias</strong><br><button class=copy-to-clipboard title="What's in a Name? Auditing Large Language Models for Race and Gender Bias" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14875v1.pdf filename=2402.14875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We employ an audit design to investigate biases in state-of-the-art <b>large</b> <b>language</b> <b>models,</b> including <b>GPT-4.</b> In our study, we elicit <b>prompt</b> the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 <b>prompt</b> templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the <b>prompt</b> can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of <b>LLM</b> deployment and implementation to mitigate their potential for harm against marginalized communities.</p></p class="citation"></blockquote><h3 id=3094--92306-semantic-mirror-jailbreak-genetic-algorithm-based-jailbreak-prompts-against-open-source-llms-xiaoxia-li-et-al-2024>(30/94 | 92/306) Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs (Xiaoxia Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, Ee-Chien Chang. (2024)<br><strong>Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs</strong><br><button class=copy-to-clipboard title="Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-NE, cs.CL<br>Keyword Score: 50<br>Keywords: Automatic Speech Recognition, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14872v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14872v2.pdf filename=2402.14872v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> used in creative writing, <b>code</b> <b>generation,</b> and translation, generate text based on input sequences but are vulnerable to jailbreak attacks, where crafted <b>prompts</b> induce harmful outputs. Most jailbreak <b>prompt</b> methods use a combination of jailbreak templates followed by questions to ask to create jailbreak <b>prompts.</b> However, existing jailbreak <b>prompt</b> designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds. Jailbreak <b>prompts</b> are semantically more varied than the original questions used for queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach that bypasses <b>LLMs</b> by generating jailbreak <b>prompts</b> that are semantically similar to the original question. We model the search for jailbreak <b>prompts</b> that satisfy both semantic similarity and jailbreak validity as a multi-objective optimization problem and employ a standardized set of genetic algorithms for generating eligible <b>prompts.</b> Compared to the baseline AutoDAN-GA, SMJ achieves attack success rates <b>(ASR)</b> that are at most 35.4% higher without ONION defense and 85.2% higher with ONION defense. SMJ&rsquo;s better performance in all three semantic meaningfulness metrics of Jailbreak <b>Prompt,</b> Similarity, and Outlier, also means that SMJ is resistant to defenses that use those metrics as thresholds.</p></p class="citation"></blockquote><h3 id=3194--93306-beyond-hate-speech-nlps-challenges-and-opportunities-in-uncovering-dehumanizing-language-hezhao-zhang-et-al-2024>(31/94 | 93/306) Beyond Hate Speech: NLP&rsquo;s Challenges and Opportunities in Uncovering Dehumanizing Language (Hezhao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hezhao Zhang, Lasana Harris, Nafise Sadat Moosavi. (2024)<br><strong>Beyond Hate Speech: NLP&rsquo;s Challenges and Opportunities in Uncovering Dehumanizing Language</strong><br><button class=copy-to-clipboard title="Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, LLaMA<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13818v1.pdf filename=2402.13818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dehumanization, characterized as a subtle yet harmful manifestation of hate speech, involves denying individuals of their human qualities and often results in violence against marginalized groups. Despite significant progress in Natural Language Processing across various domains, its application in detecting dehumanizing language is limited, largely due to the scarcity of publicly available annotated data for this domain. This paper evaluates the performance of cutting-edge NLP models, including <b>GPT-4,</b> <b>GPT-3.5,</b> and <b>LLAMA-2,</b> in identifying dehumanizing language. Our findings reveal that while these models demonstrate potential, achieving a 70% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanization for other target groups. Moreover, leveraging one of the best-performing models, we automatically annotated a larger dataset for training more accessible models. However, our findings indicate that these models currently do not meet the high-quality data generation threshold necessary for this task.</p></p class="citation"></blockquote><h3 id=3294--94306-longrope-extending-llm-context-window-beyond-2-million-tokens-yiran-ding-et-al-2024>(32/94 | 94/306) LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens (Yiran Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, Mao Yang. (2024)<br><strong>LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens</strong><br><button class=copy-to-clipboard title="LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13753v1.pdf filename=2402.13753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>context</b> <b>window</b> is a desirable feature in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, due to high <b>fine-tuning</b> costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained <b>LLMs</b> to an impressive 2048k tokens, with up to only 1k <b>fine-tuning</b> steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for <b>fine-tuning</b> and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first <b>fine-tunes</b> a 256k length <b>LLM</b> and then conducts a second positional interpolation on the <b>fine-tuned</b> extended <b>LLM</b> to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and <b>Mistral</b> across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.</p></p class="citation"></blockquote><h3 id=3394--95306-gcof-self-iterative-text-generation-for-copywriting-using-large-language-model-jianghui-zhou-et-al-2024>(33/94 | 95/306) GCOF: Self-iterative Text Generation for Copywriting Using Large Language Model (Jianghui Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianghui Zhou, Ya Gao, Jie Liu, Xuemin Zhao, Zhaohua Yang, Yue Wu, Lirong Shi. (2024)<br><strong>GCOF: Self-iterative Text Generation for Copywriting Using Large Language Model</strong><br><button class=copy-to-clipboard title="GCOF: Self-iterative Text Generation for Copywriting Using Large Language Model" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: ChatGPT, Text Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13667v1.pdf filename=2402.13667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models(LLM)</b> such as <b>ChatGPT</b> have substantially simplified the generation of marketing copy, yet producing content satisfying domain specific requirements, such as effectively engaging customers, remains a significant challenge. In this work, we introduce the Genetic Copy Optimization Framework (GCOF) designed to enhance both efficiency and engagememnt of marketing copy creation. We conduct explicit feature engineering within the <b>prompts</b> of <b>LLM.</b> Additionally, we modify the crossover operator in Genetic Algorithm (GA), integrating it into the GCOF to enable automatic feature engineering. This integration facilitates a self-iterative refinement of the marketing copy. Compared to human curated copy, Online results indicate that copy produced by our framework achieves an average increase in click-through rate (CTR) of over $50%$.</p></p class="citation"></blockquote><h3 id=3494--96306-user-llm-efficient-llm-contextualization-with-user-embeddings-lin-ning-et-al-2024>(34/94 | 96/306) User-LLM: Efficient LLM Contextualization with User Embeddings (Lin Ning et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Ning, Luyang Liu, Jiaxing Wu, Neo Wu, Devora Berlowitz, Sushant Prakash, Bradley Green, Shawn O&rsquo;Banion, Jun Xie. (2024)<br><strong>User-LLM: Efficient LLM Contextualization with User Embeddings</strong><br><button class=copy-to-clipboard title="User-LLM: Efficient LLM Contextualization with User Embeddings" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Self-supervised Learning, Self-supervised Pre-training, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13598v1.pdf filename=2402.13598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize <b>LLMs.</b> These embeddings, <b>distilled</b> from diverse user interactions using <b>self-supervised</b> <b>pretraining,</b> capture latent user preferences and their evolution over time. We integrate these user embeddings with <b>LLMs</b> through cross-attention and soft-prompting, enabling <b>LLMs</b> to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorporate Perceiver layers to streamline the integration between user encoders and <b>LLMs,</b> reducing computational demands.</p></p class="citation"></blockquote><h3 id=3594--97306-winoviz-probing-visual-properties-of-objects-under-different-states-woojeong-jin-et-al-2024>(35/94 | 97/306) WinoViz: Probing Visual Properties of Objects Under Different States (Woojeong Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Woojeong Jin, Tejas Srinivasan, Jesse Thomason, Xiang Ren. (2024)<br><strong>WinoViz: Probing Visual Properties of Objects Under Different States</strong><br><button class=copy-to-clipboard title="WinoViz: Probing Visual Properties of Objects Under Different States" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Reasoning, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13584v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13584v1.pdf filename=2402.13584v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans perceive and comprehend different visual properties of an object based on specific contexts. For instance, we know that a banana turns brown <code>when it becomes rotten,'' whereas it appears green </code>when it is unripe.&rsquo;&rsquo; Previous studies on probing visual commonsense knowledge have primarily focused on examining language models&rsquo; understanding of typical properties (e.g., colors and shapes) of objects. We present WinoViz, a text-only evaluation dataset, consisting of 1,380 examples that probe the <b>reasoning</b> abilities of language models regarding variant visual properties of objects under different contexts or states. Our task is challenging since it requires pragmatic <b>reasoning</b> (finding intended meanings) and visual knowledge <b>reasoning.</b> We also present multi-hop data, a more challenging version of our data, which requires multi-step <b>reasoning</b> chains to solve our task. In our experimental analysis, our findings are: a) <b>Large</b> <b>language</b> <b>models</b> such as <b>GPT-4</b> demonstrate effective performance, but when it comes to multi-hop data, their performance is significantly degraded. b) <b>Large</b> <b>models</b> <b>perform</b> well on pragmatic <b>reasoning,</b> but visual knowledge <b>reasoning</b> is a bottleneck in our task. c) <b>Vision-language</b> models outperform their language-model counterparts. d) A model with machine-generated images performs poorly in our task. This is due to the poor quality of the generated images.</p></p class="citation"></blockquote><h3 id=3694--98306-are-llms-effective-negotiators-systematic-evaluation-of-the-multifaceted-capabilities-of-llms-in-negotiation-dialogues-deuksin-kwon-et-al-2024>(36/94 | 98/306) Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues (Deuksin Kwon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deuksin Kwon, Emily Weiss, Tara Kulshrestha, Kushal Chawla, Gale M. Lucas, Jonathan Gratch. (2024)<br><strong>Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues</strong><br><button class=copy-to-clipboard title="Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Dialogue System, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13550v1.pdf filename=2402.13550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A successful negotiation demands a deep comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer the partner&rsquo;s motives, as well as strategic <b>reasoning</b> and effective communication, making it challenging for automated systems. Given the remarkable performance of <b>LLMs</b> across a variety of NLP tasks, in this work, we aim to understand how <b>LLMs</b> can advance different aspects of negotiation research, ranging from designing <b>dialogue</b> <b>systems</b> to providing pedagogical feedback and scaling up data collection practices. To this end, we devise a methodology to analyze the multifaceted capabilities of <b>LLMs</b> across diverse <b>dialogue</b> <b>scenarios</b> covering all the time stages of a typical negotiation interaction. Our analysis adds to the increasing evidence for the superiority of <b>GPT-4</b> across various tasks while also providing insights into specific tasks that remain difficult for <b>LLMs.</b> For instance, the models correlate poorly with human players when making subjective assessments about the negotiation <b>dialogues</b> <b>and</b> often struggle to generate responses that are contextually appropriate as well as strategically advantageous.</p></p class="citation"></blockquote><h3 id=3794--99306-self-dc-when-to-retrieve-and-when-to-generate-self-divide-and-conquer-for-compositional-unknown-questions-hongru-wang-et-al-2024>(37/94 | 99/306) Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions (Hongru Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Guanhua Chen, Huimin Wang, Kam-fai Wong. (2024)<br><strong>Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions</strong><br><button class=copy-to-clipboard title="Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Open-Domain Question Answering, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13514v1.pdf filename=2402.13514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known <b>questions</b> <b>in</b> <b>open-domain</b> <b>question-answering,</b> <b>while</b> the former retrieves necessary external knowledge and the later <b>prompt</b> the <b>large</b> <b>language</b> <b>models</b> to generate internal known knowledge encoded in the parameters. However, few of previous works consider the compositional unknown <b>questions,</b> <b>which</b> consist of several known or unknown sub-questions. Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown <b>question.</b> <b>To</b> this end, we propose the first Compositional unknown <b>Question-Answering</b> <b>dataset</b> (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower <b>LLMs</b> to adaptively call different methods on-demand, resulting in better performance and efficiency. Experimental results on two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve comparable or even better performance with much more less retrieval times compared with several strong baselines.</p></p class="citation"></blockquote><h3 id=3894--100306-the-lay-persons-guide-to-biomedicine-orchestrating-large-language-models-zheheng-luo-et-al-2024>(38/94 | 100/306) The Lay Person&rsquo;s Guide to Biomedicine: Orchestrating Large Language Models (Zheheng Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheheng Luo, Qianqian Xie, Sophia Ananiadou. (2024)<br><strong>The Lay Person&rsquo;s Guide to Biomedicine: Orchestrating Large Language Models</strong><br><button class=copy-to-clipboard title="The Lay Person's Guide to Biomedicine: Orchestrating Large Language Models" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Supervised Learning, Zero-shot, Large Language Model, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13498v1.pdf filename=2402.13498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated lay summarisation (LS) aims to simplify complex technical documents into a more accessible format to non-experts. Existing approaches using <b>pre-trained</b> <b>language</b> <b>models,</b> possibly augmented with external background knowledge, tend to struggle with effective simplification and explanation. Moreover, automated methods that can effectively assess the `layness&rsquo; of generated summaries are lacking. Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated a remarkable capacity for text simplification, background information generation, and text evaluation. This has motivated our systematic exploration into using <b>LLMs</b> to generate and evaluate lay summaries of biomedical articles. We propose a novel \textit{Explain-then-Summarise} LS framework, which leverages <b>LLMs</b> to generate high-quality background knowledge to improve <b>supervised</b> LS. We also evaluate the performance of <b>LLMs</b> for <b>zero-shot</b> LS and propose two novel <b>LLM-based</b> LS evaluation metrics, which assess layness from multiple perspectives. Finally, we conduct a human assessment of generated lay summaries. Our experiments reveal that <b>LLM-generated</b> background information can support improved <b>supervised</b> LS. Furthermore, our novel <b>zero-shot</b> LS evaluation metric demonstrates a high degree of alignment with human preferences. We conclude that <b>LLMs</b> have an important part to play in improving both the performance and evaluation of LS methods.</p></p class="citation"></blockquote><h3 id=3994--101306-retrieval-augmented-data-augmentation-for-low-resource-domain-tasks-minju-seo-et-al-2024>(39/94 | 101/306) Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks (Minju Seo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minju Seo, Jinheon Baek, James Thorne, Sung Ju Hwang. (2024)<br><strong>Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks</strong><br><button class=copy-to-clipboard title="Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, Low-Resource, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13482v1.pdf filename=2402.13482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite <b>large</b> <b>successes</b> <b>of</b> recent language models on diverse tasks, they suffer from severe performance degeneration in <b>low-resource</b> settings with limited training <b>data</b> <b>available.</b> Many existing works tackle this problem by generating synthetic <b>data</b> <b>from</b> the training <b>data</b> <b>and</b> then training models on them, recently using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> However, in <b>low-resource</b> settings, the amount of seed <b>data</b> <b>samples</b> to use for <b>data</b> <b>augmentation</b> is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training <b>data</b> <b>by</b> incorporating a wealth of examples from other datasets, along with the given training <b>data.</b> <b>Specifically,</b> we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed <b>data,</b> <b>and</b> then <b>prompt</b> <b>LLMs</b> to generate new samples with the contextual information within and across the original and retrieved samples. This approach can ensure that the generated <b>data</b> <b>is</b> not only relevant but also more diverse than what could be achieved using the limited seed <b>data</b> <b>alone.</b> We validate our proposed Retrieval-Augmented <b>Data</b> <b>Augmentation</b> (RADA) framework on multiple datasets under <b>low-resource</b> settings of training and test-time <b>data</b> <b>augmentation</b> scenarios, on which it outperforms existing <b>LLM-powered</b> <b>data</b> <b>augmentation</b> baselines.</p></p class="citation"></blockquote><h3 id=4094--102306-how-important-is-domain-specificity-in-language-models-and-instruction-finetuning-for-biomedical-relation-extraction-aviv-brokman-et-al-2024>(40/94 | 102/306) How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction? (Aviv Brokman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aviv Brokman, Ramakanth Kavuluru. (2024)<br><strong>How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?</strong><br><button class=copy-to-clipboard title="How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Fine-tuning, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13470v1.pdf filename=2402.13470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain. The past few years have seen generative language models (LMs), instruction <b>finetuning,</b> and <b>few-shot</b> <b>learning</b> become foci of NLP research. As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction <b>finetuning</b> has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks. Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of <b>relation</b> <b>extraction.</b> Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora? (2) Do models instruction <b>finetuned</b> on biomedical datasets outperform those <b>finetuned</b> on assorted datasets or those simply pretrained? We tackle these questions using existing LMs, testing across four datasets. In a surprising result, general-domain models typically outperformed biomedical-domain models. However, biomedical instruction <b>finetuning</b> improved performance to a similar degree as general instruction <b>finetuning,</b> despite having orders of magnitude fewer instructions. Our findings suggest it may be more fruitful to focus research effort on larger-scale biomedical instruction <b>finetuning</b> of general LMs over building domain-specific biomedical LMs</p></p class="citation"></blockquote><h3 id=4194--103306-ranking-large-language-models-without-ground-truth-amit-dhurandhar-et-al-2024>(41/94 | 103/306) Ranking Large Language Models without Ground Truth (Amit Dhurandhar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly, Karthikeyan Natesan Ramamurthy. (2024)<br><strong>Ranking Large Language Models without Ground Truth</strong><br><button class=copy-to-clipboard title="Ranking Large Language Models without Ground Truth" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Low-Resource, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14860v1.pdf filename=2402.14860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluation and ranking of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of <b>LLMs</b> to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of <b>prompts</b> (viz. questions, instructions, etc.) and a set of <b>LLMs,</b> we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank <b>LLMs.</b> In experiments on different generative tasks <b>(summarization,</b> multiple-choice, and dialog), our methods reliably recover close to true rankings without reference data. This points to a viable <b>low-resource</b> mechanism for practical use.</p></p class="citation"></blockquote><h3 id=4294--104306-olympiadbench-a-challenging-benchmark-for-promoting-agi-with-olympiad-level-bilingual-multimodal-scientific-problems-chaoqun-he-et-al-2024>(42/94 | 104/306) OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems (Chaoqun He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems</strong><br><button class=copy-to-clipboard title="OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14008v1.pdf filename=2402.14008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements have seen <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>Large</b> <b>Multimodal</b> <b>Models</b> (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional <b>benchmarks</b> becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual <b>multimodal</b> scientific <b>benchmark,</b> featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step <b>reasoning.</b> Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, <b>GPT-4V,</b> attains an average score of 17.23% on OlympiadBench, with a mere 11.28% in physics, highlighting the <b>benchmark</b> rigor and the intricacy of physical <b>reasoning.</b> Our analysis orienting <b>GPT-4V</b> points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging <b>benchmark</b> can serve as a valuable resource for helping future AGI research endeavors.</p></p class="citation"></blockquote><h3 id=4394--105306-knowledge-graph-enhanced-large-language-model-editing-mengqi-zhang-et-al-2024>(43/94 | 105/306) Knowledge Graph Enhanced Large Language Model Editing (Mengqi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengqi Zhang, Xiaotian Ye, Qiang Liu, Pengjie Ren, Shu Wu, Zhumin Chen. (2024)<br><strong>Knowledge Graph Enhanced Large Language Model Editing</strong><br><button class=copy-to-clipboard title="Knowledge Graph Enhanced Large Language Model Editing" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 48<br>Keywords: Graph, Knowledge Graph, GPT, GPT-2, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13593v1.pdf filename=2402.13593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated <b>knowledge.</b> <b>Model</b> editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in <b>knowledge</b> <b>associated</b> with edits, which limits the generalization ability of postedit <b>LLMs</b> in processing edited <b>knowledge.</b> <b>To</b> tackle these problems, we propose a novel model editing method that leverages <b>knowledge</b> <b>graphs</b> for enhancing <b>LLM</b> editing, namely GLAME. Specifically, we first utilize a <b>knowledge</b> <b>graph</b> augmentation module to uncover associated <b>knowledge</b> <b>that</b> has changed due to editing, obtaining its internal representations within <b>LLMs.</b> This approach allows <b>knowledge</b> <b>alterations</b> within <b>LLMs</b> to be reflected through an external <b>graph</b> structure. Subsequently, we design a <b>graph-based</b> <b>knowledge</b> <b>edit</b> module to integrate structured <b>knowledge</b> <b>into</b> the model editing. This ensures that the updated parameters reflect not only the modifications of the edited <b>knowledge</b> <b>but</b> also the changes in other associated <b>knowledge</b> <b>resulting</b> from the editing process. Comprehensive experiments conducted on <b>GPT-J</b> and <b>GPT-2</b> XL demonstrate that GLAME significantly improves the generalization capabilities of post-edit <b>LLMs</b> in employing edited knowledge.</p></p class="citation"></blockquote><h3 id=4494--106306-calibrating-large-language-models-with-sample-consistency-qing-lyu-et-al-2024>(44/94 | 106/306) Calibrating Large Language Models with Sample Consistency (Qing Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, Chris Callison-Burch. (2024)<br><strong>Calibrating Large Language Models with Sample Consistency</strong><br><button class=copy-to-clipboard title="Calibrating Large Language Models with Sample Consistency" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Sample Size, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13904v1.pdf filename=2402.13904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurately gauging the confidence level of <b>Large</b> <b>Language</b> <b>Models&rsquo;</b> <b>(LLMs)</b> predictions is pivotal for their reliable application. However, <b>LLMs</b> are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly <b>sampled</b> <b>model</b> generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine <b>reasoning</b> datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger <b>sample</b> <b>sizes</b> enhance calibration, while <b>instruction-tuning</b> <b>makes</b> calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.</p></p class="citation"></blockquote><h3 id=4594--107306-factual-consistency-evaluation-of-summarisation-in-the-era-of-large-language-models-zheheng-luo-et-al-2024>(45/94 | 107/306) Factual Consistency Evaluation of Summarisation in the Era of Large Language Models (Zheheng Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheheng Luo, Qianqian Xie, Sophia Ananiadou. (2024)<br><strong>Factual Consistency Evaluation of Summarisation in the Era of Large Language Models</strong><br><button class=copy-to-clipboard title="Factual Consistency Evaluation of Summarisation in the Era of Large Language Models" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13758v1.pdf filename=2402.13758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary <b>LLMs,</b> leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation <b>benchmarks</b> are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of <b>LLM-generated</b> summaries of clinical texts, annotated for FC by domain experts. Moreover, we <b>benchmark</b> 11 <b>LLMs</b> for FC evaluation across news and clinical domains and analyse the impact of model size, <b>prompts,</b> pre-training and <b>fine-tuning</b> data. Our findings reveal that despite proprietary models prevailing on the task, open-source <b>LLMs</b> lag behind. Nevertheless, there is potential for enhancing the performance of open-source <b>LLMs</b> through increasing model size, expanding pre-training data, and developing well-curated <b>fine-tuning</b> data. Experiments on TreatFact suggest that both previous methods and <b>LLM-based</b> evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.</p></p class="citation"></blockquote><h3 id=4694--108306-inftybench-extending-long-context-evaluation-beyond-100k-tokens-xinrong-zhang-et-al-2024>(46/94 | 108/306) $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens (Xinrong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens</strong><br><button class=copy-to-clipboard title="$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Reasoning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13718v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13718v3.pdf filename=2402.13718v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Processing and <b>reasoning</b> over long contexts is crucial for many practical applications of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> such as document comprehension and agent construction. Despite recent strides in making <b>LLMs</b> process contexts with more than 100K tokens, there is currently a lack of a standardized <b>benchmark</b> to evaluate this long-context capability. Existing public <b>benchmarks</b> typically focus on contexts around 10K tokens, limiting the assessment and comparison of <b>LLMs</b> in processing longer contexts. In this paper, we propose $\infty$Bench, the first <b>LLM</b> <b>benchmark</b> featuring an average data length surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\infty$Bench, we evaluate the state-of-the-art proprietary and open-source <b>LLMs</b> tailored for processing long contexts. The results indicate that existing long context <b>LLMs</b> still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of <b>LLMs</b> processing long context.</p></p class="citation"></blockquote><h3 id=4794--109306-graph-representation-of-narrative-context-coherence-dependency-via-retrospective-questions-liyan-xu-et-al-2024>(47/94 | 109/306) Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions (Liyan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liyan Xu, Jiangnan Li, Mo Yu, Jie Zhou. (2024)<br><strong>Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions</strong><br><button class=copy-to-clipboard title="Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 43<br>Keywords: Graph, Question Answering, Stemming, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13551v1.pdf filename=2402.13551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work introduces a novel and practical paradigm for narrative comprehension, <b>stemming</b> from the observation that individual passages within narratives are often cohesively related than being isolated. We therefore propose to formulate a <b>graph</b> upon narratives dubbed NARCO that depicts a task-agnostic coherence dependency of the entire context. Especially, edges in NARCO encompass retrospective free-form questions between two context snippets reflecting high-level coherent relations, inspired by the cognitive perception of humans who constantly reinstate relevant events from prior context. Importantly, our <b>graph</b> is instantiated through our designed two-stage <b>LLM</b> <b>prompting,</b> thereby without reliance on human annotations. We present three unique studies on its practical utility, examining the edge efficacy via recap identification, local context augmentation via plot retrieval, and broader applications exemplified by long document <b>QA.</b> Experiments suggest that our approaches leveraging NARCO yield performance boost across all three tasks.</p></p class="citation"></blockquote><h3 id=4894--110306-llms-meet-long-video-advancing-long-video-comprehension-with-an-interactive-visual-adapter-in-llms-yunxin-li-et-al-2024>(48/94 | 110/306) LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs (Yunxin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunxin Li, Xinyu Chen, Baotain Hu, Min Zhang. (2024)<br><strong>LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs</strong><br><button class=copy-to-clipboard title="LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Transformer, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13546v1.pdf filename=2402.13546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within <b>LLMs,</b> designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal <b>transformer,</b> then feed them into <b>LLMs</b> with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector and a spatial feature interactor, within the internal blocks of <b>LLMs</b> to capture instruction-aware and fine-grained visual signals. Consequently, the proposed video-LLM facilitates a comprehensive understanding of long video content through appropriate long video modeling and precise visual interactions. We conducted extensive experiments on nine video understanding <b>benchmarks</b> and experimental results show that our interactive visual adapter significantly improves the performance of video <b>LLMs</b> on long video <b>QA</b> tasks. Ablation studies further verify the effectiveness of IVA in long and short video understandings.</p></p class="citation"></blockquote><h3 id=4994--111306-can-similarity-based-domain-ordering-reduce-catastrophic-forgetting-for-intent-recognition-amogh-mannekote-et-al-2024>(49/94 | 111/306) Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for Intent Recognition? (Amogh Mannekote et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amogh Mannekote, Xiaoyi Tian, Kristy Elizabeth Boyer, Bonnie J. Dorr. (2024)<br><strong>Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for Intent Recognition?</strong><br><button class=copy-to-clipboard title="Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for Intent Recognition?" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Continual Learning, T5, Dialogue System, Intent Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14155v1.pdf filename=2402.14155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Task-oriented <b>dialogue</b> <b>systems</b> are expected to handle a constantly expanding set of <b>intents</b> <b>and</b> domains even after they have been deployed to support more and more functionalities. To live up to this expectation, it becomes critical to mitigate the catastrophic forgetting problem (CF) that occurs in <b>continual</b> <b>learning</b> (CL) settings for a task such as <b>intent</b> <b>recognition.</b> While existing <b>dialogue</b> <b>systems</b> research has explored replay-based and regularization-based methods to this end, the effect of domain ordering on the CL performance of <b>intent</b> <b>recognition</b> models remains unexplored. If understood well, domain ordering has the potential to be an orthogonal technique that can be leveraged alongside existing techniques such as experience replay. Our work fills this gap by comparing the impact of three domain-ordering strategies (min-sum path, max-sum path, random) on the CL performance of a generative <b>intent</b> <b>recognition</b> model. Our findings reveal that the min-sum path strategy outperforms the others in reducing catastrophic forgetting when training on the 220M <b>T5-Base</b> model. However, this advantage diminishes with the larger 770M <b>T5-Large</b> model. These results underscores the potential of domain ordering as a complementary strategy for mitigating catastrophic forgetting in continually learning <b>intent</b> <b>recognition</b> models, particularly in resource-constrained scenarios.</p></p class="citation"></blockquote><h3 id=5094--112306-what-linguistic-features-and-languages-are-important-in-llm-translation-ryandito-diandaru-et-al-2024>(50/94 | 112/306) What Linguistic Features and Languages are Important in LLM Translation? (Ryandito Diandaru et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryandito Diandaru, Lucky Susanto, Zilu Tang, Ayu Purwarianti, Derry Wijaya. (2024)<br><strong>What Linguistic Features and Languages are Important in LLM Translation?</strong><br><button class=copy-to-clipboard title="What Linguistic Features and Languages are Important in LLM Translation?" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Neural Machine Translation, BLEU, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13917v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13917v1.pdf filename=2402.13917v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrate strong capability across multiple tasks, including <b>machine</b> <b>translation.</b> Our study focuses on evaluating Llama2&rsquo;s <b>machine</b> <b>translation</b> capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 <b>BLEU</b> score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the current landscape of <b>LLMs,</b> raising the possibility that <b>LLMs</b> centered around languages other than English may offer a more effective foundation for a multilingual model.</p></p class="citation"></blockquote><h3 id=5194--113306-retrieval-helps-or-hurts-a-deeper-dive-into-the-efficacy-of-retrieval-augmentation-to-language-models-seiji-maekawa-et-al-2024>(51/94 | 113/306) Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models (Seiji Maekawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seiji Maekawa, Hayate Iso, Sairam Gurajada, Nikita Bhutani. (2024)<br><strong>Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models</strong><br><button class=copy-to-clipboard title="Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Question Answering, Large Language Model, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13492v1.pdf filename=2402.13492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>large</b> <b>language</b> <b>models</b> (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of <b>retrieval</b> <b>may</b> adversely affect overall performance. Previous research has primarily focused on examining how entities influence <b>retrieval</b> <b>models</b> and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new <b>question</b> <b>answering</b> <b>(QA)</b> dataset called WiTQA (Wikipedia Triple <b>Question</b> <b>Answers).</b> This dataset includes <b>questions</b> <b>about</b> entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive experiments with diverse LMs and retrievers reveal when <b>retrieval</b> <b>does</b> not consistently enhance LMs from the viewpoints of fact-centric popularity.Confirming earlier findings, we observe that larger LMs excel in recalling popular facts. However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers. Interestingly, they can effectively retain popular relations of less common entities. We demonstrate the efficacy of our finer-grained metric and insights through an adaptive <b>retrieval</b> <b>system</b> that selectively employs <b>retrieval</b> <b>and</b> recall based on the frequencies of entities and relations in the question.</p></p class="citation"></blockquote><h3 id=5294--114306-large-language-models-for-data-annotation-a-survey-zhen-tan-et-al-2024>(52/94 | 114/306) Large Language Models for Data Annotation: A Survey (Zhen Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu. (2024)<br><strong>Large Language Models for Data Annotation: A Survey</strong><br><button class=copy-to-clipboard title="Large Language Models for Data Annotation: A Survey" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13446v1.pdf filename=2402.13446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is labor-intensive and expensive. The emergence of advanced <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> exemplified by <b>GPT-4,</b> presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered <b>LLM</b> architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: <b>LLM-Based</b> Data Annotation, Assessing <b>LLM-generated</b> Annotations, and Learning with <b>LLM-generated</b> annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing <b>LLMs</b> for data annotation, a comprehensive review of learning strategies for models incorporating <b>LLM-generated</b> annotations, and a detailed discussion on primary challenges and limitations associated with using <b>LLMs</b> for data annotation. As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest <b>LLMs</b> for data annotation, fostering future advancements in this critical domain. We provide a comprehensive papers list at \url{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git}.</p></p class="citation"></blockquote><h3 id=5394--115306-toolverifier-generalization-to-new-tools-via-self-verification-dheeraj-mekala-et-al-2024>(53/94 | 115/306) TOOLVERIFIER: Generalization to New Tools via Self-Verification (Dheeraj Mekala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria Lomeli, Jingbo Shang, Jane Dwivedi-Yu. (2024)<br><strong>TOOLVERIFIER: Generalization to New Tools via Self-Verification</strong><br><button class=copy-to-clipboard title="TOOLVERIFIER: Generalization to New Tools via Self-Verification" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Few-shot, Fine-tuning, LLaMA<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14158v1.pdf filename=2402.14158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via <b>fine-tuning,</b> language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation. We construct synthetic, high-quality, self-generated data for this goal using <b>Llama-2</b> 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench <b>benchmark,</b> consisting of 17 unseen tools, demonstrate an average improvement of 22% over <b>few-shot</b> baselines, even in scenarios where the distinctions between candidate tools are finely nuanced.</p></p class="citation"></blockquote><h3 id=5494--116306-measuring-social-biases-in-masked-language-models-by-proxy-of-prediction-quality-rahul-zalkikar-et-al-2024>(54/94 | 116/306) Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality (Rahul Zalkikar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rahul Zalkikar, Kanchan Chandra. (2024)<br><strong>Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality</strong><br><button class=copy-to-clipboard title="Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Transformer, Masked Language Model, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13954v1.pdf filename=2402.13954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social and political scientists often aim to discover and measure distinct biases from text data representations (embeddings). Innovative <b>transformer-based</b> language models produce contextually-aware token embeddings and have achieved state-of-the-art performance for a variety of natural language tasks, but have been shown to encode unwanted biases for downstream applications. In this paper, we evaluate the social biases encoded by <b>transformers</b> trained with the <b>masked</b> <b>language</b> <b>modeling</b> objective using proposed proxy functions within an iterative masking experiment to measure the quality of <b>transformer</b> models&rsquo; predictions, and assess the preference of <b>MLMs</b> towards disadvantaged and advantaged groups. We compare bias estimations with those produced by other evaluation methods using two <b>benchmark</b> datasets, finding relatively high religious and disability biases across considered <b>MLMs</b> and low gender bias in one dataset relative to the other. Our measures outperform others in their agreement with human annotators. We extend on previous work by evaluating social biases introduced after re-training an <b>MLM</b> under the <b>masked</b> <b>language</b> <b>modeling</b> objective (w.r.t. the model&rsquo;s pre-trained base), and find that proposed measures produce more accurate estimations of relative preference for biased sentences between <b>transformers</b> than others based on our methods.</p></p class="citation"></blockquote><h3 id=5594--117306-technical-report-on-the-checkforai-ai-generated-text-classifier-bradley-emi-et-al-2024>(55/94 | 117/306) Technical Report on the Checkfor.ai AI-Generated Text Classifier (Bradley Emi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bradley Emi, Max Spero. (2024)<br><strong>Technical Report on the Checkfor.ai AI-Generated Text Classifier</strong><br><button class=copy-to-clipboard title="Technical Report on the Checkfor.ai AI-Generated Text Classifier" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Zero-shot, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14873v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14873v2.pdf filename=2402.14873v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the CheckforAI text classifier, a <b>transformer-based</b> neural network trained to distinguish text written by <b>large</b> <b>language</b> <b>models</b> from text written by humans. CheckforAI outperforms <b>zero-shot</b> methods such as DetectGPT as well as leading commercial AI detection tools with over 9 times lower error rates on a comprehensive <b>benchmark</b> comprised of ten text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q&amp;A) and 8 open- and closed-source <b>large</b> <b>language</b> <b>models.</b> We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that CheckforAI is not biased against nonnative English speakers and generalizes to domains and models unseen during training.</p></p class="citation"></blockquote><h3 id=5694--118306-more-multi-modal-retrieval-augmented-generative-commonsense-reasoning-wanqing-cui-et-al-2024>(56/94 | 118/306) MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning (Wanqing Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanqing Cui, Keping Bi, Jiafeng Guo, Xueqi Cheng. (2024)<br><strong>MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning</strong><br><button class=copy-to-clipboard title="MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Multi-modal, Common-sense Reasoning, Reasoning, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13625v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13625v1.pdf filename=2402.13625v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since <b>commonsense</b> <b>information</b> has been recorded significantly less frequently than its existence, language models pre-trained by <b>text</b> <b>generation</b> have difficulty to learn sufficient <b>commonsense</b> <b>knowledge.</b> Several studies have leveraged <b>text</b> <b>retrieval</b> to augment the models&rsquo; <b>commonsense</b> <b>ability.</b> Unlike <b>text,</b> <b>images</b> capture <b>commonsense</b> <b>information</b> inherently but little effort has been paid to effectively utilize them. In this work, we propose a novel <b>Multi-mOdal</b> REtrieval (MORE) augmentation framework, to leverage both <b>text</b> <b>and</b> images to enhance the <b>commonsense</b> <b>ability</b> of language models. Extensive experiments on the Common-Gen task have demonstrated the efficacy of MORE based on the pre-trained models of both single and multiple modalities.</p></p class="citation"></blockquote><h3 id=5794--119306-dyval-2-dynamic-evaluation-of-large-language-models-by-meta-probing-agents-kaijie-zhu-et-al-2024>(57/94 | 119/306) DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents (Kaijie Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, Xing Xie. (2024)<br><strong>DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents</strong><br><button class=copy-to-clipboard title="DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Data Augmentation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14865v1.pdf filename=2402.14865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has raised great concerns in the community due to the issue of <b>data</b> <b>contamination.</b> Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation <b>benchmarks</b> can only provide the overall <b>benchmark</b> results and cannot support a fine-grained and multifaceted analysis of <b>LLMs&rsquo;</b> abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate <b>LLMs.</b> MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are also dynamically configurable, allowing multifaceted analysis. We conducted extensive evaluations using MPA and found that most <b>LLMs</b> achieve poorer performance, indicating room for improvement. Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Matthew effect on model size, i.e., larger models possess stronger correlations of the abilities. MPA can also be used as a <b>data</b> <b>augmentation</b> approach to enhance <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=5894--120306-potential-and-challenges-of-model-editing-for-social-debiasing-jianhao-yan-et-al-2024>(58/94 | 120/306) Potential and Challenges of Model Editing for Social Debiasing (Jianhao Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhao Yan, Futing Wang, Yafu Li, Yue Zhang. (2024)<br><strong>Potential and Challenges of Model Editing for Social Debiasing</strong><br><button class=copy-to-clipboard title="Potential and Challenges of Model Editing for Social Debiasing" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13462v1.pdf filename=2402.13462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with <b>fine-tuning</b> could be both costly and data-hungry. Model editing methods, which focus on modifying <b>LLMs</b> in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing. To mitigate this gap, we carefully formulate social debiasing into an editing problem and <b>benchmark</b> seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing. Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from edited sentences to semantically equivalent sentences is limited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et al. 2022b), while internal editing methods degenerate with the number of edits. (3) Model editing algorithms achieve generalization towards unseen biases both within the same type and from different types. In light of these findings, we further propose two simple but effective methods to improve debias editing, and experimentally show the effectiveness of the proposed methods.</p></p class="citation"></blockquote><h3 id=5994--121306-a-study-on-the-vulnerability-of-test-questions-against-chatgpt-based-cheating-shanker-ram-et-al-2024>(59/94 | 121/306) A Study on the Vulnerability of Test Questions against ChatGPT-based Cheating (Shanker Ram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanker Ram, Chen Qian. (2024)<br><strong>A Study on the Vulnerability of Test Questions against ChatGPT-based Cheating</strong><br><button class=copy-to-clipboard title="A Study on the Vulnerability of Test Questions against ChatGPT-based Cheating" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs-CY, cs.CL<br>Keyword Score: 30<br>Keywords: ChatGPT, Chatbot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14881v1.pdf filename=2402.14881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>ChatGPT</b> is a <b>chatbot</b> that can answer text <b>prompts</b> fairly accurately, even performing very well on postgraduate-level questions. Many educators have found that their take-home or remote tests and exams are vulnerable to <b>ChatGPT-based</b> cheating because students may directly use answers provided by tools like <b>ChatGPT.</b> In this paper, we try to provide an answer to an important question: how well <b>ChatGPT</b> can answer test questions and how we can detect whether the questions of a test can be answered correctly by <b>ChatGPT.</b> We generated <b>ChatGPT&rsquo;s</b> responses to the MedMCQA dataset, which contains over 10,000 medical school entrance exam questions. We analyzed the responses and uncovered certain types of questions <b>ChatGPT</b> answers more inaccurately than others. In addition, we have created a basic natural language processing model to single out the most vulnerable questions to <b>ChatGPT</b> in a collection of questions or a sample exam. Our tool can be used by test-makers to avoid <b>ChatGPT-vulnerable</b> test questions.</p></p class="citation"></blockquote><h3 id=6094--122306-bangla-ai-a-framework-for-machine-translation-utilizing-large-language-models-for-ethnic-media-md-ashraful-goni-et-al-2024>(60/94 | 122/306) Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media (MD Ashraful Goni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>MD Ashraful Goni, Fahad Mostafa, Kerk F. Kee. (2024)<br><strong>Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media</strong><br><button class=copy-to-clipboard title="Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14179v1.pdf filename=2402.14179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ethnic media, which caters to diaspora communities in host nations, serves as a vital platform for these communities to both produce content and access information. Rather than utilizing the language of the host nation, ethnic media delivers news in the language of the immigrant community. For instance, in the USA, Bangla ethnic media presents news in Bangla rather than English. This research delves into the prospective integration of <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> and multi-lingual <b>machine</b> <b>translations</b> (MMT) within the ethnic media industry. It centers on the transformative potential of using <b>LLM</b> in MMT in various facets of news translation, searching, and categorization. The paper outlines a theoretical framework elucidating the integration of <b>LLM</b> and MMT into the news searching and translation processes for ethnic media. Additionally, it briefly addresses the potential ethical challenges associated with the incorporation of <b>LLM</b> and MMT in news translation procedures.</p></p class="citation"></blockquote><h3 id=6194--123306-improving-language-understanding-from-screenshots-tianyu-gao-et-al-2024>(61/94 | 123/306) Improving Language Understanding from Screenshots (Tianyu Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyu Gao, Zirui Wang, Adithya Bhaskar, Danqi Chen. (2024)<br><strong>Improving Language Understanding from Screenshots</strong><br><button class=copy-to-clipboard title="Improving Language Understanding from Screenshots" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: BERT, GLUE, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14073v1.pdf filename=2402.14073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An emerging family of language models (LMs), capable of processing both text and images within a single visual view, has the promise to unlock complex tasks such as chart understanding and UI navigation. We refer to these models as screenshot language models. Despite their appeal, existing screenshot LMs substantially lag behind text-only models on language understanding tasks. To close this gap, we adopt a simplified setting where the model inputs are plain-text-rendered screenshots, and we focus on improving the text ability of screenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective, which masks and recovers both image patches of screenshots and text within screenshots. We also conduct extensive ablation studies on masking rates and patch sizes, as well as designs for improving training stability. Our pre-trained model, while solely taking visual inputs, achieves comparable performance with <b>BERT</b> on 6 out of 8 <b>GLUE</b> tasks (within 2%) and improves up to 8% over prior work. Additionally, we extend PTP to train autoregressive screenshot LMs and demonstrate its effectiveness&ndash;our models can significantly reduce <b>perplexity</b> by utilizing the screenshot context. Together, we hope our findings can inspire future research on developing powerful screenshot LMs and extending their reach to broader applications.</p></p class="citation"></blockquote><h3 id=6294--124306-on-leveraging-encoder-only-pre-trained-language-models-for-effective-keyphrase-generation-di-wu-et-al-2024>(62/94 | 124/306) On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation (Di Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Di Wu, Wasi Uddin Ahmad, Kai-Wei Chang. (2024)<br><strong>On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation</strong><br><button class=copy-to-clipboard title="On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14052v1.pdf filename=2402.14052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the application of encoder-only <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> in keyphrase generation (KPG) amidst the broader availability of domain-tailored encoder-only models compared to encoder-decoder models. We investigate three core inquiries: (1) the efficacy of encoder-only <b>PLMs</b> in KPG, (2) optimal architectural decisions for employing encoder-only <b>PLMs</b> in KPG, and (3) a performance comparison between in-domain encoder-only and encoder-decoder <b>PLMs</b> across varied resource settings. Our findings, derived from extensive experimentation in two domains reveal that with encoder-only <b>PLMs,</b> although KPE with Conditional Random Fields slightly excels in identifying present keyphrases, the KPG formulation renders a broader spectrum of keyphrase predictions. Additionally, prefix-LM <b>fine-tuning</b> of encoder-only <b>PLMs</b> emerges as a strong and data-efficient strategy for KPG, outperforming general-domain seq2seq <b>PLMs.</b> We also identify a favorable parameter allocation towards model depth rather than width when employing encoder-decoder architectures initialized with encoder-only <b>PLMs.</b> The study sheds light on the potential of utilizing encoder-only <b>PLMs</b> for advancing KPG systems and provides a groundwork for future KPG methods. Our code and <b>pre-trained</b> <b>checkpoints</b> <b>are</b> released at <a href=https://github.com/uclanlp/DeepKPG>https://github.com/uclanlp/DeepKPG</a>.</p></p class="citation"></blockquote><h3 id=6394--125306-large-language-models-are-vulnerable-to-bait-and-switch-attacks-for-generating-harmful-content-federico-bianchi-et-al-2024>(63/94 | 125/306) Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content (Federico Bianchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Federico Bianchi, James Zou. (2024)<br><strong>Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content</strong><br><button class=copy-to-clipboard title="Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13926v1.pdf filename=2402.13926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The risks derived from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts. In our study, we shift the focus to how even safe text coming from <b>LLMs</b> can be easily turned into potentially dangerous content through Bait-and-Switch attacks. In such attacks, the user first <b>prompts</b> <b>LLMs</b> with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives. The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for <b>LLMs.</b> In particular, we stress that focusing on the safety of the verbatim <b>LLM</b> outputs is insufficient and that we also need to consider post-hoc transformations.</p></p class="citation"></blockquote><h3 id=6494--126306-llm-based-multi-agent-generation-of-semi-structured-documents-from-semantic-templates-in-the-public-administration-domain-emanuele-musumeci-et-al-2024>(64/94 | 126/306) LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain (Emanuele Musumeci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emanuele Musumeci, Michele Brienza, Vincenzo Suriani, Daniele Nardi, Domenico Daniele Bloisi. (2024)<br><strong>LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain</strong><br><button class=copy-to-clipboard title="LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14871v1.pdf filename=2402.14871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the last years&rsquo; digitalization process, the creation and management of documents in various domains, particularly in Public Administration (PA), have become increasingly complex and diverse. This complexity arises from the need to handle a wide range of document types, often characterized by semi-structured forms. Semi-structured documents present a fixed set of data without a fixed format. As a consequence, a template-based solution cannot be used, as understanding a document requires the extraction of the data structure. The recent introduction of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has enabled the creation of customized text output satisfying user requests. In this work, we propose a novel approach that combines the <b>LLMs</b> with <b>prompt</b> engineering and multi-agent systems for generating new documents compliant with a desired structure. The main contribution of this work concerns replacing the commonly used manual <b>prompting</b> with a task description generated by semantic retrieval from an <b>LLM.</b> The potential of this approach is demonstrated through a series of experiments and case studies, showcasing its effectiveness in real-world PA scenarios.</p></p class="citation"></blockquote><h3 id=6594--127306-ouroboros-speculative-decoding-with-large-model-enhanced-drafting-weilin-zhao-et-al-2024>(65/94 | 127/306) Ouroboros: Speculative Decoding with Large Model Enhanced Drafting (Weilin Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>Ouroboros: Speculative Decoding with Large Model Enhanced Drafting</strong><br><button class=copy-to-clipboard title="Ouroboros: Speculative Decoding with Large Model Enhanced Drafting" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13720v1.pdf filename=2402.13720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Drafting-then-verifying decoding methods such as speculative decoding are widely adopted training-free methods to accelerate the inference of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Instead of employing an autoregressive process to decode tokens sequentially, speculative decoding initially creates drafts with an efficient small model. Then <b>LLMs</b> are required to conduct verification and correction in a non-autoregressive fashion to minimize time overhead. Generating longer drafts can lead to even more significant speedups once verified, but also incurs substantial trial and error costs if it fails. Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration. In this paper, we introduce Ouroboros, which constructs a phrase candidate pool from the verification process of <b>LLMs</b> to provide candidates for draft generation of the small model. Thereby, Ouroboros can further improve the efficiency and effectiveness of the initial drafts. The experimental results on typical <b>text</b> <b>generation</b> tasks show that Ouroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead decoding and speculative decoding, respectively. The source code of Ouroboros is available at <a href=https://github.com/thunlp/Ouroboros>https://github.com/thunlp/Ouroboros</a>.</p></p class="citation"></blockquote><h3 id=6694--128306-neeko-leveraging-dynamic-lora-for-efficient-multi-character-role-playing-agent-xiaoyan-yu-et-al-2024>(66/94 | 128/306) Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent (Xiaoyan Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyan Yu, Tongxu Luo, Yifan Wei, Fangyu Lei, Yiming Huang, Peng Hao, Liehuang Zhu. (2024)<br><strong>Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent</strong><br><button class=copy-to-clipboard title="Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Open-Domain Dialogue, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13717v1.pdf filename=2402.13717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have revolutionized <b>open-domain</b> <b>dialogue</b> agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko&rsquo;s adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at <a href=https://github.com/weiyifan1023/Neeko>https://github.com/weiyifan1023/Neeko</a>.</p></p class="citation"></blockquote><h3 id=6794--129306-a-comprehensive-study-of-multilingual-confidence-estimation-on-large-language-models-boyang-xue-et-al-2024>(67/94 | 129/306) A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models (Boyang Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyang Xue, Hongru Wang, Weichao Wang, Rui Wang, Sheng Wang, Zeming Liu, Kam-Fai Wong. (2024)<br><strong>A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models</strong><br><button class=copy-to-clipboard title="A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13606v1.pdf filename=2402.13606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The tendency of <b>Large</b> <b>Language</b> <b>Models</b> to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model&rsquo;s response are essential to developing reliable AI systems. Current research primarily focuses on <b>LLM</b> confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on <b>LLMs.</b> First, we introduce an elaborated and expert-checked multilingual <b>QA</b> dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance <b>LLM</b> performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more precise confidence scores. The experimental results showcase the performance of various confidence estimation methods across different languages as well as present that our proposed cross-lingual confidence estimation technique significantly enhances confidence estimation and outperforms several baseline methods.</p></p class="citation"></blockquote><h3 id=6894--130306-recmind-japanese-movie-recommendation-dialogue-with-seekers-internal-state-takashi-kodama-et-al-2024>(68/94 | 130/306) RecMind: Japanese Movie Recommendation Dialogue with Seeker&rsquo;s Internal State (Takashi Kodama et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takashi Kodama, Hirokazu Kiyomaru, Yin Jou Huang, Sadao Kurohashi. (2024)<br><strong>RecMind: Japanese Movie Recommendation Dialogue with Seeker&rsquo;s Internal State</strong><br><button class=copy-to-clipboard title="RecMind: Japanese Movie Recommendation Dialogue with Seeker's Internal State" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Recommendation, Chain-of-thought Prompt, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13522v1.pdf filename=2402.13522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans pay careful attention to the interlocutor&rsquo;s internal state in dialogues. For example, in <b>recommendation</b> dialogues, we make <b>recommendations</b> while estimating the seeker&rsquo;s internal state, such as his/her level of knowledge and interest. Since there are no existing annotated resources for the analysis, we constructed RecMind, a Japanese movie <b>recommendation</b> dialogue dataset with annotations of the seeker&rsquo;s internal state at the entity level. Each entity has a subjective label annotated by the seeker and an objective label annotated by the recommender. RecMind also features engaging dialogues with long seeker&rsquo;s utterances, enabling a detailed analysis of the seeker&rsquo;s internal state. Our analysis based on RecMind reveals that entities that the seeker has no knowledge about but has an interest in contribute to <b>recommendation</b> success. We also propose a response generation framework that explicitly considers the seeker&rsquo;s internal state, utilizing the <b>chain-of-thought</b> <b>prompting.</b> The human evaluation results show that our proposed method outperforms the baseline method in both consistency and the success of <b>recommendations.</b></p></p class="citation"></blockquote><h3 id=6994--131306-round-trip-translation-defence-against-large-language-model-jailbreaking-attacks-canaan-yung-et-al-2024>(69/94 | 131/306) Round Trip Translation Defence against Large Language Model Jailbreaking Attacks (Canaan Yung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Canaan Yung, Hadi Mohaghegh Dolatabadi, Sarah Erfani, Christopher Leckie. (2024)<br><strong>Round Trip Translation Defence against Large Language Model Jailbreaking Attacks</strong><br><button class=copy-to-clipboard title="Round Trip Translation Defence against Large Language Model Jailbreaking Attacks" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13517v1.pdf filename=2402.13517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for <b>LLMs</b> to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on <b>LLMs.</b> RTT paraphrases the adversarial <b>prompt</b> and generalizes the idea conveyed, making it easier for <b>LLMs</b> to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different <b>LLMs.</b> Our defense successfully mitigated over 70% of <b>Prompt</b> Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at <a href=https://github.com/Cancanxxx/Round_Trip_Translation_Defence>https://github.com/Cancanxxx/Round_Trip_Translation_Defence</a></p></p class="citation"></blockquote><h3 id=7094--132306-evaluation-of-a-semi-autonomous-attentive-listening-system-with-takeover-prompting-haruki-kawai-et-al-2024>(70/94 | 132/306) Evaluation of a semi-autonomous attentive listening system with takeover prompting (Haruki Kawai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haruki Kawai, Divesh Lala, Koji Inoue, Keiko Ochi, Tatsuya Kawahara. (2024)<br><strong>Evaluation of a semi-autonomous attentive listening system with takeover prompting</strong><br><button class=copy-to-clipboard title="Evaluation of a semi-autonomous attentive listening system with takeover prompting" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Human Intervention, Dialogue System, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14863v1.pdf filename=2402.14863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The handling of communication breakdowns and loss of engagement is an important aspect of spoken <b>dialogue</b> <b>systems,</b> particularly for chatting systems such as attentive listening, where the user is mostly speaking. We presume that a <b>human</b> <b>is</b> best equipped to handle this task and rescue the flow of conversation. To this end, we propose a semi-autonomous system, where a remote operator can take control of an autonomous attentive listening system in real-time. In order to make <b>human</b> <b>intervention</b> easy and consistent, we introduce automatic detection of low interest and engagement to provide explicit takeover <b>prompts</b> to the remote operator. We implement this semi-autonomous system which detects takeover points for the operator and compare it to fully tele-operated and fully autonomous attentive listening systems. We find that the semi-autonomous system is generally perceived more positively than the autonomous system. The results suggest that identifying points of conversation when the user starts to lose interest may help us improve a fully autonomous <b>dialogue</b> <b>system.</b></p></p class="citation"></blockquote><h3 id=7194--133306-distinctive-image-captioning-leveraging-ground-truth-captions-in-clip-guided-reinforcement-learning-antoine-chaffin-et-al-2024>(71/94 | 133/306) Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning (Antoine Chaffin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antoine Chaffin, Ewa Kijak, Vincent Claveau. (2024)<br><strong>Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 26<br>Keywords: Generative Adversarial Network, Multi-modal, Multi-modal, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13936v1.pdf filename=2402.13936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility. <b>Reinforcement</b> <b>Learning</b> (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions. Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions. However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework. We propose a new image captioning model training strategy that makes use of GT captions in different ways. Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions, resulting in a textual <b>GAN</b> setup extended for <b>multimodal</b> inputs. Secondly, they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image. This objective acts as an additional learning signal grounded to the distribution of the GT captions. Thirdly, they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate. Experiments on MS-COCO demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality.</p></p class="citation"></blockquote><h3 id=7294--134306-cmner-a-chinese-multimodal-ner-dataset-based-on-social-media-yuanze-ji-et-al-2024>(72/94 | 134/306) CMNER: A Chinese Multimodal NER Dataset based on Social Media (Yuanze Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanze Ji, Bobo Li, Jun Zhou, Fei Li, Chong Teng, Donghong Ji. (2024)<br><strong>CMNER: A Chinese Multimodal NER Dataset based on Social Media</strong><br><button class=copy-to-clipboard title="CMNER: A Chinese Multimodal NER Dataset based on Social Media" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13693v1.pdf filename=2402.13693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Named</b> <b>Entity</b> <b>Recognition</b> (MNER) is a pivotal task designed to extract <b>named</b> <b>entities</b> <b>from</b> text with the support of pertinent images. Nonetheless, a notable paucity of data for Chinese MNER has considerably impeded the progress of this natural language processing task within the Chinese domain. Consequently, in this study, we compile a Chinese <b>Multimodal</b> <b>NER</b> dataset (CMNER) utilizing data sourced from Weibo, China&rsquo;s largest social media platform. Our dataset encompasses 5,000 Weibo posts paired with 18,326 corresponding images. The entities are classified into four distinct categories: person, location, organization, and miscellaneous. We perform baseline experiments on CMNER, and the outcomes underscore the effectiveness of incorporating images for <b>NER.</b> Furthermore, we conduct cross-lingual experiments on the publicly available English MNER dataset (Twitter2015), and the results substantiate our hypothesis that Chinese and English <b>multimodal</b> <b>NER</b> data can mutually enhance the performance of the <b>NER</b> model.</p></p class="citation"></blockquote><h3 id=7394--135306-criticbench-evaluating-large-language-models-as-critic-tian-lan-et-al-2024>(73/94 | 135/306) CriticBench: Evaluating Large Language Models as Critic (Tian Lan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, Xian-ling Mao. (2024)<br><strong>CriticBench: Evaluating Large Language Models as Critic</strong><br><button class=copy-to-clipboard title="CriticBench: Evaluating Large Language Models as Critic" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13764v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13764v3.pdf filename=2402.13764v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Critique ability are crucial in the scalable oversight and self-improvement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> While many recent studies explore the critique ability of <b>LLMs</b> to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of <b>LLMs</b> is under-explored. This paper introduces CriticBench, a novel <b>benchmark</b> designed to comprehensively and reliably evaluate four key critique ability dimensions of <b>LLMs:</b> feedback, comparison, refinement and meta-feedback. CriticBench encompasses nine diverse tasks, each assessing the <b>LLMs&rsquo;</b> ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source <b>LLMs</b> reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for CriticBench will be publicly released at <a href=https://github.com/open-compass/CriticBench>https://github.com/open-compass/CriticBench</a>.</p></p class="citation"></blockquote><h3 id=7494--136306-the-da-vinci-code-of-large-pre-trained-language-models-deciphering-degenerate-knowledge-neurons-yuheng-chen-et-al-2024>(74/94 | 136/306) The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons (Yuheng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuheng Chen, Pengfei Cao, Yubo Chen, Yining Wang, Shengping Liu, Kang Liu, Jun Zhao. (2024)<br><strong>The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons</strong><br><button class=copy-to-clipboard title="The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Clustering, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13731v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13731v1.pdf filename=2402.13731v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores the mechanism of factual knowledge storage in <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs).</b> Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in <b>PLMs&rsquo;</b> factual knowledge storage units. Based on this, we introduce the Neurological Topology <b>Clustering</b> method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of <b>PLMs.</b> Within this framework, our execution of 34 experiments across 2 <b>PLMs,</b> 4 datasets, and 6 settings highlights the critical role of DKNs. The code will be available soon.</p></p class="citation"></blockquote><h3 id=7594--137306-exploiting-adaptive-contextual-masking-for-aspect-based-sentiment-analysis-s-m-rafiuddin-et-al-2024>(75/94 | 137/306) Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment Analysis (S M Rafiuddin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S M Rafiuddin, Mohammed Rakib, Sadia Kamal, Arunkumar Bagavathi. (2024)<br><strong>Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment Analysis</strong><br><button class=copy-to-clipboard title="Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment Analysis" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Aspect-based Sentiment Analysis, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13722v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13722v1.pdf filename=2402.13722v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Aspect-Based</b> <b>Sentiment</b> <b>Analysis</b> (ABSA) is a fine-grained linguistics problem that entails the extraction of multifaceted aspects, opinions, and <b>sentiments</b> <b>from</b> the given text. Both standalone and compound ABSA tasks have been extensively used in the literature to examine the nuanced information present in online reviews and social media posts. Current ABSA methods often rely on static hyperparameters for attention-masking mechanisms, which can struggle with context adaptation and may overlook the unique relevance of words in varied situations. This leads to challenges in accurately analyzing complex sentences containing multiple aspects with differing <b>sentiments.</b> <b>In</b> this work, we present adaptive masking methods that remove irrelevant tokens based on context to assist in Aspect Term Extraction and Aspect <b>Sentiment</b> <b>Classification</b> subtasks of ABSA. We show with our experiments that the proposed methods outperform the baseline methods in terms of accuracy and F1 scores on four <b>benchmark</b> online review datasets. Further, we show that the proposed methods can be extended with multiple adaptations and demonstrate a qualitative analysis of the proposed approach using sample text for aspect term extraction.</p></p class="citation"></blockquote><h3 id=7694--138306-sage-evaluating-moral-consistency-in-large-language-models-vamshi-krishna-bonagiri-et-al-2024>(76/94 | 138/306) SaGE: Evaluating Moral Consistency in Large Language Models (Vamshi Krishna Bonagiri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vamshi Krishna Bonagiri, Sreeram Vennam, Priyanshul Govil, Ponnurangam Kumaraguru, Manas Gaur. (2024)<br><strong>SaGE: Evaluating Moral Consistency in Large Language Models</strong><br><button class=copy-to-clipboard title="SaGE: Evaluating Moral Consistency in Large Language Models" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Graph, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13709v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13709v1.pdf filename=2402.13709v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite recent advancements showcasing the impressive capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in conversational systems, we show that even state-of-the-art <b>LLMs</b> are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in <b>LLM</b> evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic <b>Graph</b> Entropy (SaGE), grounded in the concept of &ldquo;Rules of Thumb&rdquo; (RoTs) to measure a model&rsquo;s moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by <b>LLMs,</b> and the RoTs that these models followed. Furthermore, to illustrate the generalizability of SaGE, we use it to investigate <b>LLM</b> consistency on two popular datasets &ndash; TruthfulQA and HellaSwag. Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further.</p></p class="citation"></blockquote><h3 id=7794--139306-kornat-llm-alignment-benchmark-for-korean-social-values-and-common-knowledge-jiyoung-lee-et-al-2024>(77/94 | 139/306) KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge (Jiyoung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiyoung Lee, Minwoo Kim, Seungho Kim, Junghwan Kim, Seunghyun Won, Hwaran Lee, Edward Choi. (2024)<br><strong>KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge</strong><br><button class=copy-to-clipboard title="KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13605v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13605v2.pdf filename=2402.13605v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to be effectively deployed in a specific country, they must possess an understanding of the nation&rsquo;s culture and basic knowledge. To this end, we introduce National Alignment, which measures an alignment between an <b>LLM</b> and a targeted country from two aspects: social value alignment and common knowledge alignment. Social value alignment evaluates how well the model understands nation-specific social values, while common knowledge alignment examines how well the model captures basic knowledge related to the nation. We constructed KorNAT, the first <b>benchmark</b> that measures national alignment with South Korea. For the social value dataset, we obtained ground truth labels from a <b>large-scale</b> <b>survey</b> <b>involving</b> 6,174 unique Korean participants. For the common knowledge dataset, we constructed samples based on Korean textbooks and GED reference materials. KorNAT contains 4K and 6K multiple-choice questions for social value and common knowledge, respectively. Our dataset creation process is meticulously designed and based on statistical sampling theory and was refined through multiple rounds of human review. The experiment results of seven <b>LLMs</b> reveal that only a few models met our reference score, indicating a potential for further enhancement. KorNAT has received government approval after passing an assessment conducted by a government-affiliated organization dedicated to evaluating dataset quality. Samples and detailed evaluation protocols of our dataset can be found in <a href=https://selectstar.ai/ko/papers-national-alignment>https://selectstar.ai/ko/papers-national-alignment</a></p></p class="citation"></blockquote><h3 id=7894--140306-ed-copilot-reduce-emergency-department-wait-time-with-language-model-diagnostic-assistance-liwen-sun-et-al-2024>(78/94 | 140/306) ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance (Liwen Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liwen Sun, Abhineet Agarwal, Aaron Kornblith, Bin Yu, Chenyan Xiong. (2024)<br><strong>ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance</strong><br><button class=copy-to-clipboard title="ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Recommendation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13448v1.pdf filename=2402.13448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a <b>benchmark</b> that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and <b>reinforcement</b> <b>learning</b> to minimize ED wait time and maximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist, ED-Copilot improves prediction accuracy over baselines while halving average wait time from four hours to two hours. Ablation studies demonstrate the importance of model scale and use of a bio-medical language model. Further analyses reveal the necessity of personalized laboratory test suggestions for diagnosing patients with severe cases, as well as the potential of ED-Copilot in providing ED clinicians with informative laboratory test <b>recommendations.</b> Our code is available at <a href=https://github.com/cxcscmu/ED-Copilot>https://github.com/cxcscmu/ED-Copilot</a>.</p></p class="citation"></blockquote><h3 id=7994--141306-understanding-the-dataset-practitioners-behind-large-language-model-development-crystal-qian-et-al-2024>(79/94 | 141/306) Understanding the Dataset Practitioners Behind Large Language Model Development (Crystal Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Crystal Qian, Emily Reif, Minsuk Kahng. (2024)<br><strong>Understanding the Dataset Practitioners Behind Large Language Model Development</strong><br><button class=copy-to-clipboard title="Understanding the Dataset Practitioners Behind Large Language Model Development" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16611v1.pdf filename=2402.16611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> become more advanced and impactful, it is increasingly important to scrutinize the data that they rely upon and produce. What is it to be a dataset practitioner doing this work? We approach this in two parts: first, we define the role of &ldquo;dataset practitioner&rdquo; by performing a retrospective analysis on the responsibilities of teams contributing to <b>LLM</b> development at Google. Then, we conduct semi-structured interviews with a cross-section of these practitioners (N=10). We find that data quality is the top priority. To evaluate data quality, practitioners either rely on their own intuition or write custom evaluation logic. There is a lack of consensus across practitioners on what quality is and how to evaluate it. We discuss potential reasons for this phenomenon and opportunities for alignment.</p></p class="citation"></blockquote><h3 id=8094--142306-automatic-histograms-leveraging-language-models-for-text-dataset-exploration-emily-reif-et-al-2024>(80/94 | 142/306) Automatic Histograms: Leveraging Language Models for Text Dataset Exploration (Emily Reif et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emily Reif, Crystal Qian, James Wexler, Minsuk Kahng. (2024)<br><strong>Automatic Histograms: Leveraging Language Models for Text Dataset Exploration</strong><br><button class=copy-to-clipboard title="Automatic Histograms: Leveraging Language Models for Text Dataset Exploration" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14880v1.pdf filename=2402.14880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Making sense of unstructured text datasets is perennially difficult, yet increasingly relevant with <b>Large</b> <b>Language</b> <b>Models.</b> Data workers often rely on dataset summaries, especially distributions of various derived features. Some features, like toxicity or topics, are relevant to many datasets, but many interesting features are domain specific: instruments and genres for a music dataset, or diseases and symptoms for a medical dataset. Accordingly, data workers often run custom analyses for each dataset, which is cumbersome and difficult. We present AutoHistograms, a visualization tool leveragingLLMs. AutoHistograms automatically identifies relevant features, visualizes them with histograms, and allows the user to interactively query the dataset for categories of entities and create new histograms. In a user study with 10 data workers (n=10), we observe that participants can quickly identify insights and explore the data using AutoHistograms, and conceptualize a broad range of applicable use cases. Together, this tool and user study contributeto the growing field of <b>LLM-assisted</b> sensemaking tools.</p></p class="citation"></blockquote><h3 id=8194--143306-reinforcement-learning-with-dynamic-multi-reward-weighting-for-multi-style-controllable-generation-karin-de-langis-et-al-2024>(81/94 | 143/306) Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation (Karin de Langis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karin de Langis, Ryan Koo, Dongyeop Kang. (2024)<br><strong>Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation</strong><br><button class=copy-to-clipboard title="Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14146v1.pdf filename=2402.14146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Style is an integral component of text that expresses a diverse set of information, including interpersonal dynamics (e.g. formality) and the author&rsquo;s emotions or attitudes (e.g. disgust). Humans often employ multiple styles simultaneously. An open question is how <b>large</b> <b>language</b> <b>models</b> can be explicitly controlled so that they weave together target styles when generating text: for example, to produce text that is both negative and non-toxic. Previous work investigates the controlled generation of a single style, or else controlled generation of a style and other attributes. In this paper, we expand this into controlling multiple styles simultaneously. Specifically, we investigate various formulations of multiple style rewards for a <b>reinforcement</b> <b>learning</b> (RL) approach to controlled multi-style generation. These reward formulations include calibrated outputs from discriminators and dynamic weighting by discriminator gradient magnitudes. We find that dynamic weighting generally outperforms static weighting approaches, and we explore its effectiveness in 2- and 3-style control, even compared to strong baselines like plug-and-play model. All code and data for RL pipelines with multiple style attributes will be publicly available.</p></p class="citation"></blockquote><h3 id=8294--144306-can-watermarks-survive-translation-on-the-cross-lingual-consistency-of-text-watermark-for-large-language-models-zhiwei-he-et-al-2024>(82/94 | 144/306) Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models (Zhiwei He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang. (2024)<br><strong>Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models</strong><br><button class=copy-to-clipboard title="Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14007v1.pdf filename=2402.14007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text watermarking technology aims to tag and identify content produced by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to prevent misuse. In this study, we introduce the concept of &lsquo;&lsquo;cross-lingual consistency&rsquo;&rsquo; in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two <b>LLMs</b> and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an <b>LLM</b> in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.</p></p class="citation"></blockquote><h3 id=8394--145306-analysing-the-impact-of-sequence-composition-on-language-model-pre-training-yu-zhao-et-al-2024>(83/94 | 145/306) Analysing The Impact of Sequence Composition on Language Model Pre-Training (Yu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Zhao, Yuanbin Qu, Konrad Staniszewski, Szymon Tworkowski, Wei Liu, Piotr Miłoś, Yuxiang Wu, Pasquale Minervini. (2024)<br><strong>Analysing The Impact of Sequence Composition on Language Model Pre-Training</strong><br><button class=copy-to-clipboard title="Analysing The Impact of Sequence Composition on Language Model Pre-Training" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13991v1.pdf filename=2402.13991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored. In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, BM25Chunk, can improve <b>in-context</b> <b>learning</b> (+11.6%), knowledge memorisation (+9.8%), and context utilisation (+7.2%) abilities of language models without sacrificing efficiency.</p></p class="citation"></blockquote><h3 id=8494--146306-beyond-probabilities-unveiling-the-misalignment-in-evaluating-large-language-models-chenyang-lyu-et-al-2024>(84/94 | 146/306) Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models (Chenyang Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyang Lyu, Minghao Wu, Alham Fikri Aji. (2024)<br><strong>Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models</strong><br><button class=copy-to-clipboard title="Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13887v1.pdf filename=2402.13887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of <b>LLMs</b> for predictions, primarily due to computational constraints, diverging from real-world <b>LLM</b> usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using <b>LLMs</b> for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess <b>LLMs</b> through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of <b>LLM</b> evaluation methodologies and provide insights for future research in this domain.</p></p class="citation"></blockquote><h3 id=8594--147306-from-text-to-cql-bridging-natural-language-and-corpus-search-engine-luming-lu-et-al-2024>(85/94 | 147/306) From Text to CQL: Bridging Natural Language and Corpus Search Engine (Luming Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luming Lu, Jiyuan An, Yujie Wang, Liner yang, Cunliang Kong, Zhenghao Liu, Shuo Wang, Haozhe Lin, Mingwei Fang, Yaping Huang, Erhong Yang. (2024)<br><strong>From Text to CQL: Bridging Natural Language and Corpus Search Engine</strong><br><button class=copy-to-clipboard title="From Text to CQL: Bridging Natural Language and Corpus Search Engine" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13740v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13740v1.pdf filename=2402.13740v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural Language Processing (NLP) technologies have revolutionized the way we interact with information systems, with a significant focus on converting natural language queries into formal query languages such as SQL. However, less emphasis has been placed on the Corpus Query Language (CQL), a critical tool for linguistic research and detailed analysis within text corpora. The manual construction of CQL queries is a complex and time-intensive task that requires a great deal of expertise, which presents a notable challenge for both researchers and practitioners. This paper presents the first text-to-CQL task that aims to automate the translation of natural language into CQL. We present a comprehensive framework for this task, including a specifically curated <b>large-scale</b> <b>dataset</b> <b>and</b> methodologies leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for effective text-to-CQL task. In addition, we established advanced evaluation metrics to assess the syntactic and semantic accuracy of the generated queries. We created innovative <b>LLM-based</b> conversion approaches and detailed experiments. The results demonstrate the efficacy of our methods and provide insights into the complexities of text-to-CQL task.</p></p class="citation"></blockquote><h3 id=8694--148306-data-driven-discovery-with-large-generative-models-bodhisattwa-prasad-majumder-et-al-2024>(86/94 | 148/306) Data-driven Discovery with Large Generative Models (Bodhisattwa Prasad Majumder et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Sanchaita Hazra, Ashish Sabharwal, Peter Clark. (2024)<br><strong>Data-driven Discovery with Large Generative Models</strong><br><button class=copy-to-clipboard title="Data-driven Discovery with Large Generative Models" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13610v1.pdf filename=2402.13610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery &ndash; a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DATAVOYAGER, a proof-of-concept utilizing <b>GPT-4,</b> we demonstrate how LGMs fulfill several of these desiderata &ndash; a feat previously unattainable &ndash; while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely through the current capabilities of LGMs is challenging. We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility.</p></p class="citation"></blockquote><h3 id=8794--149306-longwanjuan-towards-systematic-measurement-for-long-text-quality-kai-lv-et-al-2024>(87/94 | 149/306) LongWanjuan: Towards Systematic Measurement for Long Text Quality (Kai Lv et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Lv, Xiaoran Liu, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, Dahua Lin. (2024)<br><strong>LongWanjuan: Towards Systematic Measurement for Long Text Quality</strong><br><button class=copy-to-clipboard title="LongWanjuan: Towards Systematic Measurement for Long Text Quality" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Foundation Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13583v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13583v2.pdf filename=2402.13583v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quality of training data are crucial for enhancing the long-text capabilities of <b>foundation</b> <b>models.</b> Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there&rsquo;s a lack of systematic approaches specifically tailored for assessing long texts. Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity. Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and <b>pre-trained</b> <b>language</b> <b>model-based</b> ones. Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens. In LongWanjuan, we categorize long texts into holistic, aggregated, and chaotic types, enabling a detailed analysis of long-text quality. Furthermore, we devise a data mixture recipe that strategically balances different types of long texts within LongWanjuan, leading to significant improvements in model performance on long-text tasks. The code and dataset are available at <a href=https://github.com/OpenLMLab/LongWanjuan>https://github.com/OpenLMLab/LongWanjuan</a>.</p></p class="citation"></blockquote><h3 id=8894--150306-multilingual-coreference-resolution-in-low-resource-south-asian-languages-ritwik-mishra-et-al-2024>(88/94 | 150/306) Multilingual Coreference Resolution in Low-resource South Asian Languages (Ritwik Mishra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ritwik Mishra, Pooja Desur, Rajiv Ratn Shah, Ponnurangam Kumaraguru. (2024)<br><strong>Multilingual Coreference Resolution in Low-resource South Asian Languages</strong><br><button class=copy-to-clipboard title="Multilingual Coreference Resolution in Low-resource South Asian Languages" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Low-Resource, Coreference Resolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13571v1.pdf filename=2402.13571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Coreference</b> <b>resolution</b> involves the task of identifying text spans within a discourse that pertain to the same real-world entity. While this task has been extensively explored in the English language, there has been a notable scarcity of publicly accessible resources and models for <b>coreference</b> <b>resolution</b> in South Asian languages. We introduce a Translated dataset for Multilingual <b>Coreference</b> <b>Resolution</b> (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools for translation and word-alignment. Nearly all of the predicted translations successfully pass a sanity check, and 75% of English references align with their predicted translations. Using multilingual encoders, two off-the-shelf <b>coreference</b> <b>resolution</b> models were trained on a concatenation of TransMuCoRes and a Hindi <b>coreference</b> <b>resolution</b> dataset with manual annotations. The best performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1, respectively, on our test-split of Hindi golden set. This study is the first to evaluate an end-to-end <b>coreference</b> <b>resolution</b> model on a Hindi golden set. Furthermore, this work underscores the limitations of current <b>coreference</b> <b>evaluation</b> metrics when applied to datasets with split antecedents, advocating for the development of more suitable evaluation metrics.</p></p class="citation"></blockquote><h3 id=8994--151306-leveraging-collection-wide-similarities-for-unsupervised-document-structure-extraction-gili-lior-et-al-2024>(89/94 | 151/306) Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction (Gili Lior et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gili Lior, Yoav Goldberg, Gabriel Stanovsky. (2024)<br><strong>Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction</strong><br><button class=copy-to-clipboard title="Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Graph, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13906v1.pdf filename=2402.13906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models. We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations. These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents. Subsequently, we develop an <b>unsupervised</b> <b>graph-based</b> method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure. Our evaluations on three diverse domains in both English and Hebrew indicate that our method extracts meaningful collection-wide structure, and we hope that future work will leverage our method for multi-document applications and structure-aware models.</p></p class="citation"></blockquote><h3 id=9094--152306-cost-efficient-subjective-task-annotation-and-modeling-through-few-shot-annotator-adaptation-preni-golazizian-et-al-2024>(90/94 | 152/306) Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation (Preni Golazizian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Preni Golazizian, Ali Omrani, Alireza S. Ziabari, Morteza Dehghani. (2024)<br><strong>Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation</strong><br><button class=copy-to-clipboard title="Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14101v1.pdf filename=2402.14101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In subjective NLP tasks, where a single ground truth does not exist, the inclusion of diverse annotators becomes crucial as their unique perspectives significantly influence the annotations. In realistic scenarios, the annotation budget often becomes the main determinant of the number of perspectives (i.e., annotators) included in the data and subsequent modeling. We introduce a novel framework for annotation collection and modeling in subjective tasks that aims to minimize the annotation budget while maximizing the predictive performance for each annotator. Our framework has a two-stage design: first, we rely on a small set of annotators to build a multitask model, and second, we augment the model for a new perspective by strategically annotating a few samples per annotator. To test our framework at scale, we introduce and release a unique dataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by 24 annotators for moral sentiment. We demonstrate that our framework surpasses the previous SOTA in capturing the annotators&rsquo; individual perspectives with as little as 25% of the original annotation budget on two datasets. Furthermore, our framework results in more equitable models, reducing the performance disparity among annotators.</p></p class="citation"></blockquote><h3 id=9194--153306-effects-of-term-weighting-approach-with-and-without-stop-words-removing-on-arabic-text-classification-esraa-alhenawi-et-al-2024>(91/94 | 153/306) Effects of term weighting approach with and without stop words removing on Arabic text classification (Esra&rsquo;a Alhenawi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Esra&rsquo;a Alhenawi, Ruba Abu Khurma, Pedro A. Castillo, Maribel G. Arenas. (2024)<br><strong>Effects of term weighting approach with and without stop words removing on Arabic text classification</strong><br><button class=copy-to-clipboard title="Effects of term weighting approach with and without stop words removing on Arabic text classification" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14867v1.pdf filename=2402.14867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classifying <b>text</b> <b>is</b> a method for categorizing documents into pre-established groups. <b>Text</b> <b>documents</b> must be prepared and represented in a way that is appropriate for the algorithms used for data mining prior to classification. As a result, a number of term weighting strategies have been created in the literature to enhance <b>text</b> <b>categorization</b> algorithms&rsquo; functionality. This study compares the effects of Binary and Term frequency weighting feature methodologies on the <b>text&rsquo;s</b> <b>classification</b> method when stop words are eliminated once and when they are not. In recognition of assessing the effects of prior weighting of features approaches on classification results in terms of accuracy, recall, precision, and F-measure values, we used an Arabic data set made up of 322 documents divided into six main topics (agriculture, economy, health, politics, science, and sport), each of which contains 50 documents, with the exception of the health category, which contains 61 documents. The results demonstrate that for all metrics, the term frequency feature weighting approach with stop word removal outperforms the binary approach, while for accuracy, recall, and F-Measure, the binary approach outperforms the TF approach without stop word removal. However, for precision, the two approaches produce results that are very similar. Additionally, it is clear from the data that, using the same phrase weighting approach, stop word removing increases classification accuracy.</p></p class="citation"></blockquote><h3 id=9294--154306-dress-dataset-for-rubric-based-essay-scoring-on-efl-writing-haneul-yoo-et-al-2024>(92/94 | 154/306) DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing (Haneul Yoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haneul Yoo, Jieun Han, So-Yeon Ahn, Alice Oh. (2024)<br><strong>DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing</strong><br><button class=copy-to-clipboard title="DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Essay Scoring<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16733v1.pdf filename=2402.16733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated <b>essay</b> <b>scoring</b> (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time <b>essay</b> <b>scores</b> for students and instructors. However, previous AES models were trained on <b>essays</b> <b>and</b> scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated <b>essay</b> <b>scoring.</b> DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with 1.7K <b>essays</b> <b>authored</b> by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based <b>essay</b> <b>scoring</b> datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for <b>essays,</b> <b>which</b> generates 20K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. DREsS will enable further research to provide a more accurate and practical AES system for EFL writing education.</p></p class="citation"></blockquote><h3 id=9394--155306-breaking-the-hisco-barrier-automatic-occupational-standardization-with-occcanine-christian-møller-dahl-et-al-2024>(93/94 | 155/306) Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE (Christian Møller Dahl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Møller Dahl, Christian Vedel. (2024)<br><strong>Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE</strong><br><button class=copy-to-clipboard title="Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; I-7-0, cs-CL, cs.CL, econ-EM<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13604v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13604v1.pdf filename=2402.13604v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a new tool, OccCANINE, to automatically transform occupational descriptions into the HISCO classification system. The manual work involved in processing and classifying occupational descriptions is error-prone, tedious, and time-consuming. We <b>finetune</b> a preexisting language model (CANINE) to do this automatically thereby performing in seconds and minutes what previously took days and weeks. The model is trained on 14 million pairs of occupational descriptions and HISCO codes in 13 different languages contributed by 22 different sources. Our approach is shown to have accuracy, recall and precision above 90 percent. Our tool breaks the metaphorical HISCO barrier and makes this data readily available for analysis of occupational structures with broad applicability in economics, economic history and various related disciplines.</p></p class="citation"></blockquote><h3 id=9494--156306-an-effective-incorporating-heterogeneous-knowledge-curriculum-learning-for-sequence-labeling-xuemei-tang-et-al-2024>(94/94 | 156/306) An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling (Xuemei Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuemei Tang, Qi Su. (2024)<br><strong>An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling</strong><br><button class=copy-to-clipboard title="An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13534v1.pdf filename=2402.13534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage <b>curriculum</b> <b>learning</b> (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models.</p></p class="citation"></blockquote><h2 id=cscv-41>cs.CV (41)</h2><h3 id=141--157306-vl-trojan-multimodal-instruction-backdoor-attacks-against-autoregressive-visual-language-models-jiawei-liang-et-al-2024>(1/41 | 157/306) VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models (Jiawei Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao. (2024)<br><strong>VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models</strong><br><button class=copy-to-clipboard title="VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 84<br>Keywords: Black Box, Clustering, Few-shot, Few-shot Learning, Multi-modal, Multi-modal, Automatic Speech Recognition, Instruction Following, Reasoning, In-context Learning, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13851v1.pdf filename=2402.13851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autoregressive Visual Language Models (VLMs) showcase impressive <b>few-shot</b> <b>learning</b> capabilities in a <b>multimodal</b> context. Recently, <b>multimodal</b> <b>instruction</b> <b>tuning</b> has been proposed to further enhance <b>instruction-following</b> <b>abilities.</b> However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during <b>instruction</b> <b>tuning.</b> Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in <b>instructions</b> <b>or</b> images, enabling malicious manipulation of the victim model&rsquo;s predictions with predefined triggers. Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers. Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model. To address these challenges, we propose a <b>multimodal</b> <b>instruction</b> <b>backdoor</b> attack, namely VL-Trojan. Our approach facilitates image trigger learning through an isolating and <b>clustering</b> strategy and enhance <b>black-box-attack</b> <b>efficacy</b> via an iterative character-level text trigger generation method. Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52%) in <b>ASR.</b> Moreover, it demonstrates robustness across various model scales and <b>few-shot</b> <b>in-context</b> <b>reasoning</b> scenarios.</p></p class="citation"></blockquote><h3 id=241--158306-multi-organ-self-supervised-contrastive-learning-for-breast-lesion-segmentation-hugo-figueiras-et-al-2024>(2/41 | 158/306) Multi-organ Self-supervised Contrastive Learning for Breast Lesion Segmentation (Hugo Figueiras et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugo Figueiras, Helena Aidos, Nuno Cruz Garcia. (2024)<br><strong>Multi-organ Self-supervised Contrastive Learning for Breast Lesion Segmentation</strong><br><button class=copy-to-clipboard title="Multi-organ Self-supervised Contrastive Learning for Breast Lesion Segmentation" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Contrastive Learning, Fine-tuning, Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14114v1.pdf filename=2402.14114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> has proven to be an effective way to learn representations in domains where annotated labels are scarce, such as medical imaging. A widely adopted framework for this purpose is <b>contrastive</b> <b>learning</b> and it has been applied to different scenarios. This paper seeks to advance our understanding of the <b>contrastive</b> <b>learning</b> framework by exploring a novel perspective: employing multi-organ datasets for pre-training models tailored to specific organ-related target tasks. More specifically, our target task is breast tumour segmentation in ultrasound images. The pre-training datasets include ultrasound images from other organs, such as the lungs and heart, and large datasets of natural images. Our results show that conventional <b>contrastive</b> <b>learning</b> pre-training improves performance compared to <b>supervised</b> baseline approaches. Furthermore, our pre-trained models achieve comparable performance when <b>fine-tuned</b> with only half of the available labelled data. Our findings also show the advantages of pre-training on diverse organ data for improving performance in the downstream task.</p></p class="citation"></blockquote><h3 id=341--159306-zero-shot-generalization-across-architectures-for-visual-classification-evan-gerrtiz-et-al-2024>(3/41 | 159/306) Zero-shot generalization across architectures for visual classification (Evan Gerrtiz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evan Gerrtiz, Luciano Dyballa, Steven W. Zucker. (2024)<br><strong>Zero-shot generalization across architectures for visual classification</strong><br><button class=copy-to-clipboard title="Zero-shot generalization across architectures for visual classification" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14095v1.pdf filename=2402.14095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep <b>convolutional</b> <b>networks</b> <b>(CNNs)</b> to <b>transformers,</b> vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at <a href=https://github.com/dyballa/zero-shot-generalization>https://github.com/dyballa/zero-shot-generalization</a>.</p></p class="citation"></blockquote><h3 id=441--160306-srndiff-short-term-rainfall-nowcasting-with-condition-diffusion-model-xudong-ling-et-al-2024>(4/41 | 160/306) SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model (Xudong Ling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Ling, Chaorong Li, Fengqing Qin, Peng Yang, Yuanyuan Huang. (2024)<br><strong>SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model</strong><br><button class=copy-to-clipboard title="SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Autoencoder, Generative Adversarial Network, Generative Adversarial Network, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13737v1.pdf filename=2402.13737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> are widely used in image generation because they can generate high-quality and realistic samples. This is in contrast to <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> and <b>variational</b> <b>autoencoders</b> (VAEs), which have some limitations in terms of image quality.We introduce the <b>diffusion</b> <b>model</b> to the precipitation forecasting task and propose a short-term precipitation nowcasting with condition <b>diffusion</b> <b>model</b> based on historical observational data, which is referred to as SRNDiff. By incorporating an additional conditional decoder module in the denoising process, SRNDiff achieves end-to-end conditional rainfall prediction. SRNDiff is composed of two networks: a denoising network and a conditional Encoder network. The conditional network is composed of multiple independent UNet networks. These networks extract conditional feature maps at different resolutions, providing accurate conditional information that guides the <b>diffusion</b> <b>model</b> for conditional generation.SRNDiff surpasses <b>GANs</b> in terms of prediction accuracy, although it requires more computational resources.The SRNDiff model exhibits higher stability and efficiency during training than <b>GANs-based</b> approaches, and generates high-quality precipitation distribution samples that better reflect future actual precipitation conditions. This fully validates the advantages and potential of <b>diffusion</b> <b>models</b> in precipitation forecasting, providing new insights for enhancing rainfall prediction.</p></p class="citation"></blockquote><h3 id=541--161306-on-large-visual-language-models-for-medical-imaging-analysis-an-empirical-study-minh-hao-van-et-al-2024>(5/41 | 161/306) On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study (Minh-Hao Van et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh-Hao Van, Prateek Verma, Xintao Wu. (2024)<br><strong>On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study</strong><br><button class=copy-to-clipboard title="On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Few-shot, Multi-modal, Multi-modal, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14162v1.pdf filename=2402.14162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have taken the spotlight in natural language processing. Further, integrating <b>LLMs</b> with vision enables the users to explore emergent abilities with <b>multimodal</b> data. Visual language models (VLMs), such as LLaVA, Flamingo, or CLIP, have demonstrated impressive performance on various visio-linguistic tasks. Consequently, there are enormous applications of <b>large</b> <b>models</b> <b>that</b> could be potentially used in the biomedical imaging field. Along that direction, there is a lack of related work to show the ability of <b>large</b> <b>models</b> <b>to</b> diagnose the diseases. In this work, we study the <b>zero-shot</b> and <b>few-shot</b> robustness of VLMs on the medical imaging analysis tasks. Our comprehensive experiments demonstrate the effectiveness of VLMs in analyzing biomedical images such as brain MRIs, microscopic images of blood cells, and chest X-rays.</p></p class="citation"></blockquote><h3 id=641--162306-real-time-3d-aware-portrait-editing-from-a-single-image-qingyan-bai-et-al-2024>(6/41 | 162/306) Real-time 3D-aware Portrait Editing from a Single Image (Qingyan Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingyan Bai, Yinghao Xu, Zifan Shi, Hao Ouyang, Qiuyu Wang, Ceyuan Yang, Xuan Wang, Gordon Wetzstein, Yujun Shen, Qifeng Chen. (2024)<br><strong>Real-time 3D-aware Portrait Editing from a Single Image</strong><br><button class=copy-to-clipboard title="Real-time 3D-aware Portrait Editing from a Single Image" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Fine-tuning, Geometry, Knowledge Distillation, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14000v1.pdf filename=2402.14000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents 3DPE, a practical tool that can efficiently edit a face image following given <b>prompts,</b> like reference images or text descriptions, in the 3D-aware manner. To this end, a lightweight module is <b>distilled</b> from a 3D portrait generator and a <b>text-to-image</b> model, which provide prior knowledge of face <b>geometry</b> and open-vocabulary editing capability, respectively. Such a design brings two compelling advantages over existing approaches. First, our system achieves real-time editing with a feedforward network (i.e., ~0.04s per image), over 100x faster than the second competitor. Second, thanks to the powerful priors, our module could focus on the learning of editing-related variations, such that it manages to handle various types of editing simultaneously in the training phase and further supports fast adaptation to user-specified novel types of editing during inference (e.g., with ~5min <b>fine-tuning</b> per case). The code, the model, and the interface will be made publicly available to facilitate future research.</p></p class="citation"></blockquote><h3 id=741--163306-weakly-supervised-localisation-of-prostate-cancer-using-reinforcement-learning-for-bi-parametric-mr-images-martynas-pocius-et-al-2024>(7/41 | 163/306) Weakly supervised localisation of prostate cancer using reinforcement learning for bi-parametric MR images (Martynas Pocius et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martynas Pocius, Wen Yan, Dean C. Barratt, Mark Emberton, Matthew J. Clarkson, Yipeng Hu, Shaheer U. Saeed. (2024)<br><strong>Weakly supervised localisation of prostate cancer using reinforcement learning for bi-parametric MR images</strong><br><button class=copy-to-clipboard title="Weakly supervised localisation of prostate cancer using reinforcement learning for bi-parametric MR images" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Multiple Instance Learning, Reinforcement Learning, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13778v1.pdf filename=2402.13778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we propose a <b>reinforcement</b> <b>learning</b> based weakly <b>supervised</b> system for localisation. We train a controller function to localise regions of interest within an image by introducing a novel reward definition that utilises non-binarised classification probability, generated by a pre-trained binary classifier which classifies object presence in images or image crops. The object-presence classifier may then inform the controller of its localisation quality by quantifying the likelihood of the image containing an object. Such an approach allows us to minimize any potential labelling or human bias propagated via human labelling for fully <b>supervised</b> localisation. We evaluate our proposed approach for a task of cancerous lesion localisation on a large dataset of real clinical bi-parametric MR images of the prostate. Comparisons to the commonly used <b>multiple-instance</b> <b>learning</b> <b>weakly</b> <b>supervised</b> localisation and to a fully <b>supervised</b> baseline show that our proposed method outperforms the multi-instance learning and performs comparably to fully-supervised learning, using only image-level classification labels for training.</p></p class="citation"></blockquote><h3 id=841--164306-hybrid-reasoning-based-on-large-language-models-for-autonomous-car-driving-mehdi-azarafza-et-al-2024>(8/41 | 164/306) Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving (Mehdi Azarafza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg. (2024)<br><strong>Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving</strong><br><button class=copy-to-clipboard title="Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13602v1.pdf filename=2402.13602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex <b>reasoning</b> tasks. However, their ability to generalize this advanced <b>reasoning</b> with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well <b>LLMs</b> can adapt and apply a combination of arithmetic and <b>common-sense</b> <b>reasoning,</b> particularly in autonomous driving scenarios. We hypothesize that <b>LLMs</b> hybrid <b>reasoning</b> abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> based on accuracy by comparing their answers with human-generated ground truth inside CARLA. The results showed that when a combination of images (detected objects) and sensor data is fed into the <b>LLM,</b> it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions. This formulation and answers can assist in decision-making for auto-pilot systems.</p></p class="citation"></blockquote><h3 id=941--165306-transgop-transformer-based-gaze-object-prediction-binglu-wang-et-al-2024>(9/41 | 165/306) TransGOP: Transformer-Based Gaze Object Prediction (Binglu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binglu Wang, Chenxi Guo, Yang Jin, Haisheng Xia, Nian Liu. (2024)<br><strong>TransGOP: Transformer-Based Gaze Object Prediction</strong><br><button class=copy-to-clipboard title="TransGOP: Transformer-Based Gaze Object Prediction" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Autoencoder, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13578v1.pdf filename=2402.13578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gaze <b>object</b> <b>prediction</b> aims to predict the location and category of the <b>object</b> <b>that</b> is watched by a human. Previous gaze <b>object</b> <b>prediction</b> works use <b>CNN-based</b> <b>object</b> <b>detectors</b> to predict the <b>object&rsquo;s</b> <b>location.</b> However, we find that <b>Transformer-based</b> <b>object</b> <b>detectors</b> can predict more accurate <b>object</b> <b>location</b> for dense <b>objects</b> <b>in</b> retail scenarios. Moreover, the long-distance modeling capability of the <b>Transformer</b> can help to build relationships between the human head and the gaze <b>object,</b> <b>which</b> is important for the GOP task. To this end, this paper introduces <b>Transformer</b> into the fields of gaze <b>object</b> <b>prediction</b> and proposes an end-to-end <b>Transformer-based</b> gaze <b>object</b> <b>prediction</b> method named TransGOP. Specifically, TransGOP uses an off-the-shelf <b>Transformer-based</b> <b>object</b> <b>detector</b> to detect the location of <b>objects</b> <b>and</b> designs a <b>Transformer-based</b> gaze <b>autoencoder</b> in the gaze regressor to establish long-distance gaze relationships. Moreover, to improve gaze heatmap regression, we propose an <b>object-to-gaze</b> <b>cross-attention</b> mechanism to let the queries of the gaze <b>autoencoder</b> learn the global-memory position knowledge from the <b>object</b> <b>detector.</b> Finally, to make the whole framework end-to-end trained, we propose a Gaze Box loss to jointly optimize the <b>object</b> <b>detector</b> and gaze regressor by enhancing the gaze heatmap energy in the box of the gaze <b>object.</b> <b>Extensive</b> experiments on the GOO-Synth and GOO-Real datasets demonstrate that our TransGOP achieves state-of-the-art performance on all tracks, i.e., <b>object</b> <b>detection,</b> gaze estimation, and gaze <b>object</b> <b>prediction.</b> Our code will be available at <a href=https://github.com/chenxi-Guo/TransGOP.git>https://github.com/chenxi-Guo/TransGOP.git</a>.</p></p class="citation"></blockquote><h3 id=1041--166306-effloc-lightweight-vision-transformer-for-efficient-6-dof-camera-relocalization-zhendong-xiao-et-al-2024>(10/41 | 166/306) EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization (Zhendong Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhendong Xiao, Changhao Chen, Shan Yang, Wu Wei. (2024)<br><strong>EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization</strong><br><button class=copy-to-clipboard title="EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Vision Transformer, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13537v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13537v1.pdf filename=2402.13537v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Camera relocalization is pivotal in computer <b>vision,</b> <b>with</b> applications in AR, drones, robotics, and autonomous driving. It estimates 3D camera position and orientation (6-DoF) from images. Unlike traditional methods like SLAM, recent strides use deep learning for direct end-to-end pose estimation. We propose EffLoc, a novel efficient <b>Vision</b> <b>Transformer</b> for single-image camera relocalization. EffLoc&rsquo;s hierarchical layout, memory-bound <b>self-attention,</b> and feed-forward layers boost memory efficiency and inter-channel communication. Our introduced sequential group attention (SGA) module enhances computational efficiency by diversifying input features, reducing redundancy, and expanding model capacity. EffLoc excels in efficiency and accuracy, outperforming prior methods, such as AtLoc and MapNet. It thrives on large-scale outdoor car-driving scenario, ensuring simplicity, end-to-end trainability, and eliminating handcrafted loss functions.</p></p class="citation"></blockquote><h3 id=1141--167306-contrastive-prompts-improve-disentanglement-in-text-to-image-diffusion-models-chen-wu-et-al-2024>(11/41 | 167/306) Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models (Chen Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Wu, Fernando De la Torre. (2024)<br><strong>Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Zero-shot, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13490v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13490v1.pdf filename=2402.13490v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> <b>diffusion</b> <b>models</b> have achieved remarkable performance in image synthesis, while the text interface does not always provide fine-grained control over certain image factors. For instance, changing a single token in the text can have unintended effects on the image. This paper shows a simple modification of classifier-free guidance can help disentangle image factors in <b>text-to-image</b> models. The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two <b>prompts</b> that differ in minimal tokens: the positive <b>prompt</b> describes the image to be synthesized, and the baseline <b>prompt</b> serves as a &ldquo;baseline&rdquo; that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific <b>diffusion</b> <b>models</b> trained on an object class, (2) to gain continuous, rig-like controls for <b>text-to-image</b> generation, and (3) to improve the performance of <b>zero-shot</b> image editors.</p></p class="citation"></blockquote><h3 id=1241--168306-unsupervised-learning-based-object-detection-using-contrastive-learning-chandan-kumar-et-al-2024>(12/41 | 168/306) Unsupervised learning based object detection using Contrastive Learning (Chandan Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chandan Kumar, Jansel Herrera-Gerena, John Just, Matthew Darr, Ali Jannesari. (2024)<br><strong>Unsupervised learning based object detection using Contrastive Learning</strong><br><button class=copy-to-clipboard title="Unsupervised learning based object detection using Contrastive Learning" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Contrastive Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13465v1.pdf filename=2402.13465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training image-based <b>object</b> <b>detectors</b> presents formidable challenges, as it entails not only the complexities of <b>object</b> <b>detection</b> but also the added intricacies of precisely localizing <b>objects</b> <b>within</b> potentially diverse and noisy environments. However, the collection of imagery itself can often be straightforward; for instance, cameras mounted in vehicles can effortlessly capture vast amounts of data in various real-world scenarios. In light of this, we introduce a groundbreaking method for training single-stage <b>object</b> <b>detectors</b> through unsupervised/self-supervised learning. Our state-of-the-art approach has the potential to revolutionize the labeling process, substantially reducing the time and cost associated with manual annotation. Furthermore, it paves the way for previously unattainable research opportunities, particularly for large, diverse, and challenging datasets lacking extensive labels. In contrast to prevalent <b>unsupervised</b> <b>learning</b> methods that primarily target classification tasks, our approach takes on the unique challenge of <b>object</b> <b>detection.</b> We pioneer the concept of intra-image <b>contrastive</b> <b>learning</b> alongside inter-image counterparts, enabling the acquisition of crucial location information essential for <b>object</b> <b>detection.</b> The method adeptly learns and represents this location information, yielding informative heatmaps. Our results showcase an outstanding accuracy of \textbf{89.2%}, marking a significant breakthrough of approximately \textbf{15x} over random initialization in the realm of <b>unsupervised</b> <b>object</b> <b>detection</b> within the field of computer vision.</p></p class="citation"></blockquote><h3 id=1341--169306-a-unified-framework-and-dataset-for-assessing-gender-bias-in-vision-language-models-ashutosh-sathe-et-al-2024>(13/41 | 169/306) A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models (Ashutosh Sathe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashutosh Sathe, Prachi Jain, Sunayana Sitaram. (2024)<br><strong>A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models</strong><br><button class=copy-to-clipboard title="A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-CY, cs.CV<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Image2text, Text2image, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13636v1.pdf filename=2402.13636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>vision-language</b> models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including <b>image-to-text,</b> text-to-text, <b>text-to-image,</b> and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to <b>benchmark</b> gender bias. In our <b>benchmarking</b> of recent <b>vision-language</b> models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.</p></p class="citation"></blockquote><h3 id=1441--170306-event-aware-video-corpus-moment-retrieval-danyang-hou-et-al-2024>(14/41 | 170/306) Event-aware Video Corpus Moment Retrieval (Danyang Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng. (2024)<br><strong>Event-aware Video Corpus Moment Retrieval</strong><br><button class=copy-to-clipboard title="Event-aware Video Corpus Moment Retrieval" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IR, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Contrastive Learning, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13566v1.pdf filename=2402.13566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video Corpus Moment Retrieval (VCMR) is a practical video retrieval task focused on identifying a specific moment within a vast corpus of untrimmed videos using the natural language query. Existing methods for VCMR typically rely on frame-aware video retrieval, calculating similarities between the query and video frames to rank videos based on maximum frame similarity.However, this approach overlooks the semantic structure embedded within the information between frames, namely, the event, a crucial element for human comprehension of videos. Motivated by this, we propose EventFormer, a model that explicitly utilizes events within videos as fundamental units for video retrieval. The model extracts event representations through event <b>reasoning</b> and hierarchical event encoding. The event <b>reasoning</b> module groups consecutive and visually similar frame representations into events, while the hierarchical event encoding encodes information at both the frame and event levels. We also introduce anchor multi-head self-attenion to encourage <b>Transformer</b> to capture the relevance of adjacent content in the video. The training of EventFormer is conducted by two-branch <b>contrastive</b> <b>learning</b> and dual optimization for two sub-tasks of VCMR. Extensive experiments on TVR, ANetCaps, and DiDeMo <b>benchmarks</b> show the effectiveness and efficiency of EventFormer in VCMR, achieving new state-of-the-art results. Additionally, the effectiveness of EventFormer is also validated on partially relevant video retrieval task.</p></p class="citation"></blockquote><h3 id=1541--171306-codis-benchmarking-context-dependent-visual-comprehension-for-multimodal-large-language-models-fuwen-luo-et-al-2024>(15/41 | 171/306) CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models (Fuwen Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li, Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, Peng Li, Ning Ma, Maosong Sun, Yang Liu. (2024)<br><strong>CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 32<br>Keywords: Benchmarking, Benchmarking, Multi-modal, Multi-modal, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13607v1.pdf filename=2402.13607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing <b>benchmarks</b> fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new <b>benchmark,</b> named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this <b>benchmark.</b> Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a context-dependent manner. View our project website at <a href=https://thunlp-mt.github.io/CODIS>https://thunlp-mt.github.io/CODIS</a>.</p></p class="citation"></blockquote><h3 id=1641--172306-t-stitch-accelerating-sampling-in-pre-trained-diffusion-models-with-trajectory-stitching-zizheng-pan-et-al-2024>(16/41 | 172/306) T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching (Zizheng Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zizheng Pan, Bohan Zhuang, De-An Huang, Weili Nie, Zhiding Yu, Chaowei Xiao, Jianfei Cai, Anima Anandkumar. (2024)<br><strong>T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching</strong><br><button class=copy-to-clipboard title="T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Probabilistic Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14167v1.pdf filename=2402.14167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sampling from <b>diffusion</b> <b>probabilistic</b> <b>models</b> (DPMs) is often expensive for high-quality image generation and typically requires many steps with a large model. In this paper, we introduce sampling Trajectory Stitching T-Stitch, a simple yet efficient technique to improve the sampling efficiency with little or no generation degradation. Instead of solely using a large DPM for the entire sampling trajectory, T-Stitch first leverages a smaller DPM in the initial steps as a cheap drop-in replacement of the larger DPM and switches to the larger DPM at a later stage. Our key insight is that different <b>diffusion</b> <b>models</b> learn similar encodings under the same training data distribution and smaller models are capable of generating good global structures in the early steps. Extensive experiments demonstrate that T-Stitch is training-free, generally applicable for different architectures, and complements most existing fast sampling techniques with flexible speed and quality trade-offs. On DiT-XL, for example, 40% of the early timesteps can be safely replaced with a 10x faster DiT-S without performance drop on class-conditional ImageNet generation. We further show that our method can also be used as a drop-in technique to not only accelerate the popular pretrained stable <b>diffusion</b> <b>(SD)</b> models but also improve the <b>prompt</b> alignment of stylized SD models from the public model zoo. Code is released at <a href=https://github.com/NVlabs/T-Stitch>https://github.com/NVlabs/T-Stitch</a></p></p class="citation"></blockquote><h3 id=1741--173306-sdxl-lightning-progressive-adversarial-diffusion-distillation-shanchuan-lin-et-al-2024>(17/41 | 173/306) SDXL-Lightning: Progressive Adversarial Diffusion Distillation (Shanchuan Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanchuan Lin, Anran Wang, Xiao Yang. (2024)<br><strong>SDXL-Lightning: Progressive Adversarial Diffusion Distillation</strong><br><button class=copy-to-clipboard title="SDXL-Lightning: Progressive Adversarial Diffusion Distillation" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13929v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13929v2.pdf filename=2402.13929v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a diffusion <b>distillation</b> method that achieves new state-of-the-art in one-step/few-step 1024px <b>text-to-image</b> generation based on SDXL. Our method combines progressive and adversarial <b>distillation</b> to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our <b>distilled</b> SDXL-Lightning models both as LoRA and full UNet weights.</p></p class="citation"></blockquote><h3 id=1841--174306-high-throughput-visual-nano-drone-to-nano-drone-relative-localization-using-onboard-fully-convolutional-networks-luca-crupi-et-al-2024>(18/41 | 174/306) High-throughput Visual Nano-drone to Nano-drone Relative Localization using Onboard Fully Convolutional Networks (Luca Crupi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Crupi, Alessandro Giusti, Daniele Palossi. (2024)<br><strong>High-throughput Visual Nano-drone to Nano-drone Relative Localization using Onboard Fully Convolutional Networks</strong><br><button class=copy-to-clipboard title="High-throughput Visual Nano-drone to Nano-drone Relative Localization using Onboard Fully Convolutional Networks" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13756v1.pdf filename=2402.13756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Relative drone-to-drone localization is a fundamental building block for any swarm operations. We address this task in the context of miniaturized nano-drones, i.e., 10cm in diameter, which show an ever-growing interest due to novel use cases enabled by their reduced form factor. The price for their versatility comes with limited onboard resources, i.e., sensors, processing units, and memory, which limits the complexity of the onboard algorithms. A traditional solution to overcome these limitations is represented by lightweight deep learning models directly deployed aboard nano-drones. This work tackles the challenging relative pose estimation between nano-drones using only a gray-scale low-resolution camera and an ultra-low-power System-on-Chip (SoC) hosted onboard. We present a vertically integrated system based on a novel vision-based fully <b>convolutional</b> <b>neural</b> <b>network</b> (FCNN), which runs at 39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8 SoC. We compare our FCNN against three State-of-the-Art (SoA) systems. Considering the best-performing SoA approach, our model results in an R-squared improvement from 32 to 47% on the horizontal image coordinate and from 18 to 55% on the vertical image coordinate, on a real-world dataset of 30k images. Finally, our in-field tests show a reduction of the average tracking error of 37% compared to a previous SoA work and an endurance performance up to the entire battery lifetime of 4 minutes.</p></p class="citation"></blockquote><h3 id=1941--175306-hybrid-video-diffusion-models-with-2d-triplane-and-3d-wavelet-representation-kihong-kim-et-al-2024>(19/41 | 175/306) Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation (Kihong Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kihong Kim, Haneol Lee, Jihye Park, Seyeon Kim, Kwanghee Lee, Seungryong Kim, Jaejun Yoo. (2024)<br><strong>Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation</strong><br><button class=copy-to-clipboard title="Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Autoencoder, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13729v1.pdf filename=2402.13729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating high-quality videos that synthesize desired realistic content is a challenging task due to their intricate high-dimensionality and complexity of videos. Several recent <b>diffusion-based</b> <b>methods</b> have shown comparable performance by compressing videos to a lower-dimensional latent space, using traditional video <b>autoencoder</b> architecture. However, such method that employ standard frame-wise 2D and 3D <b>convolution</b> fail to fully exploit the spatio-temporal nature of videos. To address this issue, we propose a novel hybrid video <b>diffusion</b> <b>model,</b> called HVDM, which can capture spatio-temporal dependencies more effectively. The HVDM is trained by a hybrid video <b>autoencoder</b> which extracts a disentangled representation of the video including: (i) a global context information captured by a 2D projected latent (ii) a local volume information captured by 3D <b>convolutions</b> with wavelet decomposition (iii) a frequency information for improving the video reconstruction. Based on this disentangled representation, our hybrid <b>autoencoder</b> provide a more comprehensive video latent enriching the generated videos with fine structures and details. Experiments on video generation benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed approach achieves state-of-the-art video generation quality, showing a wide range of video applications (e.g., long video generation, image-to-video, and video dynamics control).</p></p class="citation"></blockquote><h3 id=2041--176306-push-quantization-aware-training-toward-full-precision-performances-via-consistency-regularization-junbiao-pang-et-al-2024>(20/41 | 176/306) Push Quantization-Aware Training Toward Full Precision Performances via Consistency Regularization (Junbiao Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junbiao Pang, Tianyang Cai, Baochang Zhang, Jiaqi Wu, Ye Tao. (2024)<br><strong>Push Quantization-Aware Training Toward Full Precision Performances via Consistency Regularization</strong><br><button class=copy-to-clipboard title="Push Quantization-Aware Training Toward Full Precision Performances via Consistency Regularization" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13497v1.pdf filename=2402.13497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>Quantization-Aware</b> Training (QAT) methods intensively depend on the complete labeled dataset or <b>knowledge</b> <b>distillation</b> to guarantee the performances toward Full Precision (FP) accuracies. However, empirical results show that QAT still has inferior results compared to its FP counterpart. One question is how to push QAT toward or even surpass FP performances. In this paper, we address this issue from a new perspective by injecting the vicinal data distribution information to improve the generalization performances of QAT effectively. We present a simple, novel, yet powerful method introducing an Consistency Regularization (CR) for QAT. Concretely, CR assumes that augmented samples should be consistent in the latent feature space. Our method generalizes well to different network architectures and various QAT methods. Extensive experiments demonstrate that our approach significantly outperforms the current state-of-the-art QAT methods and even FP counterparts.</p></p class="citation"></blockquote><h3 id=2141--177306-mask-up-investigating-biases-in-face-re-identification-for-masked-faces-siddharth-d-jaiswal-et-al-2024>(21/41 | 177/306) Mask-up: Investigating Biases in Face Re-identification for Masked Faces (Siddharth D Jaiswal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siddharth D Jaiswal, Ankit Kr. Verma, Animesh Mukherjee. (2024)<br><strong>Mask-up: Investigating Biases in Face Re-identification for Masked Faces</strong><br><button class=copy-to-clipboard title="Mask-up: Investigating Biases in Face Re-identification for Masked Faces" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-CY, cs-HC, cs.CV<br>Keyword Score: 23<br>Keywords: Face Recognition, Benchmarking, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13771v1.pdf filename=2402.13771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI based <b>Face</b> <b>Recognition</b> Systems (FRSs) are now widely distributed and deployed as MLaaS solutions all over the world, moreso since the COVID-19 pandemic for tasks ranging from validating individuals&rsquo; <b>faces</b> <b>while</b> buying SIM cards to surveillance of citizens. Extensive biases have been reported against marginalized groups in these systems and have led to highly discriminatory outcomes. The post-pandemic world has normalized wearing <b>face</b> <b>masks</b> but FRSs have not kept up with the changing times. As a result, these systems are susceptible to mask based <b>face</b> <b>occlusion.</b> In this study, we audit four commercial and nine open-source FRSs for the task of <b>face</b> <b>re-identification</b> between different varieties of masked and unmasked images across five <b>benchmark</b> datasets (total 14,722 images). These simulate a realistic validation/surveillance task as deployed in all major countries around the world. Three of the commercial and five of the open-source FRSs are highly inaccurate; they further perpetuate biases against non-White individuals, with the lowest accuracy being 0%. A survey for the same task with 85 human participants also results in a low accuracy of 40%. Thus a <b>human-in-the-loop</b> moderation in the pipeline does not alleviate the concerns, as has been frequently hypothesized in literature. Our large-scale study shows that developers, lawmakers and users of such services need to rethink the design principles behind FRSs, especially for the task of <b>face</b> <b>re-identification,</b> taking cognizance of observed biases.</p></p class="citation"></blockquote><h3 id=2241--178306-bee-net-a-deep-neural-network-to-identify-in-the-wild-bodily-expression-of-emotions-mohammad-mahdi-dehshibi-et-al-2024>(22/41 | 178/306) BEE-NET: A deep neural network to identify in-the-wild Bodily Expression of Emotions (Mohammad Mahdi Dehshibi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Mahdi Dehshibi, David Masip. (2024)<br><strong>BEE-NET: A deep neural network to identify in-the-wild Bodily Expression of Emotions</strong><br><button class=copy-to-clipboard title="BEE-NET: A deep neural network to identify in-the-wild Bodily Expression of Emotions" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13955v1.pdf filename=2402.13955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language. To this end, we introduce a novel multi-stream deep <b>convolutional</b> <b>neural</b> <b>network</b> named BEE-NET. We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process. Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space. Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation. To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE). Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%.</p></p class="citation"></blockquote><h3 id=2341--179306-tumor-segmentation-on-whole-slide-images-training-or-prompting-huaqian-wu-et-al-2024>(23/41 | 179/306) Tumor segmentation on whole slide images: training or prompting? (Huaqian Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huaqian Wu, Clara Brémond-Martin, Kévin Bouaou, Cédric Clouchoux. (2024)<br><strong>Tumor segmentation on whole slide images: training or prompting?</strong><br><button class=copy-to-clipboard title="Tumor segmentation on whole slide images: training or prompting?" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13932v1.pdf filename=2402.13932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level. However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask. Downsampling the WSI and performing semantic segmentation is another possible approach. While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss. Visual <b>prompting</b> is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself. Such approach has demonstrated promising results on many computer vision tasks. In this paper, we show the efficacy of visual <b>prompting</b> in the context of tumor segmentation for three distinct organs. In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate <b>prompt</b> examples, visual <b>prompting</b> can achieve comparable or better performance without extensive <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=2441--180306-zero-bev-zero-shot-projection-of-any-first-person-modality-to-bev-maps-gianluca-monaci-et-al-2024>(24/41 | 180/306) Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps (Gianluca Monaci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianluca Monaci, Leonid Antsfeld, Boris Chidlovskii, Christian Wolf. (2024)<br><strong>Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps</strong><br><button class=copy-to-clipboard title="Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13848v1.pdf filename=2402.13848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bird&rsquo;s-eye view (BEV) maps are an important geometrically structured representation widely used in robotics, in particular self-driving vehicles and terrestrial robots. Existing algorithms either require depth information for the geometric projection, which is not always reliably available, or are trained end-to-end in a fully <b>supervised</b> way to map visual first-person observations to BEV representation, and are therefore restricted to the output modality they have been trained for. In contrast, we propose a new model capable of performing <b>zero-shot</b> projections of any modality available in a first person view to the corresponding BEV map. This is achieved by disentangling the geometric inverse perspective projection from the modality transformation, eg. RGB to occupancy. The method is general and we showcase experiments projecting to BEV three different modalities: semantic segmentation, motion vectors and object bounding boxes detected in first person. We experimentally show that the model outperforms competing methods, in particular the widely used baseline resorting to monocular depth estimation.</p></p class="citation"></blockquote><h3 id=2541--181306-mstar-multi-scale-backbone-architecture-search-for-timeseries-classification-tue-m-cao-et-al-2024>(25/41 | 181/306) MSTAR: Multi-Scale Backbone Architecture Search for Timeseries Classification (Tue M. Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tue M. Cao, Nhat H. Tran, Hieu H. Pham, Hung T. Nguyen, Le P. Nguyen. (2024)<br><strong>MSTAR: Multi-Scale Backbone Architecture Search for Timeseries Classification</strong><br><button class=copy-to-clipboard title="MSTAR: Multi-Scale Backbone Architecture Search for Timeseries Classification" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13822v1.pdf filename=2402.13822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most of the previous approaches to Time Series Classification (TSC) highlight the significance of receptive fields and frequencies while overlooking the time resolution. Hence, unavoidably suffered from scalability issues as they integrated an extensive range of receptive fields into classification models. Other methods, while having a better adaptation for large datasets, require manual design and yet not being able to reach the optimal architecture due to the uniqueness of each dataset. We overcome these challenges by proposing a novel multi-scale search space and a framework for Neural architecture search (NAS), which addresses both the problem of frequency and time resolution, discovering the suitable scale for a specific dataset. We further show that our model can serve as a backbone to employ a powerful <b>Transformer</b> module with both untrained and pre-trained weights. Our search space reaches the state-of-the-art performance on four datasets on four different domains while introducing more than ten highly <b>fine-tuned</b> models for each data.</p></p class="citation"></blockquote><h3 id=2641--182306-generalizable-semantic-vision-query-generation-for-zero-shot-panoptic-and-semantic-segmentation-jialei-chen-et-al-2024>(26/41 | 182/306) Generalizable Semantic Vision Query Generation for Zero-shot Panoptic and Semantic Segmentation (Jialei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialei Chen, Daisuke Deguchi, Chenkai Zhang, Hiroshi Murase. (2024)<br><strong>Generalizable Semantic Vision Query Generation for Zero-shot Panoptic and Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Generalizable Semantic Vision Query Generation for Zero-shot Panoptic and Semantic Segmentation" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13697v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13697v1.pdf filename=2402.13697v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> Panoptic Segmentation (ZPS) aims to recognize foreground instances and background stuff without images containing unseen categories in training. Due to the visual data sparsity and the difficulty of generalizing from seen to unseen categories, this task remains challenging. To better generalize to unseen classes, we propose Conditional tOken aligNment and Cycle trAnsiTion (CONCAT), to produce generalizable semantic vision queries. First, a feature extractor is trained by CON to link the vision and semantics for providing target queries. Formally, CON is proposed to align the semantic queries with the CLIP visual CLS token extracted from complete and masked images. To address the lack of unseen categories, a generator is required. However, one of the gaps in synthesizing pseudo vision queries, ie, vision queries for unseen categories, is describing fine-grained visual details through semantic embeddings. Therefore, we approach CAT to train the generator in semantic-vision and vision-semantic manners. In semantic-vision, visual query contrast is proposed to model the high granularity of vision by pulling the pseudo vision queries with the corresponding targets containing segments while pushing those without segments away. To ensure the generated queries retain semantic information, in vision-semantic, the pseudo vision queries are mapped back to semantic and <b>supervised</b> by real semantic embeddings. Experiments on ZPS achieve a 5.2% hPQ increase surpassing SOTA. We also examine inductive ZPS and open-vocabulary semantic segmentation and obtain comparative results while being 2 times faster in testing.</p></p class="citation"></blockquote><h3 id=2741--183306-yolov9-learning-what-you-want-to-learn-using-programmable-gradient-information-chien-yao-wang-et-al-2024>(27/41 | 183/306) YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information (Chien-Yao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chien-Yao Wang, I-Hau Yeh, Hong-Yuan Mark Liao. (2024)<br><strong>YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</strong><br><button class=copy-to-clipboard title="YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13616v1.pdf filename=2402.13616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Today&rsquo;s deep learning methods focus on how to design the most appropriate objective functions so that the prediction results of the model can be closest to the ground truth. Meanwhile, an appropriate architecture that can facilitate acquisition of enough information for prediction has to be designed. Existing methods ignore a fact that when input data undergoes layer-by-layer feature extraction and spatial transformation, large amount of information will be lost. This paper will delve into the important issues of data loss when data is transmitted through deep networks, namely information bottleneck and reversible functions. We proposed the concept of programmable gradient information (PGI) to cope with the various changes required by deep networks to achieve multiple objectives. PGI can provide complete input information for the target task to calculate objective function, so that reliable gradient information can be obtained to update network weights. In addition, a new lightweight network architecture &ndash; Generalized Efficient Layer Aggregation Network (GELAN), based on gradient path planning is designed. GELAN&rsquo;s architecture confirms that PGI has gained superior results on lightweight models. We verified the proposed GELAN and PGI on MS COCO dataset based <b>object</b> <b>detection.</b> The results show that GELAN only uses conventional <b>convolution</b> operators to achieve better parameter utilization than the state-of-the-art methods developed based on depth-wise <b>convolution.</b> PGI can be used for variety of models from lightweight to large. It can be used to obtain complete information, so that train-from-scratch models can achieve better results than state-of-the-art models pre-trained using large datasets, the comparison results are shown in Figure 1. The source codes are at: <a href=https://github.com/WongKinYiu/yolov9>https://github.com/WongKinYiu/yolov9</a>.</p></p class="citation"></blockquote><h3 id=2841--184306-explainable-classification-techniques-for-quantum-dot-device-measurements-daniel-schug-et-al-2024>(28/41 | 184/306) Explainable Classification Techniques for Quantum Dot Device Measurements (Daniel Schug et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak. (2024)<br><strong>Explainable Classification Techniques for Quantum Dot Device Measurements</strong><br><button class=copy-to-clipboard title="Explainable Classification Techniques for Quantum Dot Device Measurements" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cond-mat-mes-hall, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13699v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13699v2.pdf filename=2402.13699v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where <b>human</b> <b>intervention</b> is necessary at the current stage of development.</p></p class="citation"></blockquote><h3 id=2941--185306-robustness-of-deep-neural-networks-for-micro-doppler-radar-classification-mikolaj-czerkawski-et-al-2024>(29/41 | 185/306) Robustness of Deep Neural Networks for Micro-Doppler Radar Classification (Mikolaj Czerkawski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mikolaj Czerkawski, Carmine Clemente, Craig Michie, Christos Tachtatzis. (2024)<br><strong>Robustness of Deep Neural Networks for Micro-Doppler Radar Classification</strong><br><button class=copy-to-clipboard title="Robustness of Deep Neural Networks for Micro-Doppler Radar Classification" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, eess-SP<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13651v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13651v2.pdf filename=2402.13651v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the great capabilities of deep classifiers for radar data processing come the risks of learning dataset-specific features that do not generalize well. In this work, the robustness of two deep <b>convolutional</b> architectures, trained and tested on the same data, is evaluated. When standard training practice is followed, both classifiers exhibit sensitivity to subtle temporal shifts of the input representation, an augmentation that carries minimal semantic content. Furthermore, the models are extremely susceptible to adversarial examples. Both small temporal shifts and adversarial examples are a result of a model overfitting on features that do not generalize well. As a remedy, it is shown that training on adversarial examples and temporally augmented samples can reduce this effect and lead to models that generalise better. Finally, models operating on cadence-velocity diagram representation rather than Doppler-time are demonstrated to be naturally more immune to adversarial examples.</p></p class="citation"></blockquote><h3 id=3041--186306-delving-into-dark-regions-for-robust-shadow-detection-huankang-guan-et-al-2024>(30/41 | 186/306) Delving into Dark Regions for Robust Shadow Detection (Huankang Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huankang Guan, Ke Xu, Rynson W. H. Lau. (2024)<br><strong>Delving into Dark Regions for Robust Shadow Detection</strong><br><button class=copy-to-clipboard title="Delving into Dark Regions for Robust Shadow Detection" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13631v1.pdf filename=2402.13631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Shadow detection is a challenging task as it requires a comprehensive understanding of shadow characteristics and global/local illumination conditions. We observe from our experiment that state-of-the-art deep methods tend to have higher error rates in differentiating shadow pixels from non-shadow pixels in dark regions (ie, regions with low-intensity values). Our key insight to this problem is that existing methods typically learn discriminative shadow features from the whole image globally, covering the full range of intensity values, and may not learn the subtle differences between shadow and non-shadow pixels in dark regions. Hence, if we can design a model to focus on a narrower range of low-intensity regions, it may be able to learn better discriminative features for shadow detection. Inspired by this insight, we propose a novel shadow detection approach that first learns global contextual cues over the entire image and then zooms into the dark regions to learn local shadow representations. To this end, we formulate an effective dark-region <b>recommendation</b> (DRR) module to recommend regions of low-intensity values, and a novel dark-aware shadow analysis (DASA) module to learn dark-aware shadow features from the recommended dark regions. Extensive experiments show that the proposed method outperforms the state-of-the-art methods on three popular shadow detection datasets. Code is available at <a href=https://github.com/guanhuankang/ShadowDetection2021.git>https://github.com/guanhuankang/ShadowDetection2021.git</a>.</p></p class="citation"></blockquote><h3 id=3141--187306-flexible-physical-camouflage-generation-based-on-a-differential-approach-yang-li-et-al-2024>(31/41 | 187/306) Flexible Physical Camouflage Generation Based on a Differential Approach (Yang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Li, Wenyi Tan, Chenxing Zhao, Shuangju Zhou, Xinkai Liang, Quan Pan. (2024)<br><strong>Flexible Physical Camouflage Generation Based on a Differential Approach</strong><br><button class=copy-to-clipboard title="Flexible Physical Camouflage Generation Based on a Differential Approach" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13575v1.pdf filename=2402.13575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a <b>diffusion</b> <b>model.</b> This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferability. Additionally, the designed sticker-mode camouflage, coupled with a concealment constraint, adapts to the environment, yielding diverse styles of texture. Our findings highlight the versatility and efficacy of the FPA approach in adversarial camouflage applications.</p></p class="citation"></blockquote><h3 id=3241--188306-todo-token-downsampling-for-efficient-generation-of-high-resolution-images-ethan-smith-et-al-2024>(32/41 | 188/306) ToDo: Token Downsampling for Efficient Generation of High-Resolution Images (Ethan Smith et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ethan Smith, Nayan Saxena, Aninda Saha. (2024)<br><strong>ToDo: Token Downsampling for Efficient Generation of High-Resolution Images</strong><br><button class=copy-to-clipboard title="ToDo: Token Downsampling for Efficient Generation of High-Resolution Images" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13573v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13573v1.pdf filename=2402.13573v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Attention mechanism has been crucial for image <b>diffusion</b> <b>models,</b> however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable <b>Diffusion</b> <b>inference</b> by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.</p></p class="citation"></blockquote><h3 id=3341--189306-exploring-the-limits-of-semantic-image-compression-at-micro-bits-per-pixel-jordan-dotzel-et-al-2024>(33/41 | 189/306) Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel (Jordan Dotzel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jordan Dotzel, Bahaa Kotb, James Dotzel, Mohamed Abdelfattah, Zhiru Zhang. (2024)<br><strong>Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel</strong><br><button class=copy-to-clipboard title="Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13536v1.pdf filename=2402.13536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional methods, such as JPEG, perform image compression by operating on structural information, such as pixel values or frequency content. These methods are effective to bitrates around one bit per pixel (bpp) and higher at standard image sizes. In contrast, text-based semantic compression directly stores concepts and their relationships using natural language, which has evolved with humans to efficiently represent these salient concepts. These methods can operate at extremely low bitrates by disregarding structural information like location, size, and orientation. In this work, we use <b>GPT-4V</b> and DALL-E3 from OpenAI to explore the quality-compression frontier for image compression and identify the limitations of current technology. We push semantic compression as low as 100 $\mu$bpp (up to $10,000\times$ smaller than JPEG) by introducing an iterative reflection process to improve the decoded image. We further hypothesize this 100 $\mu$bpp level represents a soft limit on semantic compression at standard image resolutions.</p></p class="citation"></blockquote><h3 id=3441--190306-multi-scale-spatio-temporal-transformer-based-imbalanced-longitudinal-learning-for-glaucoma-forecasting-from-irregular-time-series-images-xikai-yang-et-al-2024>(34/41 | 190/306) Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning for Glaucoma Forecasting from Irregular Time Series Images (Xikai Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xikai Yang, Jian Wu, Xi Wang, Yuchen Yuan, Ning Li Wang, Pheng-Ann Heng. (2024)<br><strong>Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning for Glaucoma Forecasting from Irregular Time Series Images</strong><br><button class=copy-to-clipboard title="Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning for Glaucoma Forecasting from Irregular Time Series Images" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13475v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13475v1.pdf filename=2402.13475v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Glaucoma is one of the major eye diseases that leads to progressive optic nerve fiber damage and irreversible blindness, afflicting millions of individuals. Glaucoma forecast is a good solution to early screening and intervention of potential patients, which is helpful to prevent further deterioration of the disease. It leverages a series of historical fundus images of an eye and forecasts the likelihood of glaucoma occurrence in the future. However, the irregular sampling nature and the imbalanced class distribution are two challenges in the development of disease forecasting approaches. To this end, we introduce the Multi-scale Spatio-temporal <b>Transformer</b> Network (MST-former) based on the <b>transformer</b> architecture tailored for sequential image inputs, which can effectively learn representative semantic information from sequential images on both temporal and spatial dimensions. Specifically, we employ a multi-scale structure to extract features at various resolutions, which can largely exploit rich spatial information encoded in each image. Besides, we design a time distance matrix to scale time attention in a non-linear manner, which could effectively deal with the irregularly sampled data. Furthermore, we introduce a temperature-controlled Balanced Softmax Cross-entropy loss to address the class imbalance issue. Extensive experiments on the Sequential fundus Images for Glaucoma Forecast (SIGF) dataset demonstrate the superiority of the proposed MST-former method, achieving an AUC of 98.6% for glaucoma forecasting. Besides, our method shows excellent generalization capability on the Alzheimer&rsquo;s Disease Neuroimaging Initiative (ADNI) MRI dataset, with an accuracy of 90.3% for mild cognitive impairment and Alzheimer&rsquo;s disease prediction, outperforming the compared method by a large margin.</p></p class="citation"></blockquote><h3 id=3541--191306-seald-nerf-interactive-pixel-level-editing-for-dynamic-scenes-by-neural-radiance-fields-zhentao-huang-et-al-2024>(35/41 | 191/306) SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural Radiance Fields (Zhentao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhentao Huang, Yukun Shi, Neil Bruce, Minglun Gong. (2024)<br><strong>SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural Radiance Fields</strong><br><button class=copy-to-clipboard title="SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural Radiance Fields" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T45, cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13510v1.pdf filename=2402.13510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread adoption of implicit neural representations, especially Neural Radiance Fields (NeRF), highlights a growing need for editing capabilities in implicit 3D models, essential for tasks like scene post-processing and 3D content creation. Despite previous efforts in NeRF editing, challenges remain due to limitations in editing flexibility and quality. The key issue is developing a neural representation that supports local edits for real-time updates. Current NeRF editing methods, offering pixel-level adjustments or detailed <b>geometry</b> and color modifications, are mostly limited to static scenes. This paper introduces SealD-NeRF, an extension of Seal-3D for pixel-level editing in dynamic settings, specifically targeting the D-NeRF network. It allows for consistent edits across sequences by mapping editing actions to a specific timeframe, freezing the deformation network responsible for dynamic scene representation, and using a teacher-student approach to integrate changes.</p></p class="citation"></blockquote><h3 id=3641--192306-benchcloudvision-a-benchmark-analysis-of-deep-learning-approaches-for-cloud-detection-and-segmentation-in-remote-sensing-imagery-loddo-fabio-et-al-2024>(36/41 | 192/306) BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery (Loddo Fabio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Loddo Fabio, Dario Piga, Michelucci Umberto, El Ghazouali Safouane. (2024)<br><strong>BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery</strong><br><button class=copy-to-clipboard title="BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13918v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13918v2.pdf filename=2402.13918v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline. This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a <b>benchmark</b> analysis to evaluate their architectural approaches and identify the most performing ones. To increase the model&rsquo;s adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed. Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations. The model&rsquo;s flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets. This <b>benchmark</b> can be reproduced using the material from this github link: <a href=https://github.com/toelt-llc/cloud_segmentation_comparative>https://github.com/toelt-llc/cloud_segmentation_comparative</a>.</p></p class="citation"></blockquote><h3 id=3741--193306-scene-prior-filtering-for-depth-map-super-resolution-zhengxue-wang-et-al-2024>(37/41 | 193/306) Scene Prior Filtering for Depth Map Super-Resolution (Zhengxue Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengxue Wang, Zhiqiang Yan, Ming-Hsuan Yang, Jinshan Pan, Jian Yang, Ying Tai, Guangwei Gao. (2024)<br><strong>Scene Prior Filtering for Depth Map Super-Resolution</strong><br><button class=copy-to-clipboard title="Scene Prior Filtering for Depth Map Super-Resolution" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13876v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13876v2.pdf filename=2402.13876v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> fusion is vital to the success of super-resolution of depth maps. However, commonly used fusion strategies, such as addition and concatenation, fall short of effectively bridging the modal gap. As a result, guided image filtering methods have been introduced to mitigate this issue. Nevertheless, it is observed that their filter kernels usually encounter significant texture interference and edge inaccuracy. To tackle these two challenges, we introduce a Scene Prior Filtering network, SPFNet, which utilizes the priors surface normal and semantic map from large-scale models. Specifically, we design an All-in-one Prior Propagation that computes the similarity between <b>multi-modal</b> scene priors, i.e., RGB, normal, semantic, and depth, to reduce the texture interference. In addition, we present a One-to-one Prior Embedding that continuously embeds each single-modal prior into depth using Mutual Guided Filtering, further alleviating the texture interference while enhancing edges. Our SPFNet has been extensively evaluated on both real and synthetic datasets, achieving state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=3841--194306-identifying-unnecessary-3d-gaussians-using-clustering-for-fast-rendering-of-3d-gaussian-splatting-joongho-jo-et-al-2024>(38/41 | 194/306) Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting (Joongho Jo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joongho Jo, Hyeongwon Kim, Jongsun Park. (2024)<br><strong>Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting</strong><br><button class=copy-to-clipboard title="Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AR, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13827v1.pdf filename=2402.13827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline <b>clustering</b> of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.</p></p class="citation"></blockquote><h3 id=3941--195306-class-aware-mask-guided-feature-refinement-for-scene-text-recognition-mingkun-yang-et-al-2024>(39/41 | 195/306) Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition (Mingkun Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingkun Yang, Biao Yang, Minghui Liao, Yingying Zhu, Xiang Bai. (2024)<br><strong>Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition</strong><br><button class=copy-to-clipboard title="Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13643v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13643v1.pdf filename=2402.13643v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene text recognition is a rapidly developing field that faces numerous challenges due to the complexity and diversity of scene text, including complex backgrounds, diverse fonts, flexible arrangements, and accidental occlusions. In this paper, we propose a novel approach called Class-Aware Mask-guided feature refinement (CAM) to address these challenges. Our approach introduces canonical class-aware glyph masks generated from a standard font to effectively suppress background and text style noise, thereby enhancing feature discrimination. Additionally, we design a feature alignment and fusion module to incorporate the canonical mask guidance for further feature refinement for text recognition. By enhancing the alignment between the canonical mask feature and the text feature, the module ensures more effective fusion, ultimately leading to improved recognition performance. We first evaluate CAM on six standard text recognition <b>benchmarks</b> to demonstrate its effectiveness. Furthermore, CAM exhibits superiority over the state-of-the-art method by an average performance gain of 4.1% across six more challenging datasets, despite utilizing a smaller model size. Our study highlights the importance of incorporating canonical mask guidance and aligned feature refinement techniques for robust scene text recognition. The code is available at <a href=https://github.com/MelosY/CAM>https://github.com/MelosY/CAM</a>.</p></p class="citation"></blockquote><h3 id=4041--196306-learning-pixel-wise-continuous-depth-representation-via-clustering-for-depth-completion-chen-shenglun-et-al-2024>(40/41 | 196/306) Learning Pixel-wise Continuous Depth Representation via Clustering for Depth Completion (Chen Shenglun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Shenglun, Zhang Hong, Ma XinZhu, Wang Zhihui, Li Haojie. (2024)<br><strong>Learning Pixel-wise Continuous Depth Representation via Clustering for Depth Completion</strong><br><button class=copy-to-clipboard title="Learning Pixel-wise Continuous Depth Representation via Clustering for Depth Completion" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13579v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13579v1.pdf filename=2402.13579v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Depth completion is a long-standing challenge in computer vision, where classification-based methods have made tremendous progress in recent years. However, most existing classification-based methods rely on pre-defined pixel-shared and discrete depth values as depth categories. This representation fails to capture the continuous depth values that conform to the real depth distribution, leading to depth smearing in boundary regions. To address this issue, we revisit depth completion from the <b>clustering</b> perspective and propose a novel <b>clustering-based</b> framework called CluDe which focuses on learning the pixel-wise and continuous depth representation. The key idea of CluDe is to iteratively update the pixel-shared and discrete depth representation to its corresponding pixel-wise and continuous counterpart, driven by the real depth distribution. Specifically, CluDe first utilizes depth value <b>clustering</b> to learn a set of depth centers as the depth representation. While these depth centers are pixel-shared and discrete, they are more in line with the real depth distribution compared to pre-defined depth categories. Then, CluDe estimates offsets for these depth centers, enabling their dynamic adjustment along the depth axis of the depth distribution to generate the pixel-wise and continuous depth representation. Extensive experiments demonstrate that CluDe successfully reduces depth smearing around object boundaries by utilizing pixel-wise and continuous depth representation. Furthermore, CluDe achieves state-of-the-art performance on the VOID datasets and outperforms classification-based methods on the KITTI dataset.</p></p class="citation"></blockquote><h3 id=4141--197306-improving-video-corpus-moment-retrieval-with-partial-relevance-enhancement-danyang-hou-et-al-2024>(41/41 | 197/306) Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement (Danyang Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danyang Hou, Liang Pang, Huawei Shen, Xueqi Cheng. (2024)<br><strong>Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement</strong><br><button class=copy-to-clipboard title="Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IR, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13576v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13576v1.pdf filename=2402.13576v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video corpus moment retrieval~(VCMR) is a new video retrieval task aimed at retrieving a relevant moment from a large corpus of untrimmed videos using a natural language text as query. The relevance between the video and query is partial, mainly evident in two aspects: (1) Scope: The untrimmed video contains information-rich frames, and not all are relevant to the query. Strong correlation is typically observed only within the relevant moment, emphasizing the importance of capturing key content. (2) Modality: The relevance of query to different modalities varies; action descriptions align more with the visual elements, while character conversations are more related to textual information. Recognizing and addressing these modality-specific nuances is crucial for effective retrieval in VCMR. However, existing methods often treat all video contents equally, leading to sub-optimal moment retrieval. We argue that effectively capturing the partial relevance between the query and video is essential for the VCMR task. To this end, we propose a Partial Relevance Enhanced Model~(PREM) to improve VCMR. VCMR involves two sub-tasks: video retrieval and moment localization. To align with their distinct objectives, we implement specialized partial relevance enhancement strategies. For video retrieval, we introduce a <b>multi-modal</b> collaborative video retriever, generating distinct query representations tailored for different modalities by modality-specific pooling, ensuring a more effective match. For moment localization, we propose the focus-then-fuse moment localizer, utilizing modality-specific gates to capture essential content, followed by fusing <b>multi-modal</b> information for moment localization. Experimental results on TVR and DiDeMo datasets show that the proposed model outperforms the baselines, achieving a new state-of-the-art of VCMR.</p></p class="citation"></blockquote><h2 id=cscr-7>cs.CR (7)</h2><h3 id=17--198306-privacy-preserving-instructions-for-aligning-large-language-models-da-yu-et-al-2024>(1/7 | 198/306) Privacy-Preserving Instructions for Aligning Large Language Models (Da Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu. (2024)<br><strong>Privacy-Preserving Instructions for Aligning Large Language Models</strong><br><button class=copy-to-clipboard title="Privacy-Preserving Instructions for Aligning Large Language Models" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs.CR<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Supervised Learning, Large Language Model, Large Language Model, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13659v1.pdf filename=2402.13659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Service providers of <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> applications collect user instructions in the wild and use them in further aligning <b>LLMs</b> with users&rsquo; intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model <b>fine-tuning.</b> Formal <b>differential</b> <b>privacy</b> is guaranteed by generating those synthetic instructions using privately <b>fine-tuned</b> generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both <b>supervised</b> <b>fine-tuning</b> and <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback,</b> our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In <b>supervised</b> <b>fine-tuning,</b> models trained with private synthetic instructions outperform leading open-source models such as Vicuna.</p></p class="citation"></blockquote><h3 id=27--199306-llm-jailbreak-attack-versus-defense-techniques----a-comprehensive-study-zihao-xu-et-al-2024>(2/7 | 199/306) LLM Jailbreak Attack versus Defense Techniques &ndash; A Comprehensive Study (Zihao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, Stjepan Picek. (2024)<br><strong>LLM Jailbreak Attack versus Defense Techniques &ndash; A Comprehensive Study</strong><br><button class=copy-to-clipboard title="LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 70<br>Keywords: GPT, GPT-3, GPT-3.5, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13457v1.pdf filename=2402.13457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMS)</b> have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of &ldquo;jailbreaking&rdquo;, where carefully crafted <b>prompts</b> elicit harmful responses from models, persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking <b>LLMs</b> and their defense techniques. We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, <b>LLama,</b> and <b>GPT-3.5</b> Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks underperform compared to universal techniques and that including special tokens in the input significantly affects the likelihood of successful attacks. This research highlights the need to concentrate on the security facets of <b>LLMs.</b> Additionally, we contribute to the field by releasing our datasets and testing framework, aiming to foster further research into <b>LLM</b> security. We believe these contributions will facilitate the exploration of security measures within this domain.</p></p class="citation"></blockquote><h3 id=37--200306-generative-ai-for-secure-physical-layer-communications-a-survey-changyuan-zhao-et-al-2024>(3/7 | 200/306) Generative AI for Secure Physical Layer Communications: A Survey (Changyuan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changyuan Zhao, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim, Xuemin, Shen, Khaled B. Letaief. (2024)<br><strong>Generative AI for Secure Physical Layer Communications: A Survey</strong><br><button class=copy-to-clipboard title="Generative AI for Secure Physical Layer Communications: A Survey" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 60<br>Keywords: Diffusion Model, Autoencoder, Generative AI, Generative Adversarial Network, Generative Adversarial Network, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13553v1.pdf filename=2402.13553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Artificial</b> <b>Intelligence</b> (GAI) stands at the forefront of AI innovation, demonstrating rapid advancement and unparalleled proficiency in generating diverse content. Beyond content creation, GAI has significant analytical abilities to learn complex data distribution, offering numerous opportunities to resolve security issues. In the realm of security from physical layer perspectives, traditional AI approaches frequently struggle, primarily due to their limited capacity to dynamically adjust to the evolving physical attributes of transmission channels and the complexity of contemporary cyber threats. This adaptability and analytical depth are precisely where GAI excels. Therefore, in this paper, we offer an extensive survey on the various applications of GAI in enhancing security within the physical layer of communication networks. We first emphasize the importance of advanced GAI models in this area, including <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs),</b> <b>Autoencoders</b> (AEs), <b>Variational</b> <b>Autoencoders</b> (VAEs), and <b>Diffusion</b> <b>Models</b> (DMs). We delve into the roles of GAI in addressing challenges of physical layer security, focusing on communication confidentiality, authentication, availability, resilience, and integrity. Furthermore, we also present future research directions focusing model improvements, multi-scenario deployment, resource-efficient optimization, and secure semantic communication, highlighting the multifaceted potential of GAI to address emerging challenges in secure physical layer communications and sensing.</p></p class="citation"></blockquote><h3 id=47--201306-finding-incompatibles-blocks-for-reliable-jpeg-steganalysis-etienne-levecque-et-al-2024>(4/7 | 201/306) Finding Incompatibles Blocks for Reliable JPEG Steganalysis (Etienne Levecque et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Etienne Levecque, Jan Butora, Patrick Bas. (2024)<br><strong>Finding Incompatibles Blocks for Reliable JPEG Steganalysis</strong><br><button class=copy-to-clipboard title="Finding Incompatibles Blocks for Reliable JPEG Steganalysis" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13660v1.pdf filename=2402.13660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article presents a refined notion of incompatible JPEG images for a quality factor of 100. It can be used to detect the presence of steganographic schemes embedding in DCT coefficients. We show that, within the JPEG pipeline, the combination of the DCT transform with the <b>quantization</b> function can map several distinct blocks in the pixel domain to the same block in the DCT domain. However, not every DCT block can be obtained: we call those blocks incompatible. In particular, incompatibility can happen when DCT coefficients are manually modified to embed a message. We show that the problem of distinguishing compatible blocks from incompatible ones is an inverse problem with or without solution and we propose two different methods to solve it. The first one is heuristic-based, fast to find a solution if it exists. The second is formulated as an Integer Linear Programming problem and can detect incompatible blocks only for a specific DCT transform in a reasonable amount of time. We show that the probability for a block to become incompatible only relies on the number of modifications. Finally, using the heuristic algorithm we can derive a Likelihood Ratio Test depending on the number of compatible blocks per image to perform steganalysis. We simulate the result of this test and show that it outperforms a deep learning detector e-SRNet for every payload between 0.001 and 0.01 bpp by using only 10% of the blocks from 256x256 images. A Selection-Channel-Aware version of the test is even more powerful and outperforms e-SRNet while using only 1% of the blocks.</p></p class="citation"></blockquote><h3 id=57--202306-sissa-real-time-monitoring-of-hardware-functional-safety-and-cybersecurity-with-in-vehicle-someip-ethernet-traffic-qi-liu-et-al-2024>(5/7 | 202/306) SISSA: Real-time Monitoring of Hardware Functional Safety and Cybersecurity with In-vehicle SOME/IP Ethernet Traffic (Qi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Liu, Xingyu Li, Ke Sun, Yufeng Li, Yanchen Liu. (2024)<br><strong>SISSA: Real-time Monitoring of Hardware Functional Safety and Cybersecurity with In-vehicle SOME/IP Ethernet Traffic</strong><br><button class=copy-to-clipboard title="SISSA: Real-time Monitoring of Hardware Functional Safety and Cybersecurity with In-vehicle SOME/IP Ethernet Traffic" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs-NI, cs.CR<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14862v1.pdf filename=2402.14862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scalable service-Oriented Middleware over IP (SOME/IP) is an Ethernet communication standard protocol in the Automotive Open System Architecture (AUTOSAR), promoting ECU-to-ECU communication over the IP stack. However, SOME/IP lacks a robust security architecture, making it susceptible to potential attacks. Besides, random hardware failure of ECU will disrupt SOME/IP communication. In this paper, we propose SISSA, a SOME/IP communication traffic-based approach for modeling and analyzing in-vehicle functional safety and cyber security. Specifically, SISSA models hardware failures with the Weibull distribution and addresses five potential attacks on SOME/IP communication, including Distributed Denial-of-Services, Man-in-the-Middle, and abnormal communication processes, assuming a malicious user accesses the in-vehicle network. Subsequently, SISSA designs a series of deep learning models with various backbones to extract features from SOME/IP sessions among ECUs. We adopt residual <b>self-attention</b> to accelerate the model&rsquo;s convergence and enhance detection accuracy, determining whether an ECU is under attack, facing functional failure, or operating normally. Additionally, we have created and annotated a dataset encompassing various classes, including indicators of attack, functionality, and normalcy. This contribution is noteworthy due to the scarcity of publicly accessible datasets with such characteristics.Extensive experimental results show the effectiveness and efficiency of SISSA.</p></p class="citation"></blockquote><h3 id=67--203306-a-unified-knowledge-graph-to-permit-interoperability-of-heterogeneous-digital-evidence-ali-alshumrani-et-al-2024>(6/7 | 203/306) A Unified Knowledge Graph to Permit Interoperability of Heterogeneous Digital Evidence (Ali Alshumrani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Alshumrani, Nathan Clarke, Bogdan Ghita. (2024)<br><strong>A Unified Knowledge Graph to Permit Interoperability of Heterogeneous Digital Evidence</strong><br><button class=copy-to-clipboard title="A Unified Knowledge Graph to Permit Interoperability of Heterogeneous Digital Evidence" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13746v1.pdf filename=2402.13746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The modern digital world is highly heterogeneous, encompassing a wide variety of communications, devices, and services. This interconnectedness generates, synchronises, stores, and presents digital information in multidimensional, complex formats, often fragmented across multiple sources. When linked to misuse, this digital information becomes vital digital evidence. Integrating and harmonising these diverse formats into a unified system is crucial for comprehensively understanding evidence and its relationships. However, existing approaches to date have faced challenges limiting investigators&rsquo; ability to query heterogeneous evidence across large datasets. This paper presents a novel approach in the form of a modern unified data <b>graph.</b> The proposed approach aims to seamlessly integrate, harmonise, and unify evidence data, enabling cross-platform interoperability, efficient data queries, and improved digital investigation performance. To demonstrate its efficacy, a case study is conducted, highlighting the benefits of the proposed approach and showcasing its effectiveness in enabling the interoperability required for advanced analytics in digital investigations.</p></p class="citation"></blockquote><h3 id=77--204306-towards-efficient-verification-of-constant-time-cryptographic-implementations-luwei-cai-et-al-2024>(7/7 | 204/306) Towards Efficient Verification of Constant-Time Cryptographic Implementations (Luwei Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luwei Cai, Fu Song, Taolue Chen. (2024)<br><strong>Towards Efficient Verification of Constant-Time Cryptographic Implementations</strong><br><button class=copy-to-clipboard title="Towards Efficient Verification of Constant-Time Cryptographic Implementations" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SE, cs.CR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13506v1.pdf filename=2402.13506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Timing side-channel attacks exploit secret-dependent execution time to fully or partially recover secrets of cryptographic implementations, posing a severe threat to software security. Constant-time programming discipline is an effective software-based countermeasure against timing side-channel attacks, but developing constant-time implementations turns out to be challenging and error-prone. Current verification approaches/tools suffer from scalability and precision issues when applied to production software in practice. In this paper, we put forward practical verification approaches based on a novel synergy of taint analysis and safety verification of self-composed programs. Specifically, we first use an IFDS-based lightweight taint analysis to prove that a large number of potential (timing) side-channel sources do not actually leak secrets. We then resort to a precise taint analysis and a safety verification approach to determine whether the remaining potential side-channel sources can actually leak secrets. These include novel constructions of taint-directed semi-cross-product of the original program and its Boolean abstraction, and a taint-directed self-composition of the program. Our approach is implemented as a cross-platform and fully automated tool CT-Prover. The experiments confirm its efficiency and effectiveness in verifying real-world <b>benchmarks</b> from modern cryptographic and SSL/TLS libraries. In particular, CT-Prover identify new, confirmed vulnerabilities of open-source SSL libraries (e.g., Mbed SSL, BearSSL) and significantly outperforms the state-of-the-art tools.</p></p class="citation"></blockquote><h2 id=csro-13>cs.RO (13)</h2><h3 id=113--205306-cofrida-self-supervised-fine-tuning-for-human-robot-co-painting-peter-schaldenbrand-et-al-2024>(1/13 | 205/306) CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting (Peter Schaldenbrand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Schaldenbrand, Gaurav Parmar, Jun-Yan Zhu, James McCann, Jean Oh. (2024)<br><strong>CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting</strong><br><button class=copy-to-clipboard title="CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Fine-tuning, Foundation Model, Self-supervised Learning, Text2image, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13442v1.pdf filename=2402.13442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior robot painting and drawing work, such as FRIDA, has focused on decreasing the sim-to-real gap and expanding input modalities for users, but the interaction with these systems generally exists only in the input stages. To support interactive, human-robot collaborative painting, we introduce the Collaborative FRIDA (CoFRIDA) robot painting framework, which can co-paint by modifying and engaging with content already painted by a human collaborator. To improve <b>text-image</b> alignment, FRIDA&rsquo;s major weakness, our system uses pre-trained <b>text-to-image</b> models; however, pre-trained models in the context of real-world co-painting do not perform well because they (1) do not understand the constraints and abilities of the robot and (2) cannot perform co-painting without making unrealistic edits to the canvas and overwriting content. We propose a <b>self-supervised</b> <b>fine-tuning</b> procedure that can tackle both issues, allowing the use of pre-trained state-of-the-art <b>text-image</b> alignment models with robots to enable co-painting in the physical world. Our open-source approach, CoFRIDA, creates paintings and drawings that match the input text <b>prompt</b> more clearly than FRIDA, both from a blank canvas and one with human created work. More generally, our <b>fine-tuning</b> procedure successfully encodes the robot&rsquo;s constraints and abilities into a <b>foundation</b> <b>model,</b> showcasing promising results as an effective method for reducing sim-to-real gaps.</p></p class="citation"></blockquote><h3 id=213--206306-leveraging-demonstrator-perceived-precision-for-safe-interactive-imitation-learning-of-clearance-limited-tasks-hanbit-oh-et-al-2024>(2/13 | 206/306) Leveraging Demonstrator-perceived Precision for Safe Interactive Imitation Learning of Clearance-limited Tasks (Hanbit Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanbit Oh, Takamitsu Matsubara. (2024)<br><strong>Leveraging Demonstrator-perceived Precision for Safe Interactive Imitation Learning of Clearance-limited Tasks</strong><br><button class=copy-to-clipboard title="Leveraging Demonstrator-perceived Precision for Safe Interactive Imitation Learning of Clearance-limited Tasks" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Human Intervention, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13466v1.pdf filename=2402.13466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interactive imitation learning is an efficient, model-free method through which a robot can learn a task by repetitively iterating an execution of a learning policy and a data collection by querying <b>human</b> <b>demonstrations.</b> However, deploying unmatured policies for clearance-limited tasks, like industrial insertion, poses significant collision risks. For such tasks, a robot should detect the collision risks and request intervention by ceding control to a <b>human</b> <b>when</b> collisions are imminent. The former requires an accurate model of the environment, a need that significantly limits the scope of IIL applications. In contrast, <b>humans</b> <b>implicitly</b> demonstrate environmental precision by adjusting their behavior to avoid collisions when performing tasks. Inspired by <b>human</b> <b>behavior,</b> this paper presents a novel interactive learning method that uses demonstrator-perceived precision as a criterion for <b>human</b> <b>intervention</b> called Demonstrator-perceived Precision-aware Interactive Imitation Learning (DPIIL). DPIIL captures precision by observing the speed-accuracy trade-off exhibited in <b>human</b> <b>demonstrations</b> and cedes control to a <b>human</b> <b>to</b> avoid collisions in states where high precision is estimated. DPIIL improves the safety of interactive policy learning and ensures efficiency without explicitly providing precise information of the environment. We assessed DPIIL&rsquo;s effectiveness through <b>simulations</b> and real-robot experiments that trained a UR5e 6-DOF robotic arm to perform assembly tasks. Our results significantly improved training safety, and our best performance compared favorably with other learning methods.</p></p class="citation"></blockquote><h3 id=313--207306-autonomous-mapless-navigation-on-uneven-terrains-hassan-jardali-et-al-2024>(3/13 | 207/306) Autonomous Mapless Navigation on Uneven Terrains (Hassan Jardali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hassan Jardali, Mahmoud Ali, Lantao Liu. (2024)<br><strong>Autonomous Mapless Navigation on Uneven Terrains</strong><br><button class=copy-to-clipboard title="Autonomous Mapless Navigation on Uneven Terrains" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13443v1.pdf filename=2402.13443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a new method for autonomous navigation in uneven terrains by utilizing a sparse <b>Gaussian</b> <b>Process</b> (SGP) based local perception model. The SGP local perception model is trained on local ranging observation (pointcloud) to learn the terrain elevation profile and extract the feasible navigation subgoals around the robot. Subsequently, a cost function, which prioritizes the safety of the robot in terms of keeping the robot&rsquo;s roll and pitch angles bounded within a specified range, is used to select a safety-aware subgoal that leads the robot to its final destination. The algorithm is designed to run in real-time and is intensively evaluated in <b>simulation</b> and real world experiments. The results compellingly demonstrate that our proposed algorithm consistently navigates uneven terrains with high efficiency and surpasses the performance of other planners. The code and video can be found here: <a href=https://rb.gy/3ov2r8>https://rb.gy/3ov2r8</a></p></p class="citation"></blockquote><h3 id=413--208306-realdex-towards-human-like-grasping-for-robotic-dexterous-hand-yumeng-liu-et-al-2024>(4/13 | 208/306) RealDex: Towards Human-like Grasping for Robotic Dexterous Hand (Yumeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yumeng Liu, Yaxun Yang, Youzhuo Wang, Xiaofei Wu, Jiamin Wang, Yichen Yao, Sören Schwertfeger, Sibei Yang, Wenping Wang, Jingyi Yu, Xuming He, Yuexin Ma. (2024)<br><strong>RealDex: Towards Human-like Grasping for Robotic Dexterous Hand</strong><br><button class=copy-to-clipboard title="RealDex: Towards Human-like Grasping for Robotic Dexterous Hand" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13853v1.pdf filename=2402.13853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and <b>multimodal</b> visual data. Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time. This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely. RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios. Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models.</b> Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets. The complete dataset and code will be made available upon the publication of this work.</p></p class="citation"></blockquote><h3 id=513--209306-khronos-a-unified-approach-for-spatio-temporal-metric-semantic-slam-in-dynamic-environments-lukas-schmid-et-al-2024>(5/13 | 209/306) Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments (Lukas Schmid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Schmid, Marcus Abate, Yun Chang, Luca Carlone. (2024)<br><strong>Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments</strong><br><button class=copy-to-clipboard title="Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13817v1.pdf filename=2402.13817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Perceiving and understanding highly dynamic and changing environments is a crucial capability for robot autonomy. While large strides have been made towards developing dynamic SLAM approaches that estimate the robot pose accurately, a lesser emphasis has been put on the construction of dense spatio-temporal representations of the robot environment. A detailed understanding of the scene and its evolution through time is crucial for long-term robot autonomy and essential to tasks that require long-term <b>reasoning,</b> such as operating effectively in environments shared with humans and other agents and thus are subject to short and long-term dynamics. To address this challenge, this work defines the Spatio-temporal Metric-semantic SLAM (SMS) problem, and presents a framework to factorize and solve it efficiently. We show that the proposed factorization suggests a natural organization of a spatio-temporal perception system, where a fast process tracks short-term dynamics in an active temporal window, while a slower process reasons over long-term changes in the environment using a factor <b>graph</b> formulation. We provide an efficient implementation of the proposed spatio-temporal perception approach, that we call Khronos, and show that it unifies exiting interpretations of short-term and long-term dynamics and is able to construct a dense spatio-temporal map in real-time. We provide simulated and real results, showing that the spatio-temporal maps built by Khronos are an accurate reflection of a 3D scene over time and that Khronos outperforms baselines across multiple metrics. We further validate our approach on two heterogeneous robots in challenging, large-scale real-world environments.</p></p class="citation"></blockquote><h3 id=613--210306-generating-realistic-arm-movements-in-reinforcement-learning-a-quantitative-comparison-of-reward-terms-and-task-requirements-jhon-charaja-et-al-2024>(6/13 | 210/306) Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements (Jhon Charaja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jhon Charaja, Isabell Wochner, Pierre Schumacher, Winfried Ilg, Martin Giese, Christophe Maufroy, Andreas Bulling, Syn Schmitt, Daniel F. B. Haeufle. (2024)<br><strong>Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements</strong><br><button class=copy-to-clipboard title="Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13949v1.pdf filename=2402.13949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The mimicking of human-like arm movement characteristics involves the consideration of three factors during control policy synthesis: (a) chosen task requirements, (b) inclusion of noise during movement execution and (c) chosen optimality principles. Previous studies showed that when considering these factors (a-c) individually, it is possible to synthesize arm movements that either kinematically match the experimental data or reproduce the stereotypical triphasic muscle activation pattern. However, to date no quantitative comparison has been made on how realistic the arm movement generated by each factor is; as well as whether a partial or total combination of all factors results in arm movements with human-like kinematic characteristics and a triphasic muscle pattern. To investigate this, we used <b>reinforcement</b> <b>learning</b> to learn a control policy for a musculoskeletal arm model, aiming to discern which combination of factors (a-c) results in realistic arm movements according to four frequently reported stereotypical characteristics. Our findings indicate that incorporating velocity and acceleration requirements into the reaching task, employing reward terms that encourage minimization of mechanical work, hand jerk, and control effort, along with the inclusion of noise during movement, leads to the emergence of realistic human arm movements in <b>reinforcement</b> <b>learning.</b> We expect that the gained insights will help in the future to better predict desired arm movements and corrective forces in wearable assistive devices.</p></p class="citation"></blockquote><h3 id=713--211306-learning-dual-arm-object-rearrangement-for-cartesian-robots-shishun-zhang-et-al-2024>(7/13 | 211/306) Learning Dual-arm Object Rearrangement for Cartesian Robots (Shishun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shishun Zhang, Qijin She, Wenhao Li, Chenyang Zhu, Yongjun Wang, Ruizhen Hu, Kai Xu. (2024)<br><strong>Learning Dual-arm Object Rearrangement for Cartesian Robots</strong><br><button class=copy-to-clipboard title="Learning Dual-arm Object Rearrangement for Cartesian Robots" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13634v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13634v1.pdf filename=2402.13634v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work focuses on the dual-arm object rearrangement problem abstracted from a realistic industrial scenario of Cartesian robots. The goal of this problem is to transfer all the objects from sources to targets with the minimum total completion time. To achieve the goal, the core idea is to develop an effective object-to-arm task assignment strategy for minimizing the cumulative task execution time and maximizing the dual-arm cooperation efficiency. One of the difficulties in the task assignment is the scalability problem. As the number of objects increases, the computation time of traditional offline-search-based methods grows strongly for computational complexity. Encouraged by the adaptability of <b>reinforcement</b> <b>learning</b> (RL) in long-sequence task decisions, we propose an online task assignment decision method based on RL, and the computation time of our method only increases linearly with the number of objects. Further, we design an attention-based network to model the dependencies between the input states during the whole task execution process to help find the most reasonable object-to-arm correspondence in each task assignment round. In the experimental part, we adapt some search-based methods to this specific setting and compare our method with them. Experimental result shows that our approach achieves outperformance over search-based methods in total execution time and computational efficiency, and also verifies the generalization of our method to different numbers of objects. In addition, we show the effectiveness of our method deployed on the real robot in the supplementary video.</p></p class="citation"></blockquote><h3 id=813--212306-learning-to-model-diverse-driving-behaviors-in-highly-interactive-autonomous-driving-scenarios-with-multi-agent-reinforcement-learning-liu-weiwei-et-al-2024>(8/13 | 212/306) Learning to Model Diverse Driving Behaviors in Highly Interactive Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning (Liu Weiwei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liu Weiwei, Hu Wenxuan, Jing Wei, Lei Lanxin, Gao Lingping, Liu Yong. (2024)<br><strong>Learning to Model Diverse Driving Behaviors in Highly Interactive Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Learning to Model Diverse Driving Behaviors in Highly Interactive Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13481v1.pdf filename=2402.13481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous vehicles trained through Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL) have shown impressive results in many driving scenarios. However, the performance of these trained policies can be impacted when faced with diverse driving styles and personalities, particularly in highly interactive situations. This is because conventional MARL algorithms usually operate under the assumption of fully cooperative behavior among all agents and focus on maximizing team rewards during training. To address this issue, we introduce the Personality Modeling Network (PeMN), which includes a cooperation value function and personality parameters to model the varied interactions in high-interactive scenarios. The PeMN also enables the training of a background traffic flow with diverse behaviors, thereby improving the performance and generalization of the ego vehicle. Our extensive experimental studies, which incorporate different personality parameters in high-interactive driving scenarios, demonstrate that the personality parameters effectively model diverse driving styles and that policies trained with PeMN demonstrate better generalization compared to traditional MARL methods.</p></p class="citation"></blockquote><h3 id=913--213306-blending-data-driven-priors-in-dynamic-games-justin-lidard-et-al-2024>(9/13 | 213/306) Blending Data-Driven Priors in Dynamic Games (Justin Lidard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Justin Lidard, Haimin Hu, Asher Hancock, Zixu Zhang, Albert Gimó Contreras, Vikash Modi, Jonathan DeCastro, Deepak Gopinath, Guy Rosman, Naomi Leonard, María Santos, Jaime Fernández Fisac. (2024)<br><strong>Blending Data-Driven Priors in Dynamic Games</strong><br><button class=copy-to-clipboard title="Blending Data-Driven Priors in Dynamic Games" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs-SY, cs.RO, eess-SY, math-OC<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14174v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14174v2.pdf filename=2402.14174v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As intelligent robots like autonomous vehicles become increasingly deployed in the presence of people, the extent to which these systems should leverage model-based game-theoretic planners versus data-driven policies for safe, interaction-aware motion planning remains an open question. Existing dynamic game formulations assume all agents are task-driven and behave optimally. However, in reality, humans tend to deviate from the decisions prescribed by these models, and their behavior is better approximated under a noisy-rational paradigm. In this work, we investigate a principled methodology to blend a data-driven reference policy with an optimization-based game-theoretic policy. We formulate KLGame, a type of non-cooperative dynamic game with Kullback-Leibler (KL) regularization with respect to a general, stochastic, and possibly <b>multi-modal</b> reference policy. Our method incorporates, for each decision maker, a tunable parameter that permits modulation between task-driven and data-driven behaviors. We propose an efficient algorithm for computing <b>multimodal</b> approximate feedback Nash equilibrium strategies of KLGame in real time. Through a series of simulated and real-world autonomous driving scenarios, we demonstrate that KLGame policies can more effectively incorporate guidance from the reference policy and account for noisily-rational human behaviors versus non-regularized baselines.</p></p class="citation"></blockquote><h3 id=1013--214306-gdtm-an-indoor-geospatial-tracking-dataset-with-distributed-multimodal-sensors-ho-lyun-jeong-et-al-2024>(10/13 | 214/306) GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors (Ho Lyun Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ho Lyun Jeong, Ziqi Wang, Colin Samplawski, Jason Wu, Shiwei Fang, Lance M. Kaplan, Deepak Ganesan, Benjamin Marlin, Mani Srivastava. (2024)<br><strong>GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors</strong><br><button class=copy-to-clipboard title="GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO, eess-SP<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14136v1.pdf filename=2402.14136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Constantly locating moving objects, i.e., geospatial tracking, is essential for autonomous building infrastructure. Accurate and robust geospatial tracking often leverages <b>multimodal</b> sensor fusion algorithms, which require large datasets with time-aligned, synchronized data from various sensor types. However, such datasets are not readily available. Hence, we propose GDTM, a nine-hour dataset for <b>multimodal</b> object tracking with distributed <b>multimodal</b> sensors and reconfigurable sensor node placements. Our dataset enables the exploration of several research problems, such as optimizing architectures for processing <b>multimodal</b> data, and investigating models&rsquo; robustness to adverse sensing conditions and sensor placement variances. A GitHub repository containing the code, sample data, and checkpoints of this work is available at <a href=https://github.com/nesl/GDTM>https://github.com/nesl/GDTM</a>.</p></p class="citation"></blockquote><h3 id=1113--215306-a-combined-learning-and-optimization-framework-to-transfer-human-whole-body-loco-manipulation-skills-to-mobile-manipulators-jianzhuang-zhao-et-al-2024>(11/13 | 215/306) A Combined Learning and Optimization Framework to Transfer Human Whole-body Loco-manipulation Skills to Mobile Manipulators (Jianzhuang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianzhuang Zhao, Francesco Tassi, Yanlong Huang, Elena De Momi, Arash Ajoudani. (2024)<br><strong>A Combined Learning and Optimization Framework to Transfer Human Whole-body Loco-manipulation Skills to Mobile Manipulators</strong><br><button class=copy-to-clipboard title="A Combined Learning and Optimization Framework to Transfer Human Whole-body Loco-manipulation Skills to Mobile Manipulators" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13915v1.pdf filename=2402.13915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans&rsquo; ability to smoothly switch between locomotion and manipulation is a remarkable feature of sensorimotor coordination. Leaning and replication of such human-like strategies can lead to the development of more sophisticated robots capable of performing complex whole-body tasks in real-world environments. To this end, this paper proposes a combined learning and optimization framework for transferring human&rsquo;s loco-manipulation soft-switching skills to mobile manipulators. The methodology departs from data collection of human demonstrations for a locomotion-integrated manipulation task through a vision system. Next, the wrist and pelvis motions are mapped to mobile manipulators&rsquo; End-Effector (EE) and mobile base. A kernelized movement primitive algorithm learns the wrist and pelvis trajectories and generalizes to new desired points according to task requirements. Next, the reference trajectories are sent to a hierarchical quadratic programming controller, where the EE and the mobile base reference trajectories are provided as the first and second priority tasks, generating the feasible and optimal joint level commands. A locomotion-integrated pick-and-place task is executed to validate the proposed approach. After a human demonstrates the task, a mobile manipulator executes the task with the same and new settings, grasping a bottle at non-zero velocity. The results showed that the proposed approach successfully transfers the human loco-manipulation skills to mobile manipulators, even with different <b>geometry.</b></p></p class="citation"></blockquote><h3 id=1213--216306-learning-control-strategy-in-soft-robotics-through-a-set-of-configuration-spaces-etienne-ménager-et-al-2024>(12/13 | 216/306) Learning control strategy in soft robotics through a set of configuration spaces (Etienne Ménager et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Etienne Ménager, Christian Duriez. (2024)<br><strong>Learning control strategy in soft robotics through a set of configuration spaces</strong><br><button class=copy-to-clipboard title="Learning control strategy in soft robotics through a set of configuration spaces" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13649v1.pdf filename=2402.13649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability of a soft robot to perform specific tasks is determined by its contact configuration, and transitioning between configurations is often necessary to reach a desired position or manipulate an object. Based on this observation, we propose a method for controlling soft robots that involves defining a <b>graph</b> of configuration spaces. Different agents, whether learned or not (convex optimization, expert trajectory, and collision detection), use the structure of the <b>graph</b> to solve the desired task. The <b>graph</b> and the agents are part of the prior knowledge that is intuitively integrated into the learning process. They are used to combine different optimization methods, improve sample efficiency, and provide interpretability. We construct the <b>graph</b> based on the contact configurations and demonstrate its effectiveness through two scenarios, a deformable beam in contact with its environment and a soft manipulator, where it outperforms the baseline in terms of stability, learning speed, and interpretability.</p></p class="citation"></blockquote><h3 id=1313--217306-voom-robust-visual-object-odometry-and-mapping-using-hierarchical-landmarks-yutong-wang-et-al-2024>(13/13 | 217/306) VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks (Yutong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutong Wang, Chaoyang Jiang, Xieyuanli Chen. (2024)<br><strong>VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks</strong><br><button class=copy-to-clipboard title="VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13609v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13609v2.pdf filename=2402.13609v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, object-oriented simultaneous localization and mapping (SLAM) has attracted increasing attention due to its ability to provide high-level semantic information while maintaining computational efficiency. Some researchers have attempted to enhance localization accuracy by integrating the modeled object residuals into bundle adjustment. However, few have demonstrated better results than feature-based visual SLAM systems, as the generic coarse object models, such as cuboids or ellipsoids, are less accurate than feature points. In this paper, we propose a Visual Object Odometry and Mapping framework VOOM using high-level objects and low-level points as the hierarchical landmarks in a coarse-to-fine manner instead of directly using object residuals in bundle adjustment. Firstly, we introduce an improved observation model and a novel data association method for dual quadrics, employed to represent physical objects. It facilitates the creation of a 3D map that closely reflects reality. Next, we use object information to enhance the data association of feature points and consequently update the map. In the visual object odometry backend, the updated map is employed to further optimize the camera pose and the objects. Meanwhile, local bundle adjustment is performed utilizing the objects and points-based covisibility <b>graphs</b> in our visual object mapping process. Experiments show that VOOM outperforms both object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms of localization. The implementation of our method is available at <a href=https://github.com/yutongwangBIT/VOOM.git>https://github.com/yutongwangBIT/VOOM.git</a>.</p></p class="citation"></blockquote><h2 id=cssd-3>cs.SD (3)</h2><h3 id=13--218306-the-effect-of-batch-size-on-contrastive-self-supervised-speech-representation-learning-nik-vaessen-et-al-2024>(1/3 | 218/306) The Effect of Batch Size on Contrastive Self-Supervised Speech Representation Learning (Nik Vaessen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nik Vaessen, David A. van Leeuwen. (2024)<br><strong>The Effect of Batch Size on Contrastive Self-Supervised Speech Representation Learning</strong><br><button class=copy-to-clipboard title="The Effect of Batch Size on Contrastive Self-Supervised Speech Representation Learning" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 51<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Foundation Model, Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13723v1.pdf filename=2402.13723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> in speech are often trained using many GPUs, which implicitly leads to large effective batch sizes. In this paper we study the effect of batch size on pre-training, both in terms of statistics that can be monitored during training, and in the effect on the performance of a downstream <b>fine-tuning</b> task. By using batch sizes varying from 87.5 seconds to 80 minutes of speech we show that, for a fixed amount of iterations, larger batch sizes result in better pre-trained models. However, there is lower limit for stability, and an upper limit for effectiveness. We then show that the quality of the pre-trained model depends mainly on the amount of speech data seen during training, i.e., on the product of batch size and number of iterations. All results are produced with an independent implementation of the wav2vec 2.0 architecture, which to a large extent reproduces the results of the original work (arXiv:2006.11477). Our extensions can help researchers choose effective operating conditions when studying <b>self-supervised</b> <b>learning</b> in speech, and hints towards <b>benchmarking</b> self-supervision with a fixed amount of seen data. Code and model checkpoints are available at <a href=https://github.com/nikvaessen/w2v2-batch-size>https://github.com/nikvaessen/w2v2-batch-size</a>.</p></p class="citation"></blockquote><h3 id=23--219306-advancing-audio-fingerprinting-accuracy-addressing-background-noise-and-distortion-challenges-navin-kamuni-et-al-2024>(2/3 | 219/306) Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges (Navin Kamuni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Navin Kamuni, Sathishkumar Chintala, Naveen Kunchakuri, Jyothi Swaroop Arlagadda Narasimharaju, Venkat Kumar. (2024)<br><strong>Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges</strong><br><button class=copy-to-clipboard title="Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13957v1.pdf filename=2402.13957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition. However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability. This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built on the Dejavu Project&rsquo;s foundations, the study emphasizes real-world scenario <b>simulations</b> with diverse background noises and distortions. Signal processing, central to Dejavu&rsquo;s model, includes the Fast Fourier Transform, spectrograms, and peak extraction. The &ldquo;constellation&rdquo; concept and fingerprint hashing enable unique song identification. Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency. Storage analysis highlights the critical space-speed trade-off for practical implementation. This research advances audio fingerprinting&rsquo;s adaptability, addressing challenges in varied environments and applications.</p></p class="citation"></blockquote><h3 id=33--220306-music-style-transfer-with-time-varying-inversion-of-diffusion-models-sifei-li-et-al-2024>(3/3 | 220/306) Music Style Transfer with Time-Varying Inversion of Diffusion Models (Sifei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sifei Li, Yuxin Zhang, Fan Tang, Chongyang Ma, Weiming dong, Changsheng Xu. (2024)<br><strong>Music Style Transfer with Time-Varying Inversion of Diffusion Models</strong><br><button class=copy-to-clipboard title="Music Style Transfer with Time-Varying Inversion of Diffusion Models" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Diffusion Model, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13763v1.pdf filename=2402.13763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of <b>diffusion</b> <b>models,</b> text-guided image <b>style</b> <b>transfer</b> has demonstrated high-quality controllable synthesis results. However, the utilization of text for diverse music <b>style</b> <b>transfer</b> poses significant challenges, primarily due to the limited availability of matched audio-text datasets. Music, being an abstract and complex art form, exhibits variations and intricacies even within the same genre, thereby making accurate textual descriptions challenging. This paper presents a music <b>style</b> <b>transfer</b> approach that effectively captures musical attributes using minimal data. We introduce a novel time-varying textual inversion module to precisely capture mel-spectrogram features at different levels. During inference, we propose a bias-reduced stylization technique to obtain stable results. Experimental results demonstrate that our method can transfer the <b>style</b> <b>of</b> specific instruments, as well as incorporate natural sounds to compose melodies. Samples and source code are available at <a href=https://lsfhuihuiff.github.io/MusicTI/>https://lsfhuihuiff.github.io/MusicTI/</a>.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--221306-rfi-drunet-restoring-dynamic-spectra-corrupted-by-radio-frequency-interference----application-to-pulsar-observations-xiao-zhang-et-al-2024>(1/1 | 221/306) RFI-DRUnet: Restoring dynamic spectra corrupted by radio frequency interference &ndash; Application to pulsar observations (Xiao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Zhang, Ismaël Cognard, Nicolas Dobigeon. (2024)<br><strong>RFI-DRUnet: Restoring dynamic spectra corrupted by radio frequency interference &ndash; Application to pulsar observations</strong><br><button class=copy-to-clipboard title="RFI-DRUnet: Restoring dynamic spectra corrupted by radio frequency interference -- Application to pulsar observations" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph.IM, cs-LG, eess-IV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Simulation, Simulator, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13867v1.pdf filename=2402.13867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radio frequency interference (RFI) have been an enduring concern in radio astronomy, particularly for the observations of pulsars which require high timing precision and data sensitivity. In most works of the literature, RFI mitigation has been formulated as a detection task that consists of localizing possible RFI in dynamic spectra. This strategy inevitably leads to a potential loss of information since parts of the signal identified as possibly RFI-corrupted are generally not considered in the subsequent data processing pipeline. Conversely, this work proposes to tackle RFI mitigation as a joint detection and restoration that allows parts of the dynamic spectrum affected by RFI to be not only identified but also recovered. The proposed <b>supervised</b> method relies on a deep <b>convolutional</b> <b>network</b> whose architecture inherits the performance reached by a recent yet popular image-denoising network. To train this network, a whole <b>simulation</b> framework is built to generate large data sets according to physics-inspired and statistical models of the pulsar signals and of the RFI. The relevance of the proposed approach is quantitatively assessed by conducting extensive experiments. In particular, the results show that the restored dynamic spectra are sufficiently reliable to estimate pulsar times-of-arrivals with an accuracy close to the one that would be obtained from RFI-free signals.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--222306-an-evaluation-of-large-language-models-in-bioinformatics-research-hengchuang-yin-et-al-2024>(1/1 | 222/306) An Evaluation of Large Language Models in Bioinformatics Research (Hengchuang Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hengchuang Yin, Zhonghui Gu, Fanhao Wang, Yiparemu Abuduhaibaier, Yanqiao Zhu, Xinming Tu, Xian-Sheng Hua, Xiao Luo, Yizhou Sun. (2024)<br><strong>An Evaluation of Large Language Models in Bioinformatics Research</strong><br><button class=copy-to-clipboard title="An Evaluation of Large Language Models in Bioinformatics Research" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 50<br>Keywords: ChatGPT, GPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13714v1.pdf filename=2402.13714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>ChatGPT</b> have gained considerable interest across diverse research communities. Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving. However, the potential and efficacy of these models in bioinformatics remain incompletely explored. In this work, we study the performance <b>LLMs</b> on a wide spectrum of crucial bioinformatics tasks. These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems. Our findings indicate that, given appropriate <b>prompts,</b> <b>LLMs</b> like <b>GPT</b> variants can successfully handle most of these tasks. In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks. In conclusion, we believe that this work can provide new perspectives and motivate future research in the field of <b>LLMs</b> applications, AI for Science and bioinformatics.</p></p class="citation"></blockquote><h2 id=csir-13>cs.IR (13)</h2><h3 id=113--223306-general-debiasing-for-graph-based-collaborative-filtering-via-adversarial-graph-dropout-an-zhang-et-al-2024>(1/13 | 223/306) General Debiasing for Graph-based Collaborative Filtering via Adversarial Graph Dropout (An Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>An Zhang, Wenchang Ma, Pengbo Wei, Leheng Sheng, Xiang Wang. (2024)<br><strong>General Debiasing for Graph-based Collaborative Filtering via Adversarial Graph Dropout</strong><br><button class=copy-to-clipboard title="General Debiasing for Graph-based Collaborative Filtering via Adversarial Graph Dropout" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 48<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Adversarial Learning, Recommender System, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13769v1.pdf filename=2402.13769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have shown impressive performance in <b>recommender</b> <b>systems,</b> particularly in collaborative filtering (CF). The key lies in aggregating neighborhood information on a user-item interaction <b>graph</b> <b>to</b> <b>enhance</b> user/item <b>representations.</b> <b>However,</b> we have discovered that this aggregation mechanism comes with a drawback, which amplifies biases present in the interaction <b>graph.</b> <b>For</b> <b>instance,</b> a user&rsquo;s interactions with items can be driven by both unbiased true interest and various biased factors like item popularity or exposure. However, the current aggregation approach combines all information, both biased and unbiased, leading to biased <b>representation</b> <b>learning.</b> Consequently, <b>graph-based</b> <b>recommenders</b> <b>can</b> learn distorted views of users/items, hindering the modeling of their true preferences and generalizations. To address this issue, we introduce a novel framework called <b>Adversarial</b> <b>Graph</b> <b>Dropout</b> <b>(AdvDrop).</b> It differentiates between unbiased and biased interactions, enabling unbiased <b>representation</b> <b>learning.</b> For each user/item, AdvDrop employs <b>adversarial</b> <b>learning</b> to split the neighborhood into two views: one with bias-mitigated interactions and the other with bias-aware interactions. After view-specific aggregation, AdvDrop ensures that the bias-mitigated and bias-aware <b>representations</b> <b>remain</b> invariant, shielding them from the influence of bias. We validate AdvDrop&rsquo;s effectiveness on five public datasets that cover both general and specific biases, demonstrating significant improvements. Furthermore, our method exhibits meaningful separation of subgraphs and achieves unbiased <b>representations</b> <b>for</b> <b>graph-based</b> <b>CF</b> <b>models,</b> as revealed by in-depth analysis. Our code is publicly available at <a href=https://github.com/Arthurma71/AdvDrop>https://github.com/Arthurma71/AdvDrop</a>.</p></p class="citation"></blockquote><h3 id=213--224306-linear-time-graph-neural-networks-for-scalable-recommendations-jiahao-zhang-et-al-2024>(2/13 | 224/306) Linear-Time Graph Neural Networks for Scalable Recommendations (Jiahao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Zhang, Rui Xue, Wenqi Fan, Xin Xu, Qing Li, Jian Pei, Xiaorui Liu. (2024)<br><strong>Linear-Time Graph Neural Networks for Scalable Recommendations</strong><br><button class=copy-to-clipboard title="Linear-Time Graph Neural Networks for Scalable Recommendations" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13973v1.pdf filename=2402.13973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In an era of information explosion, <b>recommender</b> <b>systems</b> are vital tools to deliver personalized <b>recommendations</b> for users. The key of <b>recommender</b> <b>systems</b> is to forecast users&rsquo; future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> to boost the prediction performance of <b>recommender</b> <b>systems.</b> Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale <b>recommender</b> <b>systems</b> due to their scalability advantages. Despite the existence of <b>GNN-acceleration</b> solutions, it remains an open question whether <b>GNN-based</b> <b>recommender</b> <b>systems</b> can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time <b>Graph</b> <b>Neural</b> <b>Network</b> (LTGNN) to scale up <b>GNN-based</b> <b>recommender</b> <b>systems</b> to achieve comparable scalability as classic MF approaches while maintaining <b>GNNs&rsquo;</b> powerful expressiveness for superior prediction accuracy. Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm. Our implementation based on PyTorch is available.</p></p class="citation"></blockquote><h3 id=313--225306-the-effectiveness-of-graph-contrastive-learning-on-mathematical-information-retrieval-pei-syuan-wang-et-al-2024>(3/13 | 225/306) The Effectiveness of Graph Contrastive Learning on Mathematical Information Retrieval (Pei-Syuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pei-Syuan Wang, Hung-Hsuan Chen. (2024)<br><strong>The Effectiveness of Graph Contrastive Learning on Mathematical Information Retrieval</strong><br><button class=copy-to-clipboard title="The Effectiveness of Graph Contrastive Learning on Mathematical Information Retrieval" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 43<br>Keywords: Graph, Graph Contrastive Learning, Graph Contrastive Learning, Contrastive Learning, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13444v1.pdf filename=2402.13444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper details an empirical investigation into using <b>Graph</b> <b>Contrastive</b> <b>Learning</b> <b>(GCL)</b> to generate mathematical equation representations, a critical aspect of Mathematical <b>Information</b> <b>Retrieval</b> (MIR). Our findings reveal that this simple approach consistently exceeds the performance of the current leading formula retrieval model, TangentCFT. To support ongoing research and development in this field, we have made our source code accessible to the public at <a href=https://github.com/WangPeiSyuan/GCL-Formula-Retrieval/>https://github.com/WangPeiSyuan/GCL-Formula-Retrieval/</a>.</p></p class="citation"></blockquote><h3 id=413--226306-llm4sbr-a-lightweight-and-effective-framework-for-integrating-large-language-models-in-session-based-recommendation-shutong-qiao-et-al-2024>(4/13 | 226/306) LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation (Shutong Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shutong Qiao, Chen Gao, Junhao Wen, Wei Zhou, Qun Luo, Peixuan Chen, Yong Li. (2024)<br><strong>LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation</strong><br><button class=copy-to-clipboard title="LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Recommendation, Recommender System, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13840v1.pdf filename=2402.13840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional session-based <b>recommendation</b> (SBR) utilizes session behavior sequences from anonymous users for <b>recommendation.</b> Although this strategy is highly efficient, it sacrifices the inherent semantic information of the items, making it difficult for the model to understand the true intent of the session and resulting in a lack of interpretability in the recommended results. Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have flourished across various domains, offering a glimpse of hope in addressing the aforementioned challenges. Inspired by the impact of <b>LLMs,</b> research exploring the integration of <b>LLMs</b> with the <b>Recommender</b> <b>system</b> (RS) has surged like mushrooms after rain. However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first <b>LLM</b> <b>recommendation</b> framework suitable for industrial deployment has yet to emerge in the field of SBR. To address the aforementioned challenges, we have proposed the <b>LLM</b> Integration Framework for SBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR adopts a two-step strategy. Firstly, we transform session data into a bimodal form of text and behavior. In the first step, leveraging the inferential capabilities of <b>LLMs,</b> we conduct inference on session text data from different perspectives and design the component for auxiliary enhancement. In the second step, the SBR model is trained on behavior data, aligning and averaging two modal session representations from different perspectives. Finally, we fuse session representations from different perspectives and modalities as the ultimate session representation for <b>recommendation.</b> We conducted experiments on two real-world datasets, and the results demonstrate that LLM4SBR significantly improves the performance of traditional SBR models and is highly lightweight and efficient, making it suitable for industrial deployment.</p></p class="citation"></blockquote><h3 id=513--227306-breaking-the-barrier-utilizing-large-language-models-for-industrial-recommendation-systems-through-an-inferential-knowledge-graph-qian-zhao-et-al-2024>(5/13 | 227/306) Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph (Qian Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Zhao, Hao Qian, Ziqi Liu, Gong-Duo Zhang, Lihong Gu. (2024)<br><strong>Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph</strong><br><button class=copy-to-clipboard title="Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs.IR<br>Keyword Score: 38<br>Keywords: Graph, Knowledge Graph, Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13750v1.pdf filename=2402.13750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommendation</b> systems are widely used in e-commerce websites and online platforms to address information overload. However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions. Recently, <b>Knowledge</b> <b>Base</b> (KB)-based models are proposed to incorporate expert <b>knowledge,</b> <b>but</b> it struggle to adapt to new items and the evolving e-commerce environment. To address these challenges, we propose a novel <b>Large</b> <b>Language</b> <b>Model</b> based Complementary <b>Knowledge</b> <b>Enhanced</b> <b>Recommendation</b> System <b>(LLM-KERec).</b> It introduces an entity extractor that extracts unified concept terms from item and user information. To provide cost-effective and reliable prior <b>knowledge,</b> <b>entity</b> pairs are generated based on entity popularity and specific strategies. The <b>large</b> <b>language</b> <b>model</b> determines complementary relationships in each entity pair, constructing a complementary <b>knowledge</b> <b>graph.</b> Furthermore, a new complementary recall module and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of the ranking model using real complementary exposure-click samples. Extensive experiments conducted on three industry datasets demonstrate the significant performance improvement of our model compared to existing approaches. Additionally, detailed analysis shows that <b>LLM-KERec</b> enhances users&rsquo; enthusiasm for consumption by recommending complementary items. In summary, <b>LLM-KERec</b> addresses the limitations of traditional <b>recommendation</b> systems by incorporating complementary <b>knowledge</b> <b>and</b> utilizing a <b>large</b> <b>language</b> <b>model</b> to capture user intent transitions, adapt to new items, and enhance <b>recommendation</b> efficiency in the evolving e-commerce landscape.</p></p class="citation"></blockquote><h3 id=613--228306-birco-a-benchmark-of-information-retrieval-tasks-with-complex-objectives-xiaoyue-wang-et-al-2024>(6/13 | 228/306) BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives (Xiaoyue Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyue Wang, Jianyou Wang, Weili Cao, Kaicheng Wang, Ramamohan Paturi, Leon Bergen. (2024)<br><strong>BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives</strong><br><button class=copy-to-clipboard title="BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 33<br>Keywords: Benchmarking, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14151v1.pdf filename=2402.14151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the <b>Benchmark</b> of <b>Information</b> <b>Retrieval</b> (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The <b>benchmark&rsquo;s</b> complexity and compact size make it suitable for evaluating <b>large</b> <b>language</b> <b>model</b> <b>(LLM)-based</b> <b>information</b> <b>retrieval</b> systems. We present a modular framework for investigating factors that may influence <b>LLM</b> performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all <b>benchmark</b> tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.</p></p class="citation"></blockquote><h3 id=713--229306-combining-language-and-graph-models-for-semi-structured-information-extraction-on-the-web-zhi-hong-et-al-2024>(7/13 | 229/306) Combining Language and Graph Models for Semi-structured Information Extraction on the Web (Zhi Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhi Hong, Kyle Chard, Ian Foster. (2024)<br><strong>Combining Language and Graph Models for Semi-structured Information Extraction on the Web</strong><br><button class=copy-to-clipboard title="Combining Language and Graph Models for Semi-structured Information Extraction on the Web" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 33<br>Keywords: Graph, Zero-shot, Information Retrieval, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14129v1.pdf filename=2402.14129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Relation</b> <b>extraction</b> is an efficient way of mining the extraordinary wealth of human knowledge on the Web. Existing methods rely on domain-specific training data or produce noisy outputs. We focus here on extracting targeted <b>relations</b> <b>from</b> semi-structured web pages given only a short description of the <b>relation.</b> <b>We</b> present GraphScholarBERT, an open-domain <b>information</b> <b>extraction</b> method based on a joint <b>graph</b> and language model structure. GraphScholarBERT can generalize to previously unseen domains without additional data or training and produces only clean extraction results matched to the search keyword. Experiments show that GraphScholarBERT can improve extraction F1 scores by as much as 34.8% compared to previous work in a <b>zero-shot</b> domain and <b>zero-shot</b> website setting.</p></p class="citation"></blockquote><h3 id=813--230306-leveraging-translation-for-optimal-recall-tailoring-llm-personalization-with-user-profiles-karthik-ravichandran-et-al-2024>(8/13 | 230/306) Leveraging Translation For Optimal Recall: Tailoring LLM Personalization With User Profiles (Karthik Ravichandran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karthik Ravichandran, Sarmistha Sarna Gomasta. (2024)<br><strong>Leveraging Translation For Optimal Recall: Tailoring LLM Personalization With User Profiles</strong><br><button class=copy-to-clipboard title="Leveraging Translation For Optimal Recall: Tailoring LLM Personalization With User Profiles" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: F-2-2; I-2-7, cs-CL, cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Information Retrieval, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13500v1.pdf filename=2402.13500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores a novel technique for improving recall in cross-language <b>information</b> <b>retrieval</b> (CLIR) systems using iterative query refinement grounded in the user&rsquo;s lexical-semantic space. The proposed methodology combines multi-level translation, semantic embedding-based expansion, and user profile-centered augmentation to address the challenge of matching variance between user queries and relevant documents. Through an initial BM25 retrieval, translation into intermediate languages, embedding lookup of similar terms, and iterative re-ranking, the technique aims to expand the scope of potentially relevant results personalized to the individual user. Comparative experiments on news and Twitter datasets demonstrate superior performance over baseline BM25 ranking for the proposed approach across <b>ROUGE</b> metrics. The translation methodology also showed maintained semantic accuracy through the multi-step process. This personalized CLIR framework paves the path for improved context-aware retrieval attentive to the nuances of user language.</p></p class="citation"></blockquote><h3 id=913--231306-can-one-embedding-fit-all-a-multi-interest-learning-paradigm-towards-improving-user-interest-diversity-fairness-yuying-zhao-et-al-2024>(9/13 | 231/306) Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards Improving User Interest Diversity Fairness (Yuying Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuying Zhao, Minghua Xu, Huiyuan Chen, Yuzhong Chen, Yiwei Cai, Rashidul Islam, Yu Wang, Tyler Derr. (2024)<br><strong>Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards Improving User Interest Diversity Fairness</strong><br><button class=copy-to-clipboard title="Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards Improving User Interest Diversity Fairness" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Fairness, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13495v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13495v1.pdf filename=2402.13495v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems</b> (RSs) have gained widespread applications across various domains owing to the superior ability to capture users&rsquo; interests. However, the complexity and nuanced nature of users&rsquo; interests, which span a wide range of diversity, pose a significant challenge in delivering fair <b>recommendations.</b> In practice, user preferences vary significantly; some users show a clear preference toward certain item categories, while others have a broad interest in diverse ones. Even though it is expected that all users should receive high-quality <b>recommendations,</b> the effectiveness of RSs in catering to this disparate interest diversity remains under-explored. In this work, we investigate whether users with varied levels of interest diversity are treated fairly. Our empirical experiments reveal an inherent disparity: users with broader interests often receive lower-quality <b>recommendations.</b> To mitigate this, we propose a multi-interest framework that uses multiple (virtual) interest embeddings rather than single ones to represent users. Specifically, the framework consists of stacked multi-interest representation layers, which include an interest embedding generator that derives virtual interests from shared parameters, and a center embedding aggregator that facilitates multi-hop aggregation. Experiments demonstrate the effectiveness of the framework in achieving better trade-off between <b>fairness</b> and utility across various datasets and backbones.</p></p class="citation"></blockquote><h3 id=1013--232306-science-checker-reloaded-a-bidirectional-paradigm-for-transparency-and-logical-reasoning-loïc-rakotoson-et-al-2024>(10/13 | 232/306) Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning (Loïc Rakotoson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Loïc Rakotoson, Sylvain Massip, Fréjus A. A. Laleye. (2024)<br><strong>Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning</strong><br><button class=copy-to-clipboard title="Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: H-3-1; H-3-3; I-7; K-4, cs-AI, cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Information Retrieval, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13897v1.pdf filename=2402.13897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Information</b> <b>retrieval</b> is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of <b>information,</b> <b>such</b> as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated <b>information</b> <b>in</b> generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the <b>information</b> <b>spread</b> in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system&rsquo;s <b>reasoning.</b> We believe this bidirectional approach brings significant advancements in terms of transparency, logical thinking, and comprehensive understanding in the field of scientific <b>information</b> <b>retrieval.</b></p></p class="citation"></blockquote><h3 id=1113--233306-learning-to-retrieve-for-job-matching-jianqiang-shen-et-al-2024>(11/13 | 233/306) Learning to Retrieve for Job Matching (Jianqiang Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianqiang Shen, Yuchin Juan, Shaobo Zhang, Ping Liu, Wen Pu, Sriram Vasudevan, Qingquan Song, Fedor Borisyuk, Kay Qianqi Shen, Haichao Wei, Yunxiang Ren, Yeou S. Chiou, Sicong Kuang, Yuan Yin, Ben Zheng, Muchen Wu, Shaghayegh Gharghabi, Xiaoqing Wang, Huichao Xue, Qi Guo, Daniel Hewlett, Luke Simon, Liangjie Hong, Wenjing Zhang. (2024)<br><strong>Learning to Retrieve for Job Matching</strong><br><button class=copy-to-clipboard title="Learning to Retrieve for Job Matching" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 13<br>Keywords: Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13435v1.pdf filename=2402.13435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Web-scale search systems typically tackle the scalability challenge with a two-step paradigm: retrieval and ranking. The retrieval step, also known as candidate selection, often involves extracting standardized entities, creating an inverted index, and performing term matching for retrieval. Such traditional methods require manual and time-consuming development of query models. In this paper, we discuss applying learning-to-retrieve technology to enhance LinkedIns job search and <b>recommendation</b> systems. In the realm of promoted jobs, the key objective is to improve the quality of applicants, thereby delivering value to recruiter customers. To achieve this, we leverage confirmed hire data to construct a <b>graph</b> that evaluates a seeker&rsquo;s qualification for a job, and utilize learned links for retrieval. Our learned model is easy to explain, debug, and adjust. On the other hand, the focus for organic jobs is to optimize seeker engagement. We accomplished this by training embeddings for personalized retrieval, fortified by a set of rules derived from the categorization of member feedback. In addition to a solution based on a conventional inverted index, we developed an on-GPU solution capable of supporting both KNN and term matching efficiently.</p></p class="citation"></blockquote><h3 id=1213--234306-retention-induced-biases-in-a-recommendation-system-with-heterogeneous-users-shichao-ma-2024>(12/13 | 234/306) Retention Induced Biases in a Recommendation System with Heterogeneous Users (Shichao Ma, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shichao Ma. (2024)<br><strong>Retention Induced Biases in a Recommendation System with Heterogeneous Users</strong><br><button class=copy-to-clipboard title="Retention Induced Biases in a Recommendation System with Heterogeneous Users" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13959v1.pdf filename=2402.13959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>I examine a conceptual model of a <b>recommendation</b> system (RS) with user inflow and churn dynamics. When inflow and churn balance out, the user distribution reaches a steady state. Changing the <b>recommendation</b> algorithm alters the steady state and creates a transition period. During this period, the RS behaves differently from its new steady state. In particular, A/B experiment metrics obtained in transition periods are biased indicators of the RS&rsquo;s long term performance. Scholars and practitioners, however, often conduct A/B tests shortly after introducing new algorithms to validate their effectiveness. This A/B experiment paradigm, widely regarded as the gold standard for assessing RS improvements, may consequently yield false conclusions. I also briefly discuss the data bias caused by the user retention dynamics.</p></p class="citation"></blockquote><h3 id=1313--235306-diversity-aware-k-maximum-inner-product-search-revisited-qiang-huang-et-al-2024>(13/13 | 235/306) Diversity-Aware $k$-Maximum Inner Product Search Revisited (Qiang Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiang Huang, Yanhao Wang, Yiqun Sun, Anthony K. H. Tung. (2024)<br><strong>Diversity-Aware $k$-Maximum Inner Product Search Revisited</strong><br><button class=copy-to-clipboard title="Diversity-Aware $k$-Maximum Inner Product Search Revisited" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-DB, cs-DS, cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13858v1.pdf filename=2402.13858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The $k$-Maximum Inner Product Search ($k$MIPS) serves as a foundational component in <b>recommender</b> <b>systems</b> and various data mining tasks. However, while most existing $k$MIPS approaches prioritize the efficient retrieval of highly relevant items for users, they often neglect an equally pivotal facet of search results: \emph{diversity}. To bridge this gap, we revisit and refine the diversity-aware $k$MIPS (D$k$MIPS) problem by incorporating two well-known diversity objectives &ndash; minimizing the average and maximum pairwise item similarities within the results &ndash; into the original relevance objective. This enhancement, inspired by Maximal Marginal Relevance (MMR), offers users a controllable trade-off between relevance and diversity. We introduce \textsc{Greedy} and \textsc{DualGreedy}, two linear scan-based algorithms tailored for D$k$MIPS. They both achieve data-dependent approximations and, when aiming to minimize the average pairwise similarity, \textsc{DualGreedy} attains an approximation ratio of $1/4$ with an additive term for regularization. To further improve query efficiency, we integrate a lightweight Ball-Cone Tree (BC-Tree) index with the two algorithms. Finally, comprehensive experiments on ten real-world data sets demonstrate the efficacy of our proposed methods, showcasing their capability to efficiently deliver diverse and relevant search results to users.</p></p class="citation"></blockquote><h2 id=csit-5>cs.IT (5)</h2><h3 id=15--236306-near-field-multiuser-beam-training-for-extremely-large-scale-mimo-systems-wang-liu-et-al-2024>(1/5 | 236/306) Near-Field Multiuser Beam-Training for Extremely Large-Scale MIMO Systems (Wang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wang Liu, Cunhua Pan, Hong Ren, Jiangzhou Wang, Robert Schober, Lajos Hanzo. (2024)<br><strong>Near-Field Multiuser Beam-Training for Extremely Large-Scale MIMO Systems</strong><br><button class=copy-to-clipboard title="Near-Field Multiuser Beam-Training for Extremely Large-Scale MIMO Systems" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 46<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13597v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13597v1.pdf filename=2402.13597v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extremely large-scale multiple-input multiple-output (XL-MIMO) systems are capable of improving spectral efficiency by employing far more antennas than conventional massive MIMO at the base station (BS). However, beam training in multiuser XL-MIMO systems is challenging. To tackle these issues, we conceive a three-phase <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)-based</b> beam training scheme for multiuser XL-MIMO systems. In the first phase, only far-field wide beams have to be tested for each user and the <b>GNN</b> is utilized to map the beamforming gain information of the far-field wide beams to the optimal near-field beam for each user. In addition, the proposed <b>GNN-based</b> scheme can exploit the position-correlation between adjacent users for further improvement of the accuracy of beam training. In the second phase, a beam allocation scheme based on the probability vectors produced at the outputs of <b>GNNs</b> is proposed to address the above beam-direction conflicts between users. In the third phase, the hybrid TBF is designed for further reducing the inter-user interference. Our <b>simulation</b> results show that the proposed scheme improves the beam training performance of the <b>benchmarks.</b> Moreover, the performance of the proposed beam training scheme approaches that of an exhaustive search, despite requiring only about 7% of the pilot overhead.</p></p class="citation"></blockquote><h3 id=25--237306-coding-theorems-for-repetition-and-superposition-codes-over-binary-input-output-symmetric-channels-yixin-wang-et-al-2024>(2/5 | 237/306) Coding Theorems for Repetition and Superposition Codes over Binary-Input Output-Symmetric Channels (Yixin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixin Wang, Xiao Ma. (2024)<br><strong>Coding Theorems for Repetition and Superposition Codes over Binary-Input Output-Symmetric Channels</strong><br><button class=copy-to-clipboard title="Coding Theorems for Repetition and Superposition Codes over Binary-Input Output-Symmetric Channels" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 30<br>Keywords: Convolution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13603v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13603v1.pdf filename=2402.13603v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper is concerned with a class of low density generator matrix codes (LDGM), called repetition and superposition (RaS) codes, which have been proved to be capacity-achieving over binary-input output-symmetric (BIOS) channels in terms of bit-error rate (BER). We prove with a recently proposed framework that the RaS codes are also capacity-achieving over BIOS channels in terms of frame-error rate (FER). With this new framework, the theorem for the RaS codes can be generalized to source coding and joint source and channel coding (JSCC). In particular, we prove with this framework that the corresponding low-density parity-check (LDPC) codes, as an enlarged ensemble of quasi-cyclic LDPC (QC-LDPC) codes, can also achieve the capacity. To further improve the iterative decoding performance, we consider the <b>convolutional</b> RaS (Conv-RaS) code ensemble and prove it to be capacity-achieving over BIOS channels in terms of the first error event probability. The construction of Conv-RaS codes is flexible with rate (defined as the ratio of the input length to the encoding output length) ranging from less than one (typically for channel codes) to greater than one (typically for source codes), which can be implemented as a universal JSCC scheme, as confirmed by <b>simulations.</b></p></p class="citation"></blockquote><h3 id=35--238306-q-learning-based-joint-design-of-adaptive-modulation-and-precoding-for-physical-layer-security-in-visible-light-communications-duc-m-t-hoang-et-al-2024>(3/5 | 238/306) Q-learning-based Joint Design of Adaptive Modulation and Precoding for Physical Layer Security in Visible Light Communications (Duc M. T. Hoang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duc M. T. Hoang, Thanh V. Pham, Anh T. Pham, Chuyen T Nguyen. (2024)<br><strong>Q-learning-based Joint Design of Adaptive Modulation and Precoding for Physical Layer Security in Visible Light Communications</strong><br><button class=copy-to-clipboard title="Q-learning-based Joint Design of Adaptive Modulation and Precoding for Physical Layer Security in Visible Light Communications" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-SY, cs.IT, eess-SY, math-IT<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13549v1.pdf filename=2402.13549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There has been an increasing interest in physical layer security (PLS), which, compared with conventional cryptography, offers a unique approach to guaranteeing information confidentiality against eavesdroppers. In this paper, we study a joint design of adaptive $M$-ary pulse amplitude modulation (PAM) and precoding, which aims to optimize wiretap visible-light channels&rsquo; secrecy capacity and bit error rate (BER) performances. The proposed design is motivated by higher-order modulation, which results in better secrecy capacity at the expense of a higher BER. On the other hand, a proper precoding design, which can manipulate the received signal quality at the legitimate user and the eavesdropper, can also enhance secrecy performance and influence the BER. A reward function that considers the secrecy capacity and the BERs of the legitimate user&rsquo;s (Bob) and the eavesdropper&rsquo;s (Eve) channels is introduced and maximized. Due to the non-linearity and complexity of the reward function, it is challenging to solve the optical design using classical optimization techniques. Therefore, <b>reinforcement</b> <b>learning-based</b> designs using Q-learning and Deep Q-learning are proposed to maximize the reward function. <b>Simulation</b> results verify that compared with the baseline designs, the proposed joint designs achieve better reward values while maintaining the BER of Bob&rsquo;s channel (Eve&rsquo;s channel) well below (above) the pre-FEC (forward error correction) BER threshold.</p></p class="citation"></blockquote><h3 id=45--239306-secrecy-performance-analysis-of-space-to-ground-optical-satellite-communications-thang-v-nguyen-et-al-2024>(4/5 | 239/306) Secrecy Performance Analysis of Space-to-Ground Optical Satellite Communications (Thang V. Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thang V. Nguyen, Thanh V. Pham, Anh T. Pham, Dang T. Ngoc. (2024)<br><strong>Secrecy Performance Analysis of Space-to-Ground Optical Satellite Communications</strong><br><button class=copy-to-clipboard title="Secrecy Performance Analysis of Space-to-Ground Optical Satellite Communications" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13554v1.pdf filename=2402.13554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Free-space optics (FSO)-based satellite communication systems have recently received considerable attention due to their enhanced capacity compared to their radio frequency (RF) counterparts. This paper analyzes the performance of physical layer security of space-to-ground intensity modulation/direct detection FSO satellite links under the effect of atmospheric loss, misalignment, cloud attenuation, and atmospheric turbulence-induced fading. Specifically, a wiretap channel consisting of a legitimate transmitter Alice (i.e., the satellite), a legitimate user Bob, and an eavesdropper Eve over turbulence channels modeled by the Fisher-Snedecor $\mathcal{F}$ distribution is considered. The secrecy performance in terms of the average secrecy capacity, secrecy outage probability, and strictly positive secrecy capacity are derived in closed-form. <b>Simulation</b> results reveal significant impacts of satellite altitude, zenith angle, and turbulence strength on the secrecy performance.</p></p class="citation"></blockquote><h3 id=55--240306-reconfigurable-intelligent-surfaces-for-thz-hardware-impairments-and-switching-technologies-sérgio-matos-et-al-2024>(5/5 | 240/306) Reconfigurable Intelligent Surfaces for THz: Hardware Impairments and Switching Technologies (Sérgio Matos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sérgio Matos, Yihan Ma, Qi Luo, Jonas Deuermeier, Luca Lucci, Panagiotis Gavriilidis, Asal Kiazadeh, Verónica Lain-Rubio, Tung D. Phan, Ping Jack Soh, Antonio Clemente, Luís M. Pessoa, George C. Alexandropoulos. (2024)<br><strong>Reconfigurable Intelligent Surfaces for THz: Hardware Impairments and Switching Technologies</strong><br><button class=copy-to-clipboard title="Reconfigurable Intelligent Surfaces for THz: Hardware Impairments and Switching Technologies" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-ET, cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13804v1.pdf filename=2402.13804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The demand for unprecedented performance in the upcoming 6G wireless networks is fomenting the research on THz communications empowered by Reconfigurable Inteligent Surfaces (RISs). A wide range of use cases have been proposed, most of them, assuming high-level RIS models that overlook some of the hardware impairments that this technology faces. The expectation is that the emergent reconfigurable THz technologies will eventually overcome its current limitations. This disassociation from the hardware may mask nonphysical assumptions, perceived as hardware limitations. In this paper, a top-down approach bounded by physical constraints is presented, <b>distilling</b> from system-level specifications, hardware requirements, and upper bounds for the RIS-aided system performance. We consider D-band indoor and outdoor scenarios where a more realistic assessment of the state-of-the-art solution can be made. The goal is to highlight the intricacies of the design procedure based on sound assumptions for the RIS performance. For a given signal range and angular coverage, we quantify the required RIS size, number of switching elements, and maximum achievable bandwidth and capacity.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--241306-verifying-message-passing-neural-networks-via-topology-based-bounds-tightening-christopher-hojny-et-al-2024>(1/2 | 241/306) Verifying message-passing neural networks via topology-based bounds tightening (Christopher Hojny et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher Hojny, Shiqiang Zhang, Juan S. Campos, Ruth Misener. (2024)<br><strong>Verifying message-passing neural networks via topology-based bounds tightening</strong><br><button class=copy-to-clipboard title="Verifying message-passing neural networks via topology-based bounds tightening" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 43<br>Keywords: Message-Passing, Graph Classification, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13937v1.pdf filename=2402.13937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> are often vulnerable to attack, we need to know when we can trust them. We develop a computationally effective approach towards providing robust certificates for <b>message-passing</b> neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications. Our key technology, topology-based bounds tightening, uses <b>graph</b> <b>structure</b> <b>to</b> tighten bounds. We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds. To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP. We test on both node and <b>graph</b> <b>classification</b> <b>problems</b> and consider topological attacks that both add and remove edges.</p></p class="citation"></blockquote><h3 id=22--242306-a-cutting-plane-algorithm-for-globally-solving-low-dimensional-k-means-clustering-problems-martin-ryner-et-al-2024>(2/2 | 242/306) A cutting plane algorithm for globally solving low dimensional k-means clustering problems (Martin Ryner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Ryner, Jan Kronqvist, Johan Karlsson. (2024)<br><strong>A cutting plane algorithm for globally solving low dimensional k-means clustering problems</strong><br><button class=copy-to-clipboard title="A cutting plane algorithm for globally solving low dimensional k-means clustering problems" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 90C26 (Primary) 90C27 (Secondary), G-1-6, cs-LG, math-OC, math.OC, stat-ML<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13595v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13595v1.pdf filename=2402.13595v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Clustering</b> is one of the most fundamental tools in data science and machine learning, and k-means <b>clustering</b> is one of the most common such methods. There is a variety of approximate algorithms for the k-means problem, but computing the globally optimal solution is in general NP-hard. In this paper we consider the k-means problem for instances with low dimensional data and formulate it as a structured concave assignment problem. This allows us to exploit the low dimensional structure and solve the problem to global optimality within reasonable time for large data sets with several clusters. The method builds on iteratively solving a small concave problem and a large linear programming problem. This gives a sequence of feasible solutions along with bounds which we show converges to zero optimality gap. The paper combines methods from global optimization theory to accelerate the procedure, and we provide numerical results on their performance.</p></p class="citation"></blockquote><h2 id=csai-8>cs.AI (8)</h2><h3 id=18--243306-synthesis-of-hierarchical-controllers-based-on-deep-reinforcement-learning-policies-florent-delgrange-et-al-2024>(1/8 | 243/306) Synthesis of Hierarchical Controllers Based on Deep Reinforcement Learning Policies (Florent Delgrange et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florent Delgrange, Guy Avni, Anna Lukina, Christian Schilling, Ann Nowé, Guillermo A. Pérez. (2024)<br><strong>Synthesis of Hierarchical Controllers Based on Deep Reinforcement Learning Policies</strong><br><button class=copy-to-clipboard title="Synthesis of Hierarchical Controllers Based on Deep Reinforcement Learning Policies" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 43<br>Keywords: Graph, Knowledge Distillation, Markov Decision Process, Model Distillation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13785v1.pdf filename=2402.13785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel approach to the problem of controller design for environments <b>modeled</b> <b>as</b> Markov decision processes <b>(MDPs).</b> Specifically, we consider a hierarchical MDP a <b>graph</b> with each vertex populated by an MDP called a &ldquo;room&rdquo;. We first apply deep <b>reinforcement</b> <b>learning</b> (DRL) to obtain low-level policies for each room, scaling to large rooms of unknown structure. We then apply reactive synthesis to obtain a high-level planner that chooses which low-level policy to execute in each room. The central challenge in synthesizing the planner is the need for <b>modeling</b> <b>rooms.</b> We address this challenge by developing a DRL procedure to train concise &ldquo;latent&rdquo; policies together with PAC guarantees on their performance. Unlike previous approaches, ours circumvents a <b>model</b> <b>distillation</b> step. Our approach combats sparse rewards in DRL and enables reusability of low-level policies. We demonstrate feasibility in a case study involving agent navigation amid moving obstacles.</p></p class="citation"></blockquote><h3 id=28--244306-social-environment-design-edwin-zhang-et-al-2024>(2/8 | 244/306) Social Environment Design (Edwin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edwin Zhang, Sadie Zhao, Tonghan Wang, Safwan Hossain, Henry Gasztowtt, Stephan Zheng, David C. Parkes, Milind Tambe, Yiling Chen. (2024)<br><strong>Social Environment Design</strong><br><button class=copy-to-clipboard title="Social Environment Design" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI, econ-GN, q-fin-EC, stat-ML<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14090v1.pdf filename=2402.14090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence (AI) holds promise as a technology that can be used to improve government and economic policy-making. This paper proposes a new research agenda towards this end by introducing Social Environment Design, a general framework for the use of AI for automated policy-making that connects with the <b>Reinforcement</b> <b>Learning,</b> EconCS, and Computational Social Choice communities. The framework seeks to capture general economic environments, includes voting on policy objectives, and gives a direction for the systematic analysis of government and economic policy through AI <b>simulation.</b> We highlight key open problems for future research in AI-based policy-making. By solving these challenges, we hope to achieve various social welfare objectives, thereby promoting more ethical and responsible decision making.</p></p class="citation"></blockquote><h3 id=38--245306-beyond-a-better-planning-with-transformers-via-search-dynamics-bootstrapping-lucas-lehnert-et-al-2024>(3/8 | 245/306) Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping (Lucas Lehnert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, Yuandong Tian. (2024)<br><em><em>Beyond A</em>: Better Planning with Transformers via Search Dynamics Bootstrapping</em>*<br><button class=copy-to-clipboard title="Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14083v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14083v1.pdf filename=2402.14083v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Transformers</b> have enabled tremendous progress in various application settings, such architectures still lag behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train <b>Transformers</b> to solve complex planning tasks and present Searchformer, a <b>Transformer</b> model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than standard $A^<em>$ search. Searchformer is an encoder-decoder <b>Transformer</b> model trained to predict the search dynamics of $A^</em>$. This model is then <b>fine-tuned</b> via expert iterations to perform fewer search steps than $A^<em>$ search while still generating an optimal plan. In our training method, $A^</em>$&rsquo;s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. In our ablation studies on maze navigation, we find that Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10$\times$ smaller model size and a 10$\times$ smaller training dataset. We also demonstrate how Searchformer scales to larger and more complex decision making tasks like Sokoban with improved percentage of solved tasks and shortened search dynamics.</p></p class="citation"></blockquote><h3 id=48--246306-the-delusional-hedge-algorithm-as-a-model-of-human-learning-from-diverse-opinions-yun-shiuan-chuang-et-al-2024>(4/8 | 246/306) The Delusional Hedge Algorithm as a Model of Human Learning from Diverse Opinions (Yun-Shiuan Chuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yun-Shiuan Chuang, Jerry Zhu, Timothy T. Rogers. (2024)<br><strong>The Delusional Hedge Algorithm as a Model of Human Learning from Diverse Opinions</strong><br><button class=copy-to-clipboard title="The Delusional Hedge Algorithm as a Model of Human Learning from Diverse Opinions" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13927v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13927v1.pdf filename=2402.13927v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Whereas cognitive models of learning often assume direct experience with both the features of an event and with a true label or outcome, much of everyday learning arises from hearing the opinions of others, without direct access to either the experience or the ground truth outcome. We consider how people can learn which opinions to trust in such scenarios by extending the hedge algorithm: a classic solution for learning from diverse information sources. We first introduce a semi-supervised variant we call the delusional hedge capable of learning from both <b>supervised</b> and <b>unsupervised</b> experiences. In two experiments, we examine the alignment between human judgments and predictions from the standard hedge, the delusional hedge, and a heuristic baseline model. Results indicate that humans effectively incorporate both labeled and unlabeled information in a manner consistent with the delusional hedge algorithm &ndash; suggesting that human learners not only gauge the accuracy of information sources but also their consistency with other reliable sources. The findings advance our understanding of human learning from diverse opinions, with implications for the development of algorithms that better capture how people learn to weigh conflicting information sources.</p></p class="citation"></blockquote><h3 id=58--247306-large-language-models-are-advanced-anonymizers-robin-staab-et-al-2024>(5/8 | 247/306) Large Language Models are Advanced Anonymizers (Robin Staab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robin Staab, Mark Vero, Mislav Balunović, Martin Vechev. (2024)<br><strong>Large Language Models are Advanced Anonymizers</strong><br><button class=copy-to-clipboard title="Large Language Models are Advanced Anonymizers" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-7, cs-AI, cs-CL, cs-CR, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13846v1.pdf filename=2402.13846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work in privacy research on <b>large</b> <b>language</b> <b>models</b> has shown that they achieve near human-level performance at inferring personal data from real-world online texts. With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. This raises the question of how individuals can effectively protect their personal data in sharing online texts. In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial <b>LLMs</b> inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. We then present our <b>LLM-based</b> adversarial anonymization framework leveraging the strong inferential capabilities of <b>LLMs</b> to inform our anonymization procedure. In our experimental evaluation, we show on real-world and synthetic online texts how adversarial anonymization outperforms current industry-grade anonymizers both in terms of the resulting utility and privacy.</p></p class="citation"></blockquote><h3 id=68--248306-a-neuro-symbolic-approach-to-multi-agent-rl-for-interpretability-and-probabilistic-decision-making-chitra-subramanian-et-al-2024>(6/8 | 248/306) A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and Probabilistic Decision Making (Chitra Subramanian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chitra Subramanian, Miao Liu, Naweed Khan, Jonathan Lenchner, Aporva Amarnath, Sarathkrishna Swaminathan, Ryan Riegel, Alexander Gray. (2024)<br><strong>A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and Probabilistic Decision Making</strong><br><button class=copy-to-clipboard title="A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and Probabilistic Decision Making" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-6, cs-AI, cs-NE, cs.AI<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13440v1.pdf filename=2402.13440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-agent <b>reinforcement</b> <b>learning</b> (MARL) is well-suited for runtime decision-making in optimizing the performance of systems where multiple agents coexist and compete for shared resources. However, applying common deep learning-based MARL solutions to real-world problems suffers from issues of interpretability, sample efficiency, partial observability, etc. To address these challenges, we present an event-driven formulation, where decision-making is handled by distributed co-operative MARL agents using neuro-symbolic methods. The recently introduced neuro-symbolic Logical Neural Networks (LNN) framework serves as a function approximator for the RL, to train a rules-based policy that is both logical and interpretable by construction. To enable decision-making under uncertainty and partial observability, we developed a novel probabilistic neuro-symbolic framework, Probabilistic Logical Neural Networks (PLNN), which combines the capabilities of logical <b>reasoning</b> with probabilistic graphical models. In PLNN, the upward/downward inference strategy, inherited from LNN, is coupled with belief bounds by setting the activation function for the logical operator associated with each neural network node to a probability-respecting generalization of the Fr'echet inequalities. These PLNN nodes form the unifying element that combines probabilistic logic and Bayes Nets, permitting inference for variables with unobserved states. We demonstrate our contributions by addressing key MARL challenges for power sharing in a system-on-chip application.</p></p class="citation"></blockquote><h3 id=78--249306-semirings-for-probabilistic-and-neuro-symbolic-logic-programming-vincent-derkinderen-et-al-2024>(7/8 | 249/306) Semirings for Probabilistic and Neuro-Symbolic Logic Programming (Vincent Derkinderen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincent Derkinderen, Robin Manhaeve, Pedro Zuidberg Dos Martires, Luc De Raedt. (2024)<br><strong>Semirings for Probabilistic and Neuro-Symbolic Logic Programming</strong><br><button class=copy-to-clipboard title="Semirings for Probabilistic and Neuro-Symbolic Logic Programming" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13782v1.pdf filename=2402.13782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of <b>probabilistic</b> <b>logic</b> programming (PLP) focuses on integrating <b>probabilistic</b> <b>models</b> into programming languages based on logic. Over the past 30 years, numerous languages and frameworks have been developed for modeling, inference and learning in <b>probabilistic</b> <b>logic</b> programs. While originally PLP focused on discrete probability, more recent approaches have incorporated continuous distributions as well as neural networks, effectively yielding neural-symbolic methods. We provide a unified algebraic perspective on PLP, showing that many if not most of the extensions of PLP can be cast within a common algebraic logic programming framework, in which facts are labeled with elements of a semiring and disjunction and conjunction are replaced by addition and multiplication. This does not only hold for the PLP variations itself but also for the underlying execution mechanism that is based on (algebraic) model counting.</p></p class="citation"></blockquote><h3 id=88--250306-mastering-the-game-of-guandan-with-deep-reinforcement-learning-and-behavior-regulating-yifan-yanggong-et-al-2024>(8/8 | 250/306) Mastering the Game of Guandan with Deep Reinforcement Learning and Behavior Regulating (Yifan Yanggong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Yanggong, Hao Pan, Lei Wang. (2024)<br><strong>Mastering the Game of Guandan with Deep Reinforcement Learning and Behavior Regulating</strong><br><button class=copy-to-clipboard title="Mastering the Game of Guandan with Deep Reinforcement Learning and Behavior Regulating" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13582v1.pdf filename=2402.13582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Games are a simplified model of reality and often serve as a favored platform for Artificial Intelligence (AI) research. Much of the research is concerned with game-playing agents and their decision making processes. The game of Guandan (literally, &ldquo;throwing eggs&rdquo;) is a challenging game where even professional human players struggle to make the right decision at times. In this paper we propose a framework named GuanZero for AI agents to master this game using Monte-Carlo methods and deep neural networks. The main contribution of this paper is about regulating agents&rsquo; behavior through a carefully designed neural network encoding scheme. We then demonstrate the effectiveness of the proposed framework by comparing it with state-of-the-art approaches.</p></p class="citation"></blockquote><h2 id=eessiv-3>eess.IV (3)</h2><h3 id=13--251306-exact-netehr-guided-lung-tumor-auto-segmentation-for-non-small-cell-lung-cancer-radiotherapy-hamed-hooshangnejad-et-al-2024>(1/3 | 251/306) EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy (Hamed Hooshangnejad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamed Hooshangnejad, Xue Feng, Gaofeng Huang, Rui Zhang, Quan Chen, Kai Ding. (2024)<br><strong>EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy</strong><br><button class=copy-to-clipboard title="EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, physics-med-ph<br>Keyword Score: 40<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14099v1.pdf filename=2402.14099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lung cancer is a devastating disease with the highest mortality rate among cancer types. Over 60% of non-small cell lung cancer (NSCLC) patients, which accounts for 87% of diagnoses, require radiation therapy. Rapid treatment initiation significantly increases the patient&rsquo;s survival rate and reduces the mortality rate. Accurate tumor segmentation is a critical step in the diagnosis and treatment of NSCLC. Manual segmentation is time and labor-consuming and causes delays in treatment initiation. Although many lung nodule detection methods, including deep learning-based models, have been proposed, there is still a long-standing problem of high false positives (FPs) with most of these methods. Here, we developed an electronic health record (EHR) guided lung tumor auto-segmentation called EXACT-Net (EHR-enhanced eXACtitude in Tumor segmentation), where the extracted information from EHRs using a pre-trained <b>large</b> <b>language</b> <b>model</b> <b>(LLM),</b> was used to remove the FPs and keep the TP nodules only. The auto-segmentation model was trained on NSCLC patients&rsquo; computed tomography (CT), and the pre-trained <b>LLM</b> was used with the <b>zero-shot</b> <b>learning</b> approach. Our approach resulted in a 250% boost in successful nodule detection using the data from ten NSCLC patients treated in our institution.</p></p class="citation"></blockquote><h3 id=23--252306-adversarial-purification-and-fine-tuning-for-robust-udc-image-restoration-zhenbo-song-et-al-2024>(2/3 | 252/306) Adversarial Purification and Fine-tuning for Robust UDC Image Restoration (Zhenbo Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenbo Song, Zhenyuan Zhang, Kaihao Zhang, Wenhan Luo, Zhaoxin Fan, Jianfeng Lu. (2024)<br><strong>Adversarial Purification and Fine-tuning for Robust UDC Image Restoration</strong><br><button class=copy-to-clipboard title="Adversarial Purification and Fine-tuning for Robust UDC Image Restoration" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 35<br>Keywords: Black Box, Fine-tuning, Adversarial Attack, Adversarial Purification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13629v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13629v1.pdf filename=2402.13629v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study delves into the enhancement of Under-Display Camera (UDC) image restoration models, focusing on their robustness against <b>adversarial</b> <b>attacks.</b> Despite its innovative approach to seamless display integration, UDC technology faces unique image degradation challenges exacerbated by the susceptibility to <b>adversarial</b> <b>perturbations.</b> Our research initially conducts an in-depth robustness evaluation of deep-learning-based UDC image restoration models by employing several white-box and <b>black-box</b> <b>attacking</b> methods. This evaluation is pivotal in understanding the vulnerabilities of current UDC image restoration techniques. Following the assessment, we introduce a defense framework integrating <b>adversarial</b> <b>purification</b> with subsequent <b>fine-tuning</b> processes. First, our approach employs diffusion-based <b>adversarial</b> <b>purification,</b> effectively neutralizing <b>adversarial</b> <b>perturbations.</b> Then, we apply the <b>fine-tuning</b> methodologies to refine the image restoration models further, ensuring that the quality and fidelity of the restored images are maintained. The effectiveness of our proposed approach is validated through extensive experiments, showing marked improvements in resilience against typical <b>adversarial</b> <b>attacks.</b></p></p class="citation"></blockquote><h3 id=33--253306-cas-diffcom-cascaded-diffusion-model-for-infant-longitudinal-super-resolution-3d-medical-image-completion-lianghu-guo-et-al-2024>(3/3 | 253/306) Cas-DiffCom: Cascaded diffusion model for infant longitudinal super-resolution 3D medical image completion (Lianghu Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lianghu Guo, Tianli Tao, Xinyi Cai, Zihao Zhu, Jiawei Huang, Lixuan Zhu, Zhuoyang Gu, Haifeng Tang, Rui Zhou, Siyan Han, Yan Liang, Qing Yang, Dinggang Shen, Han Zhang. (2024)<br><strong>Cas-DiffCom: Cascaded diffusion model for infant longitudinal super-resolution 3D medical image completion</strong><br><button class=copy-to-clipboard title="Cas-DiffCom: Cascaded diffusion model for infant longitudinal super-resolution 3D medical image completion" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13776v1.pdf filename=2402.13776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Early infancy is a rapid and dynamic neurodevelopmental period for behavior and neurocognition. Longitudinal magnetic resonance imaging (MRI) is an effective tool to investigate such a crucial stage by capturing the developmental trajectories of the brain structures. However, longitudinal MRI acquisition always meets a serious data-missing problem due to participant dropout and failed scans, making longitudinal infant brain atlas construction and developmental trajectory delineation quite challenging. Thanks to the development of an AI-based generative model, neuroimage completion has become a powerful technique to retain as much available data as possible. However, current image completion methods usually suffer from inconsistency within each individual subject in the time dimension, compromising the overall quality. To solve this problem, our paper proposed a two-stage cascaded <b>diffusion</b> <b>model,</b> Cas-DiffCom, for dense and longitudinal 3D infant brain MRI completion and super-resolution. We applied our proposed method to the Baby Connectome Project (BCP) dataset. The experiment results validate that Cas-DiffCom achieves both individual consistency and high fidelity in longitudinal infant brain image completion. We further applied the generated infant brain images to two downstream tasks, brain tissue segmentation and developmental trajectory delineation, to declare its task-oriented potential in the neuroscience field.</p></p class="citation"></blockquote><h2 id=csse-6>cs.SE (6)</h2><h3 id=16--254306-test-driven-development-for-code-generation-noble-saji-mathews-et-al-2024>(1/6 | 254/306) Test-Driven Development for Code Generation (Noble Saji Mathews et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noble Saji Mathews, Meiyappan Nagappan. (2024)<br><strong>Test-Driven Development for Code Generation</strong><br><button class=copy-to-clipboard title="Test-Driven Development for Code Generation" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: GPT-4, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13521v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13521v1.pdf filename=2402.13521v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>GPT4,</b> have shown proficiency in generating <b>code</b> <b>snippets</b> from problem statements. Traditionally software development by humans followed a similar methodology of writing <b>code</b> <b>from</b> problem statements or requirements. However, in the past, there have been several studies that have shown the value of test-driven development (TDD) where humans write tests based on problem statements before the <b>code</b> <b>for</b> the functionality is written. In the context of <b>LLM-based</b> <b>code</b> <b>generation,</b> one obvious benefit of TDD is that the developer then knows for sure if the generated <b>code</b> <b>has</b> passed all the given tests or not. Therefore, in this paper, we want to empirically evaluate the hypothesis: giving the problem statements and tests as input to <b>GPT4</b> is better than just giving the problem statement as input. To test our hypothesis, we build a framework TGen. In our experiments on the MBPP, HumanEval and CodeChef datasets, we consistently find that including tests solves more programming problems than not including them. Thus we show that TDD is a better development model than just using a problem statement when using <b>GPT4</b> for <b>code</b> <b>generation</b> tasks.</p></p class="citation"></blockquote><h3 id=26--255306-ritfis-robust-input-testing-framework-for-llms-based-intelligent-software-mingxuan-xiao-et-al-2024>(2/6 | 255/306) RITFIS: Robust input testing framework for LLMs-based intelligent software (Mingxuan Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingxuan Xiao, Yan Xiao, Hai Dong, Shunhui Ji, Pengcheng Zhang. (2024)<br><strong>RITFIS: Robust input testing framework for LLMs-based intelligent software</strong><br><button class=copy-to-clipboard title="RITFIS: Robust input testing framework for LLMs-based intelligent software" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13518v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13518v1.pdf filename=2402.13518v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The dependence of Natural Language Processing (NLP) intelligent software on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is increasingly prominent, underscoring the necessity for robustness testing. Current testing methods focus solely on the robustness of <b>LLM-based</b> software to <b>prompts.</b> Given the complexity and diversity of real-world inputs, studying the robustness of LLMbased software in handling comprehensive inputs (including <b>prompts</b> and examples) is crucial for a thorough understanding of its performance. To this end, this paper introduces RITFIS, a Robust Input Testing Framework for <b>LLM-based</b> Intelligent Software. To our knowledge, RITFIS is the first framework designed to assess the robustness of <b>LLM-based</b> intelligent software against natural language inputs. This framework, based on given threat models and <b>prompts,</b> primarily defines the testing process as a combinatorial optimization problem. Successful test cases are determined by a goal function, creating a transformation space for the original examples through perturbation means, and employing a series of search methods to filter cases that meet both the testing objectives and language constraints. RITFIS, with its modular design, offers a comprehensive method for evaluating the robustness of LLMbased intelligent software. RITFIS adapts 17 automated testing methods, originally designed for Deep Neural Network (DNN)-based intelligent software, to the <b>LLM-based</b> software testing scenario. It demonstrates the effectiveness of RITFIS in evaluating <b>LLM-based</b> intelligent software through empirical validation. However, existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models. Therefore, we conducted a comprehensive analysis based on five metrics and provided insightful testing method optimization strategies, benefiting both researchers and everyday users.</p></p class="citation"></blockquote><h3 id=36--256306-eyetrans-merging-human-and-machine-attention-for-neural-code-summarization-yifan-zhang-et-al-2024>(3/6 | 256/306) EyeTrans: Merging Human and Machine Attention for Neural Code Summarization (Yifan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Zhang, Jiliang Li, Zachary Karas, Aakash Bansal, Toby Jia-Jun Li, Collin McMillan, Kevin Leach, Yu Huang. (2024)<br><strong>EyeTrans: Merging Human and Machine Attention for Neural Code Summarization</strong><br><button class=copy-to-clipboard title="EyeTrans: Merging Human and Machine Attention for Neural Code Summarization" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-HC, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Transformer, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14096v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14096v2.pdf filename=2402.14096v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural code <b>summarization</b> leverages deep learning models to automatically generate brief natural language summaries of code snippets. The development of <b>Transformer</b> models has led to extensive use of attention during model design. While existing work has primarily and almost exclusively focused on static properties of source code and related structural representations like the Abstract Syntax Tree (AST), few studies have considered human attention, that is, where programmers focus while examining and comprehending code. In this paper, we develop a method for incorporating human attention into machine attention to enhance neural code <b>summarization.</b> To facilitate this incorporation and vindicate this hypothesis, we introduce EyeTrans, which consists of three steps: (1) we conduct an extensive eye-tracking human study to collect and pre-analyze data for model training, (2) we devise a data-centric approach to integrate human attention with machine attention in the <b>Transformer</b> architecture, and (3) we conduct comprehensive experiments on two code <b>summarization</b> tasks to demonstrate the effectiveness of incorporating human attention into <b>Transformers.</b> Integrating human attention leads to an improvement of up to 29.91% in Functional <b>Summarization</b> and up to 6.39% in General Code <b>Summarization</b> performance, demonstrating the substantial benefits of this combination. We further explore performance in terms of robustness and efficiency by creating challenging <b>summarization</b> scenarios in which EyeTrans exhibits interesting properties. We also visualize the attention map to depict the simplifying effect of machine attention in the <b>Transformer</b> by incorporating human attention. This work has the potential to propel AI research in software engineering by introducing more human-centered approaches and data.</p></p class="citation"></blockquote><h3 id=46--257306-using-large-language-models-for-natural-language-processing-tasks-in-requirements-engineering-a-systematic-guideline-andreas-vogelsang-et-al-2024>(4/6 | 257/306) Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline (Andreas Vogelsang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Vogelsang, Jannik Fischbach. (2024)<br><strong>Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline</strong><br><button class=copy-to-clipboard title="Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13823v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13823v2.pdf filename=2402.13823v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To use <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in a targeted way for NLP problems in RE, we require both (1) basic knowledge about the inner workings of <b>LLMs</b> and (2) a guideline on how to select and systematically utilize or repurpose <b>LLMs</b> for NLP4RE tasks. This chapter establishes the required knowledge and introduces the fundamentals of <b>LLMs</b> in the first part. In the second part, we present a detailed guideline for students, researchers, and practitioners on using <b>LLMs</b> for their purposes.</p></p class="citation"></blockquote><h3 id=56--258306-green-ai-a-preliminary-empirical-study-on-energy-consumption-in-dl-models-across-different-runtime-infrastructures-negar-alizadeh-et-al-2024>(5/6 | 258/306) Green AI: A Preliminary Empirical Study on Energy Consumption in DL Models Across Different Runtime Infrastructures (Negar Alizadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Negar Alizadeh, Fernando Castor. (2024)<br><strong>Green AI: A Preliminary Empirical Study on Energy Consumption in DL Models Across Different Runtime Infrastructures</strong><br><button class=copy-to-clipboard title="Green AI: A Preliminary Empirical Study on Energy Consumption in DL Models Across Different Runtime Infrastructures" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13640v1.pdf filename=2402.13640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning (DL) frameworks such as PyTorch and TensorFlow include runtime infrastructures responsible for executing trained models on target hardware, managing memory, data transfers, and multi-accelerator execution, if applicable. Additionally, it is a common practice to deploy pre-trained models on environments distinct from their native development settings. This led to the introduction of interchange formats such as ONNX, which includes its runtime infrastructure, and ONNX Runtime, which work as standard formats that can be used across diverse DL frameworks and languages. Even though these runtime infrastructures have a great impact on inference performance, no previous paper has investigated their energy efficiency. In this study, we monitor the energy consumption and inference time in the runtime infrastructures of three well-known DL frameworks as well as ONNX, using three various DL models. To have nuance in our investigation, we also examine the impact of using different execution providers. We find out that the performance and energy efficiency of DL are difficult to predict. One framework, MXNet, outperforms both PyTorch and TensorFlow for the computer vision models using batch size 1, due to efficient GPU usage and thus low CPU usage. However, batch size 64 makes PyTorch and MXNet practically indistinguishable, while TensorFlow is outperformed consistently. For <b>BERT,</b> PyTorch exhibits the best performance. Converting the models to ONNX usually yields significant performance improvements but the ONNX converted ResNet model with batch size 64 consumes approximately 10% more energy and time than the original PyTorch model.</p></p class="citation"></blockquote><h3 id=66--259306-restruler-towards-automatically-identifying-violations-of-restful-design-rules-in-web-apis-justus-bogner-et-al-2024>(6/6 | 259/306) RESTRuler: Towards Automatically Identifying Violations of RESTful Design Rules in Web APIs (Justus Bogner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Justus Bogner, Sebastian Kotstein, Daniel Abajirov, Timothy Ernst, Manuel Merkel. (2024)<br><strong>RESTRuler: Towards Automatically Identifying Violations of RESTful Design Rules in Web APIs</strong><br><button class=copy-to-clipboard title="RESTRuler: Towards Automatically Identifying Violations of RESTful Design Rules in Web APIs" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13710v1.pdf filename=2402.13710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>RESTful APIs based on HTTP are one of the most important ways to make data and functionality available to applications and software services. However, the quality of the API design strongly impacts API understandability and usability, and many rules have been specified for this. While we have evidence for the effectiveness of many design rules, it is still difficult for practitioners to identify rule violations in their design. We therefore present RESTRuler, a Java-based open-source tool that uses static analysis to detect design rule violations in OpenAPI descriptions. The current prototype supports 14 rules that go beyond simple syntactic checks and partly rely on natural language processing. The modular architecture also makes it easy to implement new rules. To evaluate RESTRuler, we conducted a <b>benchmark</b> with over 2,300 public OpenAPI descriptions and asked 7 API experts to construct 111 complicated rule violations. For robustness, RESTRuler successfully analyzed 99% of the used real-world OpenAPI definitions, with some failing due to excessive size. For performance efficiency, the tool performed well for the majority of files and could analyze 84% in less than 23 seconds with low CPU and RAM usage. Lastly, for effectiveness, RESTRuler achieved a precision of 91% (ranging from 60% to 100% per rule) and recall of 68% (ranging from 46% to 100%). Based on these variations between rule implementations, we identified several opportunities for improvements. While RESTRuler is still a research prototype, the evaluation suggests that the tool is quite robust to errors, resource-efficient for most APIs, and shows good precision and decent recall. Practitioners can use it to improve the quality of their API design.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--260306-localtweets-to-localhealth-a-mental-health-surveillance-framework-based-on-twitter-data-vijeta-deshpande-et-al-2024>(1/2 | 260/306) LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data (Vijeta Deshpande et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian Gibbons, Hong Yu. (2024)<br><strong>LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data</strong><br><button class=copy-to-clipboard title="LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CL, cs-LG, cs-SI, cs.SI<br>Keyword Score: 33<br>Keywords: Benchmarking, Zero-shot, GPT-3, GPT-3.5<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13452v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13452v1.pdf filename=2402.13452v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems. In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes. We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA. We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a <b>benchmark</b> dataset, LocalTweets. With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems. We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets. When used with <b>GPT3.5,</b> LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78%, respectively, a 59% improvement in F1-score over the <b>GPT3.5</b> in <b>zero-shot</b> setting. We also utilize LocalHealth to extrapolate CDC&rsquo;s estimates to proxy unreported neighborhoods, achieving an F1-score of 0.7291. Our work suggests that Twitter data can be effectively leveraged to simulate neighborhood-level MH outcomes.</p></p class="citation"></blockquote><h3 id=22--261306-fairness-rising-from-the-ranks-hits-and-pagerank-on-homophilic-networks-ana-andreea-stoica-et-al-2024>(2/2 | 261/306) Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks (Ana-Andreea Stoica et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ana-Andreea Stoica, Nelly Litvak, Augustin Chaintreau. (2024)<br><strong>Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks</strong><br><button class=copy-to-clipboard title="Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-IR, cs-SI, cs.SI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13787v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13787v2.pdf filename=2402.13787v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the conditions under which link analysis algorithms prevent minority groups from reaching high ranking slots. We find that the most common link-based algorithms using centrality metrics, such as PageRank and HITS, can reproduce and even amplify bias against minority groups in networks. Yet, their behavior differs: one one hand, we empirically show that PageRank mirrors the degree distribution for most of the ranking positions and it can equalize representation of minorities among the top ranked nodes; on the other hand, we find that HITS amplifies pre-existing bias in homophilic networks through a novel theoretical analysis, supported by empirical results. We find the root cause of bias amplification in HITS to be the level of homophily present in the network, modeled through an evolving network model with two communities. We illustrate our theoretical analysis on both synthetic and real datasets and we present directions for future work.</p></p class="citation"></blockquote><h2 id=physicsgeo-ph-1>physics.geo-ph (1)</h2><h3 id=11--262306-neural-networks-and-friction-slide-hold-learn-joaquin-garcia-suarez-2024>(1/1 | 262/306) Neural Networks and Friction: Slide, Hold, Learn (Joaquin Garcia-Suarez, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joaquin Garcia-Suarez. (2024)<br><strong>Neural Networks and Friction: Slide, Hold, Learn</strong><br><button class=copy-to-clipboard title="Neural Networks and Friction: Slide, Hold, Learn" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.geo-ph<br>Categories: cs-LG, physics-geo-ph, physics.geo-ph<br>Keyword Score: 30<br>Keywords: Graph Attention Networks, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14148v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14148v2.pdf filename=2402.14148v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, it is demonstrated that <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs),</b> specifically those utilizing <b>Gated</b> <b>Recurrent</b> <b>Unit</b> <b>(GRU)</b> architecture, possess the capability to learn the complex dynamics of rate-and-state friction laws from synthetic data. The data employed for training the network is generated through the application of traditional rate-and-state friction equations coupled with the aging law for state evolution. A novel aspect of our approach is the formulation of a loss function that explicitly accounts for initial conditions, the direct effect, and the evolution of state variables during training. It is found that the <b>RNN,</b> with its GRU architecture, effectively learns to predict changes in the friction coefficient resulting from velocity jumps, thereby showcasing the potential of machine learning models in understanding and simulating the physics of frictional processes.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--263306-reinforcement-learning-assisted-quantum-architecture-search-for-variational-quantum-algorithms-akash-kundu-2024>(1/1 | 263/306) Reinforcement learning-assisted quantum architecture search for variational quantum algorithms (Akash Kundu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akash Kundu. (2024)<br><strong>Reinforcement learning-assisted quantum architecture search for variational quantum algorithms</strong><br><button class=copy-to-clipboard title="Reinforcement learning-assisted quantum architecture search for variational quantum algorithms" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, cs-LG, quant-ph, quant-ph<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13754v1.pdf filename=2402.13754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms <b>(VQAs),</b> a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of <b>VQAs</b> depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of <b>VQAs</b> by automating the search for an optimal structure for the variational circuits using <b>reinforcement</b> <b>learning</b> (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accuracy in solving the given problem. The task of automating the search for optimal quantum circuits is known as quantum architecture search <b>(QAS).</b> The majority of research in <b>QAS</b> is primarily focused on a noiseless scenario. Yet, the impact of noise on the <b>QAS</b> remains inadequately explored. In this thesis, we tackle the issue by introducing a tensor-based quantum circuit encoding, restrictions on environment dynamics to explore the search space of possible circuits efficiently, an episode halting scheme to steer the agent to find shorter circuits, a double deep Q-network (DDQN) with an $\epsilon$-greedy policy for better stability. The numerical experiments on noiseless and noisy quantum hardware show that in dealing with various <b>VQAs,</b> our RL-based <b>QAS</b> outperforms existing <b>QAS.</b> Meanwhile, the methods we propose in the thesis can be readily adapted to address a wide range of other <b>VQAs.</b></p></p class="citation"></blockquote><h2 id=astro-phep-1>astro-ph.EP (1)</h2><h3 id=11--264306-computing-transiting-exoplanet-parameters-with-1d-convolutional-neural-networks-santiago-iglesias-álvarez-et-al-2024>(1/1 | 264/306) Computing Transiting Exoplanet Parameters with 1D Convolutional Neural Networks (Santiago Iglesias Álvarez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Santiago Iglesias Álvarez, Enrique Díez Alonso, María Luisa Sánchez Rodríguez, Javier Rodríguez Rodríguez, Saúl Pérez Fernández, Francisco Javier de Cos Juez. (2024)<br><strong>Computing Transiting Exoplanet Parameters with 1D Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Computing Transiting Exoplanet Parameters with 1D Convolutional Neural Networks" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.EP<br>Categories: astro-ph-EP, astro-ph-IM, astro-ph.EP, cs-LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13673v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13673v1.pdf filename=2402.13673v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The transit method allows the detection and characterization of planetary systems by analyzing stellar light curves. <b>Convolutional</b> <b>neural</b> <b>networks</b> appear to offer a viable solution for automating these analyses. In this research, two 1D <b>convolutional</b> <b>neural</b> <b>network</b> models, which work with simulated light curves in which transit-like signals were injected, are presented. One model operates on complete light curves and estimates the orbital period, and the other one operates on phase-folded light curves and estimates the semimajor axis of the orbit and the square of the planet-to-star radius ratio. Both models were tested on real data from TESS light curves with confirmed planets to ensure that they are able to work with real data. The results obtained show that 1D <b>CNNs</b> are able to characterize transiting exoplanets from their host star&rsquo;s detrended light curve and, furthermore, reducing both the required time and computational costs compared with the current detection and characterization algorithms.</p></p class="citation"></blockquote><h2 id=eesssy-6>eess.SY (6)</h2><h3 id=16--265306-pi-cof-a-bilevel-optimization-framework-for-solving-active-learning-problems-using-physics-information-liqiu-dong-et-al-2024>(1/6 | 265/306) PI-CoF: A Bilevel Optimization Framework for Solving Active Learning Problems using Physics-Information (Liqiu Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liqiu Dong, Marta Zagorowska, Tong Liu, Alex Durkin, Mehmet Mercangöz. (2024)<br><strong>PI-CoF: A Bilevel Optimization Framework for Solving Active Learning Problems using Physics-Information</strong><br><button class=copy-to-clipboard title="PI-CoF: A Bilevel Optimization Framework for Solving Active Learning Problems using Physics-Information" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Active Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13588v1.pdf filename=2402.13588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physics informed neural networks (PINNs) have recently been proposed as surrogate models for solving process optimization problems. However, in an <b>active</b> <b>learning</b> setting collecting enough data for reliably training PINNs poses a challenge. This study proposes a broadly applicable method for incorporating physics information into existing machine learning (ML) models of any type. The proposed method - referred to as PI-CoF for Physics-Informed Correction Factors - introduces additive or multiplicative correction factors for pointwise inference, which are identified by solving a regularized unconstrained optimization problem for reconciliation of physics information and ML model predictions. When ML models are used in an optimization context, using the proposed approach translates into a bilevel optimization problem, where the reconciliation problem is solved as an inner problem each time before evaluating the objective and constraint functions of the outer problem. The utility of the proposed approach is demonstrated through a numerical example, emphasizing constraint satisfaction in a safe Bayesian optimization (BO) setting. Furthermore, a <b>simulation</b> study is carried out by using PI-CoF for the real-time optimization of a fuel cell system. The results show reduced fuel consumption and better reference tracking performance when using the proposed PI-CoF approach in comparison to a constrained BO algorithm not using physics information.</p></p class="citation"></blockquote><h3 id=26--266306-driving-towards-stability-and-efficiency-a-variable-time-gap-strategy-for-adaptive-cruise-control-shaimaa-k-el-baklish-et-al-2024>(2/6 | 266/306) Driving Towards Stability and Efficiency: A Variable Time Gap Strategy for Adaptive Cruise Control (Shaimaa K. El-Baklish et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaimaa K. El-Baklish, Anastasios Kouvelas, Michail A. Makridis. (2024)<br><strong>Driving Towards Stability and Efficiency: A Variable Time Gap Strategy for Adaptive Cruise Control</strong><br><button class=copy-to-clipboard title="Driving Towards Stability and Efficiency: A Variable Time Gap Strategy for Adaptive Cruise Control" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14110v1.pdf filename=2402.14110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated vehicle technologies offer a promising avenue for enhancing traffic efficiency, safety, and energy consumption. Among these, Adaptive Cruise Control (ACC) systems stand out as a prevalent form of automation on today&rsquo;s roads, with their time gap settings holding paramount importance. While decreasing the average time headway tends to enhance traffic capacity, it simultaneously raises concerns regarding safety and string stability. This study introduces a novel variable time gap feedback control policy aimed at striking a balance between maintaining a minimum time gap setting under equilibrium car-following conditions, thereby improving traffic capacity, while ensuring string stability to mitigate disturbances away from the equilibrium flow. Leveraging nonlinear $H_\infty$ control technique, the strategy employs a variable time gap component as the manipulated control signal, complemented by a constant time gap component that predominates during car-following equilibrium. The effectiveness of the proposed scheme is evaluated against its constant time-gap counterpart calibrated using field platoon data from the OpenACC dataset. Through numerical and traffic <b>simulations,</b> our findings illustrate that the proposed algorithm effectively dampens perturbations within vehicle platoons, leading to a more efficient and safer mixed traffic flow.</p></p class="citation"></blockquote><h3 id=36--267306-improving-a-proportional-integral-controller-with-reinforcement-learning-on-a-throttle-valve-benchmark-paul-daoudi-et-al-2024>(3/6 | 267/306) Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark (Paul Daoudi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Daoudi, Bojan Mavkov, Bogdan Robu, Christophe Prieur, Emmanuel Witrant, Merwan Barlier, Ludovic Dos Santos. (2024)<br><strong>Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark</strong><br><button class=copy-to-clipboard title="Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13654v1.pdf filename=2402.13654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a learning-based control strategy for non-linear throttle valves with an asymmetric hysteresis, leading to a near-optimal controller without requiring any prior knowledge about the environment. We start with a carefully tuned Proportional Integrator (PI) controller and exploit the recent advances in <b>Reinforcement</b> <b>Learning</b> (RL) with Guides to improve the closed-loop behavior by learning from the additional interactions with the valve. We test the proposed control method in various scenarios on three different valves, all highlighting the benefits of combining both PI and RL frameworks to improve control performance in non-linear stochastic systems. In all the experimental test cases, the resulting agent has a better sample efficiency than traditional RL agents and outperforms the PI controller.</p></p class="citation"></blockquote><h3 id=46--268306-extending-identifiability-results-from-isolated-networks-to-embedded-networks-eduardo-mapurunga-et-al-2024>(4/6 | 268/306) Extending identifiability results from isolated networks to embedded networks (Eduardo Mapurunga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eduardo Mapurunga, Michel Gevers, Alexandre S. Bazanella. (2024)<br><strong>Extending identifiability results from isolated networks to embedded networks</strong><br><button class=copy-to-clipboard title="Extending identifiability results from isolated networks to embedded networks" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14144v1.pdf filename=2402.14144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper deals with the design of Excitation and Measurement Patterns (EMPs) for the identification of dynamical networks, when the objective is to identify only a subnetwork embedded in a larger network. Recent results have shown how to construct EMPs that guarantee identifiability for a range of networks with specific <b>graph</b> topologies, such as trees, loops, or Directed Acyclic <b>Graphs</b> (DAGs). However, an EMP that is valid for the identification of a subnetwork taken in isolation may no longer be valid when that subnetwork is embedded in a larger network. Our main contribution is to exhibit conditions under which it does remain valid, and to propose ways to enhance such EMP when these conditions are not satisfied.</p></p class="citation"></blockquote><h3 id=56--269306-a-monolithic-cybersecurity-architecture-for-power-electronic-systems-kirti-gupta-et-al-2024>(5/6 | 269/306) A Monolithic Cybersecurity Architecture for Power Electronic Systems (Kirti Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kirti Gupta, Subham Sahoo, Bijaya Ketan Panigrahi. (2024)<br><strong>A Monolithic Cybersecurity Architecture for Power Electronic Systems</strong><br><button class=copy-to-clipboard title="A Monolithic Cybersecurity Architecture for Power Electronic Systems" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13617v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13617v2.pdf filename=2402.13617v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Power electronic systems (PES) face significant threats from various data availability and integrity attacks, significantly affecting the performance of communication networks and power system operation. As a result, several attack detection and reconstruction techniques are deployed, which makes it a costly & complex cybersecurity operational platform with significant room for incremental extensions for mitigation against future threats. Unlike the said traditional arrangements, our paper introduces a foundational approach by establishing a monolithic cybersecurity architecture (MCA) via incorporating semantic principles into the sampling process for distributed energy resources (DERs). This unified approach concurrently compensates for the intrusion challenges posed by cyber attacks by reconstructing signals using the dynamics of the inner control layer. This reconstruction considers essential semantic attributes, like Priority, Freshness, and Relevance to ensure resilient dynamic performance. Hence, the proposed scheme promises a generalized route to concurrently tackle a global set of cyber attacks in elevating the resilience of PES. Finally, rigorous validation on a modified IEEE 69-bus distribution system and a real-world South California Edison (SCE) 47-bus network, using OPAL-RT under diverse operating conditions, underscores its robustness, model-free design capability, scalability, and adaptability to dynamic cyber <b>graphs</b> and system reconfiguration.</p></p class="citation"></blockquote><h3 id=66--270306-delay-aware-semantic-sampling-in-power-electronic-systems-kirti-gupta-et-al-2024>(6/6 | 270/306) Delay-Aware Semantic Sampling in Power Electronic Systems (Kirti Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kirti Gupta, Subham Sahoo, Bijaya Ketan Panigrahi. (2024)<br><strong>Delay-Aware Semantic Sampling in Power Electronic Systems</strong><br><button class=copy-to-clipboard title="Delay-Aware Semantic Sampling in Power Electronic Systems" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13586v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13586v2.pdf filename=2402.13586v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In power electronic systems (PES), attacks on data availability such as latency attacks, data dropouts, and time-synchronization attacks (TSAs) continue to pose significant threats to both the communication network and the control system performance. As per the conventional norms of communication engineering, PES still rely on time synchronized sampling, which translates every received message with equal importance. In this paper, we go beyond event-triggered sampling/estimation to integrate semantic principles into the sampling process for each distributed energy resource (DER), which not only compensates for delayed communicated signals by reconstruction of a new signal from the inner control layer dynamics, but also evaluates the reconstruction stage using key semantic requirements, namely Freshness, Relevance and Priority for good dynamic performance. As a result, the sparsity provided by event-driven sampling of internal control loop dynamics translates as semantics in PES. The proposed scheme has been extensively tested and validated on a modified IEEE 37-bus AC distribution system, under many operating conditions and noisy environment in OPAL-RT environment to establish its robustness, model-free design ability and adaptive behavior to dynamic cyber <b>graph</b> topologies.</p></p class="citation"></blockquote><h2 id=csms-1>cs.MS (1)</h2><h3 id=11--271306-democratizing-uncertainty-quantification-linus-seelinger-et-al-2024>(1/1 | 271/306) Democratizing Uncertainty Quantification (Linus Seelinger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linus Seelinger, Anne Reinarz, Mikkel B. Lykkegaard, Amal Mohammed A. Alghamdi, David Aristoff, Wolfgang Bangerth, Jean Bénézech, Matteo Diez, Kurt Frey, John D. Jakeman, Jakob Sauer Jørgensen, Ki-Tae Kim, Massimiliano Martinelli, Matthew Parno, Riccardo Pellegrini, Noemi Petra, Nicolai A. B. Riis, Katherine Rosenfeld, Andrea Serani, Lorenzo Tamellini, Umberto Villa, Tim J. Dodwell, Robert Scheichl. (2024)<br><strong>Democratizing Uncertainty Quantification</strong><br><button class=copy-to-clipboard title="Democratizing Uncertainty Quantification" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MS<br>Categories: cs-MS, cs.MS, stat-AP<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13768v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13768v2.pdf filename=2402.13768v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncertainty Quantification (UQ) is vital to safety-critical model-based analyses, but the widespread adoption of sophisticated UQ methods is limited by technical complexity. In this paper, we introduce UM-Bridge (the UQ and Modeling Bridge), a high-level abstraction and software protocol that facilitates universal interoperability of UQ software with <b>simulation</b> codes. It breaks down the technical complexity of advanced UQ applications and enables separation of concerns between experts. UM-Bridge democratizes UQ by allowing effective interdisciplinary collaboration, accelerating the development of advanced UQ methods, and making it easy to perform UQ analyses from prototype to High Performance Computing (HPC) scale. In addition, we present a library of ready-to-run UQ <b>benchmark</b> problems, all easily accessible through UM-Bridge. These <b>benchmarks</b> support UQ methodology research, enabling reproducible performance comparisons. We demonstrate UM-Bridge with several scientific applications, harnessing HPC resources even using UQ codes not designed with HPC support.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--272306-agentscope-a-flexible-yet-robust-multi-agent-platform-dawei-gao-et-al-2024>(1/1 | 272/306) AgentScope: A Flexible yet Robust Multi-Agent Platform (Dawei Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dawei Gao, Zitao Li, Weirui Kuang, Xuchen Pan, Daoyuan Chen, Zhijian Ma, Bingchen Qian, Liuyi Yao, Lin Zhu, Chen Cheng, Hongzhu Shi, Yaliang Li, Bolin Ding, Jingren Zhou. (2024)<br><strong>AgentScope: A Flexible yet Robust Multi-Agent Platform</strong><br><button class=copy-to-clipboard title="AgentScope: A Flexible yet Robust Multi-Agent Platform" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 23<br>Keywords: Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14034v1.pdf filename=2402.14034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid advancement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> significant progress has been made in multi-agent applications. However, the complexities in coordinating agents&rsquo; cooperation and <b>LLMs&rsquo;</b> erratic performance pose notable challenges in developing robust and efficient multi-agent applications. To tackle these challenges, we propose AgentScope, a developer-centric multi-agent platform with message exchange as its core communication mechanism. Together with abundant syntactic tools, built-in resources, and user-friendly interactions, our communication mechanism significantly reduces the barriers to both development and understanding. Towards robust and flexible multi-agent application, AgentScope provides both built-in and customizable fault tolerance mechanisms while it is also armed with system-level supports for <b>multi-modal</b> data generation, storage and transmission. Additionally, we design an actor-based distribution framework, enabling easy conversion between local and distributed deployments and automatic parallel optimization without extra effort. With these features, AgentScope empowers developers to build applications that fully realize the potential of intelligent agents. We have released AgentScope at <a href=https://github.com/modelscope/agentscope>https://github.com/modelscope/agentscope</a>, and hope AgentScope invites wider participation and innovation in this fast-moving field.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--273306-what-is-the-focus-of-xai-in-ui-design-prioritizing-ui-design-principles-for-enhancing-xai-user-experience-dian-lei-et-al-2024>(1/3 | 273/306) What is the focus of XAI in UI design? Prioritizing UI design principles for enhancing XAI user experience (Dian Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dian Lei, Yao He, Jianyou Zeng. (2024)<br><strong>What is the focus of XAI in UI design? Prioritizing UI design principles for enhancing XAI user experience</strong><br><button class=copy-to-clipboard title="What is the focus of XAI in UI design? Prioritizing UI design principles for enhancing XAI user experience" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Explainable AI, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13939v1.pdf filename=2402.13939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the widespread application of artificial intelligence(AI), the <b>explainable</b> <b>AI</b> (XAI) field has undergone a notable resurgence. In this background, the importance of user experience in XAI has become increasingly prominent. Simultaneously, the user interface (UI) serves as a crucial link between XAI and users. However, despite the existence of UI design principles for XAI, there is a lack of prioritization based on their significance. This will lead practitioners to have a vague understanding of different design principles, making it difficult to allocate design space reasonably and emphasize design focal points. This paper aims to prioritize four design principles, providing clear guidance for UI design in XAI. Initially, we conducted a lightweight summary to derive five user experience standards for non-expert users in XAI. Subsequently, we developed four corresponding webpage prototypes for the four design principles. Nineteen participants then interacted with these prototypes, providing ratings based on five user experience standards, and We calculated the weights of the design principles. Our findings indicate that, for non-expert users, &ldquo;sensitivity&rdquo; is the optimal UI design principle (weight = 0.3296), followed by &ldquo;flexibility&rdquo; (weight = 0.3014). Finally, we engage in further discussion and <b>summarization</b> of our research results, and present future works and limitations.</p></p class="citation"></blockquote><h3 id=23--274306-meditating-in-live-stream-an-autoethnographic-and-interview-study-to-investigate-motivations-interactions-and-challenges-jingjin-li-et-al-2024>(2/3 | 274/306) Meditating in Live Stream: An Autoethnographic and Interview Study to Investigate Motivations, Interactions and Challenges (Jingjin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingjin Li, Jiajing Guo, Gilly Leshed. (2024)<br><strong>Meditating in Live Stream: An Autoethnographic and Interview Study to Investigate Motivations, Interactions and Challenges</strong><br><button class=copy-to-clipboard title="Meditating in Live Stream: An Autoethnographic and Interview Study to Investigate Motivations, Interactions and Challenges" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-2, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13992v1.pdf filename=2402.13992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mindfulness practice has many mental and physical well-being benefits. With the increased popularity of live stream technologies and the impact of COVID-19, many people have turned to live stream tools to participate in online meditation sessions. To better understand the practices, challenges, and opportunities in live-stream meditation, we conducted a three-month autoethnographic study, during which two researchers participated in live-stream meditation sessions as the audience. Then we conducted a follow-up semi-structured interview study with 10 experienced live meditation teachers who use different live-stream tools. We found that live meditation, although having a weaker social presence than in-person meditation, facilitates attendees in establishing a practice routine and connecting with other meditators. Teachers use live streams to deliver the meditation practice to the world which also enhances their practice and brand building. We identified the challenges of using live-stream tools for meditation from the perspectives of both audiences and teachers, and provided design <b>recommendations</b> to better utilize live meditation as a resource for mental wellbeing.</p></p class="citation"></blockquote><h3 id=33--275306-bring-your-own-character-a-holistic-solution-for-automatic-facial-animation-generation-of-customized-characters-zechen-bai-et-al-2024>(3/3 | 275/306) Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters (Zechen Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zechen Bai, Peng Chen, Xiaolan Peng, Lu Liu, Hui Chen, Mike Zheng Shou, Feng Tian. (2024)<br><strong>Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters</strong><br><button class=copy-to-clipboard title="Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CV, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13724v1.pdf filename=2402.13724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Animating virtual characters has always been a fundamental research problem in virtual reality (VR). Facial animations play a crucial role as they effectively convey emotions and attitudes of virtual humans. However, creating such facial animations can be challenging, as current methods often involve utilization of expensive motion capture devices or significant investments of time and effort from human animators in tuning animation parameters. In this paper, we propose a holistic solution to automatically animate virtual human faces. In our solution, a deep learning model was first trained to retarget the facial expression from input face images to virtual human faces by estimating the blendshape coefficients. This method offers the flexibility of generating animations with characters of different appearances and blendshape topologies. Second, a practical toolkit was developed using Unity 3D, making it compatible with the most popular VR applications. The toolkit accepts both image and video as input to animate the target virtual human faces and enables users to manipulate the animation results. Furthermore, inspired by the spirit of <b>Human-in-the-loop</b> (HITL), we leveraged user feedback to further improve the performance of the model and toolkit, thereby increasing the customization properties to suit user preferences. The whole solution, for which we will make the code public, has the potential to accelerate the generation of facial animations for use in VR applications.</p></p class="citation"></blockquote><h2 id=mathna-4>math.NA (4)</h2><h3 id=14--276306-development-of-multi-physics-finite-element-model-to-investigate-electromagnetic-forming-and-simultaneous-multi-point-perforations-of-aluminium-tube-avinash-chetry-et-al-2024>(1/4 | 276/306) Development of multi-physics finite element model to investigate electromagnetic forming and simultaneous multi-point perforations of aluminium tube (Avinash Chetry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avinash Chetry, Arup Nandy. (2024)<br><strong>Development of multi-physics finite element model to investigate electromagnetic forming and simultaneous multi-point perforations of aluminium tube</strong><br><button class=copy-to-clipboard title="Development of multi-physics finite element model to investigate electromagnetic forming and simultaneous multi-point perforations of aluminium tube" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-DS, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13922v1.pdf filename=2402.13922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electromagnetic forming and perforations (EMFP) are complex and innovative high strain rate processes that involve electromagnetic-mechanical interactions for simultaneous metal forming and perforations. Instead of spending costly resources on repetitive experimental work, a properly designed numerical model can be effectively used for detailed analysis and characterization of the complex process. A coupled finite element (FE) model is considered for analyzing the multi-physics of the EMFP because of its robustness and improved accuracy. In this work, a detailed understanding of the process has been achieved by numerically simulating forming and perforations of Al6061-T6 tube for 12 holes and 36 holes with two different punches, i.e., pointed and concave punches using Ls-Dyna software. In order to shed light on EMFP physics, a comparison between experimental data and the formulated numerical <b>simulation</b> has been carried out to compare the average hole diameter and the number of perforated holes, for different types of punches and a range of discharge energies. The simulated results show acceptable agreement with experimental studies, with maximum deviations being less than or equal to 6%, which clearly illustrates the efficacy and capability of the developed coupled Multi-physics FE model.</p></p class="citation"></blockquote><h3 id=24--277306-numerical-methods-for-closed-loop-systems-with-non-autonomous-data-b-baran-et-al-2024>(2/4 | 277/306) Numerical methods for closed-loop systems with non-autonomous data (B. Baran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>B. Baran, P. Benner, J. Saak, T. Stillfjord. (2024)<br><strong>Numerical methods for closed-loop systems with non-autonomous data</strong><br><button class=copy-to-clipboard title="Numerical methods for closed-loop systems with non-autonomous data" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65F45, 93A15, 93B52, 93C10, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13656v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13656v1.pdf filename=2402.13656v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>By computing a feedback control via the linear quadratic regulator (LQR) approach and simulating a non-linear non-autonomous closed-loop system using this feedback, we combine two numerically challenging tasks. For the first task, the computation of the feedback control, we use the non-autonomous generalized differential Riccati equation (DRE), whose solution determines the time-varying feedback gain matrix. Regarding the second task, we want to be able to simulate non-linear closed-loop systems for which it is known that the regulator is only valid for sufficiently small perturbations. Thus, one easily runs into numerical issues in the integrators when the closed-loop control varies greatly. For these systems, e.g., the A-stable implicit Euler methods fails.\newline On the one hand, we implement non-autonomous versions of splitting schemes and BDF methods for the solution of our non-autonomous DREs. These are well-established DRE solvers in the autonomous case. On the other hand, to tackle the numerical issues in the <b>simulation</b> of the non-linear closed-loop system, we apply a fractional-step-theta scheme with time-adaptivity tuned specifically to this kind of challenge. That is, we additionally base the time-adaptivity on the activity of the control. We compare this approach to the more classical error-based time-adaptivity.\newline We describe techniques to make these two tasks computable in a reasonable amount of time and are able to simulate closed-loop systems with strongly varying controls, while avoiding numerical issues. Our time-adaptivity approach requires fewer time steps than the error-based alternative and is more reliable.</p></p class="citation"></blockquote><h3 id=34--278306-on-the-application-of-subspace-migration-from-scattering-matrix-with-constant-valued-diagonal-elements-in-microwave-imaging-won-kwang-park-2024>(3/4 | 278/306) On the application of subspace migration from scattering matrix with constant-valued diagonal elements in microwave imaging (Won-Kwang Park, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Won-Kwang Park. (2024)<br><strong>On the application of subspace migration from scattering matrix with constant-valued diagonal elements in microwave imaging</strong><br><button class=copy-to-clipboard title="On the application of subspace migration from scattering matrix with constant-valued diagonal elements in microwave imaging" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 78A46, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13565v1.pdf filename=2402.13565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the application of a subspace migration (SM) algorithm to quickly identify small objects in microwave imaging. In various problems, it is easy to measure the diagonal elements of the scattering matrix if the location of the transmitter and the receiver is the same. To address this issue, several studies have been conducted by setting the diagonal elements to zero. In this paper, we generalize the imaging problem by setting diagonal elements of the scattering matrix as a constant with the application of SM. To show the applicability of SM and its dependence on the constant, we show that the imaging function of SM can be represented in terms of an infinite series of the Bessel functions of integer order, antenna number and arrangement, and applied constant. This result enables us to discover some further properties, including the unique determination of objects. We also demonstrated <b>simulation</b> results with synthetic data to support the theoretical result.</p></p class="citation"></blockquote><h3 id=44--279306-a-mixed-finite-element-finite-volume-semi-implicit-discretisation-for-atmospheric-dynamics-spherical-geometry-thomas-melvin-et-al-2024>(4/4 | 279/306) A mixed finite-element, finite-volume, semi-implicit discretisation for atmospheric dynamics: Spherical geometry (Thomas Melvin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Melvin, Ben Shipway, Nigel Wood, Tommaso Benacchio, Thomas Bendall, Ian Boutle, Alex Brown, Christine Johnson, James Kent, Stephen Pring, Chris Smith, Mohamed Zerroukat, Colin Cotter, John Thuburn. (2024)<br><strong>A mixed finite-element, finite-volume, semi-implicit discretisation for atmospheric dynamics: Spherical geometry</strong><br><button class=copy-to-clipboard title="A mixed finite-element, finite-volume, semi-implicit discretisation for atmospheric dynamics: Spherical geometry" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M22, cs-NA, math-MP, math-NA, math-ph, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13738v1.pdf filename=2402.13738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The reformulation of the Met Office&rsquo;s dynamical core for weather and climate prediction previously described by the authors is extended to spherical domains using a cubed-sphere mesh. This paper updates the semi-implicit mixed finite-element formulation to be suitable for spherical domains. In particular the finite-volume transport scheme is extended to take account of non-uniform, non-orthogonal meshes and uses an advective-then-flux formulation so that increment from the transport scheme is linear in the divergence. The resulting model is then applied to a standard set of dry dynamical core tests and compared to the existing semi-implicit semi-Lagrangian dynamical core currently used in the Met Office&rsquo;s operational model.</p></p class="citation"></blockquote><h2 id=statap-1>stat.AP (1)</h2><h3 id=11--280306-measurement-uncertainty-relating-the-uncertainties-of-physical-and-virtual-measurements-simon-cramer-et-al-2024>(1/1 | 280/306) Measurement Uncertainty: Relating the uncertainties of physical and virtual measurements (Simon Cramer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Cramer, Tobias Müller, Robert H. Schmitt. (2024)<br><strong>Measurement Uncertainty: Relating the uncertainties of physical and virtual measurements</strong><br><button class=copy-to-clipboard title="Measurement Uncertainty: Relating the uncertainties of physical and virtual measurements" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.AP<br>Categories: cs-LG, math-PR, stat-AP, stat.AP<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13666v1.pdf filename=2402.13666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the context of industrially mass-manufactured products, quality management is based on physically inspecting a small sample from a large batch and <b>reasoning</b> about the batch&rsquo;s quality conformance. When complementing physical inspections with predictions from machine learning models, it is crucial that the uncertainty of the prediction is known. Otherwise, the application of established quality management concepts is not legitimate. Deterministic (machine learning) models lack quantification of their predictive uncertainty and are therefore unsuitable. <b>Probabilistic</b> <b>(machine</b> learning) models provide a predictive uncertainty along with the prediction. However, a concise relationship is missing between the measurement uncertainty of physical inspections and the predictive uncertainty of <b>probabilistic</b> <b>models</b> in their application in quality management. Here, we show how the predictive uncertainty of <b>probabilistic</b> <b>(machine</b> learning) models is related to the measurement uncertainty of physical inspections. This enables the use of <b>probabilistic</b> <b>models</b> for virtual inspections and integrates them into existing quality management concepts. Thus, we can provide a virtual measurement for any quality characteristic based on the process data and achieve a 100 percent inspection rate. In the field of Predictive Quality, the virtual measurement is of great interest. Based on our results, physical inspections with a low sampling rate can be accompanied by virtual measurements that allow an inspection rate of 100 percent. We add substantial value, especially to complex process chains, as faulty products/parts are identified promptly and upcoming process steps can be aborted.</p></p class="citation"></blockquote><h2 id=statml-4>stat.ML (4)</h2><h3 id=14--281306-analysis-of-bootstrap-and-subsampling-in-high-dimensional-regularized-regression-lucas-clarté-et-al-2024>(1/4 | 281/306) Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression (Lucas Clarté et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Clarté, Adrien Vandenbroucque, Guillaume Dalle, Bruno Loureiro, Florent Krzakala, Lenka Zdeborová. (2024)<br><strong>Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression</strong><br><button class=copy-to-clipboard title="Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cond-mat-dis-nn, cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Logistic Regression, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13622v1.pdf filename=2402.13622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional <b>supervised</b> regression tasks. We provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and <b>logistic</b> <b>regression,</b> taking the limit where the number of samples $n$ and dimension $d$ of the covariates grow at a comparable fixed rate $\alpha!=! n/d$. Our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when $\alpha$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime $\alpha!&lt;!1$ relevant to modern machine learning practice, their predictions are not consistent, even with optimal regularization.</p></p class="citation"></blockquote><h3 id=24--282306-multiply-robust-estimation-for-local-distribution-shifts-with-multiple-domains-steven-wilkins-reeves-et-al-2024>(2/4 | 282/306) Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains (Steven Wilkins-Reeves et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Steven Wilkins-Reeves, Xu Chen, Qi Ma, Christine Agarwal, Aude Hofleitner. (2024)<br><strong>Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains</strong><br><button class=copy-to-clipboard title="Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14145v1.pdf filename=2402.14145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Distribution</b> <b>shifts</b> are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data <b>distribution</b> <b>to</b> another. We focus on scenarios where data <b>distributions</b> <b>vary</b> across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) <b>distributions</b> <b>within</b> each segment. We propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis. The method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment. Our method is designed to be implemented with commonly used off-the-shelf machine learning models. We establish theoretical guarantees on the generalization bound of the method on the test risk. With extensive experiments on synthetic and real datasets, we demonstrate that the proposed method substantially improves over existing alternatives in prediction accuracy and robustness on both regression and classification tasks. We also assess its effectiveness on a user city prediction dataset from a large technology company.</p></p class="citation"></blockquote><h3 id=34--283306-probabilistic-neural-networks-pnns-for-modeling-aleatoric-uncertainty-in-scientific-machine-learning-farhad-pourkamali-anaraki-et-al-2024>(3/4 | 283/306) Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty in Scientific Machine Learning (Farhad Pourkamali-Anaraki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Farhad Pourkamali-Anaraki, Jamal F. Husseini, Scott E. Stapleton. (2024)<br><strong>Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty in Scientific Machine Learning</strong><br><button class=copy-to-clipboard title="Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty in Scientific Machine Learning" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13945v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13945v1.pdf filename=2402.13945v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the use of probabilistic neural networks (PNNs) to model aleatoric uncertainty, which refers to the inherent variability in the input-output relationships of a system, often characterized by unequal variance or heteroscedasticity. Unlike traditional neural networks that produce deterministic outputs, PNNs generate probability distributions for the target variable, allowing the determination of both predicted means and intervals in regression scenarios. Contributions of this paper include the development of a probabilistic distance metric to optimize PNN architecture, and the deployment of PNNs in controlled data sets as well as a practical material science case involving fiber-reinforced composites. The findings confirm that PNNs effectively model aleatoric uncertainty, proving to be more appropriate than the commonly employed <b>Gaussian</b> <b>process</b> regression for this purpose. Specifically, in a real-world scientific machine learning context, PNNs yield remarkably accurate output mean estimates with R-squared scores approaching 0.97, and their predicted intervals exhibit a high correlation coefficient of nearly 0.80, closely matching observed data intervals. Hence, this research contributes to the ongoing exploration of leveraging the sophisticated representational capacity of neural networks to delineate complex input-output relationships in scientific problems.</p></p class="citation"></blockquote><h3 id=44--284306-a-large-dimensional-analysis-of-multi-task-semi-supervised-learning-victor-leger-et-al-2024>(4/4 | 284/306) A Large Dimensional Analysis of Multi-task Semi-Supervised Learning (Victor Leger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Leger, Romain Couillet. (2024)<br><strong>A Large Dimensional Analysis of Multi-task Semi-Supervised Learning</strong><br><button class=copy-to-clipboard title="A Large Dimensional Analysis of Multi-task Semi-Supervised Learning" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13646v1.pdf filename=2402.13646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article conducts a large dimensional study of a simple yet quite versatile classification model, encompassing at once multi-task and <b>semi-supervised</b> <b>learning,</b> and taking into account uncertain labeling. Using tools from random matrix theory, we characterize the asymptotics of some key functionals, which allows us on the one hand to predict the performances of the algorithm, and on the other hand to reveal some counter-intuitive guidance on how to use it efficiently. The model, powerful enough to provide good performance guarantees, is also straightforward enough to provide strong insights into its behavior.</p></p class="citation"></blockquote><h2 id=mathco-2>math.CO (2)</h2><h3 id=12--285306-self-adhesivity-in-lattices-of-abstract-conditional-independence-models-tobias-boege-et-al-2024>(1/2 | 285/306) Self-adhesivity in lattices of abstract conditional independence models (Tobias Boege et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Boege, Janneke H. Bolt, Milan Studený. (2024)<br><strong>Self-adhesivity in lattices of abstract conditional independence models</strong><br><button class=copy-to-clipboard title="Self-adhesivity in lattices of abstract conditional independence models" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 62B10 (primary) 06A15, 68T27, 68V05, 05B35 (secondary), cs-IT, math-CO, math-IT, math.CO<br>Keyword Score: 15<br>Keywords: Geometry, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14053v1.pdf filename=2402.14053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce an algebraic concept of the frame for abstract conditional independence (CI) models, together with basic operations with respect to which such a frame should be closed: copying and marginalization. Three standard examples of such frames are (discrete) probabilistic CI structures, semi-graphoids and structural semi-graphoids. We concentrate on those frames which are closed under the operation of set-theoretical intersection because, for these, the respective families of CI models are lattices. This allows one to apply the results from lattice theory and formal concept analysis to describe such families in terms of implications among CI statements. The central concept of this paper is that of self-adhesivity defined in algebraic terms, which is a combinatorial reflection of the self-adhesivity concept studied earlier in context of polymatroids and information theory. The generalization also leads to a self-adhesivity operator defined on the hyper-level of CI frames. We answer some of the questions related to this approach and raise other open questions. The core of the paper is in computations. The combinatorial approach to computation might overcome some memory and space limitation of software packages based on polyhedral <b>geometry,</b> in particular, if SAT solvers are utilized. We characterize some basic CI families over 4 variables in terms of canonical implications among CI statements. We apply our method in information-theoretical context to the task of entropic region demarcation over 5 variables.</p></p class="citation"></blockquote><h3 id=22--286306-grid-minors-and-products-vida-dujmović-et-al-2024>(2/2 | 286/306) Grid Minors and Products (Vida Dujmović et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vida Dujmović, Pat Morin, David R. Wood, David Worley. (2024)<br><strong>Grid Minors and Products</strong><br><button class=copy-to-clipboard title="Grid Minors and Products" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14181v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14181v2.pdf filename=2402.14181v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by recent developments regarding the product structure of planar <b>graphs,</b> we study relationships between treewidth, grid minors, and <b>graph</b> products. We show that the Cartesian product of any two connected $n$-vertex <b>graphs</b> contains an $\Omega(\sqrt{n})\times\Omega(\sqrt{n})$ grid minor. This result is tight: The lexicographic product (which includes the Cartesian product as a subgraph) of a star and any $n$-vertex tree has no $\omega(\sqrt{n})\times\omega(\sqrt{n})$ grid minor.</p></p class="citation"></blockquote><h2 id=csne-3>cs.NE (3)</h2><h3 id=13--287306-origami-unfolding-the-abstraction-of-recursion-schemes-for-program-synthesis-matheus-campos-fernandes-et-al-2024>(1/3 | 287/306) Origami: (un)folding the abstraction of recursion schemes for program synthesis (Matheus Campos Fernandes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matheus Campos Fernandes, Fabricio Olivetti de Franca, Emilio Francesquini. (2024)<br><strong>Origami: (un)folding the abstraction of recursion schemes for program synthesis</strong><br><button class=copy-to-clipboard title="Origami: (un)folding the abstraction of recursion schemes for program synthesis" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs-PL, cs.NE<br>Keyword Score: 13<br>Keywords: Benchmarking, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13828v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13828v2.pdf filename=2402.13828v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Program synthesis with Genetic Programming searches for a correct program that satisfies the input specification, which is usually provided as input-output examples. One particular challenge is how to effectively handle loops and recursion avoiding programs that never terminate. A helpful abstraction that can alleviate this problem is the employment of Recursion Schemes that generalize the combination of data production and consumption. Recursion Schemes are very powerful as they allow the construction of programs that can <b>summarize</b> data, create sequences, and perform advanced calculations. The main advantage of writing a program using Recursion Schemes is that the programs are composed of well defined templates with only a few parts that need to be synthesized. In this paper we make an initial study of the benefits of using program synthesis with fold and unfold templates, and outline some preliminary experimental results. To highlight the advantages and disadvantages of this approach, we manually solved the entire GPSB <b>benchmark</b> using recursion schemes, highlighting the parts that should be evolved compared to alternative implementations. We noticed that, once the choice of which recursion scheme is made, the synthesis process can be simplified as each of the missing parts of the template are reduced to simpler functions, which are further constrained by their own input and output types.</p></p class="citation"></blockquote><h3 id=23--288306-neuraldiffuser-controllable-fmri-reconstruction-with-primary-visual-feature-guided-diffusion-haoyu-li-et-al-2024>(2/3 | 288/306) NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion (Haoyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Li, Hao Wu, Badong Chen. (2024)<br><strong>NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion</strong><br><button class=copy-to-clipboard title="NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-CV, cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13809v1.pdf filename=2402.13809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent <b>Diffusion</b> <b>Models</b> (LDM) provides a fine-grained retrieval of the brain. A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.). Moreover, LDMs would generate different image results even under the same conditions. For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details. We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details. We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a variety of results. We obtain the state-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset (NSD), which offers more faithful details and consistent results.</p></p class="citation"></blockquote><h3 id=33--289306-an-effective-networks-intrusion-detection-approach-based-on-hybrid-harris-hawks-and-multi-layer-perceptron-moutaz-alazab-et-al-2024>(3/3 | 289/306) An Effective Networks Intrusion Detection Approach Based on Hybrid Harris Hawks and Multi-Layer Perceptron (Moutaz Alazab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moutaz Alazab, Ruba Abu Khurma, Pedro A. Castillo, Bilal Abu-Salih, Alejandro Martin, David Camacho. (2024)<br><strong>An Effective Networks Intrusion Detection Approach Based on Hybrid Harris Hawks and Multi-Layer Perceptron</strong><br><button class=copy-to-clipboard title="An Effective Networks Intrusion Detection Approach Based on Hybrid Harris Hawks and Multi-Layer Perceptron" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14037v1.pdf filename=2402.14037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes an Intrusion Detection System (IDS) employing the Harris Hawks Optimization algorithm (HHO) to optimize Multilayer Perceptron learning by optimizing bias and weight parameters. HHO-MLP aims to select optimal parameters in its learning process to minimize intrusion detection errors in networks. HHO-MLP has been implemented using EvoloPy NN framework, an open-source Python tool specialized for training MLPs using evolutionary algorithms. For purposes of comparing the HHO model against other evolutionary methodologies currently available, specificity and sensitivity measures, accuracy measures, and mse and rmse measures have been calculated using <b>KDD</b> datasets. Experiments have demonstrated the HHO MLP method is effective at identifying malicious patterns. HHO-MLP has been tested against evolutionary algorithms like Butterfly Optimization Algorithm (BOA), Grasshopper Optimization Algorithms (GOA), and Black Widow Optimizations (BOW), with validation by Random Forest (RF), XG-Boost. HHO-MLP showed superior performance by attaining top scores with accuracy rate of 93.17%, sensitivity level of 89.25%, and specificity percentage of 95.41%.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--290306-balancing-spectral-temporal-and-spatial-information-for-eeg-based-alzheimers-disease-classification-stephan-goerttler-et-al-2024>(1/1 | 290/306) Balancing Spectral, Temporal and Spatial Information for EEG-based Alzheimer&rsquo;s Disease Classification (Stephan Goerttler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephan Goerttler, Fei He, Min Wu. (2024)<br><strong>Balancing Spectral, Temporal and Spatial Information for EEG-based Alzheimer&rsquo;s Disease Classification</strong><br><button class=copy-to-clipboard title="Balancing Spectral, Temporal and Spatial Information for EEG-based Alzheimer's Disease Classification" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP, q-bio-NC<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13523v1.pdf filename=2402.13523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The prospect of future treatment warrants the development of cost-effective screening for Alzheimer&rsquo;s disease (AD). A promising candidate in this regard is electroencephalography (EEG), as it is one of the most economic imaging modalities. Recent efforts in EEG analysis have shifted towards leveraging spatial information, employing novel frameworks such as <b>graph</b> <b>signal</b> <b>processing</b> or <b>graph</b> <b>neural</b> <b>networks.</b> Here, we systematically investigate the importance of spatial information relative to spectral or temporal information by varying the proportion of each dimension for AD classification. To do so, we test various dimension resolution configurations on two routine EEG datasets. We find that spatial information is consistently more relevant than temporal information and equally relevant as spectral information. These results emphasise the necessity to consider spatial information for EEG-based AD classification. On our second dataset, we further find that well-balanced feature resolutions boost classification accuracy by up to 1.6%. Our resolution-based feature extraction has the potential to improve AD classification specifically, and multivariate signal classification generally.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--291306-fairness-and-incentive-compatibility-via-percentage-fees-shahar-dobzinski-et-al-2024>(1/1 | 291/306) Fairness and Incentive Compatibility via Percentage Fees (Shahar Dobzinski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shahar Dobzinski, Sigal Oren, Jan Vondrak. (2024)<br><strong>Fairness and Incentive Compatibility via Percentage Fees</strong><br><button class=copy-to-clipboard title="Fairness and Incentive Compatibility via Percentage Fees" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14173v1.pdf filename=2402.14173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study incentive-compatible mechanisms that maximize the Nash Social Welfare. Since traditional incentive-compatible mechanisms cannot maximize the Nash Social Welfare even approximately, we propose changing the traditional model. Inspired by a widely used charging method (e.g., royalties, a lawyer that charges some percentage of possible future compensation), we suggest charging the players some percentage of their value of the outcome. We call this model the \emph{percentage fee} model. We show that there is a mechanism that maximizes exactly the Nash Social Welfare in every setting with non-negative valuations. Moreover, we prove an analog of Roberts theorem that essentially says that if the valuations are non-negative, then the only implementable social choice functions are those that maximize weighted variants of the Nash Social Welfare. We develop polynomial time incentive compatible approximation algorithms for the Nash Social Welfare with subadditive valuations and prove some hardness results.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--292306-open-source-software-field-research-spanning-social-and-practice-networks-for-re-entering-the-field-sean-p-goggins-et-al-2024>(1/1 | 292/306) Open Source Software Field Research: Spanning Social and Practice Networks for Re-Entering the Field (Sean P. Goggins et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sean P. Goggins, Kevin Lumbard, Matt Germonprez. (2024)<br><strong>Open Source Software Field Research: Spanning Social and Practice Networks for Re-Entering the Field</strong><br><button class=copy-to-clipboard title="Open Source Software Field Research: Spanning Social and Practice Networks for Re-Entering the Field" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-SI, cs.CY<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14172v1.pdf filename=2402.14172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sociotechnical research increasingly includes the social sub-networks that emerge from large-scale sociotechnical infrastructure, including the infrastructure for building open source software. This paper addresses these numerous sub-networks as advantageous for researchers. It provides a methodological synthesis focusing on how researchers can best span adjacent social sub-networks during engaged field research. Specifically, we describe practices and artifacts that aid movement from one social subsystem within a more extensive technical infrastructure to another. To surface the importance of spanning sub-networks, we incorporate a discussion of social capital and the role of technical infrastructure in its development for sociotechnical researchers. We then characterize a five-step process for spanning social sub-networks during engaged field research: commitment, context mapping, jargon competence, returning value, and bridging. We then present our experience studying corporate open source software projects and the role of that experience in accelerating our work in open source scientific software research as described through the lens of bridging social capital. Based on our analysis, we offer <b>recommendations</b> for engaging in fieldwork in adjacent social sub-networks that share a technical context and discussion of how the relationship between social and technically acquired social capital is a missing but critical methodological dimension for research on large-scale sociotechnical research.</p></p class="citation"></blockquote><h2 id=csds-5>cs.DS (5)</h2><h3 id=15--293306-masked-matrix-multiplication-for-emergent-sparsity-brian-wheatman-et-al-2024>(1/5 | 293/306) Masked Matrix Multiplication for Emergent Sparsity (Brian Wheatman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brian Wheatman, Meghana Madhyastha, Randal Burns. (2024)<br><strong>Masked Matrix Multiplication for Emergent Sparsity</strong><br><button class=copy-to-clipboard title="Masked Matrix Multiplication for Emergent Sparsity" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-AI, cs-DS, cs.DS<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14118v1.pdf filename=2402.14118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial intelligence workloads, especially <b>transformer</b> models, exhibit emergent sparsity in which computations perform selective sparse access to dense data. The workloads are inefficient on hardware designed for dense computations and do not map well onto sparse data representations. We build a vectorized and parallel matrix-multiplication system A X B = C that eliminates unnecessary computations and avoids branches based on a runtime evaluation of sparsity. We use a combination of dynamic code lookup to adapt to the specific sparsity encoded in the B matrix and preprocessing of sparsity maps of the A and B matrices to compute conditional branches once for the whole computation. For a wide range of sparsity, from 60% to 95% zeros, our implementation performs fewer instructions and increases performance when compared with Intel MKL&rsquo;s dense or sparse matrix multiply routines. Benefits can be as large as 2 times speedup and 4 times fewer instructions.</p></p class="citation"></blockquote><h3 id=25--294306-practical-algorithms-for-hierarchical-overlap-graphs-saumya-talera-et-al-2024>(2/5 | 294/306) Practical algorithms for Hierarchical overlap graphs (Saumya Talera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saumya Talera, Parth Bansal, Shabnam Khan, Shahbaz Khan. (2024)<br><strong>Practical algorithms for Hierarchical overlap graphs</strong><br><button class=copy-to-clipboard title="Practical algorithms for Hierarchical overlap graphs" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 8<br>Keywords: Graph, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13920v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13920v1.pdf filename=2402.13920v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Genome assembly is a prominent problem studied in bioinformatics, which computes the source string using a set of its overlapping substrings. Classically, genome assembly uses assembly <b>graphs</b> built using this set of substrings to compute the source string efficiently, having a tradeoff between scalability and avoiding information loss. The scalable de Bruijn <b>graphs</b> come at the price of losing crucial overlap information. The complete overlap information is stored in overlap <b>graphs</b> using quadratic space. Hierarchical overlap <b>graphs</b> [IPL20] (HOG) overcome these limitations, avoiding information loss despite using linear space. After a series of suboptimal improvements, Khan and Park et al. simultaneously presented two optimal algorithms [CPM2021], where only the former was seemingly practical. We empirically analyze all the practical algorithms for computing HOG, where the optimal algorithm [CPM2021] outperforms the previous algorithms as expected, though at the expense of extra memory. However, it uses non-intuitive approach and non-trivial data structures. We present arguably the most intuitive algorithm, using only elementary arrays, which is also optimal. Our algorithm empirically proves even better for both time and memory over all the algorithms, highlighting its significance in both theory and practice. We further explore the applications of hierarchical overlap <b>graphs</b> to solve various forms of suffix-prefix queries on a set of strings. Loukides et al. [CPM2023] recently presented state-of-the-art algorithms for these queries. However, these algorithms require complex <b>black-box</b> <b>data</b> structures and are seemingly impractical. Our algorithms, despite failing to match the state-of-the-art algorithms theoretically, answer different queries ranging from 0.01-100 milliseconds for a data set having around a billion characters.</p></p class="citation"></blockquote><h3 id=35--295306-robust-recovery-for-stochastic-block-models-simplified-and-generalized-sidhanth-mohanty-et-al-2024>(3/5 | 295/306) Robust recovery for stochastic block models, simplified and generalized (Sidhanth Mohanty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sidhanth Mohanty, Prasad Raghavendra, David X. Wu. (2024)<br><strong>Robust recovery for stochastic block models, simplified and generalized</strong><br><button class=copy-to-clipboard title="Robust recovery for stochastic block models, simplified and generalized" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, math-PR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13921v1.pdf filename=2402.13921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of $\textit{robust community recovery}$: efficiently recovering communities in sparse stochastic block models in the presence of adversarial corruptions. In the absence of adversarial corruptions, there are efficient algorithms when the $\textit{signal-to-noise ratio}$ exceeds the $\textit{Kesten&ndash;Stigum (KS) threshold}$, widely believed to be the computational threshold for this problem. The question we study is: does the computational threshold for robust community recovery also lie at the KS threshold? We answer this question affirmatively, providing an algorithm for robust community recovery for arbitrary stochastic block models on any constant number of communities, generalizing the work of Ding, d&rsquo;Orsi, Nasser & Steurer on an efficient algorithm above the KS threshold in the case of $2$-community block models. There are three main ingredients to our work: (i) The Bethe Hessian of the <b>graph</b> is defined as $H_G(t) \triangleq (D_G-I)t^2 - A_Gt + I$ where $D_G$ is the diagonal matrix of degrees and $A_G$ is the adjacency matrix. Empirical work suggested that the Bethe Hessian for the stochastic block model has outlier eigenvectors corresponding to the communities right above the Kesten-Stigum threshold. We formally confirm the existence of outlier eigenvalues for the Bethe Hessian, by explicitly constructing outlier eigenvectors from the community vectors. (ii) We develop an algorithm for a variant of robust PCA on sparse matrices. Specifically, an algorithm to partially recover top eigenspaces from adversarially corrupted sparse matrices under mild delocalization constraints. (iii) A rounding algorithm to turn vector assignments of vertices into a community assignment, inspired by the algorithm of Charikar & Wirth \cite{CW04} for $2$XOR.</p></p class="citation"></blockquote><h3 id=45--296306-a-uniformly-random-solution-to-algorithmic-redistricting-jin-yi-cai-et-al-2024>(4/5 | 296/306) A Uniformly Random Solution to Algorithmic Redistricting (Jin-Yi Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin-Yi Cai, Jacob Kruse, Kenneth Mayer, Daniel P. Szabo. (2024)<br><strong>A Uniformly Random Solution to Algorithmic Redistricting</strong><br><button class=copy-to-clipboard title="A Uniformly Random Solution to Algorithmic Redistricting" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13868v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13868v1.pdf filename=2402.13868v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The process of drawing electoral district boundaries is known as political redistricting. Within this context, gerrymandering is the practice of drawing these boundaries such that they unfairly favor a particular political party, often leading to unequal representation and skewed electoral outcomes. One of the few ways to detect gerrymandering is by algorithmically sampling redistricting plans. Previous methods mainly focus on sampling from some neighborhood of ``realistic&rsquo; districting plans, rather than a uniform sample of the entire space. We present a deterministic subexponential time algorithm to uniformly sample from the space of all possible $ k $-partitions of a bounded degree planar <b>graph,</b> and with this construct a sample of the entire space of redistricting plans. We also give a way to restrict this sample space to plans that match certain compactness and population constraints at the cost of added complexity. The algorithm runs in $ 2^{O(\sqrt{n}\log n)} $ time, although we only give a heuristic implementation. Our method generalizes an algorithm to count self-avoiding walks on a square to count paths that split general planar <b>graphs</b> into $ k $ regions, and uses this to sample from the space of all $ k $-partitions of a planar <b>graph.</b></p></p class="citation"></blockquote><h3 id=55--297306-multi-agent-online-graph-exploration-on-cycles-and-tadpole-graphs-erik-van-den-akker-et-al-2024>(5/5 | 297/306) Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs (Erik van den Akker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erik van den Akker, Kevin Buchin, Klaus-Tycho Foerster. (2024)<br><strong>Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs</strong><br><button class=copy-to-clipboard title="Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DC, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13845v1.pdf filename=2402.13845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of multi-agent online <b>graph</b> exploration, in which a team of k agents has to explore a given <b>graph,</b> starting and ending on the same node. The <b>graph</b> is initially unknown. Whenever a node is visited by an agent, its neighborhood and adjacent edges are revealed. The agents share a global view of the explored parts of the <b>graph.</b> The cost of the exploration has to be minimized, where cost either describes the time needed for the entire exploration (time model), or the length of the longest path traversed by any agent (energy model). We investigate <b>graph</b> exploration on cycles and tadpole <b>graphs</b> for 2-4 agents, providing optimal results on the competitive ratio in the energy model (1-competitive with two agents on cycles and three agents on tadpole <b>graphs),</b> and for tadpole <b>graphs</b> in the time model (1.5-competitive with four agents). We also show competitive upper bounds of 2 for the exploration of tadpole <b>graphs</b> with three agents, and 2.5 for the exploration of tadpole <b>graphs</b> with two agents in the time model.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--298306-learning-dynamic-representations-of-the-functional-connectome-in-neurobiological-networks-luciano-dyballa-et-al-2024>(1/1 | 298/306) Learning dynamic representations of the functional connectome in neurobiological networks (Luciano Dyballa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luciano Dyballa, Samuel Lang, Alexandra Haslund-Gourley, Eviatar Yemini, Steven W. Zucker. (2024)<br><strong>Learning dynamic representations of the functional connectome in neurobiological networks</strong><br><button class=copy-to-clipboard title="Learning dynamic representations of the functional connectome in neurobiological networks" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-LG, cs-SI, q-bio-NC, q-bio.NC<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14102v1.pdf filename=2402.14102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The static synaptic connectivity of neuronal circuits stands in direct contrast to the dynamics of their function. As in changing community interactions, different neurons can participate actively in various combinations to effect behaviors at different times. We introduce an <b>unsupervised</b> approach to learn the dynamic affinities between neurons in live, behaving animals, and to reveal which communities form among neurons at different times. The inference occurs in two major steps. First, pairwise non-linear affinities between neuronal traces from brain-wide calcium activity are organized by non-negative tensor factorization (NTF). Each factor specifies which groups of neurons are most likely interacting for an inferred interval in time, and for which animals. Finally, a generative model that allows for weighted community detection is applied to the functional motifs produced by NTF to reveal a dynamic functional connectome. Since time codes the different experimental variables (e.g., application of chemical stimuli), this provides an atlas of neural motifs active during separate stages of an experiment (e.g., stimulus application or spontaneous behaviors). Results from our analysis are experimentally validated, confirming that our method is able to robustly predict causal interactions between neurons to generate behavior. Code is available at <a href=https://github.com/dyballa/dynamic-connectomes>https://github.com/dyballa/dynamic-connectomes</a>.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--299306-model-checking-logical-actions-in-magic-tricks-weijun-zhu-2024>(1/1 | 299/306) Model Checking Logical Actions in Magic Tricks (Weijun Zhu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weijun Zhu. (2024)<br><strong>Model Checking Logical Actions in Magic Tricks</strong><br><button class=copy-to-clipboard title="Model Checking Logical Actions in Magic Tricks" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-CY, cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13802v1.pdf filename=2402.13802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Some Magic Tricks <b>(MT),</b> such as many kinds of Card Magic (CM), consisting of human computational or logical actions. How to ensure the logical correctness of these MTs? In this paper, the Model Checking (MC) technique is employed to study a typical CM via a case study. First, computational operations of a CM called shousuigongcishi can be described by a Magic Algorithm (MAR). Second, the logical correctness is portrayed by a temporal logic formula. On the basis of it, this <b>MT</b> logical correctness problem is reduced to the model checking problem. As a result, the Magic Trick Model Checking (MTMC) technique aims to verify whether a designed <b>MT</b> meets its architect&rsquo;s anticipation and requirements, or not, in terms of logic and computations.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=12--300306-benchmarking-and-dissecting-the-nvidia-hopper-gpu-architecture-weile-luo-et-al-2024>(1/2 | 300/306) Benchmarking and Dissecting the Nvidia Hopper GPU Architecture (Weile Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Qiang Wang, Xiaowen Chu. (2024)<br><strong>Benchmarking and Dissecting the Nvidia Hopper GPU Architecture</strong><br><button class=copy-to-clipboard title="Benchmarking and Dissecting the Nvidia Hopper GPU Architecture" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13499v1.pdf filename=2402.13499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Graphics processing units (GPUs) are continually evolving to cater to the computational demands of contemporary general-purpose workloads, particularly those driven by artificial intelligence (AI) utilizing deep learning techniques. A substantial body of studies have been dedicated to dissecting the microarchitectural metrics characterizing diverse GPU generations, which helps researchers understand the hardware details and leverage them to optimize the GPU programs. However, the latest Hopper GPUs present a set of novel attributes, including new tensor cores supporting FP8, DPX, and distributed shared memory. Their details still remain mysterious in terms of performance and operational characteristics. In this research, we propose an extensive <b>benchmarking</b> study focused on the Hopper GPU. The objective is to unveil its microarchitectural intricacies through an examination of the new instruction-set architecture (ISA) of Nvidia GPUs and the utilization of new CUDA APIs. Our approach involves two main aspects. Firstly, we conduct conventional latency and throughput comparison <b>benchmarks</b> across the three most recent GPU architectures, namely Hopper, Ada, and Ampere. Secondly, we delve into a comprehensive discussion and <b>benchmarking</b> of the latest Hopper features, encompassing the Hopper DPX dynamic programming (DP) instruction set, distributed shared memory, and the availability of FP8 tensor cores. The microbenchmarking results we present offer a deeper understanding of the novel GPU AI function units and programming features introduced by the Hopper architecture. This newfound understanding is expected to greatly facilitate software optimization and modeling efforts for GPU architectures. To the best of our knowledge, this study makes the first attempt to demystify the tensor core performance and programming instruction sets unique to Hopper GPUs.</p></p class="citation"></blockquote><h3 id=22--301306-guac-energy-aware-and-ssa-based-generation-of-coarse-grained-merged-accelerators-from-llvm-ir-iulian-brumar-et-al-2024>(2/2 | 301/306) Guac: Energy-Aware and SSA-Based Generation of Coarse-Grained Merged Accelerators from LLVM-IR (Iulian Brumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iulian Brumar, Rodrigo Rocha, Alex Bernat, Devashree Tripathy, David Brooks, Gu-Yeon Wei. (2024)<br><strong>Guac: Energy-Aware and SSA-Based Generation of Coarse-Grained Merged Accelerators from LLVM-IR</strong><br><button class=copy-to-clipboard title="Guac: Energy-Aware and SSA-Based Generation of Coarse-Grained Merged Accelerators from LLVM-IR" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13513v1.pdf filename=2402.13513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Designing accelerators for resource- and power-constrained applications is a daunting task. High-level Synthesis (HLS) addresses these constraints through resource sharing, an optimization at the HLS binding stage that maps multiple operations to the same functional unit. However, resource sharing is often limited to reusing instructions within a basic block. Instead of searching globally for the best control and dataflow <b>graphs</b> (CDFGs) to combine, it is constrained by existing instruction mappings and schedules. Coarse-grained function merging (CGFM) at the intermediate representation (IR) level can reuse control and dataflow patterns without dealing with the post-scheduling complexity of mapping operations onto functional units, wires, and registers. The merged functions produced by CGFM can be translated to RTL by HLS, yielding Coarse Grained Merged Accelerators (CGMAs). CGMAs are especially profitable across applications with similar data- and control-flow patterns. Prior work has used CGFM to generate CGMAs without regard for which CGFM algorithms best optimize area, power, and energy costs. We propose Guac, an energy-aware and SSA-based (static single assignment) CGMA generation methodology. Guac implements a novel ensemble of cost models for efficient CGMA generation. We also show that CGFM algorithms using SSA form to merge control- and dataflow <b>graphs</b> outperform prior non-SSA CGFM designs. We demonstrate significant area, power, and energy savings with respect to the state of the art. In particular, Guac more than doubles energy savings with respect to the closest related work while using a strong resource-sharing baseline.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--302306-improved-lower-bound-on-the-number-of-pseudoline-arrangements-justin-dallant-2024>(1/1 | 302/306) Improved Lower Bound on the Number of Pseudoline Arrangements (Justin Dallant, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Justin Dallant. (2024)<br><strong>Improved Lower Bound on the Number of Pseudoline Arrangements</strong><br><button class=copy-to-clipboard title="Improved Lower Bound on the Number of Pseudoline Arrangements" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG, math-CO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13923v1.pdf filename=2402.13923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that for large enough $n$, the number of non-isomorphic pseudoline arrangements of order $n$ is greater than $2^{c\cdot n^2}$ for some constant $c > 0.2604$, improving the previous best bound of $c>0.2083$ by Dumitrescu and Mandal (2020). Arrangements of pseudolines (and in particular arrangements of lines) are important objects appearing in many forms in discrete and computational <b>geometry.</b> They have strong ties for example with oriented matroids, sorting networks and point configurations. Let $B_n$ be the number of non-isomorphic pseudoline arrangements of order $n$ and let $b_n := \log_2(B_n)$. The problem of estimating $b_n$ dates back to Knuth, who conjectured that $b_n \leq 0.5n^2 + o(n^2)$ and derived the first bounds $n^2/6-O(n) \leq b_n \leq 0.7924(n^2+n)$. Both the upper and the lower bound have been improved a couple of times since. For the upper bound, it was first improved to $b_n &lt; 0.6988n^2$ (Felsner, 1997), then $b_n &lt; 0.6571 n^2$ by Felsner and Valtr (2011), for large enough $n$. In the same paper, Felsner and Valtr improved the constant in the lower bound to $c> 0.1887$, which was subsequently improved by Dumitrescu and Mandal to $c>0.2083$. Our new bound is based on a construction which starts with one of the constructions of Dumitrescu and Mandal and breaks it into constant sized pieces. We then use software to compute the contribution of each piece to the overall number of pseudoline arrangements. This method adds a lot of flexibility to the construction and thus offers many avenues for future tweaks and improvements which could lead to further tightening of the lower bound.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--303306-on-distributed-computation-of-the-minimum-triangle-edge-transversal-keren-censor-hillel-et-al-2024>(1/2 | 303/306) On Distributed Computation of the Minimum Triangle Edge Transversal (Keren Censor-Hillel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keren Censor-Hillel, Majd Khoury. (2024)<br><strong>On Distributed Computation of the Minimum Triangle Edge Transversal</strong><br><button class=copy-to-clipboard title="On Distributed Computation of the Minimum Triangle Edge Transversal" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-DS, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13985v1.pdf filename=2402.13985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The distance of a <b>graph</b> from being triangle-free is a fundamental <b>graph</b> parameter, counting the number of edges that need to be removed from a <b>graph</b> in order for it to become triangle-free. Its corresponding computational problem is the classic minimum triangle edge transversal problem, and its normalized value is the baseline for triangle-freeness testing algorithms. While triangle-freeness testing has been successfully studied in the distributed setting, computing the distance itself in a distributed setting is unknown, to the best of our knowledge, despite being well-studied in the centralized setting. This work addresses the computation of the minimum triangle edge transversal in distributed networks. We show with a simple warm-up construction that this is a global task, requiring $\Omega(D)$ rounds even in the $\mathsf{LOCAL}$ model with unbounded messages, where $D$ is the diameter of the network. However, we show that approximating this value can be done much faster. A $(1+\epsilon)$-approximation can be obtained in $\text{poly}\log{n}$ rounds, where $n$ is the size of the network <b>graph.</b> Moreover, faster approximations can be obtained, at the cost of increasing the approximation factor to roughly 3, by a reduction to the minimum hypergraph vertex cover problem. With a time overhead of the maximum degree $\Delta$, this can also be applied to the $\mathsf{CONGEST}$ model, in which messages are bounded. Our key technical contribution is proving that computing an exact solution is ``as hard as it gets&rsquo;&rsquo; in $\mathsf{CONGEST}$, requiring a near-quadratic number of rounds. Because this problem is an edge selection problem, as opposed to previous lower bounds that were for node selection problems, major challenges arise in constructing the lower bound, requiring us to develop novel ingredients.</p></p class="citation"></blockquote><h3 id=22--304306-adaptive-massively-parallel-coloring-in-sparse-graphs-rustam-latypov-et-al-2024>(2/2 | 304/306) Adaptive Massively Parallel Coloring in Sparse Graphs (Rustam Latypov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rustam Latypov, Yannic Maus, Shreyas Pai, Jara Uitto. (2024)<br><strong>Adaptive Massively Parallel Coloring in Sparse Graphs</strong><br><button class=copy-to-clipboard title="Adaptive Massively Parallel Coloring in Sparse Graphs" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-DS, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13755v1.pdf filename=2402.13755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classic symmetry-breaking problems on <b>graphs</b> have gained a lot of attention in models of modern parallel computation. The Adaptive Massively Parallel Computation (AMPC) is a model that captures central challenges in data center computations. Chang et al. [PODC'2019] gave an extremely fast, constant time, algorithm for the $(\Delta + 1)$-coloring problem, where $\Delta$ is the maximum degree of an input <b>graph</b> of $n$ nodes. The algorithm works in the most restrictive low-space setting, where each machine has $n^{\delta}$ local space for a constant $0 &lt; \delta &lt; 1$. In this work, we study the vertex-coloring problem in sparse <b>graphs</b> parameterized by their arboricity $\alpha$, a standard measure for sparsity. We give deterministic algorithms that in constant, or almost constant, time give $\text{poly}(\alpha)$ and $O(\alpha)$-colorings, where $\alpha$ can be arbitrarily smaller than $\Delta$. A strong and standard approach to compute arboricity-dependent colorings is through the Nash-Williams forest decomposition, which gives rise to an (acyclic) orientation of the edges such that each node has a small outdegree. Our main technical contribution is giving efficient deterministic algorithms to compute these orientations and showing how to leverage them to find colorings in low-space AMPC. A key technical challenge is that the color of a node may depend on almost all of the other nodes in the <b>graph</b> and these dependencies cannot be stored on a single machine. Nevertheless, our novel and careful exploration technique yields the orientation, and the arboricity-dependent coloring, with a sublinear number of adaptive queries per node.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--305306-edge-disjoint-paths-in-eulerian-digraphs-dario-cavallaro-et-al-2024>(1/1 | 305/306) Edge-Disjoint Paths in Eulerian Digraphs (Dario Cavallaro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dario Cavallaro, Ken-ichi Kawarabayashi, Stephan Kreutzer. (2024)<br><strong>Edge-Disjoint Paths in Eulerian Digraphs</strong><br><button class=copy-to-clipboard title="Edge-Disjoint Paths in Eulerian Digraphs" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DM, cs.CC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13716v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13716v1.pdf filename=2402.13716v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Disjoint paths problems are among the most prominent problems in combinatorial optimization. The edge- as well as vertex-disjoint paths problem, are NP-complete on directed and undirected <b>graphs.</b> But on undirected <b>graphs,</b> Robertson and Seymour <b>(Graph</b> Minors XIII) developed an algorithm for the vertex- and the edge-disjoint paths problem that runs in cubic time for every fixed number $p$ of terminal pairs, i.e. they proved that the problem is fixed-parameter tractable on undirected <b>graphs.</b> On directed <b>graphs,</b> Fortune, Hopcroft, and Wyllie proved that both problems are NP-complete already for $p=2$ terminal pairs. In this paper, we study the edge-disjoint paths problem (EDPP) on Eulerian digraphs, a problem that has received significant attention in the literature. Marx (Marx 2004) proved that the Eulerian EDPP is NP-complete even on structurally very simple Eulerian digraphs. On the positive side, polynomial time algorithms are known only for very restricted cases, such as $p\leq 3$ or where the demand <b>graph</b> is a union of two stars (see e.g. Ibaraki, Poljak 1991; Frank 1988; Frank, Ibaraki, Nagamochi 1995). The question of which values of $p$ the edge-disjoint paths problem can be solved in polynomial time on Eulerian digraphs has already been raised by Frank, Ibaraki, and Nagamochi (1995) almost 30 years ago. But despite considerable effort, the complexity of the problem is still wide open and is considered to be the main open problem in this area (see Chapter 4 of Bang-Jensen, Gutin 2018 for a recent survey). In this paper, we solve this long-open problem by showing that the Edge-Disjoint Paths Problem is fixed-parameter tractable on Eulerian digraphs in general (parameterized by the number of terminal pairs). The algorithm itself is reasonably simple but the proof of its correctness requires a deep structural analysis of Eulerian digraphs.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--306306-towards-linear-spanners-in-all-temporal-cliques-sebastian-angrick-et-al-2024>(1/1 | 306/306) Towards Linear Spanners in All Temporal Cliques (Sebastian Angrick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Angrick, Ben Bals, Tobias Friedrich, Hans Gawendowicz, Niko Hastrich, Nicolas Klodt, Pascal Lenzner, Jonas Schmidt, George Skretas, Armin Wells. (2024)<br><strong>Towards Linear Spanners in All Temporal Cliques</strong><br><button class=copy-to-clipboard title="Towards Linear Spanners in All Temporal Cliques" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs-DS, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13624v1.pdf filename=2402.13624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many real-world networks, like transportation networks and social networks, are dynamic in the sense that the edge set may change over time, but these changes are known in advance. This behavior is captured by the temporal <b>graphs</b> model, which has recently become a trending topic in theoretical computer science. A core open problem in the field is to prove the existence of linear-size temporal spanners in temporal cliques, i.e., sparse subgraphs of complete temporal <b>graphs</b> that ensure all-pairs reachability via temporal paths. So far, the best known result is the existence of temporal spanners with $\mathcal{O}(n\log n)$ many edges. We present significant progress towards proving that linear-size temporal spanners exist in all temporal cliques. We adapt techniques used in previous works and heavily expand and generalize them to provide a simpler and more intuitive proof of the $\mathcal{O}(n\log n)$ bound. Moreover, we use our novel approach to show that a large class of temporal cliques, called edge-pivot <b>graphs,</b> admit linear-size temporal spanners. To contrast this, we investigate other classes of temporal cliques that do not belong to the class of edge-pivot <b>graphs.</b> We introduce two such <b>graph</b> classes and we develop novel techniques for establishing the existence of linear temporal spanners in these <b>graph</b> classes as well.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.22</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.24</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cslg-62>cs.LG (62)</a><ul><li><a href=#162--1306-unigraph-learning-a-cross-domain-graph-foundation-model-from-natural-language-yufei-he-et-al-2024>(1/62 | 1/306) UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language (Yufei He et al., 2024)</a></li><li><a href=#262--2306-an-explainable-transformer-based-model-for-phishing-email-detection-a-large-language-model-approach-mohammad-amaz-uddin-et-al-2024>(2/62 | 2/306) An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach (Mohammad Amaz Uddin et al., 2024)</a></li><li><a href=#362--3306-aptq-attention-aware-post-training-mixed-precision-quantization-for-large-language-models-ziyi-guan-et-al-2024>(3/62 | 3/306) APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models (Ziyi Guan et al., 2024)</a></li><li><a href=#462--4306-fingpt-hpc-efficient-pretraining-and-finetuning-large-language-models-for-financial-applications-with-high-performance-computing-xiao-yang-liu-et-al-2024>(4/62 | 4/306) FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing (Xiao-Yang Liu et al., 2024)</a></li><li><a href=#562--5306-stencil-submodular-mutual-information-based-weak-supervision-for-cold-start-active-learning-nathan-beck-et-al-2024>(5/62 | 5/306) STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning (Nathan Beck et al., 2024)</a></li><li><a href=#662--6306-deep-generative-models-for-offline-policy-learning-tutorial-survey-and-perspectives-on-future-directions-jiayu-chen-et-al-2024>(6/62 | 6/306) Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions (Jiayu Chen et al., 2024)</a></li><li><a href=#762--7306-deisam-segment-anything-with-deictic-prompting-hikaru-shindo-et-al-2024>(7/62 | 7/306) DeiSAM: Segment Anything with Deictic Prompting (Hikaru Shindo et al., 2024)</a></li><li><a href=#862--8306-wisdom-of-committee-distilling-from-foundation-model-to-specialized-application-model-zichang-liu-et-al-2024>(8/62 | 8/306) Wisdom of Committee: Distilling from Foundation Model to Specialized Application Model (Zichang Liu et al., 2024)</a></li><li><a href=#962--9306-from-self-attention-to-markov-models-unveiling-the-dynamics-of-generative-transformers-m-emrullah-ildiz-et-al-2024>(9/62 | 9/306) From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers (M. Emrullah Ildiz et al., 2024)</a></li><li><a href=#1062--10306-pqa-zero-shot-protein-question-answering-for-free-form-scientific-enquiry-with-large-language-models-eli-m-carrami-et-al-2024>(10/62 | 10/306) PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models (Eli M Carrami et al., 2024)</a></li><li><a href=#1162--11306-packd-pattern-clustered-knowledge-distillation-for-compressing-memory-access-prediction-models-neelesh-gupta-et-al-2024>(11/62 | 11/306) PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models (Neelesh Gupta et al., 2024)</a></li><li><a href=#1262--12306-a-simple-and-yet-fairly-effective-defense-for-graph-neural-networks-sofiane-ennadir-et-al-2024>(12/62 | 12/306) A Simple and Yet Fairly Effective Defense for Graph Neural Networks (Sofiane Ennadir et al., 2024)</a></li><li><a href=#1362--13306-inductive-graph-alignment-prompt-bridging-the-gap-between-graph-pre-training-and-inductive-fine-tuning-from-spectral-perspective-yuchen-yan-et-al-2024>(13/62 | 13/306) Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective (Yuchen Yan et al., 2024)</a></li><li><a href=#1462--14306-learning-to-poison-large-language-models-during-instruction-tuning-yao-qiang-et-al-2024>(14/62 | 14/306) Learning to Poison Large Language Models During Instruction Tuning (Yao Qiang et al., 2024)</a></li><li><a href=#1562--15306-contextual-molecule-representation-learning-from-chemical-reaction-knowledge-han-tang-et-al-2024>(15/62 | 15/306) Contextual Molecule Representation Learning from Chemical Reaction Knowledge (Han Tang et al., 2024)</a></li><li><a href=#1662--16306-ai-powered-predictions-for-electricity-load-in-prosumer-communities-aleksei-kychkin-et-al-2024>(16/62 | 16/306) AI-Powered Predictions for Electricity Load in Prosumer Communities (Aleksei Kychkin et al., 2024)</a></li><li><a href=#1762--17306-misalignment-learning-and-ranking-harnessing-users-limited-attention-arpit-agarwal-et-al-2024>(17/62 | 17/306) Misalignment, Learning, and Ranking: Harnessing Users Limited Attention (Arpit Agarwal et al., 2024)</a></li><li><a href=#1862--18306-stability-aware-training-of-neural-network-interatomic-potentials-with-differentiable-boltzmann-estimators-sanjeev-raja-et-al-2024>(18/62 | 18/306) Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators (Sanjeev Raja et al., 2024)</a></li><li><a href=#1962--19306-attackgnn-red-teaming-gnns-in-hardware-security-using-reinforcement-learning-vasudev-gohil-et-al-2024>(19/62 | 19/306) AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning (Vasudev Gohil et al., 2024)</a></li><li><a href=#2062--20306-reasoning-algorithmically-in-graph-neural-networks-danilo-numeroso-2024>(20/62 | 20/306) Reasoning Algorithmically in Graph Neural Networks (Danilo Numeroso, 2024)</a></li><li><a href=#2162--21306-linear-transformers-are-versatile-in-context-learners-max-vladymyrov-et-al-2024>(21/62 | 21/306) Linear Transformers are Versatile In-Context Learners (Max Vladymyrov et al., 2024)</a></li><li><a href=#2262--22306-recursive-speculative-decoding-accelerating-llm-inference-via-sampling-without-replacement-wonseok-jeon-et-al-2024>(22/62 | 22/306) Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement (Wonseok Jeon et al., 2024)</a></li><li><a href=#2362--23306-neuroflux-memory-efficient-cnn-training-using-adaptive-local-learning-dhananjay-saikumar-et-al-2024>(23/62 | 23/306) NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning (Dhananjay Saikumar et al., 2024)</a></li><li><a href=#2462--24306-coercing-llms-to-do-and-reveal-almost-anything-jonas-geiping-et-al-2024>(24/62 | 24/306) Coercing LLMs to do and reveal (almost) anything (Jonas Geiping et al., 2024)</a></li><li><a href=#2562--25306-do-efficient-transformers-really-save-computation-kai-yang-et-al-2024>(25/62 | 25/306) Do Efficient Transformers Really Save Computation? (Kai Yang et al., 2024)</a></li><li><a href=#2662--26306-non-asymptotic-convergence-of-discrete-time-diffusion-models-new-approach-and-improved-rate-yuchen-liang-et-al-2024>(26/62 | 26/306) Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate (Yuchen Liang et al., 2024)</a></li><li><a href=#2762--27306-protect-and-extend----using-gans-for-synthetic-data-generation-of-time-series-medical-records-navid-ashrafi-et-al-2024>(27/62 | 27/306) Protect and Extend &ndash; Using GANs for Synthetic Data Generation of Time-Series Medical Records (Navid Ashrafi et al., 2024)</a></li><li><a href=#2862--28306-e2usd-efficient-yet-effective-unsupervised-state-detection-for-multivariate-time-series-zhichen-lai-et-al-2024>(28/62 | 28/306) E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series (Zhichen Lai et al., 2024)</a></li><li><a href=#2962--29306-propd-dynamic-token-tree-pruning-and-generation-for-llm-parallel-decoding-shuzhang-zhong-et-al-2024>(29/62 | 29/306) ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding (Shuzhang Zhong et al., 2024)</a></li><li><a href=#3062--30306-overcoming-saturation-in-density-ratio-estimation-by-iterated-regularization-lukas-gruber-et-al-2024>(30/62 | 30/306) Overcoming Saturation in Density Ratio Estimation by Iterated Regularization (Lukas Gruber et al., 2024)</a></li><li><a href=#3162--31306-dslr-diversity-enhancement-and-structure-learning-for-rehearsal-based-graph-continual-learning-seungyoon-choi-et-al-2024>(31/62 | 31/306) DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning (Seungyoon Choi et al., 2024)</a></li><li><a href=#3262--32306-simpro-a-simple-probabilistic-framework-towards-realistic-long-tailed-semi-supervised-learning-chaoqun-du-et-al-2024>(32/62 | 32/306) SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning (Chaoqun Du et al., 2024)</a></li><li><a href=#3362--33306-intriguing-properties-of-modern-gans-roy-friedman-et-al-2024>(33/62 | 33/306) Intriguing Properties of Modern GANs (Roy Friedman et al., 2024)</a></li><li><a href=#3462--34306-generative-adversarial-models-for-extreme-downscaling-of-climate-datasets-guiye-li-et-al-2024>(34/62 | 34/306) Generative Adversarial Models for Extreme Downscaling of Climate Datasets (Guiye Li et al., 2024)</a></li><li><a href=#3562--35306-bias-correction-of-wind-power-forecasts-with-scada-data-and-continuous-learning-stefan-jonas-et-al-2024>(35/62 | 35/306) Bias correction of wind power forecasts with SCADA data and continuous learning (Stefan Jonas et al., 2024)</a></li><li><a href=#3662--36306-simple-and-effective-transfer-learning-for-neuro-symbolic-integration-alessandro-daniele-et-al-2024>(36/62 | 36/306) Simple and Effective Transfer Learning for Neuro-Symbolic Integration (Alessandro Daniele et al., 2024)</a></li><li><a href=#3762--37306-average-gradient-outer-product-as-a-mechanism-for-deep-neural-collapse-daniel-beaglehole-et-al-2024>(37/62 | 37/306) Average gradient outer product as a mechanism for deep neural collapse (Daniel Beaglehole et al., 2024)</a></li><li><a href=#3862--38306-sparse-and-structured-hopfield-networks-saul-santos-et-al-2024>(38/62 | 38/306) Sparse and Structured Hopfield Networks (Saul Santos et al., 2024)</a></li><li><a href=#3962--39306-spot-check-equivalence-an-interpretable-metric-for-information-elicitation-mechanisms-shengwei-xu-et-al-2024>(39/62 | 39/306) Spot Check Equivalence: an Interpretable Metric for Information Elicitation Mechanisms (Shengwei Xu et al., 2024)</a></li><li><a href=#4062--40306-diffplf-a-conditional-diffusion-model-for-probabilistic-forecasting-of-ev-charging-load-siyang-li-et-al-2024>(40/62 | 40/306) DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of EV Charging Load (Siyang Li et al., 2024)</a></li><li><a href=#4162--41306-prosparse-introducing-and-enhancing-intrinsic-activation-sparsity-within-large-language-models-chenyang-song-et-al-2024>(41/62 | 41/306) ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models (Chenyang Song et al., 2024)</a></li><li><a href=#4262--42306-stealthy-adversarial-attacks-on-stochastic-multi-armed-bandits-zhiwei-wang-et-al-2024>(42/62 | 42/306) Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits (Zhiwei Wang et al., 2024)</a></li><li><a href=#4362--43306-vn-network-embedding-newly-emerging-entities-with-virtual-neighbors-yongquan-he-et-al-2024>(43/62 | 43/306) VN Network: Embedding Newly Emerging Entities with Virtual Neighbors (Yongquan He et al., 2024)</a></li><li><a href=#4462--44306-hettree-heterogeneous-tree-graph-neural-network-mingyu-guan-et-al-2024>(44/62 | 44/306) HetTree: Heterogeneous Tree Graph Neural Network (Mingyu Guan et al., 2024)</a></li><li><a href=#4562--45306-opening-the-black-box-a-systematic-review-on-explainable-ai-in-remote-sensing-adrian-höhl-et-al-2024>(45/62 | 45/306) Opening the Black-Box: A Systematic Review on Explainable AI in Remote Sensing (Adrian Höhl et al., 2024)</a></li><li><a href=#4662--46306-accuracy-preserving-calibration-via-statistical-modeling-on-probability-simplex-yasushi-esaki-et-al-2024>(46/62 | 46/306) Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex (Yasushi Esaki et al., 2024)</a></li><li><a href=#4762--47306-cloudnine-analyzing-meteorological-observation-impact-on-weather-prediction-using-explainable-graph-neural-networks-hyeon-ju-jeon-et-al-2024>(47/62 | 47/306) CloudNine: Analyzing Meteorological Observation Impact on Weather Prediction Using Explainable Graph Neural Networks (Hyeon-Ju Jeon et al., 2024)</a></li><li><a href=#4862--48306-corrective-machine-unlearning-shashwat-goel-et-al-2024>(48/62 | 48/306) Corrective Machine Unlearning (Shashwat Goel et al., 2024)</a></li><li><a href=#4962--49306-fedadmm-insa-an-inexact-and-self-adaptive-admm-for-federated-learning-yongcun-song-et-al-2024>(49/62 | 49/306) FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning (Yongcun Song et al., 2024)</a></li><li><a href=#5062--50306-enhancing-reinforcement-learning-agents-with-local-guides-paul-daoudi-et-al-2024>(50/62 | 50/306) Enhancing Reinforcement Learning Agents with Local Guides (Paul Daoudi et al., 2024)</a></li><li><a href=#5162--51306-polynet-learning-diverse-solution-strategies-for-neural-combinatorial-optimization-andré-hottung-et-al-2024>(51/62 | 51/306) PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization (André Hottung et al., 2024)</a></li><li><a href=#5262--52306-dealing-with-unbounded-gradients-in-stochastic-saddle-point-optimization-gergely-neu-et-al-2024>(52/62 | 52/306) Dealing with unbounded gradients in stochastic saddle-point optimization (Gergely Neu et al., 2024)</a></li><li><a href=#5362--53306-generative-probabilistic-time-series-forecasting-and-applications-in-grid-operations-xinyi-wang-et-al-2024>(53/62 | 53/306) Generative Probabilistic Time Series Forecasting and Applications in Grid Operations (Xinyi Wang et al., 2024)</a></li><li><a href=#5462--54306-replicable-learning-of-large-margin-halfspaces-alkis-kalavasis-et-al-2024>(54/62 | 54/306) Replicable Learning of Large-Margin Halfspaces (Alkis Kalavasis et al., 2024)</a></li><li><a href=#5562--55306-performance-improvement-bounds-for-lipschitz-configurable-markov-decision-processes-alberto-maria-metelli-2024>(55/62 | 55/306) Performance Improvement Bounds for Lipschitz Configurable Markov Decision Processes (Alberto Maria Metelli, 2024)</a></li><li><a href=#5662--56306-fld-fourier-latent-dynamics-for-structured-motion-representation-and-learning-chenhao-li-et-al-2024>(56/62 | 56/306) FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning (Chenhao Li et al., 2024)</a></li><li><a href=#5762--57306-voice-driven-mortality-prediction-in-hospitalized-heart-failure-patients-a-machine-learning-approach-enhanced-with-diagnostic-biomarkers-nihat-ahmadli-et-al-2024>(57/62 | 57/306) Voice-Driven Mortality Prediction in Hospitalized Heart Failure Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers (Nihat Ahmadli et al., 2024)</a></li><li><a href=#5862--58306-the-expected-loss-of-preconditioned-langevin-dynamics-reveals-the-hessian-rank-amitay-bar-et-al-2024>(58/62 | 58/306) The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank (Amitay Bar et al., 2024)</a></li><li><a href=#5962--59306-on-the-expressive-power-of-a-variant-of-the-looped-transformer-yihang-gao-et-al-2024>(59/62 | 59/306) On the Expressive Power of a Variant of the Looped Transformer (Yihang Gao et al., 2024)</a></li><li><a href=#6062--60306-theoretical-analysis-of-submodular-information-measures-for-targeted-data-subset-selection-nathan-beck-et-al-2024>(60/62 | 60/306) Theoretical Analysis of Submodular Information Measures for Targeted Data Subset Selection (Nathan Beck et al., 2024)</a></li><li><a href=#6162--61306-geometry-informed-neural-networks-arturs-berzins-et-al-2024>(61/62 | 61/306) Geometry-Informed Neural Networks (Arturs Berzins et al., 2024)</a></li><li><a href=#6262--62306-improving-building-temperature-forecasting-a-data-driven-approach-with-system-scenario-clustering-dafang-zhao-et-al-2024>(62/62 | 62/306) Improving Building Temperature Forecasting: A Data-driven Approach with System Scenario Clustering (Dafang Zhao et al., 2024)</a></li></ul></li><li><a href=#cscl-94>cs.CL (94)</a><ul><li><a href=#194--63306-flame-self-supervised-low-resource-taxonomy-expansion-using-large-language-models-sahil-mishra-et-al-2024>(1/94 | 63/306) FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large Language Models (Sahil Mishra et al., 2024)</a></li><li><a href=#294--64306-unsupervised-text-style-transfer-via-llms-and-attention-masking-with-multi-way-interactions-lei-pan-et-al-2024>(2/94 | 64/306) Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions (Lei Pan et al., 2024)</a></li><li><a href=#394--65306-cognitive-visual-language-mapper-advancing-multimodal-comprehension-with-enhanced-visual-knowledge-alignment-yunxin-li-et-al-2024>(3/94 | 65/306) Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment (Yunxin Li et al., 2024)</a></li><li><a href=#494--66306-a-multimodal-in-context-tuning-approach-for-e-commerce-product-description-generation-yunxin-li-et-al-2024>(4/94 | 66/306) A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation (Yunxin Li et al., 2024)</a></li><li><a href=#594--67306-fanoutqa-multi-hop-multi-document-question-answering-for-large-language-models-andrew-zhu-et-al-2024>(5/94 | 67/306) FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models (Andrew Zhu et al., 2024)</a></li><li><a href=#694--68306-synfac-edit-synthetic-imitation-edit-feedback-for-factual-alignment-in-clinical-summarization-prakamya-mishra-et-al-2024>(6/94 | 68/306) SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization (Prakamya Mishra et al., 2024)</a></li><li><a href=#794--69306-kuaiji-the-first-chinese-accounting-large-language-model-jiayuan-luo-et-al-2024>(7/94 | 69/306) Kuaiji: the First Chinese Accounting Large Language Model (Jiayuan Luo et al., 2024)</a></li><li><a href=#894--70306-arl2-aligning-retrievers-for-black-box-large-language-models-via-self-guided-adaptive-relevance-labeling-lingxi-zhang-et-al-2024>(8/94 | 70/306) ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling (Lingxi Zhang et al., 2024)</a></li><li><a href=#994--71306-distillation-contrastive-decoding-improving-llms-reasoning-with-contrastive-decoding-and-distillation-phuc-phan-et-al-2024>(9/94 | 71/306) Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation (Phuc Phan et al., 2024)</a></li><li><a href=#1094--72306-self-distillation-bridges-distribution-gap-in-language-model-fine-tuning-zhaorui-yang-et-al-2024>(10/94 | 72/306) Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning (Zhaorui Yang et al., 2024)</a></li><li><a href=#1194--73306-making-reasoning-matter-measuring-and-improving-faithfulness-of-chain-of-thought-reasoning-debjit-paul-et-al-2024>(11/94 | 73/306) Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning (Debjit Paul et al., 2024)</a></li><li><a href=#1294--74306-activerag-revealing-the-treasures-of-knowledge-via-active-learning-zhipeng-xu-et-al-2024>(12/94 | 74/306) ActiveRAG: Revealing the Treasures of Knowledge via Active Learning (Zhipeng Xu et al., 2024)</a></li><li><a href=#1394--75306-gradsafe-detecting-unsafe-prompts-for-llms-via-safety-critical-gradient-analysis-yueqi-xie-et-al-2024>(13/94 | 75/306) GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis (Yueqi Xie et al., 2024)</a></li><li><a href=#1494--76306-pca-bench-evaluating-multimodal-large-language-models-in-perception-cognition-action-chain-liang-chen-et-al-2024>(14/94 | 76/306) PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain (Liang Chen et al., 2024)</a></li><li><a href=#1594--77306-is-llm-as-a-judge-robust-investigating-universal-adversarial-attacks-on-zero-shot-llm-assessment-vyas-raina-et-al-2024>(15/94 | 77/306) Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment (Vyas Raina et al., 2024)</a></li><li><a href=#1694--78306-investigating-multilingual-instruction-tuning-do-polyglot-models-demand-for-multilingual-instructions-alexander-arno-weber-et-al-2024>(16/94 | 78/306) Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions? (Alexander Arno Weber et al., 2024)</a></li><li><a href=#1794--79306-refutebench-evaluating-refuting-instruction-following-for-large-language-models-jianhao-yan-et-al-2024>(17/94 | 79/306) RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models (Jianhao Yan et al., 2024)</a></li><li><a href=#1894--80306-camelot-towards-large-language-models-with-training-free-consolidated-associative-memory-zexue-he-et-al-2024>(18/94 | 80/306) CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory (Zexue He et al., 2024)</a></li><li><a href=#1994--81306-lexc-gen-generating-data-for-extremely-low-resource-languages-with-large-language-models-and-bilingual-lexicons-zheng-xin-yong-et-al-2024>(19/94 | 81/306) LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons (Zheng-Xin Yong et al., 2024)</a></li><li><a href=#2094--82306-textttse2-textitsequential-example-textitselection-for-in-context-learning-haoyu-liu-et-al-2024>(20/94 | 82/306) $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning (Haoyu Liu et al., 2024)</a></li><li><a href=#2194--83306-unlocking-instructive-in-context-learning-with-tabular-prompting-for-relational-triple-extraction-guozheng-li-et-al-2024>(21/94 | 83/306) Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction (Guozheng Li et al., 2024)</a></li><li><a href=#2294--84306-kinit-at-semeval-2024-task-8-fine-tuned-llms-for-multilingual-machine-generated-text-detection-michal-spiegel-et-al-2024>(22/94 | 84/306) KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection (Michal Spiegel et al., 2024)</a></li><li><a href=#2394--85306-hallucinations-or-attention-misdirection-the-path-to-strategic-value-extraction-in-business-using-large-language-models-aline-ioste-2024>(23/94 | 85/306) Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models (Aline Ioste, 2024)</a></li><li><a href=#2494--86306-towards-building-multilingual-language-model-for-medicine-pengcheng-qiu-et-al-2024>(24/94 | 86/306) Towards Building Multilingual Language Model for Medicine (Pengcheng Qiu et al., 2024)</a></li><li><a href=#2594--87306-omgeval-an-open-multilingual-generative-evaluation-benchmark-for-large-language-models-yang-liu-et-al-2024>(25/94 | 87/306) OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models (Yang Liu et al., 2024)</a></li><li><a href=#2694--88306-mm-soc-benchmarking-multimodal-large-language-models-in-social-media-platforms-yiqiao-jin-et-al-2024>(26/94 | 88/306) MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms (Yiqiao Jin et al., 2024)</a></li><li><a href=#2794--89306-bba-bi-modal-behavioral-alignment-for-reasoning-with-large-vision-language-models-xueliang-zhao-et-al-2024>(27/94 | 89/306) BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models (Xueliang Zhao et al., 2024)</a></li><li><a href=#2894--90306-driving-generative-agents-with-their-personality-lawrence-j-klinkert-et-al-2024>(28/94 | 90/306) Driving Generative Agents With Their Personality (Lawrence J. Klinkert et al., 2024)</a></li><li><a href=#2994--91306-whats-in-a-name-auditing-large-language-models-for-race-and-gender-bias-amit-haim-et-al-2024>(29/94 | 91/306) What&rsquo;s in a Name? Auditing Large Language Models for Race and Gender Bias (Amit Haim et al., 2024)</a></li><li><a href=#3094--92306-semantic-mirror-jailbreak-genetic-algorithm-based-jailbreak-prompts-against-open-source-llms-xiaoxia-li-et-al-2024>(30/94 | 92/306) Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs (Xiaoxia Li et al., 2024)</a></li><li><a href=#3194--93306-beyond-hate-speech-nlps-challenges-and-opportunities-in-uncovering-dehumanizing-language-hezhao-zhang-et-al-2024>(31/94 | 93/306) Beyond Hate Speech: NLP&rsquo;s Challenges and Opportunities in Uncovering Dehumanizing Language (Hezhao Zhang et al., 2024)</a></li><li><a href=#3294--94306-longrope-extending-llm-context-window-beyond-2-million-tokens-yiran-ding-et-al-2024>(32/94 | 94/306) LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens (Yiran Ding et al., 2024)</a></li><li><a href=#3394--95306-gcof-self-iterative-text-generation-for-copywriting-using-large-language-model-jianghui-zhou-et-al-2024>(33/94 | 95/306) GCOF: Self-iterative Text Generation for Copywriting Using Large Language Model (Jianghui Zhou et al., 2024)</a></li><li><a href=#3494--96306-user-llm-efficient-llm-contextualization-with-user-embeddings-lin-ning-et-al-2024>(34/94 | 96/306) User-LLM: Efficient LLM Contextualization with User Embeddings (Lin Ning et al., 2024)</a></li><li><a href=#3594--97306-winoviz-probing-visual-properties-of-objects-under-different-states-woojeong-jin-et-al-2024>(35/94 | 97/306) WinoViz: Probing Visual Properties of Objects Under Different States (Woojeong Jin et al., 2024)</a></li><li><a href=#3694--98306-are-llms-effective-negotiators-systematic-evaluation-of-the-multifaceted-capabilities-of-llms-in-negotiation-dialogues-deuksin-kwon-et-al-2024>(36/94 | 98/306) Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues (Deuksin Kwon et al., 2024)</a></li><li><a href=#3794--99306-self-dc-when-to-retrieve-and-when-to-generate-self-divide-and-conquer-for-compositional-unknown-questions-hongru-wang-et-al-2024>(37/94 | 99/306) Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions (Hongru Wang et al., 2024)</a></li><li><a href=#3894--100306-the-lay-persons-guide-to-biomedicine-orchestrating-large-language-models-zheheng-luo-et-al-2024>(38/94 | 100/306) The Lay Person&rsquo;s Guide to Biomedicine: Orchestrating Large Language Models (Zheheng Luo et al., 2024)</a></li><li><a href=#3994--101306-retrieval-augmented-data-augmentation-for-low-resource-domain-tasks-minju-seo-et-al-2024>(39/94 | 101/306) Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks (Minju Seo et al., 2024)</a></li><li><a href=#4094--102306-how-important-is-domain-specificity-in-language-models-and-instruction-finetuning-for-biomedical-relation-extraction-aviv-brokman-et-al-2024>(40/94 | 102/306) How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction? (Aviv Brokman et al., 2024)</a></li><li><a href=#4194--103306-ranking-large-language-models-without-ground-truth-amit-dhurandhar-et-al-2024>(41/94 | 103/306) Ranking Large Language Models without Ground Truth (Amit Dhurandhar et al., 2024)</a></li><li><a href=#4294--104306-olympiadbench-a-challenging-benchmark-for-promoting-agi-with-olympiad-level-bilingual-multimodal-scientific-problems-chaoqun-he-et-al-2024>(42/94 | 104/306) OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems (Chaoqun He et al., 2024)</a></li><li><a href=#4394--105306-knowledge-graph-enhanced-large-language-model-editing-mengqi-zhang-et-al-2024>(43/94 | 105/306) Knowledge Graph Enhanced Large Language Model Editing (Mengqi Zhang et al., 2024)</a></li><li><a href=#4494--106306-calibrating-large-language-models-with-sample-consistency-qing-lyu-et-al-2024>(44/94 | 106/306) Calibrating Large Language Models with Sample Consistency (Qing Lyu et al., 2024)</a></li><li><a href=#4594--107306-factual-consistency-evaluation-of-summarisation-in-the-era-of-large-language-models-zheheng-luo-et-al-2024>(45/94 | 107/306) Factual Consistency Evaluation of Summarisation in the Era of Large Language Models (Zheheng Luo et al., 2024)</a></li><li><a href=#4694--108306-inftybench-extending-long-context-evaluation-beyond-100k-tokens-xinrong-zhang-et-al-2024>(46/94 | 108/306) $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens (Xinrong Zhang et al., 2024)</a></li><li><a href=#4794--109306-graph-representation-of-narrative-context-coherence-dependency-via-retrospective-questions-liyan-xu-et-al-2024>(47/94 | 109/306) Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions (Liyan Xu et al., 2024)</a></li><li><a href=#4894--110306-llms-meet-long-video-advancing-long-video-comprehension-with-an-interactive-visual-adapter-in-llms-yunxin-li-et-al-2024>(48/94 | 110/306) LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs (Yunxin Li et al., 2024)</a></li><li><a href=#4994--111306-can-similarity-based-domain-ordering-reduce-catastrophic-forgetting-for-intent-recognition-amogh-mannekote-et-al-2024>(49/94 | 111/306) Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for Intent Recognition? (Amogh Mannekote et al., 2024)</a></li><li><a href=#5094--112306-what-linguistic-features-and-languages-are-important-in-llm-translation-ryandito-diandaru-et-al-2024>(50/94 | 112/306) What Linguistic Features and Languages are Important in LLM Translation? (Ryandito Diandaru et al., 2024)</a></li><li><a href=#5194--113306-retrieval-helps-or-hurts-a-deeper-dive-into-the-efficacy-of-retrieval-augmentation-to-language-models-seiji-maekawa-et-al-2024>(51/94 | 113/306) Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models (Seiji Maekawa et al., 2024)</a></li><li><a href=#5294--114306-large-language-models-for-data-annotation-a-survey-zhen-tan-et-al-2024>(52/94 | 114/306) Large Language Models for Data Annotation: A Survey (Zhen Tan et al., 2024)</a></li><li><a href=#5394--115306-toolverifier-generalization-to-new-tools-via-self-verification-dheeraj-mekala-et-al-2024>(53/94 | 115/306) TOOLVERIFIER: Generalization to New Tools via Self-Verification (Dheeraj Mekala et al., 2024)</a></li><li><a href=#5494--116306-measuring-social-biases-in-masked-language-models-by-proxy-of-prediction-quality-rahul-zalkikar-et-al-2024>(54/94 | 116/306) Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality (Rahul Zalkikar et al., 2024)</a></li><li><a href=#5594--117306-technical-report-on-the-checkforai-ai-generated-text-classifier-bradley-emi-et-al-2024>(55/94 | 117/306) Technical Report on the Checkfor.ai AI-Generated Text Classifier (Bradley Emi et al., 2024)</a></li><li><a href=#5694--118306-more-multi-modal-retrieval-augmented-generative-commonsense-reasoning-wanqing-cui-et-al-2024>(56/94 | 118/306) MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning (Wanqing Cui et al., 2024)</a></li><li><a href=#5794--119306-dyval-2-dynamic-evaluation-of-large-language-models-by-meta-probing-agents-kaijie-zhu-et-al-2024>(57/94 | 119/306) DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents (Kaijie Zhu et al., 2024)</a></li><li><a href=#5894--120306-potential-and-challenges-of-model-editing-for-social-debiasing-jianhao-yan-et-al-2024>(58/94 | 120/306) Potential and Challenges of Model Editing for Social Debiasing (Jianhao Yan et al., 2024)</a></li><li><a href=#5994--121306-a-study-on-the-vulnerability-of-test-questions-against-chatgpt-based-cheating-shanker-ram-et-al-2024>(59/94 | 121/306) A Study on the Vulnerability of Test Questions against ChatGPT-based Cheating (Shanker Ram et al., 2024)</a></li><li><a href=#6094--122306-bangla-ai-a-framework-for-machine-translation-utilizing-large-language-models-for-ethnic-media-md-ashraful-goni-et-al-2024>(60/94 | 122/306) Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media (MD Ashraful Goni et al., 2024)</a></li><li><a href=#6194--123306-improving-language-understanding-from-screenshots-tianyu-gao-et-al-2024>(61/94 | 123/306) Improving Language Understanding from Screenshots (Tianyu Gao et al., 2024)</a></li><li><a href=#6294--124306-on-leveraging-encoder-only-pre-trained-language-models-for-effective-keyphrase-generation-di-wu-et-al-2024>(62/94 | 124/306) On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation (Di Wu et al., 2024)</a></li><li><a href=#6394--125306-large-language-models-are-vulnerable-to-bait-and-switch-attacks-for-generating-harmful-content-federico-bianchi-et-al-2024>(63/94 | 125/306) Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content (Federico Bianchi et al., 2024)</a></li><li><a href=#6494--126306-llm-based-multi-agent-generation-of-semi-structured-documents-from-semantic-templates-in-the-public-administration-domain-emanuele-musumeci-et-al-2024>(64/94 | 126/306) LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain (Emanuele Musumeci et al., 2024)</a></li><li><a href=#6594--127306-ouroboros-speculative-decoding-with-large-model-enhanced-drafting-weilin-zhao-et-al-2024>(65/94 | 127/306) Ouroboros: Speculative Decoding with Large Model Enhanced Drafting (Weilin Zhao et al., 2024)</a></li><li><a href=#6694--128306-neeko-leveraging-dynamic-lora-for-efficient-multi-character-role-playing-agent-xiaoyan-yu-et-al-2024>(66/94 | 128/306) Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent (Xiaoyan Yu et al., 2024)</a></li><li><a href=#6794--129306-a-comprehensive-study-of-multilingual-confidence-estimation-on-large-language-models-boyang-xue-et-al-2024>(67/94 | 129/306) A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models (Boyang Xue et al., 2024)</a></li><li><a href=#6894--130306-recmind-japanese-movie-recommendation-dialogue-with-seekers-internal-state-takashi-kodama-et-al-2024>(68/94 | 130/306) RecMind: Japanese Movie Recommendation Dialogue with Seeker&rsquo;s Internal State (Takashi Kodama et al., 2024)</a></li><li><a href=#6994--131306-round-trip-translation-defence-against-large-language-model-jailbreaking-attacks-canaan-yung-et-al-2024>(69/94 | 131/306) Round Trip Translation Defence against Large Language Model Jailbreaking Attacks (Canaan Yung et al., 2024)</a></li><li><a href=#7094--132306-evaluation-of-a-semi-autonomous-attentive-listening-system-with-takeover-prompting-haruki-kawai-et-al-2024>(70/94 | 132/306) Evaluation of a semi-autonomous attentive listening system with takeover prompting (Haruki Kawai et al., 2024)</a></li><li><a href=#7194--133306-distinctive-image-captioning-leveraging-ground-truth-captions-in-clip-guided-reinforcement-learning-antoine-chaffin-et-al-2024>(71/94 | 133/306) Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning (Antoine Chaffin et al., 2024)</a></li><li><a href=#7294--134306-cmner-a-chinese-multimodal-ner-dataset-based-on-social-media-yuanze-ji-et-al-2024>(72/94 | 134/306) CMNER: A Chinese Multimodal NER Dataset based on Social Media (Yuanze Ji et al., 2024)</a></li><li><a href=#7394--135306-criticbench-evaluating-large-language-models-as-critic-tian-lan-et-al-2024>(73/94 | 135/306) CriticBench: Evaluating Large Language Models as Critic (Tian Lan et al., 2024)</a></li><li><a href=#7494--136306-the-da-vinci-code-of-large-pre-trained-language-models-deciphering-degenerate-knowledge-neurons-yuheng-chen-et-al-2024>(74/94 | 136/306) The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons (Yuheng Chen et al., 2024)</a></li><li><a href=#7594--137306-exploiting-adaptive-contextual-masking-for-aspect-based-sentiment-analysis-s-m-rafiuddin-et-al-2024>(75/94 | 137/306) Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment Analysis (S M Rafiuddin et al., 2024)</a></li><li><a href=#7694--138306-sage-evaluating-moral-consistency-in-large-language-models-vamshi-krishna-bonagiri-et-al-2024>(76/94 | 138/306) SaGE: Evaluating Moral Consistency in Large Language Models (Vamshi Krishna Bonagiri et al., 2024)</a></li><li><a href=#7794--139306-kornat-llm-alignment-benchmark-for-korean-social-values-and-common-knowledge-jiyoung-lee-et-al-2024>(77/94 | 139/306) KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge (Jiyoung Lee et al., 2024)</a></li><li><a href=#7894--140306-ed-copilot-reduce-emergency-department-wait-time-with-language-model-diagnostic-assistance-liwen-sun-et-al-2024>(78/94 | 140/306) ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance (Liwen Sun et al., 2024)</a></li><li><a href=#7994--141306-understanding-the-dataset-practitioners-behind-large-language-model-development-crystal-qian-et-al-2024>(79/94 | 141/306) Understanding the Dataset Practitioners Behind Large Language Model Development (Crystal Qian et al., 2024)</a></li><li><a href=#8094--142306-automatic-histograms-leveraging-language-models-for-text-dataset-exploration-emily-reif-et-al-2024>(80/94 | 142/306) Automatic Histograms: Leveraging Language Models for Text Dataset Exploration (Emily Reif et al., 2024)</a></li><li><a href=#8194--143306-reinforcement-learning-with-dynamic-multi-reward-weighting-for-multi-style-controllable-generation-karin-de-langis-et-al-2024>(81/94 | 143/306) Reinforcement Learning with Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation (Karin de Langis et al., 2024)</a></li><li><a href=#8294--144306-can-watermarks-survive-translation-on-the-cross-lingual-consistency-of-text-watermark-for-large-language-models-zhiwei-he-et-al-2024>(82/94 | 144/306) Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models (Zhiwei He et al., 2024)</a></li><li><a href=#8394--145306-analysing-the-impact-of-sequence-composition-on-language-model-pre-training-yu-zhao-et-al-2024>(83/94 | 145/306) Analysing The Impact of Sequence Composition on Language Model Pre-Training (Yu Zhao et al., 2024)</a></li><li><a href=#8494--146306-beyond-probabilities-unveiling-the-misalignment-in-evaluating-large-language-models-chenyang-lyu-et-al-2024>(84/94 | 146/306) Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models (Chenyang Lyu et al., 2024)</a></li><li><a href=#8594--147306-from-text-to-cql-bridging-natural-language-and-corpus-search-engine-luming-lu-et-al-2024>(85/94 | 147/306) From Text to CQL: Bridging Natural Language and Corpus Search Engine (Luming Lu et al., 2024)</a></li><li><a href=#8694--148306-data-driven-discovery-with-large-generative-models-bodhisattwa-prasad-majumder-et-al-2024>(86/94 | 148/306) Data-driven Discovery with Large Generative Models (Bodhisattwa Prasad Majumder et al., 2024)</a></li><li><a href=#8794--149306-longwanjuan-towards-systematic-measurement-for-long-text-quality-kai-lv-et-al-2024>(87/94 | 149/306) LongWanjuan: Towards Systematic Measurement for Long Text Quality (Kai Lv et al., 2024)</a></li><li><a href=#8894--150306-multilingual-coreference-resolution-in-low-resource-south-asian-languages-ritwik-mishra-et-al-2024>(88/94 | 150/306) Multilingual Coreference Resolution in Low-resource South Asian Languages (Ritwik Mishra et al., 2024)</a></li><li><a href=#8994--151306-leveraging-collection-wide-similarities-for-unsupervised-document-structure-extraction-gili-lior-et-al-2024>(89/94 | 151/306) Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction (Gili Lior et al., 2024)</a></li><li><a href=#9094--152306-cost-efficient-subjective-task-annotation-and-modeling-through-few-shot-annotator-adaptation-preni-golazizian-et-al-2024>(90/94 | 152/306) Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation (Preni Golazizian et al., 2024)</a></li><li><a href=#9194--153306-effects-of-term-weighting-approach-with-and-without-stop-words-removing-on-arabic-text-classification-esraa-alhenawi-et-al-2024>(91/94 | 153/306) Effects of term weighting approach with and without stop words removing on Arabic text classification (Esra&rsquo;a Alhenawi et al., 2024)</a></li><li><a href=#9294--154306-dress-dataset-for-rubric-based-essay-scoring-on-efl-writing-haneul-yoo-et-al-2024>(92/94 | 154/306) DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing (Haneul Yoo et al., 2024)</a></li><li><a href=#9394--155306-breaking-the-hisco-barrier-automatic-occupational-standardization-with-occcanine-christian-møller-dahl-et-al-2024>(93/94 | 155/306) Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE (Christian Møller Dahl et al., 2024)</a></li><li><a href=#9494--156306-an-effective-incorporating-heterogeneous-knowledge-curriculum-learning-for-sequence-labeling-xuemei-tang-et-al-2024>(94/94 | 156/306) An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling (Xuemei Tang et al., 2024)</a></li></ul></li><li><a href=#cscv-41>cs.CV (41)</a><ul><li><a href=#141--157306-vl-trojan-multimodal-instruction-backdoor-attacks-against-autoregressive-visual-language-models-jiawei-liang-et-al-2024>(1/41 | 157/306) VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models (Jiawei Liang et al., 2024)</a></li><li><a href=#241--158306-multi-organ-self-supervised-contrastive-learning-for-breast-lesion-segmentation-hugo-figueiras-et-al-2024>(2/41 | 158/306) Multi-organ Self-supervised Contrastive Learning for Breast Lesion Segmentation (Hugo Figueiras et al., 2024)</a></li><li><a href=#341--159306-zero-shot-generalization-across-architectures-for-visual-classification-evan-gerrtiz-et-al-2024>(3/41 | 159/306) Zero-shot generalization across architectures for visual classification (Evan Gerrtiz et al., 2024)</a></li><li><a href=#441--160306-srndiff-short-term-rainfall-nowcasting-with-condition-diffusion-model-xudong-ling-et-al-2024>(4/41 | 160/306) SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model (Xudong Ling et al., 2024)</a></li><li><a href=#541--161306-on-large-visual-language-models-for-medical-imaging-analysis-an-empirical-study-minh-hao-van-et-al-2024>(5/41 | 161/306) On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study (Minh-Hao Van et al., 2024)</a></li><li><a href=#641--162306-real-time-3d-aware-portrait-editing-from-a-single-image-qingyan-bai-et-al-2024>(6/41 | 162/306) Real-time 3D-aware Portrait Editing from a Single Image (Qingyan Bai et al., 2024)</a></li><li><a href=#741--163306-weakly-supervised-localisation-of-prostate-cancer-using-reinforcement-learning-for-bi-parametric-mr-images-martynas-pocius-et-al-2024>(7/41 | 163/306) Weakly supervised localisation of prostate cancer using reinforcement learning for bi-parametric MR images (Martynas Pocius et al., 2024)</a></li><li><a href=#841--164306-hybrid-reasoning-based-on-large-language-models-for-autonomous-car-driving-mehdi-azarafza-et-al-2024>(8/41 | 164/306) Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving (Mehdi Azarafza et al., 2024)</a></li><li><a href=#941--165306-transgop-transformer-based-gaze-object-prediction-binglu-wang-et-al-2024>(9/41 | 165/306) TransGOP: Transformer-Based Gaze Object Prediction (Binglu Wang et al., 2024)</a></li><li><a href=#1041--166306-effloc-lightweight-vision-transformer-for-efficient-6-dof-camera-relocalization-zhendong-xiao-et-al-2024>(10/41 | 166/306) EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization (Zhendong Xiao et al., 2024)</a></li><li><a href=#1141--167306-contrastive-prompts-improve-disentanglement-in-text-to-image-diffusion-models-chen-wu-et-al-2024>(11/41 | 167/306) Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models (Chen Wu et al., 2024)</a></li><li><a href=#1241--168306-unsupervised-learning-based-object-detection-using-contrastive-learning-chandan-kumar-et-al-2024>(12/41 | 168/306) Unsupervised learning based object detection using Contrastive Learning (Chandan Kumar et al., 2024)</a></li><li><a href=#1341--169306-a-unified-framework-and-dataset-for-assessing-gender-bias-in-vision-language-models-ashutosh-sathe-et-al-2024>(13/41 | 169/306) A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models (Ashutosh Sathe et al., 2024)</a></li><li><a href=#1441--170306-event-aware-video-corpus-moment-retrieval-danyang-hou-et-al-2024>(14/41 | 170/306) Event-aware Video Corpus Moment Retrieval (Danyang Hou et al., 2024)</a></li><li><a href=#1541--171306-codis-benchmarking-context-dependent-visual-comprehension-for-multimodal-large-language-models-fuwen-luo-et-al-2024>(15/41 | 171/306) CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models (Fuwen Luo et al., 2024)</a></li><li><a href=#1641--172306-t-stitch-accelerating-sampling-in-pre-trained-diffusion-models-with-trajectory-stitching-zizheng-pan-et-al-2024>(16/41 | 172/306) T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching (Zizheng Pan et al., 2024)</a></li><li><a href=#1741--173306-sdxl-lightning-progressive-adversarial-diffusion-distillation-shanchuan-lin-et-al-2024>(17/41 | 173/306) SDXL-Lightning: Progressive Adversarial Diffusion Distillation (Shanchuan Lin et al., 2024)</a></li><li><a href=#1841--174306-high-throughput-visual-nano-drone-to-nano-drone-relative-localization-using-onboard-fully-convolutional-networks-luca-crupi-et-al-2024>(18/41 | 174/306) High-throughput Visual Nano-drone to Nano-drone Relative Localization using Onboard Fully Convolutional Networks (Luca Crupi et al., 2024)</a></li><li><a href=#1941--175306-hybrid-video-diffusion-models-with-2d-triplane-and-3d-wavelet-representation-kihong-kim-et-al-2024>(19/41 | 175/306) Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation (Kihong Kim et al., 2024)</a></li><li><a href=#2041--176306-push-quantization-aware-training-toward-full-precision-performances-via-consistency-regularization-junbiao-pang-et-al-2024>(20/41 | 176/306) Push Quantization-Aware Training Toward Full Precision Performances via Consistency Regularization (Junbiao Pang et al., 2024)</a></li><li><a href=#2141--177306-mask-up-investigating-biases-in-face-re-identification-for-masked-faces-siddharth-d-jaiswal-et-al-2024>(21/41 | 177/306) Mask-up: Investigating Biases in Face Re-identification for Masked Faces (Siddharth D Jaiswal et al., 2024)</a></li><li><a href=#2241--178306-bee-net-a-deep-neural-network-to-identify-in-the-wild-bodily-expression-of-emotions-mohammad-mahdi-dehshibi-et-al-2024>(22/41 | 178/306) BEE-NET: A deep neural network to identify in-the-wild Bodily Expression of Emotions (Mohammad Mahdi Dehshibi et al., 2024)</a></li><li><a href=#2341--179306-tumor-segmentation-on-whole-slide-images-training-or-prompting-huaqian-wu-et-al-2024>(23/41 | 179/306) Tumor segmentation on whole slide images: training or prompting? (Huaqian Wu et al., 2024)</a></li><li><a href=#2441--180306-zero-bev-zero-shot-projection-of-any-first-person-modality-to-bev-maps-gianluca-monaci-et-al-2024>(24/41 | 180/306) Zero-BEV: Zero-shot Projection of Any First-Person Modality to BEV Maps (Gianluca Monaci et al., 2024)</a></li><li><a href=#2541--181306-mstar-multi-scale-backbone-architecture-search-for-timeseries-classification-tue-m-cao-et-al-2024>(25/41 | 181/306) MSTAR: Multi-Scale Backbone Architecture Search for Timeseries Classification (Tue M. Cao et al., 2024)</a></li><li><a href=#2641--182306-generalizable-semantic-vision-query-generation-for-zero-shot-panoptic-and-semantic-segmentation-jialei-chen-et-al-2024>(26/41 | 182/306) Generalizable Semantic Vision Query Generation for Zero-shot Panoptic and Semantic Segmentation (Jialei Chen et al., 2024)</a></li><li><a href=#2741--183306-yolov9-learning-what-you-want-to-learn-using-programmable-gradient-information-chien-yao-wang-et-al-2024>(27/41 | 183/306) YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information (Chien-Yao Wang et al., 2024)</a></li><li><a href=#2841--184306-explainable-classification-techniques-for-quantum-dot-device-measurements-daniel-schug-et-al-2024>(28/41 | 184/306) Explainable Classification Techniques for Quantum Dot Device Measurements (Daniel Schug et al., 2024)</a></li><li><a href=#2941--185306-robustness-of-deep-neural-networks-for-micro-doppler-radar-classification-mikolaj-czerkawski-et-al-2024>(29/41 | 185/306) Robustness of Deep Neural Networks for Micro-Doppler Radar Classification (Mikolaj Czerkawski et al., 2024)</a></li><li><a href=#3041--186306-delving-into-dark-regions-for-robust-shadow-detection-huankang-guan-et-al-2024>(30/41 | 186/306) Delving into Dark Regions for Robust Shadow Detection (Huankang Guan et al., 2024)</a></li><li><a href=#3141--187306-flexible-physical-camouflage-generation-based-on-a-differential-approach-yang-li-et-al-2024>(31/41 | 187/306) Flexible Physical Camouflage Generation Based on a Differential Approach (Yang Li et al., 2024)</a></li><li><a href=#3241--188306-todo-token-downsampling-for-efficient-generation-of-high-resolution-images-ethan-smith-et-al-2024>(32/41 | 188/306) ToDo: Token Downsampling for Efficient Generation of High-Resolution Images (Ethan Smith et al., 2024)</a></li><li><a href=#3341--189306-exploring-the-limits-of-semantic-image-compression-at-micro-bits-per-pixel-jordan-dotzel-et-al-2024>(33/41 | 189/306) Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel (Jordan Dotzel et al., 2024)</a></li><li><a href=#3441--190306-multi-scale-spatio-temporal-transformer-based-imbalanced-longitudinal-learning-for-glaucoma-forecasting-from-irregular-time-series-images-xikai-yang-et-al-2024>(34/41 | 190/306) Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning for Glaucoma Forecasting from Irregular Time Series Images (Xikai Yang et al., 2024)</a></li><li><a href=#3541--191306-seald-nerf-interactive-pixel-level-editing-for-dynamic-scenes-by-neural-radiance-fields-zhentao-huang-et-al-2024>(35/41 | 191/306) SealD-NeRF: Interactive Pixel-Level Editing for Dynamic Scenes by Neural Radiance Fields (Zhentao Huang et al., 2024)</a></li><li><a href=#3641--192306-benchcloudvision-a-benchmark-analysis-of-deep-learning-approaches-for-cloud-detection-and-segmentation-in-remote-sensing-imagery-loddo-fabio-et-al-2024>(36/41 | 192/306) BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery (Loddo Fabio et al., 2024)</a></li><li><a href=#3741--193306-scene-prior-filtering-for-depth-map-super-resolution-zhengxue-wang-et-al-2024>(37/41 | 193/306) Scene Prior Filtering for Depth Map Super-Resolution (Zhengxue Wang et al., 2024)</a></li><li><a href=#3841--194306-identifying-unnecessary-3d-gaussians-using-clustering-for-fast-rendering-of-3d-gaussian-splatting-joongho-jo-et-al-2024>(38/41 | 194/306) Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting (Joongho Jo et al., 2024)</a></li><li><a href=#3941--195306-class-aware-mask-guided-feature-refinement-for-scene-text-recognition-mingkun-yang-et-al-2024>(39/41 | 195/306) Class-Aware Mask-Guided Feature Refinement for Scene Text Recognition (Mingkun Yang et al., 2024)</a></li><li><a href=#4041--196306-learning-pixel-wise-continuous-depth-representation-via-clustering-for-depth-completion-chen-shenglun-et-al-2024>(40/41 | 196/306) Learning Pixel-wise Continuous Depth Representation via Clustering for Depth Completion (Chen Shenglun et al., 2024)</a></li><li><a href=#4141--197306-improving-video-corpus-moment-retrieval-with-partial-relevance-enhancement-danyang-hou-et-al-2024>(41/41 | 197/306) Improving Video Corpus Moment Retrieval with Partial Relevance Enhancement (Danyang Hou et al., 2024)</a></li></ul></li><li><a href=#cscr-7>cs.CR (7)</a><ul><li><a href=#17--198306-privacy-preserving-instructions-for-aligning-large-language-models-da-yu-et-al-2024>(1/7 | 198/306) Privacy-Preserving Instructions for Aligning Large Language Models (Da Yu et al., 2024)</a></li><li><a href=#27--199306-llm-jailbreak-attack-versus-defense-techniques----a-comprehensive-study-zihao-xu-et-al-2024>(2/7 | 199/306) LLM Jailbreak Attack versus Defense Techniques &ndash; A Comprehensive Study (Zihao Xu et al., 2024)</a></li><li><a href=#37--200306-generative-ai-for-secure-physical-layer-communications-a-survey-changyuan-zhao-et-al-2024>(3/7 | 200/306) Generative AI for Secure Physical Layer Communications: A Survey (Changyuan Zhao et al., 2024)</a></li><li><a href=#47--201306-finding-incompatibles-blocks-for-reliable-jpeg-steganalysis-etienne-levecque-et-al-2024>(4/7 | 201/306) Finding Incompatibles Blocks for Reliable JPEG Steganalysis (Etienne Levecque et al., 2024)</a></li><li><a href=#57--202306-sissa-real-time-monitoring-of-hardware-functional-safety-and-cybersecurity-with-in-vehicle-someip-ethernet-traffic-qi-liu-et-al-2024>(5/7 | 202/306) SISSA: Real-time Monitoring of Hardware Functional Safety and Cybersecurity with In-vehicle SOME/IP Ethernet Traffic (Qi Liu et al., 2024)</a></li><li><a href=#67--203306-a-unified-knowledge-graph-to-permit-interoperability-of-heterogeneous-digital-evidence-ali-alshumrani-et-al-2024>(6/7 | 203/306) A Unified Knowledge Graph to Permit Interoperability of Heterogeneous Digital Evidence (Ali Alshumrani et al., 2024)</a></li><li><a href=#77--204306-towards-efficient-verification-of-constant-time-cryptographic-implementations-luwei-cai-et-al-2024>(7/7 | 204/306) Towards Efficient Verification of Constant-Time Cryptographic Implementations (Luwei Cai et al., 2024)</a></li></ul></li><li><a href=#csro-13>cs.RO (13)</a><ul><li><a href=#113--205306-cofrida-self-supervised-fine-tuning-for-human-robot-co-painting-peter-schaldenbrand-et-al-2024>(1/13 | 205/306) CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting (Peter Schaldenbrand et al., 2024)</a></li><li><a href=#213--206306-leveraging-demonstrator-perceived-precision-for-safe-interactive-imitation-learning-of-clearance-limited-tasks-hanbit-oh-et-al-2024>(2/13 | 206/306) Leveraging Demonstrator-perceived Precision for Safe Interactive Imitation Learning of Clearance-limited Tasks (Hanbit Oh et al., 2024)</a></li><li><a href=#313--207306-autonomous-mapless-navigation-on-uneven-terrains-hassan-jardali-et-al-2024>(3/13 | 207/306) Autonomous Mapless Navigation on Uneven Terrains (Hassan Jardali et al., 2024)</a></li><li><a href=#413--208306-realdex-towards-human-like-grasping-for-robotic-dexterous-hand-yumeng-liu-et-al-2024>(4/13 | 208/306) RealDex: Towards Human-like Grasping for Robotic Dexterous Hand (Yumeng Liu et al., 2024)</a></li><li><a href=#513--209306-khronos-a-unified-approach-for-spatio-temporal-metric-semantic-slam-in-dynamic-environments-lukas-schmid-et-al-2024>(5/13 | 209/306) Khronos: A Unified Approach for Spatio-Temporal Metric-Semantic SLAM in Dynamic Environments (Lukas Schmid et al., 2024)</a></li><li><a href=#613--210306-generating-realistic-arm-movements-in-reinforcement-learning-a-quantitative-comparison-of-reward-terms-and-task-requirements-jhon-charaja-et-al-2024>(6/13 | 210/306) Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements (Jhon Charaja et al., 2024)</a></li><li><a href=#713--211306-learning-dual-arm-object-rearrangement-for-cartesian-robots-shishun-zhang-et-al-2024>(7/13 | 211/306) Learning Dual-arm Object Rearrangement for Cartesian Robots (Shishun Zhang et al., 2024)</a></li><li><a href=#813--212306-learning-to-model-diverse-driving-behaviors-in-highly-interactive-autonomous-driving-scenarios-with-multi-agent-reinforcement-learning-liu-weiwei-et-al-2024>(8/13 | 212/306) Learning to Model Diverse Driving Behaviors in Highly Interactive Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning (Liu Weiwei et al., 2024)</a></li><li><a href=#913--213306-blending-data-driven-priors-in-dynamic-games-justin-lidard-et-al-2024>(9/13 | 213/306) Blending Data-Driven Priors in Dynamic Games (Justin Lidard et al., 2024)</a></li><li><a href=#1013--214306-gdtm-an-indoor-geospatial-tracking-dataset-with-distributed-multimodal-sensors-ho-lyun-jeong-et-al-2024>(10/13 | 214/306) GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors (Ho Lyun Jeong et al., 2024)</a></li><li><a href=#1113--215306-a-combined-learning-and-optimization-framework-to-transfer-human-whole-body-loco-manipulation-skills-to-mobile-manipulators-jianzhuang-zhao-et-al-2024>(11/13 | 215/306) A Combined Learning and Optimization Framework to Transfer Human Whole-body Loco-manipulation Skills to Mobile Manipulators (Jianzhuang Zhao et al., 2024)</a></li><li><a href=#1213--216306-learning-control-strategy-in-soft-robotics-through-a-set-of-configuration-spaces-etienne-ménager-et-al-2024>(12/13 | 216/306) Learning control strategy in soft robotics through a set of configuration spaces (Etienne Ménager et al., 2024)</a></li><li><a href=#1313--217306-voom-robust-visual-object-odometry-and-mapping-using-hierarchical-landmarks-yutong-wang-et-al-2024>(13/13 | 217/306) VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks (Yutong Wang et al., 2024)</a></li></ul></li><li><a href=#cssd-3>cs.SD (3)</a><ul><li><a href=#13--218306-the-effect-of-batch-size-on-contrastive-self-supervised-speech-representation-learning-nik-vaessen-et-al-2024>(1/3 | 218/306) The Effect of Batch Size on Contrastive Self-Supervised Speech Representation Learning (Nik Vaessen et al., 2024)</a></li><li><a href=#23--219306-advancing-audio-fingerprinting-accuracy-addressing-background-noise-and-distortion-challenges-navin-kamuni-et-al-2024>(2/3 | 219/306) Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges (Navin Kamuni et al., 2024)</a></li><li><a href=#33--220306-music-style-transfer-with-time-varying-inversion-of-diffusion-models-sifei-li-et-al-2024>(3/3 | 220/306) Music Style Transfer with Time-Varying Inversion of Diffusion Models (Sifei Li et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--221306-rfi-drunet-restoring-dynamic-spectra-corrupted-by-radio-frequency-interference----application-to-pulsar-observations-xiao-zhang-et-al-2024>(1/1 | 221/306) RFI-DRUnet: Restoring dynamic spectra corrupted by radio frequency interference &ndash; Application to pulsar observations (Xiao Zhang et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--222306-an-evaluation-of-large-language-models-in-bioinformatics-research-hengchuang-yin-et-al-2024>(1/1 | 222/306) An Evaluation of Large Language Models in Bioinformatics Research (Hengchuang Yin et al., 2024)</a></li></ul></li><li><a href=#csir-13>cs.IR (13)</a><ul><li><a href=#113--223306-general-debiasing-for-graph-based-collaborative-filtering-via-adversarial-graph-dropout-an-zhang-et-al-2024>(1/13 | 223/306) General Debiasing for Graph-based Collaborative Filtering via Adversarial Graph Dropout (An Zhang et al., 2024)</a></li><li><a href=#213--224306-linear-time-graph-neural-networks-for-scalable-recommendations-jiahao-zhang-et-al-2024>(2/13 | 224/306) Linear-Time Graph Neural Networks for Scalable Recommendations (Jiahao Zhang et al., 2024)</a></li><li><a href=#313--225306-the-effectiveness-of-graph-contrastive-learning-on-mathematical-information-retrieval-pei-syuan-wang-et-al-2024>(3/13 | 225/306) The Effectiveness of Graph Contrastive Learning on Mathematical Information Retrieval (Pei-Syuan Wang et al., 2024)</a></li><li><a href=#413--226306-llm4sbr-a-lightweight-and-effective-framework-for-integrating-large-language-models-in-session-based-recommendation-shutong-qiao-et-al-2024>(4/13 | 226/306) LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation (Shutong Qiao et al., 2024)</a></li><li><a href=#513--227306-breaking-the-barrier-utilizing-large-language-models-for-industrial-recommendation-systems-through-an-inferential-knowledge-graph-qian-zhao-et-al-2024>(5/13 | 227/306) Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph (Qian Zhao et al., 2024)</a></li><li><a href=#613--228306-birco-a-benchmark-of-information-retrieval-tasks-with-complex-objectives-xiaoyue-wang-et-al-2024>(6/13 | 228/306) BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives (Xiaoyue Wang et al., 2024)</a></li><li><a href=#713--229306-combining-language-and-graph-models-for-semi-structured-information-extraction-on-the-web-zhi-hong-et-al-2024>(7/13 | 229/306) Combining Language and Graph Models for Semi-structured Information Extraction on the Web (Zhi Hong et al., 2024)</a></li><li><a href=#813--230306-leveraging-translation-for-optimal-recall-tailoring-llm-personalization-with-user-profiles-karthik-ravichandran-et-al-2024>(8/13 | 230/306) Leveraging Translation For Optimal Recall: Tailoring LLM Personalization With User Profiles (Karthik Ravichandran et al., 2024)</a></li><li><a href=#913--231306-can-one-embedding-fit-all-a-multi-interest-learning-paradigm-towards-improving-user-interest-diversity-fairness-yuying-zhao-et-al-2024>(9/13 | 231/306) Can One Embedding Fit All? A Multi-Interest Learning Paradigm Towards Improving User Interest Diversity Fairness (Yuying Zhao et al., 2024)</a></li><li><a href=#1013--232306-science-checker-reloaded-a-bidirectional-paradigm-for-transparency-and-logical-reasoning-loïc-rakotoson-et-al-2024>(10/13 | 232/306) Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning (Loïc Rakotoson et al., 2024)</a></li><li><a href=#1113--233306-learning-to-retrieve-for-job-matching-jianqiang-shen-et-al-2024>(11/13 | 233/306) Learning to Retrieve for Job Matching (Jianqiang Shen et al., 2024)</a></li><li><a href=#1213--234306-retention-induced-biases-in-a-recommendation-system-with-heterogeneous-users-shichao-ma-2024>(12/13 | 234/306) Retention Induced Biases in a Recommendation System with Heterogeneous Users (Shichao Ma, 2024)</a></li><li><a href=#1313--235306-diversity-aware-k-maximum-inner-product-search-revisited-qiang-huang-et-al-2024>(13/13 | 235/306) Diversity-Aware $k$-Maximum Inner Product Search Revisited (Qiang Huang et al., 2024)</a></li></ul></li><li><a href=#csit-5>cs.IT (5)</a><ul><li><a href=#15--236306-near-field-multiuser-beam-training-for-extremely-large-scale-mimo-systems-wang-liu-et-al-2024>(1/5 | 236/306) Near-Field Multiuser Beam-Training for Extremely Large-Scale MIMO Systems (Wang Liu et al., 2024)</a></li><li><a href=#25--237306-coding-theorems-for-repetition-and-superposition-codes-over-binary-input-output-symmetric-channels-yixin-wang-et-al-2024>(2/5 | 237/306) Coding Theorems for Repetition and Superposition Codes over Binary-Input Output-Symmetric Channels (Yixin Wang et al., 2024)</a></li><li><a href=#35--238306-q-learning-based-joint-design-of-adaptive-modulation-and-precoding-for-physical-layer-security-in-visible-light-communications-duc-m-t-hoang-et-al-2024>(3/5 | 238/306) Q-learning-based Joint Design of Adaptive Modulation and Precoding for Physical Layer Security in Visible Light Communications (Duc M. T. Hoang et al., 2024)</a></li><li><a href=#45--239306-secrecy-performance-analysis-of-space-to-ground-optical-satellite-communications-thang-v-nguyen-et-al-2024>(4/5 | 239/306) Secrecy Performance Analysis of Space-to-Ground Optical Satellite Communications (Thang V. Nguyen et al., 2024)</a></li><li><a href=#55--240306-reconfigurable-intelligent-surfaces-for-thz-hardware-impairments-and-switching-technologies-sérgio-matos-et-al-2024>(5/5 | 240/306) Reconfigurable Intelligent Surfaces for THz: Hardware Impairments and Switching Technologies (Sérgio Matos et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--241306-verifying-message-passing-neural-networks-via-topology-based-bounds-tightening-christopher-hojny-et-al-2024>(1/2 | 241/306) Verifying message-passing neural networks via topology-based bounds tightening (Christopher Hojny et al., 2024)</a></li><li><a href=#22--242306-a-cutting-plane-algorithm-for-globally-solving-low-dimensional-k-means-clustering-problems-martin-ryner-et-al-2024>(2/2 | 242/306) A cutting plane algorithm for globally solving low dimensional k-means clustering problems (Martin Ryner et al., 2024)</a></li></ul></li><li><a href=#csai-8>cs.AI (8)</a><ul><li><a href=#18--243306-synthesis-of-hierarchical-controllers-based-on-deep-reinforcement-learning-policies-florent-delgrange-et-al-2024>(1/8 | 243/306) Synthesis of Hierarchical Controllers Based on Deep Reinforcement Learning Policies (Florent Delgrange et al., 2024)</a></li><li><a href=#28--244306-social-environment-design-edwin-zhang-et-al-2024>(2/8 | 244/306) Social Environment Design (Edwin Zhang et al., 2024)</a></li><li><a href=#38--245306-beyond-a-better-planning-with-transformers-via-search-dynamics-bootstrapping-lucas-lehnert-et-al-2024>(3/8 | 245/306) Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping (Lucas Lehnert et al., 2024)</a></li><li><a href=#48--246306-the-delusional-hedge-algorithm-as-a-model-of-human-learning-from-diverse-opinions-yun-shiuan-chuang-et-al-2024>(4/8 | 246/306) The Delusional Hedge Algorithm as a Model of Human Learning from Diverse Opinions (Yun-Shiuan Chuang et al., 2024)</a></li><li><a href=#58--247306-large-language-models-are-advanced-anonymizers-robin-staab-et-al-2024>(5/8 | 247/306) Large Language Models are Advanced Anonymizers (Robin Staab et al., 2024)</a></li><li><a href=#68--248306-a-neuro-symbolic-approach-to-multi-agent-rl-for-interpretability-and-probabilistic-decision-making-chitra-subramanian-et-al-2024>(6/8 | 248/306) A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and Probabilistic Decision Making (Chitra Subramanian et al., 2024)</a></li><li><a href=#78--249306-semirings-for-probabilistic-and-neuro-symbolic-logic-programming-vincent-derkinderen-et-al-2024>(7/8 | 249/306) Semirings for Probabilistic and Neuro-Symbolic Logic Programming (Vincent Derkinderen et al., 2024)</a></li><li><a href=#88--250306-mastering-the-game-of-guandan-with-deep-reinforcement-learning-and-behavior-regulating-yifan-yanggong-et-al-2024>(8/8 | 250/306) Mastering the Game of Guandan with Deep Reinforcement Learning and Behavior Regulating (Yifan Yanggong et al., 2024)</a></li></ul></li><li><a href=#eessiv-3>eess.IV (3)</a><ul><li><a href=#13--251306-exact-netehr-guided-lung-tumor-auto-segmentation-for-non-small-cell-lung-cancer-radiotherapy-hamed-hooshangnejad-et-al-2024>(1/3 | 251/306) EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy (Hamed Hooshangnejad et al., 2024)</a></li><li><a href=#23--252306-adversarial-purification-and-fine-tuning-for-robust-udc-image-restoration-zhenbo-song-et-al-2024>(2/3 | 252/306) Adversarial Purification and Fine-tuning for Robust UDC Image Restoration (Zhenbo Song et al., 2024)</a></li><li><a href=#33--253306-cas-diffcom-cascaded-diffusion-model-for-infant-longitudinal-super-resolution-3d-medical-image-completion-lianghu-guo-et-al-2024>(3/3 | 253/306) Cas-DiffCom: Cascaded diffusion model for infant longitudinal super-resolution 3D medical image completion (Lianghu Guo et al., 2024)</a></li></ul></li><li><a href=#csse-6>cs.SE (6)</a><ul><li><a href=#16--254306-test-driven-development-for-code-generation-noble-saji-mathews-et-al-2024>(1/6 | 254/306) Test-Driven Development for Code Generation (Noble Saji Mathews et al., 2024)</a></li><li><a href=#26--255306-ritfis-robust-input-testing-framework-for-llms-based-intelligent-software-mingxuan-xiao-et-al-2024>(2/6 | 255/306) RITFIS: Robust input testing framework for LLMs-based intelligent software (Mingxuan Xiao et al., 2024)</a></li><li><a href=#36--256306-eyetrans-merging-human-and-machine-attention-for-neural-code-summarization-yifan-zhang-et-al-2024>(3/6 | 256/306) EyeTrans: Merging Human and Machine Attention for Neural Code Summarization (Yifan Zhang et al., 2024)</a></li><li><a href=#46--257306-using-large-language-models-for-natural-language-processing-tasks-in-requirements-engineering-a-systematic-guideline-andreas-vogelsang-et-al-2024>(4/6 | 257/306) Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline (Andreas Vogelsang et al., 2024)</a></li><li><a href=#56--258306-green-ai-a-preliminary-empirical-study-on-energy-consumption-in-dl-models-across-different-runtime-infrastructures-negar-alizadeh-et-al-2024>(5/6 | 258/306) Green AI: A Preliminary Empirical Study on Energy Consumption in DL Models Across Different Runtime Infrastructures (Negar Alizadeh et al., 2024)</a></li><li><a href=#66--259306-restruler-towards-automatically-identifying-violations-of-restful-design-rules-in-web-apis-justus-bogner-et-al-2024>(6/6 | 259/306) RESTRuler: Towards Automatically Identifying Violations of RESTful Design Rules in Web APIs (Justus Bogner et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--260306-localtweets-to-localhealth-a-mental-health-surveillance-framework-based-on-twitter-data-vijeta-deshpande-et-al-2024>(1/2 | 260/306) LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data (Vijeta Deshpande et al., 2024)</a></li><li><a href=#22--261306-fairness-rising-from-the-ranks-hits-and-pagerank-on-homophilic-networks-ana-andreea-stoica-et-al-2024>(2/2 | 261/306) Fairness Rising from the Ranks: HITS and PageRank on Homophilic Networks (Ana-Andreea Stoica et al., 2024)</a></li></ul></li><li><a href=#physicsgeo-ph-1>physics.geo-ph (1)</a><ul><li><a href=#11--262306-neural-networks-and-friction-slide-hold-learn-joaquin-garcia-suarez-2024>(1/1 | 262/306) Neural Networks and Friction: Slide, Hold, Learn (Joaquin Garcia-Suarez, 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--263306-reinforcement-learning-assisted-quantum-architecture-search-for-variational-quantum-algorithms-akash-kundu-2024>(1/1 | 263/306) Reinforcement learning-assisted quantum architecture search for variational quantum algorithms (Akash Kundu, 2024)</a></li></ul></li><li><a href=#astro-phep-1>astro-ph.EP (1)</a><ul><li><a href=#11--264306-computing-transiting-exoplanet-parameters-with-1d-convolutional-neural-networks-santiago-iglesias-álvarez-et-al-2024>(1/1 | 264/306) Computing Transiting Exoplanet Parameters with 1D Convolutional Neural Networks (Santiago Iglesias Álvarez et al., 2024)</a></li></ul></li><li><a href=#eesssy-6>eess.SY (6)</a><ul><li><a href=#16--265306-pi-cof-a-bilevel-optimization-framework-for-solving-active-learning-problems-using-physics-information-liqiu-dong-et-al-2024>(1/6 | 265/306) PI-CoF: A Bilevel Optimization Framework for Solving Active Learning Problems using Physics-Information (Liqiu Dong et al., 2024)</a></li><li><a href=#26--266306-driving-towards-stability-and-efficiency-a-variable-time-gap-strategy-for-adaptive-cruise-control-shaimaa-k-el-baklish-et-al-2024>(2/6 | 266/306) Driving Towards Stability and Efficiency: A Variable Time Gap Strategy for Adaptive Cruise Control (Shaimaa K. El-Baklish et al., 2024)</a></li><li><a href=#36--267306-improving-a-proportional-integral-controller-with-reinforcement-learning-on-a-throttle-valve-benchmark-paul-daoudi-et-al-2024>(3/6 | 267/306) Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark (Paul Daoudi et al., 2024)</a></li><li><a href=#46--268306-extending-identifiability-results-from-isolated-networks-to-embedded-networks-eduardo-mapurunga-et-al-2024>(4/6 | 268/306) Extending identifiability results from isolated networks to embedded networks (Eduardo Mapurunga et al., 2024)</a></li><li><a href=#56--269306-a-monolithic-cybersecurity-architecture-for-power-electronic-systems-kirti-gupta-et-al-2024>(5/6 | 269/306) A Monolithic Cybersecurity Architecture for Power Electronic Systems (Kirti Gupta et al., 2024)</a></li><li><a href=#66--270306-delay-aware-semantic-sampling-in-power-electronic-systems-kirti-gupta-et-al-2024>(6/6 | 270/306) Delay-Aware Semantic Sampling in Power Electronic Systems (Kirti Gupta et al., 2024)</a></li></ul></li><li><a href=#csms-1>cs.MS (1)</a><ul><li><a href=#11--271306-democratizing-uncertainty-quantification-linus-seelinger-et-al-2024>(1/1 | 271/306) Democratizing Uncertainty Quantification (Linus Seelinger et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--272306-agentscope-a-flexible-yet-robust-multi-agent-platform-dawei-gao-et-al-2024>(1/1 | 272/306) AgentScope: A Flexible yet Robust Multi-Agent Platform (Dawei Gao et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--273306-what-is-the-focus-of-xai-in-ui-design-prioritizing-ui-design-principles-for-enhancing-xai-user-experience-dian-lei-et-al-2024>(1/3 | 273/306) What is the focus of XAI in UI design? Prioritizing UI design principles for enhancing XAI user experience (Dian Lei et al., 2024)</a></li><li><a href=#23--274306-meditating-in-live-stream-an-autoethnographic-and-interview-study-to-investigate-motivations-interactions-and-challenges-jingjin-li-et-al-2024>(2/3 | 274/306) Meditating in Live Stream: An Autoethnographic and Interview Study to Investigate Motivations, Interactions and Challenges (Jingjin Li et al., 2024)</a></li><li><a href=#33--275306-bring-your-own-character-a-holistic-solution-for-automatic-facial-animation-generation-of-customized-characters-zechen-bai-et-al-2024>(3/3 | 275/306) Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters (Zechen Bai et al., 2024)</a></li></ul></li><li><a href=#mathna-4>math.NA (4)</a><ul><li><a href=#14--276306-development-of-multi-physics-finite-element-model-to-investigate-electromagnetic-forming-and-simultaneous-multi-point-perforations-of-aluminium-tube-avinash-chetry-et-al-2024>(1/4 | 276/306) Development of multi-physics finite element model to investigate electromagnetic forming and simultaneous multi-point perforations of aluminium tube (Avinash Chetry et al., 2024)</a></li><li><a href=#24--277306-numerical-methods-for-closed-loop-systems-with-non-autonomous-data-b-baran-et-al-2024>(2/4 | 277/306) Numerical methods for closed-loop systems with non-autonomous data (B. Baran et al., 2024)</a></li><li><a href=#34--278306-on-the-application-of-subspace-migration-from-scattering-matrix-with-constant-valued-diagonal-elements-in-microwave-imaging-won-kwang-park-2024>(3/4 | 278/306) On the application of subspace migration from scattering matrix with constant-valued diagonal elements in microwave imaging (Won-Kwang Park, 2024)</a></li><li><a href=#44--279306-a-mixed-finite-element-finite-volume-semi-implicit-discretisation-for-atmospheric-dynamics-spherical-geometry-thomas-melvin-et-al-2024>(4/4 | 279/306) A mixed finite-element, finite-volume, semi-implicit discretisation for atmospheric dynamics: Spherical geometry (Thomas Melvin et al., 2024)</a></li></ul></li><li><a href=#statap-1>stat.AP (1)</a><ul><li><a href=#11--280306-measurement-uncertainty-relating-the-uncertainties-of-physical-and-virtual-measurements-simon-cramer-et-al-2024>(1/1 | 280/306) Measurement Uncertainty: Relating the uncertainties of physical and virtual measurements (Simon Cramer et al., 2024)</a></li></ul></li><li><a href=#statml-4>stat.ML (4)</a><ul><li><a href=#14--281306-analysis-of-bootstrap-and-subsampling-in-high-dimensional-regularized-regression-lucas-clarté-et-al-2024>(1/4 | 281/306) Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression (Lucas Clarté et al., 2024)</a></li><li><a href=#24--282306-multiply-robust-estimation-for-local-distribution-shifts-with-multiple-domains-steven-wilkins-reeves-et-al-2024>(2/4 | 282/306) Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains (Steven Wilkins-Reeves et al., 2024)</a></li><li><a href=#34--283306-probabilistic-neural-networks-pnns-for-modeling-aleatoric-uncertainty-in-scientific-machine-learning-farhad-pourkamali-anaraki-et-al-2024>(3/4 | 283/306) Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty in Scientific Machine Learning (Farhad Pourkamali-Anaraki et al., 2024)</a></li><li><a href=#44--284306-a-large-dimensional-analysis-of-multi-task-semi-supervised-learning-victor-leger-et-al-2024>(4/4 | 284/306) A Large Dimensional Analysis of Multi-task Semi-Supervised Learning (Victor Leger et al., 2024)</a></li></ul></li><li><a href=#mathco-2>math.CO (2)</a><ul><li><a href=#12--285306-self-adhesivity-in-lattices-of-abstract-conditional-independence-models-tobias-boege-et-al-2024>(1/2 | 285/306) Self-adhesivity in lattices of abstract conditional independence models (Tobias Boege et al., 2024)</a></li><li><a href=#22--286306-grid-minors-and-products-vida-dujmović-et-al-2024>(2/2 | 286/306) Grid Minors and Products (Vida Dujmović et al., 2024)</a></li></ul></li><li><a href=#csne-3>cs.NE (3)</a><ul><li><a href=#13--287306-origami-unfolding-the-abstraction-of-recursion-schemes-for-program-synthesis-matheus-campos-fernandes-et-al-2024>(1/3 | 287/306) Origami: (un)folding the abstraction of recursion schemes for program synthesis (Matheus Campos Fernandes et al., 2024)</a></li><li><a href=#23--288306-neuraldiffuser-controllable-fmri-reconstruction-with-primary-visual-feature-guided-diffusion-haoyu-li-et-al-2024>(2/3 | 288/306) NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion (Haoyu Li et al., 2024)</a></li><li><a href=#33--289306-an-effective-networks-intrusion-detection-approach-based-on-hybrid-harris-hawks-and-multi-layer-perceptron-moutaz-alazab-et-al-2024>(3/3 | 289/306) An Effective Networks Intrusion Detection Approach Based on Hybrid Harris Hawks and Multi-Layer Perceptron (Moutaz Alazab et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--290306-balancing-spectral-temporal-and-spatial-information-for-eeg-based-alzheimers-disease-classification-stephan-goerttler-et-al-2024>(1/1 | 290/306) Balancing Spectral, Temporal and Spatial Information for EEG-based Alzheimer&rsquo;s Disease Classification (Stephan Goerttler et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--291306-fairness-and-incentive-compatibility-via-percentage-fees-shahar-dobzinski-et-al-2024>(1/1 | 291/306) Fairness and Incentive Compatibility via Percentage Fees (Shahar Dobzinski et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--292306-open-source-software-field-research-spanning-social-and-practice-networks-for-re-entering-the-field-sean-p-goggins-et-al-2024>(1/1 | 292/306) Open Source Software Field Research: Spanning Social and Practice Networks for Re-Entering the Field (Sean P. Goggins et al., 2024)</a></li></ul></li><li><a href=#csds-5>cs.DS (5)</a><ul><li><a href=#15--293306-masked-matrix-multiplication-for-emergent-sparsity-brian-wheatman-et-al-2024>(1/5 | 293/306) Masked Matrix Multiplication for Emergent Sparsity (Brian Wheatman et al., 2024)</a></li><li><a href=#25--294306-practical-algorithms-for-hierarchical-overlap-graphs-saumya-talera-et-al-2024>(2/5 | 294/306) Practical algorithms for Hierarchical overlap graphs (Saumya Talera et al., 2024)</a></li><li><a href=#35--295306-robust-recovery-for-stochastic-block-models-simplified-and-generalized-sidhanth-mohanty-et-al-2024>(3/5 | 295/306) Robust recovery for stochastic block models, simplified and generalized (Sidhanth Mohanty et al., 2024)</a></li><li><a href=#45--296306-a-uniformly-random-solution-to-algorithmic-redistricting-jin-yi-cai-et-al-2024>(4/5 | 296/306) A Uniformly Random Solution to Algorithmic Redistricting (Jin-Yi Cai et al., 2024)</a></li><li><a href=#55--297306-multi-agent-online-graph-exploration-on-cycles-and-tadpole-graphs-erik-van-den-akker-et-al-2024>(5/5 | 297/306) Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs (Erik van den Akker et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--298306-learning-dynamic-representations-of-the-functional-connectome-in-neurobiological-networks-luciano-dyballa-et-al-2024>(1/1 | 298/306) Learning dynamic representations of the functional connectome in neurobiological networks (Luciano Dyballa et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--299306-model-checking-logical-actions-in-magic-tricks-weijun-zhu-2024>(1/1 | 299/306) Model Checking Logical Actions in Magic Tricks (Weijun Zhu, 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#12--300306-benchmarking-and-dissecting-the-nvidia-hopper-gpu-architecture-weile-luo-et-al-2024>(1/2 | 300/306) Benchmarking and Dissecting the Nvidia Hopper GPU Architecture (Weile Luo et al., 2024)</a></li><li><a href=#22--301306-guac-energy-aware-and-ssa-based-generation-of-coarse-grained-merged-accelerators-from-llvm-ir-iulian-brumar-et-al-2024>(2/2 | 301/306) Guac: Energy-Aware and SSA-Based Generation of Coarse-Grained Merged Accelerators from LLVM-IR (Iulian Brumar et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--302306-improved-lower-bound-on-the-number-of-pseudoline-arrangements-justin-dallant-2024>(1/1 | 302/306) Improved Lower Bound on the Number of Pseudoline Arrangements (Justin Dallant, 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--303306-on-distributed-computation-of-the-minimum-triangle-edge-transversal-keren-censor-hillel-et-al-2024>(1/2 | 303/306) On Distributed Computation of the Minimum Triangle Edge Transversal (Keren Censor-Hillel et al., 2024)</a></li><li><a href=#22--304306-adaptive-massively-parallel-coloring-in-sparse-graphs-rustam-latypov-et-al-2024>(2/2 | 304/306) Adaptive Massively Parallel Coloring in Sparse Graphs (Rustam Latypov et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--305306-edge-disjoint-paths-in-eulerian-digraphs-dario-cavallaro-et-al-2024>(1/1 | 305/306) Edge-Disjoint Paths in Eulerian Digraphs (Dario Cavallaro et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--306306-towards-linear-spanners-in-all-temporal-cliques-sebastian-angrick-et-al-2024>(1/1 | 306/306) Towards Linear Spanners in All Temporal Cliques (Sebastian Angrick et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>