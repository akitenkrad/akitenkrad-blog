<!doctype html><html><head><title>arXiv @ 2024.02.18</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.18"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (1) cs.AI (7) cs.AR (1) cs.CC (1) cs.CE (1) cs.CL (68) cs.CR (3) cs.CV (19) cs.DC (1) cs.DS (3) cs.GT (1) cs.HC (3) cs.IR (5) cs.IT (4) cs.LG (42) cs.LO (1) cs.MM (1) cs.NI (3) cs.PL (1) cs.RO (4) cs.SD (2) cs.SE (2) cs.SI (1) eess.AS (1) eess.IV (6) eess.SP (3) eess.SY (2) math.NA (3) q-bio.BM (3) q-fin.CP (2) q-fin.ST (1) quant-ph (1) stat.AP (1) stat.ML (6) Keywords keyword cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240218000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-18T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.18"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240218000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Sunday, Feb 18, 2024</p></div><div class=title><h1>arXiv @ 2024.02.18</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/#csai-7>cs.AI (7)</a></li><li><a href=/akitenkrad-blog/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/#cscl-68>cs.CL (68)</a></li><li><a href=/akitenkrad-blog/#cscr-3>cs.CR (3)</a></li><li><a href=/akitenkrad-blog/#cscv-19>cs.CV (19)</a></li><li><a href=/akitenkrad-blog/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/#csds-3>cs.DS (3)</a></li><li><a href=/akitenkrad-blog/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/#csir-5>cs.IR (5)</a></li><li><a href=/akitenkrad-blog/#csit-4>cs.IT (4)</a></li><li><a href=/akitenkrad-blog/#cslg-42>cs.LG (42)</a></li><li><a href=/akitenkrad-blog/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/#csro-4>cs.RO (4)</a></li><li><a href=/akitenkrad-blog/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/#csse-2>cs.SE (2)</a></li><li><a href=/akitenkrad-blog/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/#eessas-1>eess.AS (1)</a></li><li><a href=/akitenkrad-blog/#eessiv-6>eess.IV (6)</a></li><li><a href=/akitenkrad-blog/#eesssp-3>eess.SP (3)</a></li><li><a href=/akitenkrad-blog/#eesssy-2>eess.SY (2)</a></li><li><a href=/akitenkrad-blog/#mathna-3>math.NA (3)</a></li><li><a href=/akitenkrad-blog/#q-biobm-3>q-bio.BM (3)</a></li><li><a href=/akitenkrad-blog/#q-fincp-2>q-fin.CP (2)</a></li><li><a href=/akitenkrad-blog/#q-finst-1>q-fin.ST (1)</a></li><li><a href=/akitenkrad-blog/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/#statap-1>stat.AP (1)</a></li><li><a href=/akitenkrad-blog/#statml-6>stat.ML (6)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td>1</td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>2</td></tr><tr><td>Autoencoder</td><td></td><td>2</td><td>1</td></tr><tr><td>Automatic Evaluation</td><td>1</td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td>1</td><td></td><td></td></tr><tr><td>BLOOM</td><td>1</td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td></tr><tr><td>Benchmarking</td><td>14</td><td>5</td><td>9</td></tr><tr><td>Chain-of-thought Prompt</td><td>1</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>5</td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td></td><td>1</td></tr><tr><td>Code Generation</td><td>1</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td></td><td></td></tr><tr><td>Continual Learning</td><td>1</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td>3</td><td>3</td></tr><tr><td>Convolution</td><td></td><td></td><td>4</td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>3</td></tr><tr><td>Counter-factual</td><td></td><td></td><td>1</td></tr><tr><td>Curriculum Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Data Augmentation</td><td></td><td></td><td>1</td></tr><tr><td>Dense Retrieval</td><td>1</td><td></td><td></td></tr><tr><td>Dialogue State Tracking</td><td>1</td><td></td><td></td></tr><tr><td>Dialogue System</td><td>1</td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td></td></tr><tr><td>Edge Prediction</td><td></td><td></td><td>1</td></tr><tr><td>Fairness</td><td>1</td><td></td><td>4</td></tr><tr><td>Federated Learning</td><td></td><td></td><td>3</td></tr><tr><td>Few-shot</td><td>2</td><td></td><td>2</td></tr><tr><td>Fine-tuning</td><td>23</td><td>2</td><td>5</td></tr><tr><td>Foundation Model</td><td>1</td><td></td><td></td></tr><tr><td>GPT</td><td>14</td><td>2</td><td>1</td></tr><tr><td>GPT-2</td><td>2</td><td></td><td></td></tr><tr><td>GPT-3</td><td>7</td><td>1</td><td></td></tr><tr><td>GPT-3.5</td><td>7</td><td>1</td><td></td></tr><tr><td>GPT-4</td><td>10</td><td></td><td>1</td></tr><tr><td>Gemini</td><td>2</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>1</td><td></td></tr><tr><td>Graph</td><td>3</td><td></td><td>8</td></tr><tr><td>Graph Contrastive Learning</td><td>1</td><td></td><td>2</td></tr><tr><td>Graph Convolutional Network</td><td>1</td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>6</td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td></tr><tr><td>High-Resource</td><td>2</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>14</td><td></td><td>3</td></tr><tr><td>Information Retrieval</td><td>4</td><td></td><td>1</td></tr><tr><td>Instruction Following</td><td>2</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>7</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>14</td><td>1</td><td>3</td></tr><tr><td>Knowledge Graph</td><td>2</td><td></td><td></td></tr><tr><td>LLaMA</td><td>7</td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td>3</td></tr><tr><td>Language Generation</td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>93</td><td>6</td><td>11</td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td>1</td></tr><tr><td>MNIST</td><td></td><td></td><td>1</td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>2</td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>1</td></tr><tr><td>Mistral</td><td>1</td><td></td><td></td></tr><tr><td>Model Pruning</td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>1</td><td>10</td><td></td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td></tr><tr><td>Named Entity Recognition</td><td>4</td><td></td><td></td></tr><tr><td>Natural Language Explanation</td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>2</td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>3</td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>2</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>4</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>2</td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td>1</td></tr><tr><td>Out-of-distribution</td><td>2</td><td></td><td>1</td></tr><tr><td>Pre-trained Language Model</td><td>4</td><td></td><td></td></tr><tr><td>Prompt</td><td>20</td><td>3</td><td>3</td></tr><tr><td>Pruning</td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td>2</td><td></td><td>8</td></tr><tr><td>Question Answering</td><td>6</td><td>3</td><td></td></tr><tr><td>Reasoning</td><td>13</td><td>2</td><td></td></tr><tr><td>Recommendation</td><td>1</td><td></td><td>2</td></tr><tr><td>Reconstruction Loss</td><td></td><td></td><td>1</td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>2</td></tr><tr><td>Reinforcement Learning</td><td>2</td><td></td><td>5</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>1</td><td></td><td>4</td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td></tr><tr><td>Rerank</td><td>1</td><td></td><td></td></tr><tr><td>Retrieval Augmentation</td><td>1</td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>1</td><td></td><td></td></tr><tr><td>Rouge</td><td>2</td><td></td><td></td></tr><tr><td>Rouge-L</td><td>1</td><td></td><td></td></tr><tr><td>Selective Prediction</td><td></td><td></td><td>1</td></tr><tr><td>Self-Attention</td><td></td><td>2</td><td>1</td></tr><tr><td>Self-Distillation</td><td>1</td><td></td><td>1</td></tr><tr><td>Self-supervised Learning</td><td></td><td>2</td><td>3</td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td></tr><tr><td>Simulation</td><td>1</td><td></td><td>2</td></tr><tr><td>Simulator</td><td>1</td><td></td><td>2</td></tr><tr><td>Speech-to-Speech Translation</td><td>1</td><td></td><td></td></tr><tr><td>Stemming</td><td>1</td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>1</td></tr><tr><td>Summarization</td><td>2</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>5</td><td>1</td><td>3</td></tr><tr><td>TF-IDF</td><td>2</td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td>1</td><td></td></tr><tr><td>Text Mining</td><td>1</td><td></td><td></td></tr><tr><td>Text Understanding</td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>1</td><td></td></tr><tr><td>Tokenization</td><td></td><td></td><td>1</td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>3</td></tr><tr><td>Transformer</td><td>2</td><td>3</td><td>4</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Variational Autoencoder</td><td></td><td>1</td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>4</td><td>2</td></tr><tr><td>Visual Question Answering</td><td>2</td><td>2</td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td>1</td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>2</td></tr><tr><td>Zero-shot</td><td>7</td><td>2</td><td></td></tr><tr><td>falcon</td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-68>cs.CL (68)</h2><h3 id=168--1204-linkner-linking-local-named-entity-recognition-models-to-large-language-models-using-uncertainty-zhen-zhang-et-al-2024>(1/68 | 1/204) LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty (Zhen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Zhang, Yuhua Zhao, Hang Gao, Mengting Hu. (2024)<br><strong>LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty</strong><br><button class=copy-to-clipboard title="LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 133<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Recommendation, GPT, GPT-4, Information Retrieval, Named Entity Recognition, Named Entity Recognition, Natural Language Understanding, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10573v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10573v1.pdf filename=2402.10573v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> serves as a fundamental task in <b>natural</b> <b>language</b> <b>understanding,</b> bearing direct implications for web content analysis, search engines, and <b>information</b> <b>retrieval</b> systems. <b>Fine-tuned</b> <b>NER</b> models exhibit satisfactory performance on standard <b>NER</b> <b>benchmarks.</b> However, due to limited <b>fine-tuning</b> data and lack of knowledge, it performs poorly on unseen entity recognition. As a result, the usability and reliability of <b>NER</b> models in web-related applications are compromised. Instead, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>GPT-4</b> possess extensive external knowledge, but research indicates that they lack specialty for <b>NER</b> tasks. Furthermore, non-public and <b>large-scale</b> <b>weights</b> <b>make</b> tuning <b>LLMs</b> difficult. To address these challenges, we propose a framework that combines small <b>fine-tuned</b> models with <b>LLMs</b> (LinkNER) and an uncertainty-based linking strategy called RDC that enables <b>fine-tuned</b> models to complement black-box <b>LLMs,</b> achieving better performance. We experiment with both standard <b>NER</b> test sets and noisy social media datasets. LinkNER enhances <b>NER</b> task performance, notably surpassing SOTA models in robustness tests. We also quantitatively analyze the influence of key components like uncertainty estimation methods, <b>LLMs,</b> and <b>in-context</b> <b>learning</b> on diverse <b>NER</b> tasks, offering specific web-related <b>recommendations.</b></p></p class="citation"></blockquote><h3 id=268--2204-large-language-models-as-zero-shot-dialogue-state-tracker-through-function-calling-zekun-li-et-al-2024>(2/68 | 2/204) Large Language Models as Zero-shot Dialogue State Tracker through Function Calling (Zekun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zekun Li, Zhiyu Zoey Chen, Mike Ross, Patrick Huber, Seungwhan Moon, Zhaojiang Lin, Xin Luna Dong, Adithya Sagar, Xifeng Yan, Paul A. Crook. (2024)<br><strong>Large Language Models as Zero-shot Dialogue State Tracker through Function Calling</strong><br><button class=copy-to-clipboard title="Large Language Models as Zero-shot Dialogue State Tracker through Function Calling" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 120<br>Keywords: Fine-tuning, Zero-shot, ChatGPT, GPT, GPT-3, GPT-3.5, GPT-4, Dialogue State Tracking, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10466v1.pdf filename=2402.10466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented <b>dialogues</b> <b>(TOD),</b> <b>which</b> requires not only response generation but also effective <b>dialogue</b> <b>state</b> <b>tracking</b> (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with <b>LLMs</b> through function calling. This method improves <b>zero-shot</b> DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary <b>LLMs:</b> with <b>in-context</b> <b>prompting</b> it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by <b>ChatGPT,</b> and improves <b>ChatGPT&rsquo;s</b> performance beating the SOTA by 5.6% Avg. JGA. Individual model results for <b>GPT-3.5</b> and <b>GPT-4</b> are boosted by 4.8% and 14%, respectively. We also show that by <b>fine-tuning</b> on a small collection of diverse task-oriented <b>dialogues,</b> <b>we</b> <b>can</b> equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to <b>ChatGPT</b> while maintaining their chat capabilities. We plan to open-source experimental code and model.</p></p class="citation"></blockquote><h3 id=368--3204-multi-modal-preference-alignment-remedies-regression-of-visual-instruction-tuning-on-language-model-shengzhi-li-et-al-2024>(3/68 | 3/204) Multi-modal preference alignment remedies regression of visual instruction tuning on language model (Shengzhi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengzhi Li, Rongyu Lin, Shichao Pei. (2024)<br><strong>Multi-modal preference alignment remedies regression of visual instruction tuning on language model</strong><br><button class=copy-to-clipboard title="Multi-modal preference alignment remedies regression of visual instruction tuning on language model" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 116<br>Keywords: Benchmarking, Fine-tuning, Knowledge Distillation, Multi-modal, Reinforcement Learning from Human Feedback, Supervised Learning, Gemini, Instruction Following, Neural Machine Translation, Visual Question Answering, Visual Question Answering, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10884v1.pdf filename=2402.10884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In production, <b>multi-modal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with <b>visual-question-answering</b> <b>(VQA)</b> <b>datasets</b> could suffer from degradation, as <b>VQA</b> datasets lack the diversity and complexity of the original text <b>instruction</b> <b>datasets</b> which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) <b>VQA</b> preference dataset where answers were annotated by <b>Gemini</b> for 5 quality metrics in a granular fashion, and investigate standard <b>Supervised</b> <b>Fine-tuning,</b> rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass <b>instruction-following</b> <b>capabilities</b> of the language model, achieving a 6.73 score on <b>MT-Bench,</b> compared to Vicuna&rsquo;s 6.57 and LLaVA&rsquo;s 5.99 despite small data scale. This enhancement in textual <b>instruction</b> <b>proficiency</b> correlates with boosted <b>visual</b> <b>instruction</b> <b>performance</b> (+4.9% on MM-Vet, +6% on LLaVA-Bench), with minimal alignment tax on <b>visual</b> <b>knowledge</b> <b>benchmarks</b> compared to previous <b>RLHF</b> approach. In conclusion, we propose a <b>distillation-based</b> <b>multi-modal</b> alignment model with fine-grained annotations on a small dataset that reconciles the textual and <b>visual</b> <b>performance</b> <b>of</b> MLLMs, restoring and boosting language capability after <b>visual</b> <b>instruction</b> <b>tuning.</b></p></p class="citation"></blockquote><h3 id=468--4204-multi-cultural-commonsense-knowledge-distillation-tuan-phong-nguyen-et-al-2024>(4/68 | 4/204) Multi-Cultural Commonsense Knowledge Distillation (Tuan-Phong Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuan-Phong Nguyen, Simon Razniewski, Gerhard Weikum. (2024)<br><strong>Multi-Cultural Commonsense Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Multi-Cultural Commonsense Knowledge Distillation" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 113<br>Keywords: Clustering, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, GPT, GPT-3, GPT-3.5, Dialogue System, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10689v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10689v1.pdf filename=2402.10689v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite recent progress, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> still face the challenge of appropriately reacting to the intricacies of social and cultural conventions. This paper presents MANGO, a methodology for <b>distilling</b> high-accuracy, high-recall assertions of cultural <b>knowledge.</b> <b>We</b> judiciously and iteratively <b>prompt</b> <b>LLMs</b> for this purpose from two entry points, concepts and cultures. Outputs are consolidated via <b>clustering</b> and generative <b>summarization.</b> Running the MANGO method with <b>GPT-3.5</b> as underlying <b>LLM</b> yields 167K high-accuracy assertions for 30K concepts and 11K cultures, surpassing prior resources by a <b>large</b> <b>margin.</b> <b>For</b> extrinsic evaluation, we explore augmenting <b>dialogue</b> <b>systems</b> with cultural <b>knowledge</b> <b>assertions.</b> We find that adding <b>knowledge</b> <b>from</b> MANGO improves the overall quality, specificity, and cultural sensitivity of <b>dialogue</b> <b>responses,</b> as judged by human annotators. Data and code are available for download.</p></p class="citation"></blockquote><h3 id=568--5204-openfmnav-towards-open-set-zero-shot-object-navigation-via-vision-language-foundation-models-yuxuan-kuang-et-al-2024>(5/68 | 5/204) OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models (Yuxuan Kuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Kuang, Hai Lin, Meng Jiang. (2024)<br><strong>OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models</strong><br><button class=copy-to-clipboard title="OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-RO, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Foundation Model, Reinforcement Learning, Supervised Learning, Zero-shot, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10670v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10670v1.pdf filename=2402.10670v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on <b>supervised</b> or <b>reinforcement</b> <b>learning,</b> where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a <b>zero-shot</b> manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set <b>Foundation</b> <b>Model</b> based framework for <b>zero-shot</b> object Navigation. We first unleash the <b>reasoning</b> abilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to extract proposed objects from natural language instructions that meet the user&rsquo;s demand. We then leverage the generalizability of <b>large</b> <b>vision</b> <b>language</b> models (VLMs) to actively discover and detect candidate objects from the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting common sense <b>reasoning</b> on VSSM, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the <b>reasoning</b> and generalizing abilities of <b>foundation</b> <b>models,</b> our method can understand free-form human instructions and perform effective open-set <b>zero-shot</b> navigation in diverse environments. Extensive experiments on the HM3D ObjectNav <b>benchmark</b> show that our method surpasses all the strong baselines on all metrics, proving our method&rsquo;s effectiveness. Furthermore, we perform real robot demonstrations to validate our method&rsquo;s open-set-ness and generalizability to real-world environments.</p></p class="citation"></blockquote><h3 id=668--6204-bitdistiller-unleashing-the-potential-of-sub-4-bit-llms-via-self-distillation-dayou-du-et-al-2024>(6/68 | 6/204) BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation (Dayou Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, Ningyi Xu. (2024)<br><strong>BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation</strong><br><button class=copy-to-clipboard title="BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Quantization, Quantization, Self-Distillation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10631v1.pdf filename=2402.10631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The upscaling of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight <b>quantization</b> has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes <b>Quantization-Aware</b> Training (QAT) with <b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> to boost the performance of <b>LLMs</b> at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric <b>quantization</b> and clipping technique to maximally preserve the fidelity of <b>quantized</b> weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a <b>self-distillation</b> manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit configurations on general language understanding and complex <b>reasoning</b> <b>benchmarks.</b> Notably, BitDistiller is shown to be more cost-effective, demanding fewer data and training resources. The code is available at <a href=https://github.com/DD-DuDa/BitDistiller>https://github.com/DD-DuDa/BitDistiller</a>.</p></p class="citation"></blockquote><h3 id=768--7204-lets-learn-step-by-step-enhancing-in-context-learning-ability-with-curriculum-learning-yinpeng-liu-et-al-2024>(7/68 | 7/204) Let&rsquo;s Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning (Yinpeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, Wei Lu. (2024)<br><strong>Let&rsquo;s Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning</strong><br><button class=copy-to-clipboard title="Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Curriculum Learning, Few-shot, In-context Learning, In-context Learning, In-context Learning, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10738v1.pdf filename=2402.10738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Demonstration ordering, which is an important strategy for <b>in-context</b> <b>learning</b> <b>(ICL),</b> can significantly affects the performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, most of the current approaches of ordering require additional knowledge and similarity calculation. We advocate the <b>few-shot</b> <b>in-context</b> <b>curriculum</b> <b>learning</b> (ICCL), a simple but effective demonstration ordering method for <b>ICL,</b> which implies gradually increasing the complexity of <b>prompt</b> demonstrations during the inference process. Then we design three experiments to discuss the effectiveness of ICCL, the formation mechanism of <b>LLM&rsquo;s</b> ICCL capability, and the impact of ordering subjects. Experimental results demonstrate that ICCL, developed during the <b>instruction-tuning</b> <b>stage,</b> is effective for open-source <b>LLMs.</b> Moreover, <b>LLMs</b> exhibit a weaker capacity compared to humans in discerning the difficulty levels of demonstrations. We release our code at <a href=https://github.com/61peng/curri_learning>https://github.com/61peng/curri_learning</a>.</p></p class="citation"></blockquote><h3 id=868--8204-assessing-the-reasoning-abilities-of-chatgpt-in-the-context-of-claim-verification-john-dougrez-lewis-et-al-2024>(8/68 | 8/204) Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification (John Dougrez-Lewis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John Dougrez-Lewis, Mahmud Elahi Akhter, Yulan He, Maria Liakata. (2024)<br><strong>Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification</strong><br><button class=copy-to-clipboard title="Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Zero-shot, ChatGPT, GPT, GPT-3, GPT-3.5, GPT-4, Reasoning, Stemming, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10735v1.pdf filename=2402.10735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>reasoning</b> capabilities of <b>LLMs</b> are currently hotly debated. We examine the issue from the perspective of claim/rumour verification. We propose the first logical <b>reasoning</b> framework designed to break down any claim or rumor paired with evidence into the atomic <b>reasoning</b> steps necessary for verification. Based on our framework, we curate two annotated collections of such claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world set <b>stemming</b> from rumours circulating on Twitter. We use them to evaluate the <b>reasoning</b> capabilities of <b>GPT-3.5-Turbo</b> and <b>GPT-4</b> (hereinafter referred to as <b>ChatGPT)</b> within the context of our framework, providing a thorough analysis. Our results show that <b>ChatGPT</b> struggles in abductive <b>reasoning,</b> although this can be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to Zero Shot (ZS) and ZS CoT approaches. Our study contributes to the growing body of research suggesting that <b>ChatGPT&rsquo;s</b> <b>reasoning</b> processes are unlikely to mirror human-like <b>reasoning,</b> and that <b>LLMs</b> need to be more rigorously evaluated in order to distinguish between hype and actual capabilities, especially in high stake real-world tasks such as claim verification.</p></p class="citation"></blockquote><h3 id=968--9204-can-separators-improve-chain-of-thought-prompting-yoonjeong-park-et-al-2024>(9/68 | 9/204) Can Separators Improve Chain-of-Thought Prompting? (Yoonjeong Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoonjeong Park, Hyunjin Kim, Chanyeol Choi, Junseong Kim, Jy-yong Sohn. (2024)<br><strong>Can Separators Improve Chain-of-Thought Prompting?</strong><br><button class=copy-to-clipboard title="Can Separators Improve Chain-of-Thought Prompting?" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10645v1.pdf filename=2402.10645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chain-of-thought (CoT) <b>prompting</b> is a simple and effective method for improving the <b>reasoning</b> capabilities of <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> The basic idea of CoT is to let <b>LLMs</b> break down their thought processes step-by-step by putting exemplars in the input <b>prompt.</b> However, the densely structured <b>prompt</b> exemplars of CoT may cause the cognitive overload of <b>LLMs.</b> Inspired by human cognition, we introduce CoT-Sep, a novel method that strategically employs separators at the end of each exemplar in CoT <b>prompting.</b> These separators are designed to help the <b>LLMs</b> understand their thought processes better while <b>reasoning.</b> It turns out that CoT-Sep significantly improves the <b>LLMs&rsquo;</b> performances on complex <b>reasoning</b> tasks (e.g., GSM-8K, AQuA, CSQA), compared with the vanilla CoT, which does not use separators. We also study the effects of the type and the location of separators tested on multiple <b>LLMs,</b> including <b>GPT-3.5-Turbo,</b> <b>GPT-4,</b> and <b>LLaMA-2</b> 7B. Interestingly, the type/location of separators should be chosen appropriately to boost the <b>reasoning</b> capability of CoT.</p></p class="citation"></blockquote><h3 id=1068--10204-how-reliable-are-automatic-evaluation-methods-for-instruction-tuned-llms-ehsan-doostmohammadi-et-al-2024>(10/68 | 10/204) How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs? (Ehsan Doostmohammadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Doostmohammadi, Oskar Holmström, Marco Kuhlmann. (2024)<br><strong>How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?</strong><br><button class=copy-to-clipboard title="How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Automatic Evaluation, GPT, GPT-4, Large Language Model, Large Language Model, Prompt, Rouge, Rouge-L<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10770v1.pdf filename=2402.10770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Work on instruction-tuned <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has used <b>automatic</b> <b>methods</b> based on text overlap and <b>LLM</b> judgments as cost-effective alternatives to human evaluation. In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting. In contrast to previous findings, we observe considerable variability in correlations between <b>automatic</b> <b>methods</b> and human evaluators when scores are differentiated by task type. Specifically, the widely-used <b>ROUGE-L</b> metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of <b>GPT-4</b> as an evaluator depends on including reference answers when <b>prompting</b> for assessments, which can lead to overly strict evaluations in free-form generation tasks. In summary, we find that, while <b>automatic</b> <b>evaluation</b> methods can approximate human judgements under specific conditions, their reliability is highly context-dependent. Our findings enhance the understanding of how <b>automatic</b> <b>methods</b> should be applied and interpreted when developing and evaluating instruction-tuned <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1168--11204-inference-to-the-best-explanation-in-large-language-models-dhairya-dalal-et-al-2024>(11/68 | 11/204) Inference to the Best Explanation in Large Language Models (Dhairya Dalal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhairya Dalal, Marco Valentino, André Freitas, Paul Buitelaar. (2024)<br><strong>Inference to the Best Explanation in Large Language Models</strong><br><button class=copy-to-clipboard title="Inference to the Best Explanation in Large Language Models" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, LLaMA, Natural Language Explanation, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10767v1.pdf filename=2402.10767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of <b>LLMs&rsquo;</b> explanations. IBE-Eval estimates the plausibility of <b>natural</b> <b>language</b> <b>explanations</b> through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal <b>Question</b> <b>Answering</b> (CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by <b>LLMs</b> (i.e., <b>GPT</b> 3.5 and <b>Llama</b> 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77% accuracy ($\approx 27%$ above random), improving upon a <b>GPT</b> 3.5-as-a-Judge baseline ($\approx+17%$) while being intrinsically more efficient and interpretable. Additional analyses suggest that, despite model-specific variances, <b>LLM-generated</b> explanations tend to conform to IBE criteria and that IBE-Eval is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools.</p></p class="citation"></blockquote><h3 id=1268--12204-in-search-of-needles-in-a-10m-haystack-recurrent-memory-finds-what-llms-miss-yuri-kuratov-et-al-2024>(12/68 | 12/204) In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss (Yuri Kuratov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev. (2024)<br><strong>In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss</strong><br><button class=copy-to-clipboard title="In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Retrieval-Augmented Generation, GPT, GPT-2, GPT-4, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10790v1.pdf filename=2402.10790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenge of processing long documents using generative <b>transformer</b> models. To evaluate different approaches, we introduce BABILong, a new <b>benchmark</b> designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes <b>benchmarks</b> for <b>GPT-4</b> and <b>RAG,</b> reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, <b>fine-tuning</b> <b>GPT-2</b> with recurrent memory augmentations enables it to handle tasks involving up to $10^7$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.</p></p class="citation"></blockquote><h3 id=1368--13204-ecorank-budget-constrained-text-re-ranking-using-large-language-models-muhammad-shihab-rashid-et-al-2024>(13/68 | 13/204) EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models (Muhammad Shihab Rashid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Shihab Rashid, Jannat Ara Meem, Yue Dong, Vagelis Hristidis. (2024)<br><strong>EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models</strong><br><button class=copy-to-clipboard title="EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Rerank, Supervised Learning, Unsupervised Learning, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10866v1.pdf filename=2402.10866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have achieved state-of-the-art performance in text re-ranking. This process includes queries and candidate passages in the <b>prompts,</b> utilizing pointwise, listwise, and pairwise <b>prompting</b> strategies. A limitation of these ranking strategies with <b>LLMs</b> is their cost: the process can become expensive due to API charges, which are based on the number of input and output tokens. We study how to maximize the re-ranking performance given a budget, by navigating the vast search spaces of <b>prompt</b> choices, <b>LLM</b> APIs, and budget splits. We propose a suite of budget-constrained methods to perform text re-ranking using a set of <b>LLM</b> APIs. Our most efficient method, called EcoRank, is a two-layered pipeline that jointly optimizes decisions regarding budget allocation across <b>prompt</b> strategies and <b>LLM</b> APIs. Our experimental results on four popular <b>QA</b> and passage <b>reranking</b> datasets show that EcoRank outperforms other budget-aware <b>supervised</b> and <b>unsupervised</b> baselines.</p></p class="citation"></blockquote><h3 id=1468--14204-decomposition-for-enhancing-attention-improving-llm-based-text-to-sql-through-workflow-paradigm-yuanzhen-xie-et-al-2024>(14/68 | 14/204) Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm (Yuanzhen Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanzhen Xie, Xinzhou Jin, Tao Xie, MingXiong Lin, Liang Chen, Chenyun Yu, Lei Cheng, ChengXiang Zhuo, Bo Hu, Zang Li. (2024)<br><strong>Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm</strong><br><button class=copy-to-clipboard title="Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Active Learning, Chain-of-thought Prompt, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10671v1.pdf filename=2402.10671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>learning</b> of <b>large-language</b> <b>models</b> <b>(LLMs)</b> has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step <b>chain-of-thought</b> <b>prompting</b> approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of <b>LLMs</b> in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of <b>LLMs</b> through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new <b>prompt</b> structure based on problem classification greatly enhance the model&rsquo;s attention. Additionally, the inclusion of self-correcting and <b>active</b> <b>learning</b> modules greatly expands the problem-solving scope of <b>LLMs,</b> hence improving the upper limit of <b>LLM-based</b> approaches. Extensive experiments conducted on three datasets demonstrate that our approach outperforms other methods by a significant margin. About 2-3 percentage point improvements compared to the existing baseline on the Spider Dev and Spider-Realistic datasets and new SOTA results on the Spider Test dataset are achieved. Our code is available on GitHub: \url{https://github.com/FlyingFeather/DEA-SQL}.</p></p class="citation"></blockquote><h3 id=1568--15204-jailbreaking-proprietary-large-language-models-using-word-substitution-cipher-divij-handa-et-al-2024>(15/68 | 15/204) Jailbreaking Proprietary Large Language Models using Word Substitution Cipher (Divij Handa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Divij Handa, Advait Chirmule, Bimal Gajera, Chitta Baral. (2024)<br><strong>Jailbreaking Proprietary Large Language Models using Word Substitution Cipher</strong><br><button class=copy-to-clipboard title="Jailbreaking Proprietary Large Language Models using Word Substitution Cipher" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: ChatGPT, GPT, GPT-4, Gemini, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10601v1.pdf filename=2402.10601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are aligned to moral and ethical guidelines but remain susceptible to creative <b>prompts</b> called Jailbreak that can bypass the alignment process. However, most jailbreaking <b>prompts</b> contain harmful questions in the natural language (mainly English), which can be detected by the <b>LLM</b> themselves. In this paper, we present jailbreaking <b>prompts</b> encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art <b>LLM,</b> <b>GPT-4,</b> in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking <b>prompts.</b> We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbreaking approach on state-of-the-art proprietary models including <b>ChatGPT,</b> <b>GPT-4,</b> and <b>Gemini-Pro.</b> Additionally, we discuss the over-defensiveness of these models. We believe that our work will encourage further research in making these <b>LLMs</b> more robust while maintaining their decoding capabilities.</p></p class="citation"></blockquote><h3 id=1668--16204-a-condensed-transition-graph-framework-for-zero-shot-link-prediction-with-large-language-models-mingchen-li-et-al-2024>(16/68 | 16/204) A Condensed Transition Graph Framework for Zero-shot Link Prediction with Large Language Models (Mingchen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingchen Li, Chen Ling, Rui Zhang, Liang Zhao. (2024)<br><strong>A Condensed Transition Graph Framework for Zero-shot Link Prediction with Large Language Models</strong><br><button class=copy-to-clipboard title="A Condensed Transition Graph Framework for Zero-shot Link Prediction with Large Language Models" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 68<br>Keywords: Graph, Graph Contrastive Learning, Contrastive Learning, Knowledge Graph, Zero-shot, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10779v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10779v1.pdf filename=2402.10779v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> link prediction (ZSLP) on <b>knowledge</b> <b>graphs</b> <b>aims</b> <b>at</b> automatically identifying relations between given entities. Existing methods primarily employ auxiliary information to predict tail entity given head entity and its relation, yet face challenges due to the occasional unavailability of such detailed information and the inherent simplicity of predicting tail entities based on semantic similarities. Even though <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> offer a promising solution to predict unobserved relations between the head and tail entity in a <b>zero-shot</b> manner, their performance is still restricted due to the inability to leverage all the (exponentially many) paths&rsquo; information between two entities, which are critical in collectively indicating their relation types. To address this, in this work, we introduce a Condensed Transition <b>Graph</b> <b>Framework</b> <b>for</b> <b>Zero-Shot</b> Link Prediction (CTLP), which encodes all the paths&rsquo; information in linear time complexity to predict unseen relations between entities, attaining both efficiency and information preservation. Specifically, we design a condensed transition <b>graph</b> <b>encoder</b> <b>with</b> theoretical guarantees on its coverage, expressiveness, and efficiency. It is learned by a transition <b>graph</b> <b>contrastive</b> <b>learning</b> strategy. Subsequently, we design a soft <b>instruction</b> <b>tuning</b> to learn and map the all-path embedding to the input of <b>LLMs.</b> Experimental results show that our proposed CTLP method achieves state-of-the-art performance on three standard ZSLP datasets</p></p class="citation"></blockquote><h3 id=1768--17204-exploring-precision-and-recall-to-assess-the-quality-and-diversity-of-llms-le-bronnec-florian-et-al-2024>(17/68 | 17/204) Exploring Precision and Recall to assess the quality and diversity of LLMs (Le Bronnec Florian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Le Bronnec Florian, Verine Alexandre, Negrevergne Benjamin, Chevaleyre Yann, Allauzen Alexandre. (2024)<br><strong>Exploring Precision and Recall to assess the quality and diversity of LLMs</strong><br><button class=copy-to-clipboard title="Exploring Precision and Recall to assess the quality and diversity of LLMs" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, LLaMA, Mistral, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10693v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10693v1.pdf filename=2402.10693v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel evaluation framework for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> such as <b>Llama-2</b> and <b>Mistral,</b> focusing on the adaptation of Precision and Recall metrics from image generation to <b>text</b> <b>generation.</b> This approach allows for a nuanced assessment of the quality and diversity of generated <b>text</b> <b>without</b> the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional <b>benchmarks.</b> The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are <b>fine-tuned</b> with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current <b>LLMs</b> in generating diverse and high-quality text.</p></p class="citation"></blockquote><h3 id=1868--18204-fine-tuning-named-entity-extraction-models-for-the-fantasy-domain-aravinth-sivaganeshan-et-al-2024>(18/68 | 18/204) Fine Tuning Named Entity Extraction Models for the Fantasy Domain (Aravinth Sivaganeshan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aravinth Sivaganeshan, Nisansa de Silva. (2024)<br><strong>Fine Tuning Named Entity Extraction Models for the Fantasy Domain</strong><br><button class=copy-to-clipboard title="Fine Tuning Named Entity Extraction Models for the Fantasy Domain" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Zero-shot, Information Retrieval, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10662v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10662v1.pdf filename=2402.10662v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> is a sequence classification Natural Language Processing task where entities are identified in the text and classified into predefined categories. It acts as a foundation for most <b>information</b> <b>extraction</b> systems. Dungeons and Dragons (D&amp;D) is an open-ended tabletop fantasy game with its own diverse lore. DnD entities are domain-specific and are thus unrecognizable by even the state-of-the-art off-the-shelf <b>NER</b> systems as the <b>NER</b> systems are trained on general data for pre-defined categories such as: person (PERS), location (LOC), organization (ORG), and miscellaneous (MISC). For meaningful extraction of <b>information</b> <b>from</b> fantasy text, the entities need to be classified into domain-specific entity categories as well as the models be <b>fine-tuned</b> on a domain-relevant corpus. This work uses available lore of monsters in the D&amp;D domain to <b>fine-tune</b> Trankit, which is a prolific <b>NER</b> framework that uses a pre-trained model for <b>NER.</b> Upon this training, the system acquires the ability to extract monster names from relevant domain documents under a novel <b>NER</b> tag. This work compares the accuracy of the monster name identification against; the <b>zero-shot</b> Trankit model and two FLAIR models. The <b>fine-tuned</b> Trankit model achieves an 87.86% F1 score surpassing all the other considered models.</p></p class="citation"></blockquote><h3 id=1968--19204-comparing-hallucination-detection-metrics-for-multilingual-generation-haoqiang-kang-et-al-2024>(19/68 | 19/204) Comparing Hallucination Detection Metrics for Multilingual Generation (Haoqiang Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoqiang Kang, Terra Blevins, Luke Zettlemoyer. (2024)<br><strong>Comparing Hallucination Detection Metrics for Multilingual Generation</strong><br><button class=copy-to-clipboard title="Comparing Hallucination Detection Metrics for Multilingual Generation" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: High-Resource, Hallucination Detection, Natural Language Inference, Natural Language Inference, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10496v1.pdf filename=2402.10496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While many automatic <b>hallucination</b> <b>detection</b> techniques have been proposed for English texts, their effectiveness in multilingual contexts remains unexplored. This paper aims to bridge the gap in understanding how these <b>hallucination</b> <b>detection</b> metrics perform on non-English languages. We evaluate the efficacy of various detection metrics, including lexical metrics like <b>ROUGE</b> and Named Entity Overlap and <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)-based</b> metrics, at detecting <b>hallucinations</b> <b>in</b> biographical summaries in many languages; we also evaluate how correlated these different metrics are to gauge whether they measure the same phenomena. Our empirical analysis reveals that while lexical metrics show limited effectiveness, <b>NLI-based</b> metrics perform well in <b>high-resource</b> languages at the sentence level. In contrast, <b>NLI-based</b> metrics often fail to detect atomic fact <b>hallucinations.</b> <b>Our</b> findings highlight existing gaps in multilingual <b>hallucination</b> <b>detection</b> and motivate future research to develop more robust detection methods for <b>LLM</b> <b>hallucination</b> <b>in</b> other languages.</p></p class="citation"></blockquote><h3 id=2068--20204-construction-of-a-syntactic-analysis-map-for-yi-shui-school-through-text-mining-and-natural-language-processing-research-hanqing-zhao-et-al-2024>(20/68 | 20/204) Construction of a Syntactic Analysis Map for Yi Shui School through Text Mining and Natural Language Processing Research (Hanqing Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanqing Zhao, Yuehan Li. (2024)<br><strong>Construction of a Syntactic Analysis Map for Yi Shui School through Text Mining and Natural Language Processing Research</strong><br><button class=copy-to-clipboard title="Construction of a Syntactic Analysis Map for Yi Shui School through Text Mining and Natural Language Processing Research" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 58<br>Keywords: Graph, Knowledge Graph, Information Retrieval, Information Retrieval, Question Answering, Text Mining, TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10743v1.pdf filename=2402.10743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Entity and relationship extraction is a crucial component in natural language processing tasks such as <b>knowledge</b> <b>graph</b> construction, <b>question</b> <b>answering</b> system design, and semantic analysis. Most of the <b>information</b> <b>of</b> the Yishui school of traditional Chinese Medicine (TCM) is stored in the form of unstructured classical Chinese <b>text.</b> <b>The</b> key <b>information</b> <b>extraction</b> of TCM <b>texts</b> <b>plays</b> an important role in mining and studying the academic schools of TCM. In order to solve these problems efficiently using artificial intelligence methods, this study constructs a word segmentation and entity relationship extraction model based on conditional random fields under the framework of natural language processing technology to identify and extract the entity relationship of traditional Chinese medicine <b>texts,</b> <b>and</b> uses the common weighting technology of <b>TF-IDF</b> <b>information</b> <b>retrieval</b> and data mining to extract important key entity <b>information</b> <b>in</b> different ancient books. The dependency syntactic parser based on neural network is used to analyze the grammatical relationship between entities in each ancient book article, and it is represented as a tree structure visualization, which lays the foundation for the next construction of the <b>knowledge</b> <b>graph</b> of Yishui school and the use of artificial intelligence methods to carry out the research of TCM academic schools.</p></p class="citation"></blockquote><h3 id=2168--21204-exploring-hybrid-question-answering-via-program-based-prompting-qi-shi-et-al-2024>(21/68 | 21/204) Exploring Hybrid Question Answering via Program-based Prompting (Qi Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Shi, Han Cui, Haofeng Wang, Qingfu Zhu, Wanxiang Che, Ting Liu. (2024)<br><strong>Exploring Hybrid Question Answering via Program-based Prompting</strong><br><button class=copy-to-clipboard title="Exploring Hybrid Question Answering via Program-based Prompting" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Few-shot, Code Generation, Question Answering, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10812v1.pdf filename=2402.10812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Question</b> <b>answering</b> over heterogeneous data requires <b>reasoning</b> over diverse sources of data, which is challenging due to the large scale of information and organic coupling of heterogeneous data. Various approaches have been proposed to address these challenges. One approach involves training specialized retrievers to select relevant information, thereby reducing the input length. Another approach is to transform diverse modalities of data into a single modality, simplifying the task difficulty and enabling more straightforward processing. In this paper, we propose HProPro, a novel program-based <b>prompting</b> framework for the hybrid <b>question</b> <b>answering</b> task. HProPro follows the <b>code</b> <b>generation</b> and execution paradigm. In addition, HProPro integrates various functions to tackle the hybrid <b>reasoning</b> scenario. Specifically, HProPro contains function declaration and function implementation to perform hybrid information-seeking over data from various sources and modalities, which enables <b>reasoning</b> over such data without training specialized retrievers or performing modal transformations. Experimental results on two typical hybrid <b>question</b> <b>answering</b> <b>benchmarks</b> HybridQA and MultiModalQA demonstrate the effectiveness of HProPro: it surpasses all baseline systems and achieves the best performances in the <b>few-shot</b> settings on both datasets.</p></p class="citation"></blockquote><h3 id=2268--22204-can-we-verify-step-by-step-for-incorrect-answer-detection-xin-xu-et-al-2024>(22/68 | 22/204) Can We Verify Step by Step for Incorrect Answer Detection? (Xin Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Xu, Shizhe Diao, Can Yang, Yang Wang. (2024)<br><strong>Can We Verify Step by Step for Incorrect Answer Detection?</strong><br><button class=copy-to-clipboard title="Can We Verify Step by Step for Incorrect Answer Detection?" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10528v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10528v1.pdf filename=2402.10528v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chain-of-Thought (CoT) <b>prompting</b> has marked a significant advancement in enhancing the <b>reasoning</b> capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of <b>reasoning</b> chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of <b>LLM</b> outputs by scrutinizing the <b>reasoning</b> chains they generate? To answer this research question, we introduce a <b>benchmark,</b> R2PE, designed specifically to explore the relationship between <b>reasoning</b> chains and performance in various <b>reasoning</b> tasks spanning five different domains. This <b>benchmark</b> aims to measure the falsehood of the final output of <b>LLMs</b> based on the <b>reasoning</b> steps. To make full use of information in multiple <b>reasoning</b> chains, we propose the process discernibility score (PDS) framework that beats the answer-checking baseline by a <b>large</b> <b>margin.</b> <b>Concretely,</b> this resulted in an average of 5.1% increase in the F1 score across all 45 subsets within R2PE. We further demonstrate our PDS&rsquo;s efficacy in advancing open-domain <b>QA</b> accuracy. Data and code are available at <a href=https://github.com/XinXU-USTC/R2PE>https://github.com/XinXU-USTC/R2PE</a>.</p></p class="citation"></blockquote><h3 id=2368--23204-measuring-and-reducing-llm-hallucination-without-gold-standard-answers-via-expertise-weighting-jiaheng-wei-et-al-2024>(23/68 | 23/204) Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting (Jiaheng Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaheng Wei, Yuanshun Yao, Jean-Francois Ton, Hongyi Guo, Andrew Estornell, Yang Liu. (2024)<br><strong>Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting</strong><br><button class=copy-to-clipboard title="Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Supervised Learning, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10412v1.pdf filename=2402.10412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLM</b> hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of <b>LLMs.</b> The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a <b>benchmark</b> dataset with gold-standard answers, i.e. &ldquo;best&rdquo; or &ldquo;correct&rdquo; answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting <b>LLMs</b> (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf <b>LLMs</b> that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference <b>LLMs</b> resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accurate hallucination measures than naively using reference <b>LLMs.</b> We also show how to leverage FEWL to reduce hallucination through both <b>in-context</b> <b>learning</b> and <b>supervised</b> <b>finetuning.</b> Last, we build a large-scale <b>benchmark</b> dataset to facilitate <b>LLM</b> hallucination research.</p></p class="citation"></blockquote><h3 id=2468--24204-understanding-survey-paper-taxonomy-about-large-language-models-via-graph-representation-learning-jun-zhuang-et-al-2024>(24/68 | 24/204) Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning (Jun Zhuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Zhuang, Casey Kennington. (2024)<br><strong>Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning</strong><br><button class=copy-to-clipboard title="Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 53<br>Keywords: Graph Convolutional Network, Graph, Fine-tuning, Large Language Model, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10409v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10409v1.pdf filename=2402.10409v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As new research on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 <b>LLM</b> survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging <b>graph</b> structure information on co-category <b>graphs</b> can significantly outperform the language models in two paradigms; <b>pre-trained</b> <b>language</b> <b>models&rsquo;</b> <b>fine-tuning</b> and zero-shot/few-shot classifications using <b>LLMs.</b> We find that our model surpasses an average human recognition level and that <b>fine-tuning</b> <b>LLMs</b> using weak labels generated by a smaller model, such as the <b>GCN</b> in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-strong generalization in the taxonomy classification task.</p></p class="citation"></blockquote><h3 id=2568--25204-instruction-diversity-drives-generalization-to-unseen-tasks-dylan-zhang-et-al-2024>(25/68 | 25/204) Instruction Diversity Drives Generalization To Unseen Tasks (Dylan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dylan Zhang, Justin Wang, Francois Charton. (2024)<br><strong>Instruction Diversity Drives Generalization To Unseen Tasks</strong><br><button class=copy-to-clipboard title="Instruction Diversity Drives Generalization To Unseen Tasks" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Instruction Tuning, Large Language Model, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10891v1.pdf filename=2402.10891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> &ndash; <b>fine-tuning</b> a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> on pairs of <b>instructions</b> <b>and</b> desired outcomes &ndash; is an approach that enables <b>pre-trained</b> <b>language</b> <b>models</b> to perform real-world tasks and follow human <b>instructions.</b> <b>Its</b> practical success depends on the model learning a broader set of <b>instructions</b> <b>than</b> those it was trained on. Yet the factors that determine model generalization to such \emph{unseen tasks} are not well understood. %To understand the driving factors of generalization, In this paper, we experiment with string rewrites, a symbolic task that serves as a building block for Turing complete Markov algorithms while allowing experimental control of &ldquo;inputs&rdquo; and <b>&ldquo;instructions&rdquo;.</b> <b>We</b> investigate the trade-off between the number of <b>instructions</b> <b>the</b> model is trained on and the number of training samples provided for each <b>instruction</b> <b>and</b> observe that the diversity of the <b>instruction</b> <b>set</b> determines generalization. Generalization emerges once a diverse enough set of tasks is provided, even though very few examples are provided for each task. <b>Instruction</b> <b>diversity</b> also ensures robustness with respect to non-uniform distributions of <b>instructions</b> <b>in</b> the training set.</p></p class="citation"></blockquote><h3 id=2668--26204-quantifying-the-persona-effect-in-llm-simulations-tiancheng-hu-et-al-2024>(26/68 | 26/204) Quantifying the Persona Effect in LLM Simulations (Tiancheng Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiancheng Hu, Nigel Collier. (2024)<br><strong>Quantifying the Persona Effect in LLM Simulations</strong><br><button class=copy-to-clipboard title="Quantifying the Persona Effect in LLM Simulations" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 50<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10811v1.pdf filename=2402.10811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown remarkable promise in simulating human language use and behavior. In this study, we delve into the intersection of persona variables and the capability of <b>LLMs</b> to simulate different perspectives. We find that persona variables can explain &lt;10% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating them via <b>prompting</b> in <b>LLMs</b> provides modest improvement. Persona <b>prompting</b> is most effective on data samples where disagreements among annotators are frequent yet confined to a limited range. A linear correlation exists: the more persona variables influence human annotations, the better <b>LLMs</b> predictions are using persona <b>prompting.</b> However, when the utility of persona variables is low (i.e., explaining &lt;10% of human annotations), persona <b>prompting</b> has little effect. Most subjective NLP datasets fall into this category, casting doubt on simulating diverse perspectives in the current NLP landscape.</p></p class="citation"></blockquote><h3 id=2768--27204-rethinking-human-like-translation-strategy-integrating-drift-diffusion-model-with-large-language-models-for-machine-translation-hongbin-na-et-al-2024>(27/68 | 27/204) Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion Model with Large Language Models for Machine Translation (Hongbin Na et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongbin Na, Zimu Wang, Mieradilijiang Maimaiti, Tong Chen, Wei Wang, Tao Shen, Ling Chen. (2024)<br><strong>Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion Model with Large Language Models for Machine Translation</strong><br><button class=copy-to-clipboard title="Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion Model with Large Language Models for Machine Translation" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: High-Resource, Low-Resource, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10699v1.pdf filename=2402.10699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated promising potential in various downstream tasks, including <b>machine</b> <b>translation.</b> However, prior work on <b>LLM-based</b> <b>machine</b> <b>translation</b> has mainly focused on better utilizing training data, demonstrations, or pre-defined and universal knowledge to improve performance, with a lack of consideration of decision-making like human translators. In this paper, we incorporate Thinker with the Drift-Diffusion Model (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion process to emulate human translators&rsquo; dynamic decision-making under constrained resources. We conduct extensive experiments under the <b>high-resource,</b> <b>low-resource,</b> and commonsense translation settings using the WMT22 and CommonMT datasets, in which Thinker-DDM outperforms baselines in the first two scenarios. We also perform additional analysis and evaluation on commonsense translation to illustrate the high effectiveness and efficacy of the proposed method.</p></p class="citation"></blockquote><h3 id=2868--28204-multipot-multilingual-program-of-thoughts-harnesses-multiple-programming-languages-xianzhen-luo-et-al-2024>(28/68 | 28/204) MultiPoT: Multilingual Program of Thoughts Harnesses Multiple Programming Languages (Xianzhen Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianzhen Luo, Qingfu Zhu, Zhiming Zhang, Libo Qin, Xu Wang, Qing Yang, Dongliang Xu, Wanxiang Che. (2024)<br><strong>MultiPoT: Multilingual Program of Thoughts Harnesses Multiple Programming Languages</strong><br><button class=copy-to-clipboard title="MultiPoT: Multilingual Program of Thoughts Harnesses Multiple Programming Languages" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: ChatGPT, GPT, GPT-3, GPT-3.5, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10691v1.pdf filename=2402.10691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Program of Thoughts (PoT) is an approach characterized by its executable intermediate steps, which ensure the accuracy of the numerical calculations in the <b>reasoning</b> process. Currently, PoT primarily uses Python. However, relying solely on a single language may result in suboptimal solutions and overlook the potential benefits of other programming languages. In this paper, we conduct comprehensive experiments on the programming languages used in PoT and find that no single language consistently delivers optimal performance across all tasks and models. The effectiveness of each language varies depending on the specific scenarios. Inspired by this, we propose a task and model agnostic approach called MultiPoT, which harnesses strength and diversity from various languages. Experimental results reveal that it significantly outperforms Python Self-Consistency. Furthermore, it achieves comparable or superior performance compared to the best monolingual PoT in almost all tasks across all models. In particular, MultiPoT achieves more than 4.6% improvement on average on both Starcoder and <b>ChatGPT</b> <b>(gpt-3.5-turbo).</b></p></p class="citation"></blockquote><h3 id=2968--29204-german-text-simplification-finetuning-large-language-models-with-semi-synthetic-data-lars-klöser-et-al-2024>(29/68 | 29/204) German Text Simplification: Finetuning Large Language Models with Semi-Synthetic Data (Lars Klöser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lars Klöser, Mika Beele, Jan-Niklas Schagen, Bodo Kraft. (2024)<br><strong>German Text Simplification: Finetuning Large Language Models with Semi-Synthetic Data</strong><br><button class=copy-to-clipboard title="German Text Simplification: Finetuning Large Language Models with Semi-Synthetic Data" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10675v1.pdf filename=2402.10675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study pioneers the use of synthetically generated data for training generative models in document-level text simplification of German texts. We demonstrate the effectiveness of our approach with real-world online texts. Addressing the challenge of data scarcity in language simplification, we crawled professionally simplified German texts and synthesized a corpus using <b>GPT-4.</b> We <b>finetune</b> <b>Large</b> <b>Language</b> <b>Models</b> with up to 13 billion parameters on this data and evaluate their performance. This paper employs various methodologies for evaluation and demonstrates the limitations of currently used rule-based metrics. Both automatic and manual evaluations reveal that our models can significantly simplify real-world online texts, indicating the potential of synthetic data in improving text simplification.</p></p class="citation"></blockquote><h3 id=3068--30204-generalizability-of-mixture-of-domain-specific-adapters-from-the-lens-of-signed-weight-directions-and-its-application-to-effective-model-pruning-tuc-nguyen-et-al-2024>(30/68 | 30/204) Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning (Tuc Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuc Nguyen, Thai Le. (2024)<br><strong>Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning</strong><br><button class=copy-to-clipboard title="Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Model Pruning, Pruning, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10639v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10639v1.pdf filename=2402.10639v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several parameter-efficient <b>fine-tuning</b> methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing <b>Pre-Trained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> but also multiple of them at once. Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via <b>model</b> <b>weight</b> averaging to optimize performance on novel, unseen domains with excellent computational efficiency. However, the essential generalizability of this emerging weight-space adapter mixing mechanism on unseen, in-domain examples remains unexplored. Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation. We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis on the negative correlation between their fraction of weight sign difference and their mixtures&rsquo; generalizability. All source code will be published.</p></p class="citation"></blockquote><h3 id=3168--31204-can-llms-speak-for-diverse-people-tuning-llms-via-debate-to-generate-controllable-controversial-statements-ming-li-et-al-2024>(31/68 | 31/204) Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements (Ming Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Li, Jiuhai Chen, Lichang Chen, Tianyi Zhou. (2024)<br><strong>Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements</strong><br><button class=copy-to-clipboard title="Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, GPT, GPT-4, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10614v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10614v1.pdf filename=2402.10614v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Making <b>LLMs</b> speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing <b>LLMs</b> lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of <b>LLMs</b> in generating statements supporting an argument the user defined in the <b>prompt.</b> We find that multi-round debates between two <b>LLMs</b> with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of <b>LLMs.</b> Motivated by this, we develop a novel debate & tuning (&ldquo;DEBATunE&rdquo;) pipeline <b>finetuning</b> <b>LLMs</b> to generate the statements obtained via debate. To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 controversial topics and corresponding arguments for each topic. Evaluations by the <b>GPT-4</b> judge with a novel controversy controllability metric show that <b>LLMs&rsquo;</b> capability of expressing diverse perspectives is significantly improved by DEBATunE. Moreover, such controllability can be generalized to unseen topics, generating high-quality statements supporting controversial arguments. Our codes, models, and data will be released at <a href=https://github.com/tianyi-lab/DEBATunE>https://github.com/tianyi-lab/DEBATunE</a>.</p></p class="citation"></blockquote><h3 id=3268--32204-insaaf-incorporating-safety-through-accuracy-and-fairness--are-llms-ready-for-the-indian-legal-domain-yogesh-tripathi-et-al-2024>(32/68 | 32/204) InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain? (Yogesh Tripathi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yogesh Tripathi, Raghav Donakanti, Sahil Girhepuje, Ishan Kavathekar, Bhaskara Hanuma Vedula, Gokul S Krishnan, Shreya Goyal, Anmol Goel, Balaraman Ravindran, Ponnurangam Kumaraguru. (2024)<br><strong>InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?</strong><br><button class=copy-to-clipboard title="InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fairness, Fine-tuning, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10567v1.pdf filename=2402.10567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the <b>fairness</b> and accuracy aspects of the <b>LLM.</b> We assess <b>LLMs&rsquo;</b> safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its <b>fairness</b> exhibition with respect to various axes of disparities in the Indian society. Task performance and <b>fairness</b> scores of <b>LLaMA</b> and <b>LLaMA&ndash;2</b> models indicate that the proposed $LSS_{\beta}$ metric can effectively determine the readiness of a model for safe usage in the legal sector. We also propose <b>finetuning</b> pipelines, utilising specialised legal datasets, as a potential method to mitigate bias and improve model safety. The <b>finetuning</b> procedures on <b>LLaMA</b> and <b>LLaMA&ndash;2</b> models increase the $LSS_{\beta}$, improving their usability in the Indian legal domain. Our code is publicly released.</p></p class="citation"></blockquote><h3 id=3368--33204-pushing-the-limits-of-zero-shot-end-to-end-speech-translation-ioannis-tsiamas-et-al-2024>(33/68 | 33/204) Pushing the Limits of Zero-shot End-to-End Speech Translation (Ioannis Tsiamas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ioannis Tsiamas, Gerard I. Gállego, José A. R. Fonollosa, Marta R. Costa-jussà. (2024)<br><strong>Pushing the Limits of Zero-shot End-to-End Speech Translation</strong><br><button class=copy-to-clipboard title="Pushing the Limits of Zero-shot End-to-End Speech Translation" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Supervised Learning, Zero-shot, Automatic Speech Recognition, Neural Machine Translation, Speech-to-Speech Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10422v1.pdf filename=2402.10422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data scarcity and the modality gap between the <b>speech</b> <b>and</b> text modalities are two major obstacles of end-to-end <b>Speech</b> <b>Translation</b> (ST) systems, thus hindering their performance. Prior work has attempted to mitigate these challenges by leveraging external <b>MT</b> data and optimizing distance metrics that bring closer the <b>speech-text</b> <b>representations.</b> However, achieving competitive results typically requires some ST data. For this reason, we introduce ZeroSwot, a method for <b>zero-shot</b> ST that bridges the modality gap without any paired ST data. Leveraging a novel CTC compression and Optimal Transport, we train a <b>speech</b> <b>encoder</b> using only <b>ASR</b> data, to align with the representation space of a massively multilingual <b>MT</b> model. The <b>speech</b> <b>encoder</b> seamlessly integrates with the <b>MT</b> model at inference, enabling direct translation from <b>speech</b> <b>to</b> text, across all languages supported by the <b>MT</b> model. Our experiments show that we can effectively close the modality gap without ST data, while our results on MuST-C and CoVoST demonstrate our method&rsquo;s superiority over not only previous <b>zero-shot</b> models, but also <b>supervised</b> ones, achieving state-of-the-art results.</p></p class="citation"></blockquote><h3 id=3468--34204-genres-rethinking-evaluation-for-generative-relation-extraction-in-the-era-of-large-language-models-pengcheng-jiang-et-al-2024>(34/68 | 34/204) GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models (Pengcheng Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengcheng Jiang, Jiacheng Lin, Zifeng Wang, Jimeng Sun, Jiawei Han. (2024)<br><strong>GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models</strong><br><button class=copy-to-clipboard title="GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Relation Extraction, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10744v1.pdf filename=2402.10744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of <b>relation</b> <b>extraction</b> (RE) is experiencing a notable shift towards generative <b>relation</b> <b>extraction</b> (GRE), leveraging the capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, we discovered that traditional <b>relation</b> <b>extraction</b> (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference <b>relations,</b> <b>while</b> GRE methods often produce diverse and semantically accurate <b>relations</b> <b>that</b> differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential <b>relations</b> <b>can</b> be incomplete; (3) <b>prompting</b> <b>LLMs</b> with a fixed set of <b>relations</b> <b>or</b> entities can cause hallucinations. Next, we conducted a human evaluation of GRE methods that shows GenRES is consistent with human preferences for RE quality. Last, we made a comprehensive evaluation of fourteen leading <b>LLMs</b> using GenRES across document, bag, and sentence level RE datasets, respectively, to set the <b>benchmark</b> for future research in GRE</p></p class="citation"></blockquote><h3 id=3568--35204-disordered-dabs-a-benchmark-for-dynamic-aspect-based-summarization-in-disordered-texts-xiaobo-guo-et-al-2024>(35/68 | 35/204) Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts (Xiaobo Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaobo Guo, Soroush Vosoughi. (2024)<br><strong>Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts</strong><br><button class=copy-to-clipboard title="Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10554v1.pdf filename=2402.10554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aspect-based <b>summarization</b> has seen significant advancements, especially in structured text. Yet, summarizing disordered, large-scale texts, like those found in social media and customer feedback, remains a significant challenge. Current research largely targets predefined aspects within structured texts, neglecting the complexities of dynamic and disordered environments. Addressing this gap, we introduce Disordered-DABS, a novel <b>benchmark</b> for dynamic aspect-based <b>summarization</b> tailored to unstructured text. Developed by adapting existing datasets for cost-efficiency and scalability, our comprehensive experiments and detailed human evaluations reveal that Disordered-DABS poses unique challenges to contemporary <b>summarization</b> models, including state-of-the-art language models such as <b>GPT-3.5.</b></p></p class="citation"></blockquote><h3 id=3668--36204-when-is-tree-search-useful-for-llm-planning-it-depends-on-the-discriminator-ziru-chen-et-al-2024>(36/68 | 36/204) When is Tree Search Useful for LLM Planning? It Depends on the Discriminator (Ziru Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, Huan Sun. (2024)<br><strong>When is Tree Search Useful for LLM Planning? It Depends on the Discriminator</strong><br><button class=copy-to-clipboard title="When is Tree Search Useful for LLM Planning? It Depends on the Discriminator" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10890v1.pdf filename=2402.10890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we examine how <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and <b>mathematical</b> <b>reasoning,</b> show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current <b>LLMs&rsquo;</b> discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with <b>LLM-based</b> discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10&ndash;20 times slower but leads to negligible performance gains, which hinders its real-world applications. Code and data will be released at <a href=https://github.com/OSU-NLP-Group/llm-planning-eval>https://github.com/OSU-NLP-Group/llm-planning-eval</a>.</p></p class="citation"></blockquote><h3 id=3768--37204-toolsword-unveiling-safety-issues-of-large-language-models-in-tool-learning-across-three-stages-junjie-ye-et-al-2024>(37/68 | 37/204) ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages (Junjie Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, Xuanjing Huang. (2024)<br><strong>ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages</strong><br><button class=copy-to-clipboard title="ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10753v1.pdf filename=2402.10753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tool learning is widely acknowledged as a foundational approach or deploying <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in real-world scenarios. While current research primarily emphasizes leveraging tools to augment <b>LLMs,</b> it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to <b>LLMs</b> in tool learning. Specifically, ToolSword delineates six safety scenarios for <b>LLMs</b> in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments conducted on 11 open-source and closed-source <b>LLMs</b> reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even <b>GPT-4</b> is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data is released in <a href=https://github.com/Junjie-Ye/ToolSword>https://github.com/Junjie-Ye/ToolSword</a>.</p></p class="citation"></blockquote><h3 id=3868--38204-longheads-multi-head-attention-is-secretly-a-long-context-processor-yi-lu-et-al-2024>(38/68 | 38/204) LongHeads: Multi-Head Attention is Secretly a Long Context Processor (Yi Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang. (2024)<br><strong>LongHeads: Multi-Head Attention is Secretly a Long Context Processor</strong><br><button class=copy-to-clipboard title="LongHeads: Multi-Head Attention is Secretly a Long Context Processor" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Out-of-distribution, Large Language Model, Large Language Model, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10685v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10685v1.pdf filename=2402.10685v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention&rsquo;s quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances <b>LLM&rsquo;s</b> long context ability by unlocking multi-head attention&rsquo;s untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to <b>out-of-distribution</b> (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that relies on the inherent correlation between the query and the key representations, efficiently distributing context chunks to different heads. In this way, each head ensures it can effectively process attended tokens within the trained length, while different heads in different layers can collectively process longer contexts. LongHeads works efficiently in linear time, fits seamlessly with many <b>LLMs</b> that use relative positional encoding. Our extensive empirical analyses verify LongHeads&rsquo;s efficacy in extending the usable context window for existing models, showcasing its promise for enhancing long <b>text</b> <b>understanding.</b></p></p class="citation"></blockquote><h3 id=3968--39204-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql-dingzirui-wang-et-al-2024>(39/68 | 39/204) Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL (Dingzirui Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingzirui Wang, Longxu Dou, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che. (2024)<br><strong>Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL</strong><br><button class=copy-to-clipboard title="Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10663v1.pdf filename=2402.10663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Currently, the <b>in-context</b> <b>learning</b> method based on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has become the mainstream of text-to-SQL research. Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool. However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead. Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL. We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments. Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost. Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on several mainstream datasets, which proves the effectiveness of Fused.</p></p class="citation"></blockquote><h3 id=4068--40204-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes-dingzirui-wang-et-al-2024>(40/68 | 40/204) Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes (Dingzirui Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingzirui Wang, Longxu Dou, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che. (2024)<br><strong>Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes</strong><br><button class=copy-to-clipboard title="Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10654v1.pdf filename=2402.10654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Numerical <b>reasoning</b> is an essential ability for NLP systems to handle numeric information. Recent research indicates that <b>fine-tuning</b> a small-scale model to learn generating <b>reasoning</b> processes alongside answers can significantly enhance performance. However, current methods have the limitation that most methods generate <b>reasoning</b> processes with <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> which are &ldquo;unreliable&rdquo; since such processes could contain information unrelated to the answer. To address this limitation, we introduce Enhancing NumeriCal <b>reasOning</b> with Reliable procEsses (Encore), which derives the reliable <b>reasoning</b> process by decomposing the answer formula, ensuring which fully supports the answer. Nevertheless, models could lack enough data to learn the <b>reasoning</b> process generation adequately, since our method generates only one single <b>reasoning</b> process for one formula. To overcome this difficulty, we present a series of pre-training tasks to help models learn the <b>reasoning</b> process generation with synthesized data. The experiments show that Encore yields improvement on all five experimental datasets with an average of 1.8%, proving the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=4168--41204-retrieve-only-when-it-needs-adaptive-retrieval-augmentation-for-hallucination-mitigation-in-large-language-models-hanxing-ding-et-al-2024>(41/68 | 41/204) Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models (Hanxing Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng. (2024)<br><strong>Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models</strong><br><button class=copy-to-clipboard title="Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10612v1.pdf filename=2402.10612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hallucinations pose a significant challenge for the practical implementation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of <b>LLMs,</b> potentially resulting in internal hallucinations. While incorporating external information can help fill knowledge gaps, it also introduces the risk of irrelevant information, thereby increasing the likelihood of external hallucinations. A careful and balanced integration of the parametric knowledge within <b>LLMs</b> with external information is crucial to alleviate hallucinations. In this study, we present Rowen, a novel approach that enhances <b>LLMs</b> with a selective <b>retrieval</b> <b>augmentation</b> process tailored to address hallucinated outputs. This process is governed by a multilingual semantic-aware detection module, which evaluates the consistency of the perturbed responses across various languages for the same queries. Upon detecting inconsistencies indicative of hallucinations, Rowen activates the <b>retrieval</b> <b>of</b> external information to rectify the model outputs. Rowen adeptly harmonizes the intrinsic parameters in <b>LLMs</b> with external knowledge sources, effectively mitigating hallucinations by ensuring a balanced integration of internal <b>reasoning</b> and external evidence. Through a comprehensive empirical analysis, we demonstrate that Rowen surpasses the current state-of-the-art in both detecting and mitigating hallucinated content within the outputs of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=4268--42204-efficiency-at-scale-investigating-the-performance-of-diminutive-language-models-in-clinical-tasks-niall-taylor-et-al-2024>(42/68 | 42/204) Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks (Niall Taylor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niall Taylor, Upamanyu Ghose, Omid Rohanian, Mohammadmahdi Nouriborji, Andrey Kormilitzin, David Clifton, Alejo Nevado-Holgado. (2024)<br><strong>Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks</strong><br><button class=copy-to-clipboard title="Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10597v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10597v1.pdf filename=2402.10597v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The entry of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> into research and commercial spaces has led to a trend of ever-larger models, with initial promises of generalisability, followed by a widespread desire to downsize and create specialised models without the need for complete <b>fine-tuning,</b> using Parameter Efficient <b>Fine-tuning</b> (PEFT) methods. We present an investigation into the suitability of different PEFT methods to clinical decision-making tasks, across a range of model sizes, including extremely small models with as few as $25$ million parameters. Our analysis shows that the performance of most PEFT approaches varies significantly from one task to another, with the exception of LoRA, which maintains relatively high performance across all model sizes and tasks, typically approaching or matching full <b>fine-tuned</b> performance. The effectiveness of PEFT methods in the clinical domain is evident, particularly for specialised models which can operate on low-cost, in-house computing infrastructure. The advantages of these models, in terms of speed and reduced training costs, dramatically outweighs any performance gain from <b>large</b> <b>foundation</b> <b>LLMs.</b> Furthermore, we highlight how domain-specific pre-training interacts with PEFT methods and model size, and discuss how these factors interplay to provide the best efficiency-performance trade-off. Full code available at: tbd.</p></p class="citation"></blockquote><h3 id=4368--43204-direct-preference-optimization-with-an-offset-afra-amini-et-al-2024>(43/68 | 43/204) Direct Preference Optimization with an Offset (Afra Amini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Afra Amini, Tim Vieira, Ryan Cotterell. (2024)<br><strong>Direct Preference Optimization with an Offset</strong><br><button class=copy-to-clipboard title="Direct Preference Optimization with an Offset" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Reinforcement Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10571v1.pdf filename=2402.10571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Direct preference optimization (DPO) is a successful <b>fine-tuning</b> strategy for aligning <b>large</b> <b>language</b> <b>models</b> with human preferences without the need to train a reward model or employ <b>reinforcement</b> <b>learning.</b> DPO, as originally formulated, relies on binary preference data and <b>fine-tunes</b> a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during <b>fine-tuning.</b> Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.</p></p class="citation"></blockquote><h3 id=4468--44204-zero-shot-sampling-of-adversarial-entities-in-biomedical-question-answering-r-patrick-xian-et-al-2024>(44/68 | 44/204) Zero-shot sampling of adversarial entities in biomedical question answering (R. Patrick Xian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>R. Patrick Xian, Alex J. Lee, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl. (2024)<br><strong>Zero-shot sampling of adversarial entities in biomedical question answering</strong><br><button class=copy-to-clipboard title="Zero-shot sampling of adversarial entities in biomedical question answering" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs.CL, stat-AP<br>Keyword Score: 40<br>Keywords: Zero-shot, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10527v1.pdf filename=2402.10527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing depth of parametric domain knowledge in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is fueling their rapid deployment in real-world applications. In high-stakes and knowledge-intensive tasks, understanding model vulnerabilities is essential for quantifying the trustworthiness of model predictions and regulating their use. The recent discovery of named entities as adversarial examples in natural language processing tasks raises <b>questions</b> <b>about</b> their potential guises in other settings. Here, we propose a powerscaled distance-weighted sampling scheme in embedding space to discover diverse adversarial entities as distractors. We demonstrate its advantage over random sampling in adversarial <b>question</b> <b>answering</b> on biomedical topics. Our approach enables the exploration of different regions on the attack surface, which reveals two regimes of adversarial entities that markedly differ in their characteristics. Moreover, we show that the attacks successfully manipulate token-wise Shapley value explanations, which become deceptive in the adversarial setting. Our investigations illustrate the brittleness of domain knowledge in <b>LLMs</b> and reveal a shortcoming of standard evaluations for high-capacity models.</p></p class="citation"></blockquote><h3 id=4568--45204-steering-conversational-large-language-models-for-long-emotional-support-conversations-navid-madani-et-al-2024>(45/68 | 45/204) Steering Conversational Large Language Models for Long Emotional Support Conversations (Navid Madani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Navid Madani, Sougata Saha, Rohini Srihari. (2024)<br><strong>Steering Conversational Large Language Models for Long Emotional Support Conversations</strong><br><button class=copy-to-clipboard title="Steering Conversational Large Language Models for Long Emotional Support Conversations" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10453v1.pdf filename=2402.10453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we address the challenge of consistently following emotional support strategies in long conversations by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We introduce the Strategy-Relevant Attention (SRA) metric, a model-agnostic measure designed to evaluate the effectiveness of <b>LLMs</b> in adhering to strategic <b>prompts</b> in emotional support contexts. By analyzing conversations within the Emotional Support Conversations dataset (ESConv) using <b>LLaMA</b> models, we demonstrate that SRA is significantly correlated with a model&rsquo;s ability to sustain the outlined strategy throughout the interactions. Our findings reveal that the application of SRA-informed <b>prompts</b> leads to enhanced strategic adherence, resulting in conversations that more reliably exhibit the desired emotional support strategies over longer conversations. Furthermore, we contribute a comprehensive, multi-branch synthetic conversation dataset for ESConv, featuring a variety of strategy continuations informed by our optimized <b>prompting</b> method. The code and data are publicly available on our Github.</p></p class="citation"></blockquote><h3 id=4668--46204-i-am-not-them-fluid-identities-and-persistent-out-group-bias-in-large-language-models-wenchao-dong-et-al-2024>(46/68 | 46/204) I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models (Wenchao Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenchao Dong, Assem Zhunis, Hyojin Chin, Jiyoung Han, Meeyoung Cha. (2024)<br><strong>I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models</strong><br><button class=copy-to-clipboard title="I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10436v1.pdf filename=2402.10436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explored cultural biases-individualism vs. collectivism-in <b>ChatGPT</b> across three Western languages (i.e., English, German, and French) and three Eastern languages (i.e., Chinese, Japanese, and Korean). When <b>ChatGPT</b> adopted an individualistic persona in Western languages, its collectivism scores (i.e., out-group values) exhibited a more negative trend, surpassing their positive orientation towards individualism (i.e., in-group values). Conversely, when a collectivistic persona was assigned to <b>ChatGPT</b> in Eastern languages, a similar pattern emerged with more negative responses toward individualism (i.e., out-group values) as compared to collectivism (i.e., in-group values). The results indicate that when imbued with a particular social identity, <b>ChatGPT</b> discerns in-group and out-group, embracing in-group values while eschewing out-group values. Notably, the negativity towards the out-group, from which prejudices and discrimination arise, exceeded the positivity towards the in-group. The experiment was replicated in the political domain, and the results remained consistent. Furthermore, this replication unveiled an intrinsic Democratic bias in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> aligning with earlier findings and providing integral insights into mitigating such bias through <b>prompt</b> engineering. Extensive robustness checks were performed using varying hyperparameter and persona setup methods, with or without social identity labels, across other popular language models.</p></p class="citation"></blockquote><h3 id=4768--47204-understanding-in-context-learning-with-a-pelican-soup-framework-ting-rui-chiang-et-al-2024>(47/68 | 47/204) Understanding In-Context Learning with a Pelican Soup Framework (Ting-Rui Chiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ting-Rui Chiang, Dani Yogatama. (2024)<br><strong>Understanding In-Context Learning with a Pelican Soup Framework</strong><br><button class=copy-to-clipboard title="Understanding In-Context Learning with a Pelican Soup Framework" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: GPT-2, In-context Learning, In-context Learning, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10424v1.pdf filename=2402.10424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many existing theoretical analyses of <b>in-context</b> <b>learning</b> for natural language processing are based on latent variable models that leaves gaps between theory and practice. We aim to close these gaps by proposing a theoretical framework, the Pelican Soup Framework. In this framework, we introduce (1) the notion of a common sense knowledge base, (2) a general formalism for natural language classification tasks, and the notion of (3) meaning association. Under this framework, we can establish a $\mathcal{O}(1/T)$ loss bound for <b>in-context</b> <b>learning,</b> where $T$ is the number of example-label pairs in the demonstration. Compared with previous works, our bound reflects the effect of the choice of verbalizers and the effect of <b>instruction</b> <b>tuning.</b> An additional notion of \textit{atom concepts} makes our framework possible to explain the generalization to tasks unseen in the language model training data. Finally, we propose a toy setup, Calcutec, and a digit addition task that mimics types of distribution shifts a model needs to overcome to perform <b>in-context</b> <b>learning.</b> We also experiment with <b>GPT2-Large</b> on real-world NLP tasks. Our empirical results demonstrate the efficacy of our framework to explain <b>in-context</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=4868--48204-datadreamer-a-tool-for-synthetic-data-generation-and-reproducible-llm-workflows-ajay-patel-et-al-2024>(48/68 | 48/204) DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows (Ajay Patel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ajay Patel, Colin Raffel, Chris Callison-Burch. (2024)<br><strong>DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows</strong><br><button class=copy-to-clipboard title="DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Knowledge Distillation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10379v1.pdf filename=2402.10379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use <b>LLMs</b> in synthetic data generation, task evaluation, <b>fine-tuning,</b> <b>distillation,</b> and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful <b>LLM</b> workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at <a href=https://github.com/datadreamer-dev/DataDreamer>https://github.com/datadreamer-dev/DataDreamer</a> .</p></p class="citation"></blockquote><h3 id=4968--49204-enhancing-role-playing-systems-through-aggressive-queries-evaluation-and-improvement-yihong-tang-et-al-2024>(49/68 | 49/204) Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement (Yihong Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihong Tang, Jiao Ou, Che Liu, Fuzheng Zhang, Di Zhang, Kun Gai. (2024)<br><strong>Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement</strong><br><button class=copy-to-clipboard title="Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Adversarial Learning, Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10618v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10618v1.pdf filename=2402.10618v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has propelled dialogue generation into new realms, particularly in the field of role-playing systems (RPSs). While enhanced with ordinary role-relevant training dialogues, existing <b>LLM-based</b> RPSs still struggle to align with roles when handling intricate and trapped queries in boundary scenarios. In this paper, we design the Modular ORchestrated Trap-setting Interaction SystEm (MORTISE) to <b>benchmark</b> and improve the role-playing <b>LLMs&rsquo;</b> performance. MORTISE can produce highly role-relevant aggressive queries through the collaborative effort of multiple <b>LLM-based</b> modules, and formulate corresponding responses to create an <b>adversarial</b> <b>training</b> dataset via a consistent response generator. We select 190 Chinese and English roles to construct aggressive queries to <b>benchmark</b> existing role-playing <b>LLMs.</b> Through comprehensive evaluation, we find that existing models exhibit a general deficiency in role alignment capabilities. We further select 180 of the roles to collect an <b>adversarial</b> <b>training</b> dataset (named RoleAD) and retain the other 10 roles for testing. Experiments on models improved by RoleAD indicate that our <b>adversarial</b> <b>dataset</b> ameliorates this deficiency, with the improvements demonstrating a degree of generalizability in ordinary scenarios.</p></p class="citation"></blockquote><h3 id=5068--50204-conversational-simulmt-efficient-simultaneous-translation-with-large-language-models-minghan-wang-et-al-2024>(50/68 | 50/204) Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models (Minghan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minghan Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari. (2024)<br><strong>Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models</strong><br><button class=copy-to-clipboard title="Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10552v1.pdf filename=2402.10552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Simultaneous <b>machine</b> <b>translation</b> (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that <b>LLMs</b> can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of <b>LLM-based</b> SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT <b>benchmarks</b> demonstrate the superiority of <b>LLM</b> in translation quality while achieving comparable computational latency to specialized SimulMT models.</p></p class="citation"></blockquote><h3 id=5168--51204-chain-of-logic-rule-based-reasoning-with-large-language-models-sergio-servantez-et-al-2024>(51/68 | 51/204) Chain of Logic: Rule-Based Reasoning with Large Language Models (Sergio Servantez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sergio Servantez, Joe Barrow, Kristian Hammond, Rajiv Jain. (2024)<br><strong>Chain of Logic: Rule-Based Reasoning with Large Language Models</strong><br><button class=copy-to-clipboard title="Chain of Logic: Rule-Based Reasoning with Large Language Models" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10400v1.pdf filename=2402.10400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rule-based <b>reasoning,</b> a fundamental type of legal <b>reasoning,</b> enables us to draw conclusions by accurately applying a rule to a set of facts. We explore causal language models as rule-based reasoners, specifically with respect to compositional rules - rules consisting of multiple elements which form a complex logical expression. <b>Reasoning</b> about compositional rules is challenging because it requires multiple <b>reasoning</b> steps, and attending to the logical relationships between elements. We introduce a new <b>prompting</b> method, Chain of Logic, which elicits rule-based <b>reasoning</b> through decomposition (solving elements as independent threads of logic), and recomposition (recombining these sub-answers to resolve the underlying logical expression). This method was inspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a sequential <b>reasoning</b> approach used by lawyers. We evaluate chain of logic across eight rule-based <b>reasoning</b> tasks involving three distinct compositional rules from the LegalBench <b>benchmark</b> and demonstrate it consistently outperforms other <b>prompting</b> methods, including chain of thought and self-ask, using open-source and commercial language models.</p></p class="citation"></blockquote><h3 id=5268--52204-time-series-forecasting-with-llms-understanding-and-enhancing-model-capabilities-mingyu-jin-et-al-2024>(52/68 | 52/204) Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities (Mingyu Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyu Jin, Hua Tang, Chong Zhang, Qinkai Yu, Chengzhi Liu, Suiyuan Zhu, Yongfeng Zhang, Mengnan Du. (2024)<br><strong>Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities</strong><br><button class=copy-to-clipboard title="Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10835v1.pdf filename=2402.10835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been applied in many fields with rapid development in recent years. As a classic machine learning task, time series forecasting has recently received a boost from <b>LLMs.</b> However, there is a research gap in the <b>LLMs&rsquo;</b> preferences in this field. In this paper, by comparing <b>LLMs</b> with traditional models, many properties of <b>LLMs</b> in time series prediction are found. For example, our study shows that <b>LLMs</b> excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity. We explain our findings through designing <b>prompts</b> to require <b>LLMs</b> to tell the period of the datasets. In addition, the input strategy is investigated, and it is found that incorporating external knowledge and adopting natural language paraphrases positively affects the predictive performance of <b>LLMs</b> for time series. Overall, this study contributes to insight into the advantages and limitations of <b>LLMs</b> in time series forecasting under different conditions.</p></p class="citation"></blockquote><h3 id=5368--53204-distillation-enhanced-generative-retrieval-yongqi-li-et-al-2024>(53/68 | 53/204) Distillation Enhanced Generative Retrieval (Yongqi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongqi Li, Zhen Zhang, Wenjie Wang, Liqiang Nie, Wenjie Li, Tat-Seng Chua. (2024)<br><strong>Distillation Enhanced Generative Retrieval</strong><br><button class=copy-to-clipboard title="Distillation Enhanced Generative Retrieval" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 30<br>Keywords: Dense Retrieval, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10769v1.pdf filename=2402.10769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generative language models, distinct from traditional sparse or <b>dense</b> <b>retrieval</b> methods. In this work, we identify a viable direction to further enhance generative retrieval via <b>distillation</b> and propose a feasible framework, named DGR. DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed <b>distilled</b> RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels. This framework only requires an additional <b>distillation</b> step to enhance current generative retrieval systems and does not add any burden to the inference stage. We conduct experiments on four public datasets, and the results indicate that DGR achieves state-of-the-art performance among the generative retrieval methods. Additionally, DGR demonstrates exceptional robustness and generalizability with various teacher models and <b>distillation</b> losses.</p></p class="citation"></blockquote><h3 id=5468--54204-an-empirical-study-on-cross-lingual-vocabulary-adaptation-for-efficient-generative-llm-inference-atsuki-yamaguchi-et-al-2024>(54/68 | 54/204) An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference (Atsuki Yamaguchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atsuki Yamaguchi, Aline Villavicencio, Nikolaos Aletras. (2024)<br><strong>An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference</strong><br><button class=copy-to-clipboard title="An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Natural Language Understanding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10712v1.pdf filename=2402.10712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of state-of-the-art generative <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some <b>LLMs</b> have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative <b>LLMs</b> has yet to be explored. In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative <b>LLMs</b> (including monolingual and multilingual models) across four typologically-diverse languages and four <b>natural</b> <b>language</b> <b>understanding</b> tasks. We find that cross-lingual vocabulary adaptation substantially contributes to <b>LLM</b> inference speedups of up to 271.5%. We also show that adapting <b>LLMs</b> that have been pre-trained on more balanced multilingual data results in downstream performance comparable to the original models.</p></p class="citation"></blockquote><h3 id=5568--55204-opening-the-black-box-of-large-language-models-two-views-on-holistic-interpretability-haiyan-zhao-et-al-2024>(55/68 | 55/204) Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability (Haiyan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiyan Zhao, Fan Yang, Himabindu Lakkaraju, Mengnan Du. (2024)<br><strong>Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability</strong><br><button class=copy-to-clipboard title="Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10688v1.pdf filename=2402.10688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> grow more powerful, concerns around potential harms like toxicity, unfairness, and hallucination threaten user trust. Ensuring beneficial alignment of <b>LLMs</b> with human values through model alignment is thus critical yet challenging, requiring a deeper understanding of <b>LLM</b> behaviors and mechanisms. We propose opening the black box of <b>LLMs</b> through a framework of holistic interpretability encompassing complementary bottom-up and top-down perspectives. The bottom-up view, enabled by mechanistic interpretability, focuses on component functionalities and training dynamics. The top-down view utilizes representation engineering to analyze behaviors through hidden representations. In this paper, we review the landscape around mechanistic interpretability and representation engineering, summarizing approaches, discussing limitations and applications, and outlining future challenges in using these techniques to achieve ethical, honest, and reliable <b>reasoning</b> aligned with human values.</p></p class="citation"></blockquote><h3 id=5668--56204-humans-or-llms-as-the-judge-a-study-on-judgement-biases-guiming-hardy-chen-et-al-2024>(56/68 | 56/204) Humans or LLMs as the Judge? A Study on Judgement Biases (Guiming Hardy Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang. (2024)<br><strong>Humans or LLMs as the Judge? A Study on Judgement Biases</strong><br><button class=copy-to-clipboard title="Humans or LLMs as the Judge? A Study on Judgement Biases" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: BLOOM, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10669v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10669v1.pdf filename=2402.10669v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adopting human and <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> as judges (\textit{a.k.a} human- and <b>LLM-as-a-judge)</b> for evaluating the performance of existing <b>LLMs</b> has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and <b>LLM</b> judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework for investigating 5 types of biases for <b>LLM</b> and human judges. We curate a dataset with 142 samples referring to the revised <b>Bloom&rsquo;s</b> Taxonomy and conduct thousands of human and <b>LLM</b> evaluations. Results show that human and <b>LLM</b> judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on <b>LLM</b> judges. We hope that our work can notify the community of the vulnerability of human- and <b>LLM-as-a-judge</b> against perturbations, as well as the urgency of developing robust evaluation systems.</p></p class="citation"></blockquote><h3 id=5768--57204-absinstruct-eliciting-abstraction-ability-from-llms-through-explanation-tuning-with-plausibility-estimation-zhaowei-wang-et-al-2024>(57/68 | 57/204) AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation (Zhaowei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaowei Wang, Wei Fan, Qing Zong, Hongming Zhang, Sehyun Choi, Tianqing Fang, Xin Liu, Yangqiu Song, Ginny Y. Wong, Simon See. (2024)<br><strong>AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation</strong><br><button class=copy-to-clipboard title="AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Instruction Following, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10646v1.pdf filename=2402.10646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study. Existing work shows that <b>LLMs</b> are deficient in abstract ability, and how to improve it remains unexplored. In this work, we design the framework AbsInstruct to enhance <b>LLMs&rsquo;</b> abstraction ability through <b>instruction</b> <b>tuning.</b> The framework builds <b>instructions</b> <b>with</b> in-depth explanations to assist <b>LLMs</b> in capturing the underlying rationale of abstraction. Meanwhile, we introduce a plausibility estimator to select <b>instructions</b> <b>that</b> are more consistent with the abstraction knowledge of <b>LLMs</b> to be aligned. Then, our framework combines abstraction <b>instructions</b> <b>with</b> general-purpose ones to build a hybrid dataset. Extensive experiments and analyses demonstrate that our framework can considerably enhance <b>LLMs&rsquo;</b> abstraction ability with strong generalization performance while maintaining their general <b>instruction-following</b> <b>abilities.</b></p></p class="citation"></blockquote><h3 id=5868--58204-do-llamas-work-in-english-on-the-latent-language-of-multilingual-transformers-chris-wendler-et-al-2024>(58/68 | 58/204) Do Llamas Work in English? On the Latent Language of Multilingual Transformers (Chris Wendler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West. (2024)<br><strong>Do Llamas Work in English? On the Latent Language of Multilingual Transformers</strong><br><button class=copy-to-clipboard title="Do Llamas Work in English? On the Latent Language of Multilingual Transformers" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 30<br>Keywords: LLaMA, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10588v1.pdf filename=2402.10588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language &ndash; a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the <b>Llama-2</b> family of <b>transformer</b> models, our study uses carefully constructed non-English <b>prompts</b> with a unique correct single-token continuation. From layer to layer, <b>transformers</b> gradually map an input embedding of the final <b>prompt</b> token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in &ldquo;input space&rdquo;, &ldquo;concept space&rdquo;, and &ldquo;output space&rdquo;, respectively. Crucially, our evidence suggests that the abstract &ldquo;concept space&rdquo; lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models.</p></p class="citation"></blockquote><h3 id=5968--59204-threads-of-subtlety-detecting-machine-generated-texts-through-discourse-motifs-zae-myung-kim-et-al-2024>(59/68 | 59/204) Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs (Zae Myung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zae Myung Kim, Kwang Hee Lee, Preston Zhu, Vipul Raheja, Dongyeop Kang. (2024)<br><strong>Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs</strong><br><button class=copy-to-clipboard title="Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10586v1.pdf filename=2402.10586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLM),</b> the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both <b>LLMs</b> and humans. Empirical findings demonstrate that, although both <b>LLMs</b> and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers&rsquo; overall performance in distinguishing between human-written and machine-generated texts, even on <b>out-of-distribution</b> and paraphrased samples. This underscores the significance of incorporating hierarchical discourse features in the analysis of text patterns. The code and dataset will be available at [TBA].</p></p class="citation"></blockquote><h3 id=6068--60204-neural-paraphrasing-by-automatically-crawled-and-aligned-sentence-pairs-achille-globo-et-al-2024>(60/68 | 60/204) Neural paraphrasing by automatically crawled and aligned sentence pairs (Achille Globo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Achille Globo, Antonio Trevisi, Andrea Zugarini, Leonardo Rigutini, Marco Maggini, Stefano Melacci. (2024)<br><strong>Neural paraphrasing by automatically crawled and aligned sentence pairs</strong><br><button class=copy-to-clipboard title="Neural paraphrasing by automatically crawled and aligned sentence pairs" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Language Generation, Natural Language Generation, Natural Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10558v1.pdf filename=2402.10558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Paraphrasing is the task of re-writing an input text using other words, without altering the meaning of the original content. Conversational systems can exploit automatic paraphrasing to make the conversation more <b>natural,</b> <b>e.g.,</b> <b>talking</b> about a certain topic using different paraphrases in different time instants. Recently, the task of automatically generating paraphrases has been approached in the context of <b>Natural</b> <b>Language</b> <b>Generation</b> <b>(NLG).</b> While many existing systems simply consist in rule-based models, the recent success of the Deep Neural Networks in several <b>NLG</b> tasks naturally suggests the possibility of exploiting such networks for generating paraphrases. However, the main obstacle toward neural-network-based paraphrasing is the lack of large datasets with aligned pairs of sentences and paraphrases, that are needed to efficiently train the neural models. In this paper we present a method for the automatic generation of large aligned corpora, that is based on the assumption that news and blog websites talk about the same events using different narrative styles. We propose a similarity search procedure with linguistic constraints that, given a reference sentence, is able to locate the most similar candidate paraphrases out from millions of indexed sentences. The data generation process is evaluated in the case of the Italian <b>language,</b> <b>performing</b> experiments using pointer-based deep neural architectures.</p></p class="citation"></blockquote><h3 id=6168--61204-strong-hallucinations-from-negation-and-how-to-fix-them-nicholas-asher-et-al-2024>(61/68 | 61/204) Strong hallucinations from negation and how to fix them (Nicholas Asher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Asher, Swarnadeep Bhar. (2024)<br><strong>Strong hallucinations from negation and how to fix them</strong><br><button class=copy-to-clipboard title="Strong hallucinations from negation and how to fix them" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Natural Language Inference, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10543v1.pdf filename=2402.10543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite great performance on many tasks, language models (LMs) still struggle with <b>reasoning,</b> sometimes providing responses that cannot possibly be true because they stem from logical incoherence. We call such responses \textit{strong hallucinations} and prove that they follow from an LM&rsquo;s computation of its internal representations for logical operators and outputs from those representations. Focusing on negation, we provide a novel solution in which negation is treated not as another element of a latent representation, but as \textit{an operation over an LM&rsquo;s latent representations that constrains how they may evolve}. We show that our approach improves model performance in cloze <b>prompting</b> and <b>natural</b> <b>language</b> <b>inference</b> tasks with negation without requiring training on sparse negative data.</p></p class="citation"></blockquote><h3 id=6268--62204-properties-and-challenges-of-llm-generated-explanations-jenny-kunz-et-al-2024>(62/68 | 62/204) Properties and Challenges of LLM-Generated Explanations (Jenny Kunz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jenny Kunz, Marco Kuhlmann. (2024)<br><strong>Properties and Challenges of LLM-Generated Explanations</strong><br><button class=copy-to-clipboard title="Properties and Challenges of LLM-Generated Explanations" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10532v1.pdf filename=2402.10532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The self-rationalising capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been explored in restricted settings, using task/specific data sets. However, current <b>LLMs</b> do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs. The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction <b>fine-tuning.</b> As the pre-training corpus includes a <b>large</b> <b>amount</b> <b>of</b> human-written explanations &ldquo;in the wild&rdquo;, we hypothesise that <b>LLMs</b> adopt common properties of human explanations. By analysing the outputs for a multi-domain instruction <b>fine-tuning</b> data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading. We discuss reasons and consequences of the properties&rsquo; presence or absence. In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system.</p></p class="citation"></blockquote><h3 id=6368--63204-evaluating-and-improving-continual-learning-in-spoken-language-understanding-muqiao-yang-et-al-2024>(63/68 | 63/204) Evaluating and Improving Continual Learning in Spoken Language Understanding (Muqiao Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muqiao Yang, Xiang Li, Umberto Cappellazzo, Shinji Watanabe, Bhiksha Raj. (2024)<br><strong>Evaluating and Improving Continual Learning in Spoken Language Understanding</strong><br><button class=copy-to-clipboard title="Evaluating and Improving Continual Learning in Spoken Language Understanding" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 30<br>Keywords: Continual Learning, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10427v1.pdf filename=2402.10427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> has emerged as an increasingly important challenge across various tasks, including Spoken Language Understanding (SLU). In SLU, its objective is to effectively handle the emergence of new concepts and evolving environments. The evaluation of <b>continual</b> <b>learning</b> algorithms typically involves assessing the model&rsquo;s stability, plasticity, and generalizability as fundamental aspects of standards. However, existing <b>continual</b> <b>learning</b> metrics primarily focus on only one or two of the properties. They neglect the overall performance across all tasks, and do not adequately disentangle the plasticity versus stability/generalizability trade-offs within the model. In this work, we propose an evaluation methodology that provides a unified evaluation on stability, plasticity, and generalizability in <b>continual</b> <b>learning.</b> By employing the proposed metric, we demonstrate how introducing various <b>knowledge</b> <b>distillations</b> can improve different aspects of these three properties of the SLU model. We further show that our proposed metric is more sensitive in capturing the impact of task ordering in <b>continual</b> <b>learning,</b> making it better suited for practical use-case scenarios.</p></p class="citation"></blockquote><h3 id=6468--64204-reviewer2-optimizing-review-generation-through-prompt-generation-zhaolin-gao-et-al-2024>(64/68 | 64/204) Reviewer2: Optimizing Review Generation Through Prompt Generation (Zhaolin Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaolin Gao, Kianté Brantley, Thorsten Joachims. (2024)<br><strong>Reviewer2: Optimizing Review Generation Through Prompt Generation</strong><br><button class=copy-to-clipboard title="Reviewer2: Optimizing Review Generation Through Prompt Generation" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10886v1.pdf filename=2402.10886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in <b>LLMs</b> offer new opportunities for assisting authors in improving their work. In this paper, we envision a use case where authors can receive <b>LLM-generated</b> reviews that uncover weak points in the current draft. While initial methods for automated review generation already exist, these methods tend to produce reviews that lack detail, and they do not cover the range of opinions that human reviewers produce. To address this shortcoming, we propose an efficient two-stage review generation framework called Reviewer2. Unlike prior work, this approach explicitly models the distribution of possible aspects that the review may address. We show that this leads to more detailed reviews that better cover the range of aspects that human reviewers identify in the draft. As part of the research, we generate a large-scale review dataset of 27k papers and 99k reviews that we annotate with aspect <b>prompts,</b> which we make available as a resource for future research.</p></p class="citation"></blockquote><h3 id=6568--65204-incremental-sequence-labeling-a-tale-of-two-shifts-shengjie-qiu-et-al-2024>(65/68 | 65/204) Incremental Sequence Labeling: A Tale of Two Shifts (Shengjie Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengjie Qiu, Junhao Zheng, Zhen Liu, Yicheng Luo, Qianli Ma. (2024)<br><strong>Incremental Sequence Labeling: A Tale of Two Shifts</strong><br><button class=copy-to-clipboard title="Incremental Sequence Labeling: A Tale of Two Shifts" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10447v1.pdf filename=2402.10447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The incremental sequence labeling task involves continuously learning new classes over time while retaining <b>knowledge</b> <b>of</b> the previous ones. Our investigation identifies two significant semantic shifts: E2O (where the model mislabels an old entity as a non-entity) and O2E (where the model labels a non-entity or old entity as a new entity). Previous research has predominantly focused on addressing the E2O problem, neglecting the O2E issue. This negligence results in a model bias towards classifying new data samples as belonging to the new class during the learning process. To address these challenges, we propose a novel framework, Incremental Sequential Labeling without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the E2O problem, we use <b>knowledge</b> <b>distillation</b> to maintain the model&rsquo;s discriminative ability for old entities. Simultaneously, to tackle the O2E problem, we alleviate the model&rsquo;s bias towards new entities through debiased loss and optimization levels. Our experimental evaluation, conducted on three datasets with various incremental settings, demonstrates the superior performance of IS3 compared to the previous state-of-the-art method by a significant margin.</p></p class="citation"></blockquote><h3 id=6668--66204-smaller-language-models-are-capable-of-selecting-instruction-tuning-training-data-for-larger-language-models-dheeraj-mekala-et-al-2024>(66/68 | 66/204) Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models (Dheeraj Mekala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dheeraj Mekala, Alex Nguyen, Jingbo Shang. (2024)<br><strong>Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models</strong><br><button class=copy-to-clipboard title="Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: LLaMA, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10430v1.pdf filename=2402.10430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction-tuning</b> <b>language</b> models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior <b>instruction-tuned</b> <b>model</b> compared to training on the complete dataset. Utilizing open-sourced OPT and <b>Llama-2</b> models up to 13B in size, two publicly available <b>instruction-tuning</b> <b>training</b> datasets and evaluated by both automatic metrics & humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative.</p></p class="citation"></blockquote><h3 id=6768--67204-dell-generating-reactions-and-explanations-for-llm-based-misinformation-detection-herun-wan-et-al-2024>(67/68 | 67/204) DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection (Herun Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Herun Wan, Shangbin Feng, Zhaoxuan Tan, Heng Wang, Yulia Tsvetkov, Minnan Luo. (2024)<br><strong>DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection</strong><br><button class=copy-to-clipboard title="DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10426v1.pdf filename=2402.10426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> are limited by challenges in factuality and hallucinations to be directly employed off-the-shelf for judging the veracity of news articles, where factual accuracy is paramount. In this work, we propose DELL that identifies three key stages in misinformation detection where <b>LLMs</b> could be incorporated as part of the pipeline: 1) <b>LLMs</b> could \emph{generate news reactions} to represent diverse perspectives and simulate user-news interaction networks; 2) <b>LLMs</b> could \emph{generate explanations} for proxy tasks (e.g., sentiment, stance) to enrich the contexts of news articles and produce experts specializing in various aspects of news understanding; 3) <b>LLMs</b> could \emph{merge task-specific experts} and provide an overall prediction by incorporating the predictions and confidence scores of varying experts. Extensive experiments on seven datasets with three <b>LLMs</b> demonstrate that DELL outperforms state-of-the-art baselines by up to 16.8% in macro f1-score. Further analysis reveals that the generated reactions and explanations are greatly helpful in misinformation detection, while our proposed <b>LLM-guided</b> expert merging helps produce better-calibrated predictions.</p></p class="citation"></blockquote><h3 id=6868--68204-enhancing-esg-impact-type-identification-through-early-fusion-and-multilingual-models-hariram-veeramani-et-al-2024>(68/68 | 68/204) Enhancing ESG Impact Type Identification through Early Fusion and Multilingual Models (Hariram Veeramani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hariram Veeramani, Surendrabikram Thapa, Usman Naseem. (2024)<br><strong>Enhancing ESG Impact Type Identification through Early Fusion and Multilingual Models</strong><br><button class=copy-to-clipboard title="Enhancing ESG Impact Type Identification through Early Fusion and Multilingual Models" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10772v1.pdf filename=2402.10772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving landscape of Environmental, Social, and Corporate Governance (ESG) impact assessment, the ML-ESG-2 shared task proposes identifying ESG impact types. To address this challenge, we present a comprehensive system leveraging ensemble learning techniques, capitalizing on early and late fusion approaches. Our approach employs four distinct models: mBERT, FlauBERT-base, ALBERT-base-v2, and a Multi-Layer Perceptron (MLP) incorporating Latent Semantic Analysis (LSA) and Term Frequency-Inverse Document Frequency <b>(TF-IDF)</b> features. Through extensive experimentation, we find that our early fusion ensemble approach, featuring the integration of LSA, <b>TF-IDF,</b> mBERT, FlauBERT-base, and ALBERT-base-v2, delivers the best performance. Our system offers a comprehensive ESG impact type identification solution, contributing to the responsible and sustainable decision-making processes vital in today&rsquo;s financial and corporate governance landscape.</p></p class="citation"></blockquote><h2 id=cslg-42>cs.LG (42)</h2><h3 id=142--69204-rlvf-learning-from-verbal-feedback-without-overgeneralization-moritz-stephan-et-al-2024>(1/42 | 69/204) RLVF: Learning from Verbal Feedback without Overgeneralization (Moritz Stephan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moritz Stephan, Alexander Khazatsky, Eric Mitchell, Annie S Chen, Sheryl Hsu, Archit Sharma, Chelsea Finn. (2024)<br><strong>RLVF: Learning from Verbal Feedback without Overgeneralization</strong><br><button class=copy-to-clipboard title="RLVF: Learning from Verbal Feedback without Overgeneralization" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 100<br>Keywords: Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, GPT, GPT-4, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10893v1.pdf filename=2402.10893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The diversity of contexts in which <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as &ldquo;Don&rsquo;t use emojis when drafting emails to my boss.&rdquo; However, while writing high-level feedback is far simpler than collecting annotations for <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF),</b> we find that simply <b>prompting</b> a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should not) be applied. It then <b>fine-tunes</b> the model in accordance with the synthetic preference data while minimizing the divergence from the original model for <b>prompts</b> where the feedback does not apply. Our experimental results indicate that our approach effectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts. For both human- and <b>GPT-4-generated</b> high-level feedback, C3PO effectively adheres to the given feedback comparably to <b>in-context</b> baselines while reducing overgeneralization by 30%.</p></p class="citation"></blockquote><h3 id=242--70204-symbolic-autoencoding-for-self-supervised-sequence-learning-mohammad-hossein-amani-et-al-2024>(2/42 | 70/204) Symbolic Autoencoding for Self-Supervised Sequence Learning (Mohammad Hossein Amani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Hossein Amani, Nicolas Mario Baldwin, Amin Mansouri, Martin Josifoski, Maxime Peyrard, Robert West. (2024)<br><strong>Symbolic Autoencoding for Self-Supervised Sequence Learning</strong><br><button class=copy-to-clipboard title="Symbolic Autoencoding for Self-Supervised Sequence Learning" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Reconstruction Loss, Self-supervised Learning, Supervised Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10575v1.pdf filename=2402.10575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional language models, adept at next-token prediction in text sequences, often struggle with transduction tasks between distinct symbolic systems, particularly when parallel data is scarce. Addressing this issue, we introduce \textit{symbolic autoencoding} ($\Sigma$AE), a <b>self-supervised</b> framework that harnesses the power of abundant unparallel data alongside limited parallel data. $\Sigma$AE connects two generative models via a discrete bottleneck layer and is optimized end-to-end by minimizing <b>reconstruction</b> <b>loss</b> (simultaneously with <b>supervised</b> <b>loss</b> for the parallel data), such that the sequence generated by the discrete bottleneck can be read out as the transduced input sequence. We also develop gradient-based methods allowing for efficient <b>self-supervised</b> sequence learning despite the discreteness of the bottleneck. Our results demonstrate that $\Sigma$AE significantly enhances performance on transduction tasks, even with minimal parallel data, offering a promising solution for <b>weakly</b> <b>supervised</b> <b>learning</b> scenarios.</p></p class="citation"></blockquote><h3 id=342--71204-provably-sample-efficient-rlhf-via-active-preference-optimization-nirjhar-das-et-al-2024>(3/42 | 71/204) Provably Sample Efficient RLHF via Active Preference Optimization (Nirjhar Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, Sayak Ray Chowdhury. (2024)<br><strong>Provably Sample Efficient RLHF via Active Preference Optimization</strong><br><button class=copy-to-clipboard title="Provably Sample Efficient RLHF via Active Preference Optimization" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Bandit Algorithm, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10500v1.pdf filename=2402.10500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> is pivotal in aligning <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of <b>RLHF.</b> Hence better and adaptive strategies for data collection is needed. To this end, we frame <b>RLHF</b> as a contextual preference <b>bandit</b> problem with <b>prompts</b> as contexts and show that the naive way of collecting preference data by choosing <b>prompts</b> uniformly at random leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards. Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an algorithm that actively selects <b>prompts</b> to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample efficiency without compromising on policy performance. We show that given a sample budget of $T$, the suboptimality gap of a policy learned via $\texttt{APO}$ scales as $O(1/\sqrt{T})$. Next, we propose a compute-efficient batch version of $\texttt{APO}$ with minor modification and evaluate its performance in practice. Experimental evaluations on a human preference dataset validate \texttt{APO}&rsquo;s efficacy as a sample-efficient and practical solution to data collection for <b>RLHF,</b> facilitating alignment of <b>LLMs</b> with human preferences in a cost-effective and scalable manner.</p></p class="citation"></blockquote><h3 id=442--72204-qdylora-quantized-dynamic-low-rank-adaptation-for-efficient-large-language-model-tuning-hossein-rajabzadeh-et-al-2024>(4/42 | 72/204) QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning (Hossein Rajabzadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh. (2024)<br><strong>QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning</strong><br><button class=copy-to-clipboard title="QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Quantization, Quantization, falcon, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10462v1.pdf filename=2402.10462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Finetuning</b> <b>large</b> <b>language</b> <b>models</b> requires huge GPU memory, restricting the choice to acquire Larger models. While the <b>quantized</b> version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further <b>fine-tuning</b> steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient <b>quantization</b> approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently <b>finetune</b> <b>LLMs</b> on a set of pre-defined LoRA ranks. QDyLoRA enables <b>fine-tuning</b> <b>Falcon-40b</b> for ranks 1 to 64 on a single 32 GB V100-GPU through one round of <b>fine-tuning.</b> Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.</p></p class="citation"></blockquote><h3 id=542--73204-fedd2s-personalized-data-free-federated-knowledge-distillation-kawa-atapour-et-al-2024>(5/42 | 73/204) FedD2S: Personalized Data-Free Federated Knowledge Distillation (Kawa Atapour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kawa Atapour, S. Jamal Seyedmohammadi, Jamshid Abouei, Arash Mohammadi, Konstantinos N. Plataniotis. (2024)<br><strong>FedD2S: Personalized Data-Free Federated Knowledge Distillation</strong><br><button class=copy-to-clipboard title="FedD2S: Personalized Data-Free Federated Knowledge Distillation" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG, eess-IV<br>Keyword Score: 60<br>Keywords: Fairness, Federated Learning, Knowledge Distillation, Knowledge Distillation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10846v1.pdf filename=2402.10846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenge of mitigating data heterogeneity among clients within a <b>Federated</b> <b>Learning</b> (FL) framework. The model-drift issue, arising from the noniid nature of client data, often results in suboptimal personalization of a global model compared to locally trained models for each client. To tackle this challenge, we propose a novel approach named FedD2S for Personalized <b>Federated</b> <b>Learning</b> (pFL), leveraging <b>knowledge</b> <b>distillation.</b> FedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free <b>knowledge</b> <b>distillation</b> process to enhance local model personalization. Through extensive <b>simulations</b> on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and CIFAR100-we compare FedD2S with state-of-the-art FL baselines. The proposed approach demonstrates superior performance, characterized by accelerated convergence and improved <b>fairness</b> among clients. The introduced layer-dropping technique effectively captures personalized <b>knowledge,</b> <b>resulting</b> in enhanced performance compared to alternative FL models. Moreover, we investigate the impact of key hyperparameters, such as the participation ratio and layer-dropping rate, providing valuable insights into the optimal configuration for FedD2S. The findings demonstrate the efficacy of adaptive layer-dropping in the <b>knowledge</b> <b>distillation</b> process to achieve enhanced personalization and performance across diverse datasets and tasks.</p></p class="citation"></blockquote><h3 id=642--74204-subgraph-level-universal-prompt-tuning-junhyun-lee-et-al-2024>(6/42 | 74/204) Subgraph-level Universal Prompt Tuning (Junhyun Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhyun Lee, Wooseong Yang, Jaewoo Kang. (2024)<br><strong>Subgraph-level Universal Prompt Tuning</strong><br><button class=copy-to-clipboard title="Subgraph-level Universal Prompt Tuning" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Edge Prediction, Graph, Graph Neural Network, Few-shot, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10380v1.pdf filename=2402.10380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving landscape of machine learning, the adaptation of pre-trained models through <b>prompt</b> tuning has become increasingly prominent. This trend is particularly observable in the <b>graph</b> <b>domain,</b> <b>where</b> diverse pre-training strategies present unique challenges in developing effective <b>prompt-based</b> tuning methods for <b>graph</b> <b>neural</b> <b>networks.</b> Previous approaches have been limited, focusing on specialized <b>prompting</b> functions tailored to models with <b>edge</b> <b>prediction</b> pre-training tasks. These methods, however, suffer from a lack of generalizability across different pre-training strategies. Recently, a simple <b>prompt</b> tuning method has been designed for any pre-training strategy, functioning within the input <b>graph&rsquo;s</b> <b>feature</b> <b>space.</b> This allows it to theoretically emulate any type of <b>prompting</b> function, thereby significantly increasing its versatility for a range of downstream applications. Nevertheless, the capacity of such simple <b>prompts</b> to fully grasp the complex contexts found in <b>graphs</b> <b>remains</b> <b>an</b> open question, necessitating further investigation. Addressing this challenge, our work introduces the Subgraph-level Universal <b>Prompt</b> Tuning (SUPT) approach, focusing on the detailed context within subgraphs. In SUPT, <b>prompt</b> features are assigned at the subgraph-level, preserving the method&rsquo;s universal capability. This requires extremely fewer tuning parameters than <b>fine-tuning-based</b> methods, outperforming them in 42 out of 45 full-shot scenario experiments with an average improvement of over 2.5%. In <b>few-shot</b> scenarios, it excels in 41 out of 45 experiments, achieving an average performance increase of more than 6.6%.</p></p class="citation"></blockquote><h3 id=742--75204-edgeqat-entropy-and-distribution-guided-quantization-aware-training-for-the-acceleration-of-lightweight-llms-on-the-edge-xuan-shen-et-al-2024>(7/42 | 75/204) EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge (Xuan Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuan Shen, Zhenglun Kong, Changdi Yang, Zhaoyang Han, Lei Lu, Peiyan Dong, Cheng Lyu, Chih-hsiang Li, Xuehang Guo, Zhihao Shu, Wei Niu, Miriam Leeser, Pu Zhao, Yanzhi Wang. (2024)<br><strong>EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge</strong><br><button class=copy-to-clipboard title="EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Quantization, Quantization, Large Language Model, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10787v1.pdf filename=2402.10787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the remarkable strides of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in various fields, the wide applications of <b>LLMs</b> on edge devices are limited due to their massive parameters and computations. To address this, <b>quantization</b> is commonly adopted to generate lightweight <b>LLMs</b> with efficient computations and fast inference. However, Post-Training <b>Quantization</b> (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits. Besides, many <b>Quantization-Aware</b> Training (QAT) works <b>quantize</b> model weights, leaving the activations untouched, which do not fully exploit the potential of <b>quantization</b> for inference acceleration on the edge. In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight <b>LLMs</b> to achieve inference acceleration on Edge devices. We first identify that the performance drop of <b>quantization</b> primarily stems from the information distortion in <b>quantized</b> attention maps, demonstrated by the different distributions in <b>quantized</b> query and key of the <b>self-attention</b> mechanism. Then, the entropy and distribution guided QAT is proposed to mitigate the information distortion. Moreover, we design a token importance-aware adaptive method to dynamically <b>quantize</b> the tokens with different bit widths for further optimization and acceleration. Our extensive experiments verify the substantial improvements with our framework across various datasets. Furthermore, we achieve an on-device speedup of up to 2.37x compared with its FP16 counterparts across multiple edge devices, signaling a groundbreaking advancement.</p></p class="citation"></blockquote><h3 id=842--76204-machine-learning-based-prediction-of-ditching-loads-henning-schwarz-et-al-2024>(8/42 | 76/204) Machine Learning based Prediction of Ditching Loads (Henning Schwarz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Henning Schwarz, Micha Überrück, Jens-Peter M. Zemke, Thomas Rung. (2024)<br><strong>Machine Learning based Prediction of Ditching Loads</strong><br><button class=copy-to-clipboard title="Machine Learning based Prediction of Ditching Loads" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Autoencoder, Convolution, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10724v1.pdf filename=2402.10724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present approaches to predict dynamic ditching loads on aircraft fuselages using machine learning. The employed learning procedure is structured into two parts, the reconstruction of the spatial loads using a <b>convolutional</b> <b>autoencoder</b> (CAE) and the transient evolution of these loads in a subsequent part. Different CAE strategies are assessed and combined with either <b>long</b> <b>short-term</b> <b>memory</b> <b>(LSTM)</b> networks or Koopman-operator based methods to predict the transient behaviour. The training data is compiled by an extension of the momentum method of von-Karman and Wagner and the rationale of the training approach is briefly summarised. The application included refers to a full-scale fuselage of a DLR-D150 aircraft for a range of horizontal and vertical approach velocities at 6{\deg} incidence. Results indicate a satisfactory level of predictive agreement for all four investigated surrogate models examined, with the combination of an <b>LSTM</b> and a deep decoder CAE showing the best performance.</p></p class="citation"></blockquote><h3 id=942--77204-linear-transformers-with-learnable-kernel-functions-are-better-in-context-models-yaroslav-aksenov-et-al-2024>(9/42 | 77/204) Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Yaroslav Aksenov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov. (2024)<br><strong>Linear Transformers with Learnable Kernel Functions are Better In-Context Models</strong><br><button class=copy-to-clipboard title="Linear Transformers with Learnable Kernel Functions are Better In-Context Models" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Transformer, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10644v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10644v1.pdf filename=2402.10644v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing <b>Transformer</b> performance on language modeling tasks. However, these models have revealed deficiencies in essential <b>In-Context</b> <b>Learning</b> capabilities - a domain where the <b>Transformer</b> traditionally shines. The Based model emerged as a hybrid solution, blending a Linear <b>Transformer</b> with a kernel inspired by the Taylor expansion of exponential functions, augmented by <b>convolutional</b> <b>networks.</b> Mirroring the <b>Transformer&rsquo;s</b> <b>in-context</b> <b>adeptness,</b> it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its <b>In-Context</b> <b>Learning</b> abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.</p></p class="citation"></blockquote><h3 id=1042--78204-adversarial-curriculum-graph-contrastive-learning-with-pair-wise-augmentation-xinjian-zhao-et-al-2024>(10/42 | 78/204) Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation (Xinjian Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinjian Zhao, Liang Zhang, Yang Liu, Ruocheng Guo, Xiangyu Zhao. (2024)<br><strong>Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation</strong><br><button class=copy-to-clipboard title="Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Graph, Graph Contrastive Learning, Graph Contrastive Learning, Benchmarking, Contrastive Learning, Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10468v1.pdf filename=2402.10468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>contrastive</b> <b>learning</b> <b>(GCL)</b> has emerged as a pivotal technique in the domain of <b>graph</b> <b>representation</b> <b>learning.</b> A crucial aspect of effective <b>GCL</b> is the caliber of generated positive and negative samples, which is intrinsically dictated by their resemblance to the original data. Nevertheless, precise control over similarity during sample generation presents a formidable challenge, often impeding the effective discovery of representative <b>graph</b> <b>patterns.</b> <b>To</b> address this challenge, we propose an innovative framework: Adversarial <b>Curriculum</b> <b>Graph</b> <b>Contrastive</b> <b>Learning</b> (ACGCL), which capitalizes on the merits of pair-wise augmentation to engender <b>graph-level</b> <b>positive</b> <b>and</b> negative samples with controllable similarity, alongside subgraph <b>contrastive</b> <b>learning</b> to discern effective <b>graph</b> <b>patterns</b> <b>therein.</b> Within the ACGCL framework, we have devised a novel adversarial <b>curriculum</b> <b>training</b> methodology that facilitates progressive learning by sequentially increasing the difficulty of distinguishing the generated samples. Notably, this approach transcends the prevalent sparsity issue inherent in conventional <b>curriculum</b> <b>learning</b> strategies by adaptively concentrating on more challenging training data. Finally, a comprehensive assessment of ACGCL is conducted through extensive experiments on six well-known <b>benchmark</b> datasets, wherein ACGCL conspicuously surpasses a set of state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=1142--79204-masked-attention-is-all-you-need-for-graphs-david-buterez-et-al-2024>(11/42 | 79/204) Masked Attention is All You Need for Graphs (David Buterez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Buterez, Jon Paul Janet, Dino Oglic, Pietro Lio. (2024)<br><strong>Masked Attention is All You Need for Graphs</strong><br><button class=copy-to-clipboard title="Masked Attention is All You Need for Graphs" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10793v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10793v1.pdf filename=2402.10793v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> and variations of the message passing algorithm are the predominant means for learning on <b>graphs,</b> <b>largely</b> <b>due</b> to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose <b>GNNs,</b> however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on <b>graphs</b> <b>that</b> <b>relies</b> exclusively on attention. <b>Graphs</b> <b>are</b> <b>represented</b> as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each <b>graph.</b> <b>Despite</b> <b>its</b> simplicity, masked attention for <b>graphs</b> <b>(MAG)</b> <b>has</b> state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and <b>graph-level</b> <b>tasks.</b> <b>We</b> also show significantly better <b>transfer</b> <b>learning</b> capabilities compared to <b>GNNs</b> and comparable or better time and memory scaling. MAG has sub-linear memory scaling in the number of nodes or edges, enabling learning on dense <b>graphs</b> <b>and</b> <b>future-proofing</b> the approach.</p></p class="citation"></blockquote><h3 id=1242--80204-pretext-training-algorithms-for-event-sequence-data-yimu-wang-et-al-2024>(12/42 | 80/204) Pretext Training Algorithms for Event Sequence Data (Yimu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yimu Wang, He Zhao, Ruizhi Deng, Frederick Tung, Greg Mori. (2024)<br><strong>Pretext Training Algorithms for Event Sequence Data</strong><br><button class=copy-to-clipboard title="Pretext Training Algorithms for Event Sequence Data" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Contrastive Learning, Fine-tuning, Self-supervised Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10392v1.pdf filename=2402.10392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretext training followed by task-specific <b>fine-tuning</b> has been a successful approach in vision and language domains. This paper proposes a <b>self-supervised</b> pretext training framework tailored to event sequence data. We introduce a novel alignment verification task that is specialized to event sequences, building on good practices in masked reconstruction and <b>contrastive</b> <b>learning.</b> Our pretext tasks unlock foundational representations that are generalizable across different down-stream tasks, including next-event prediction for temporal point process models, event sequence classification, and missing event interpolation. Experiments on popular public <b>benchmarks</b> demonstrate the potential of the proposed method across different tasks and data domains.</p></p class="citation"></blockquote><h3 id=1342--81204-any-precision-llm-low-cost-deployment-of-multiple-different-sized-llms-yeonhong-park-et-al-2024>(13/42 | 81/204) Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs (Yeonhong Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, Jae W. Lee. (2024)<br><strong>Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs</strong><br><button class=copy-to-clipboard title="Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Quantization, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10517v1.pdf filename=2402.10517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, considerable efforts have been directed towards compressing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their <b>large</b> <b>sizes.</b> <b>Meanwhile,</b> much less attention has been given to mitigating the costs associated with deploying multiple <b>LLMs</b> of varying sizes despite its practical significance. Thus, this paper introduces \emph{any-precision LLM}, extending the concept of any-precision DNN to <b>LLMs.</b> Addressing challenges in any-precision <b>LLM,</b> we propose a lightweight method for any-precision <b>quantization</b> of <b>LLMs,</b> leveraging a post-training <b>quantization</b> framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized <b>LLMs</b> by overlaying <b>LLMs</b> <b>quantized</b> to varying bit-widths, such as 3, 4, &mldr;, $n$ bits, into a memory footprint comparable to a single $n$-bit <b>LLM.</b> All the supported <b>LLMs</b> with varying bit-widths demonstrate state-of-the-art model quality and inference throughput, proving itself to be a compelling option for deployment of multiple, different-sized <b>LLMs.</b> The source code will be publicly available soon.</p></p class="citation"></blockquote><h3 id=1442--82204-can-transformers-predict-vibrations-fusataka-kuniyoshi-et-al-2024>(14/42 | 82/204) Can Transformers Predict Vibrations? (Fusataka Kuniyoshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fusataka Kuniyoshi, Yoshihide Sawada. (2024)<br><strong>Can Transformers Predict Vibrations?</strong><br><button class=copy-to-clipboard title="Can Transformers Predict Vibrations?" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10511v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10511v1.pdf filename=2402.10511v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Highly accurate time-series vibration prediction is an important research issue for electric vehicles (EVs). EVs often experience vibrations when driving on rough terrains, known as torsional resonance. This resonance, caused by the interaction between motor and tire vibrations, puts excessive loads on the vehicle&rsquo;s drive shaft. However, current damping technologies only detect resonance after the vibration amplitude of the drive shaft torque reaches a certain threshold, leading to significant loads on the shaft at the time of detection. In this study, we propose a novel approach to address this issue by introducing Resoformer, a <b>transformer-based</b> model for predicting torsional resonance. Resoformer utilizes time-series of the motor rotation speed as input and predicts the amplitude of torsional vibration at a specified quantile occurring in the shaft after the input series. By calculating the attention between recursive and <b>convolutional</b> <b>features</b> extracted from the measured data points, Resoformer improves the accuracy of vibration forecasting. To evaluate the model, we use a vibration dataset called VIBES (Dataset for Forecasting Vibration Transition in EVs), consisting of 2,600 simulator-generated vibration sequences. Our experiments, conducted on strong baselines built on the VIBES dataset, demonstrate that Resoformer achieves state-of-the-art results. In conclusion, our study answers the question &ldquo;Can <b>Transformers</b> Forecast Vibrations?&rdquo; While traditional <b>transformer</b> architectures show low performance in forecasting torsional resonance waves, our findings indicate that combining <b>recurrent</b> <b>neural</b> <b>network</b> and temporal <b>convolutional</b> <b>network</b> using the <b>transformer</b> architecture improves the accuracy of long-term vibration forecasting.</p></p class="citation"></blockquote><h3 id=1542--83204-prise-learning-temporal-action-abstractions-as-a-sequence-compression-problem-ruijie-zheng-et-al-2024>(15/42 | 83/204) PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem (Ruijie Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruijie Zheng, Ching-An Cheng, Hal Daumé III, Furong Huang, Andrey Kolobov. (2024)<br><strong>PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem</strong><br><button class=copy-to-clipboard title="PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Few-shot, Quantization, Tokenization, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10450v1.pdf filename=2402.10450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of <b>LLM</b> training pipelines &ndash; input <b>tokenization</b> via byte pair encoding (BPE) &ndash; to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action <b>quantization</b> with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of both multitask imitation learning as well as <b>few-shot</b> imitation learning on unseen tasks. Our code will be released at <a href=https://github.com/FrankZheng2022/PRISE>https://github.com/FrankZheng2022/PRISE</a>.</p></p class="citation"></blockquote><h3 id=1642--84204-personalised-drug-identifier-for-cancer-treatment-with-transformers-using-auxiliary-information-aishwarya-jayagopal-et-al-2024>(16/42 | 84/204) Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information (Aishwarya Jayagopal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aishwarya Jayagopal, Hansheng Xue, Ziyang He, Robert J. Walsh, Krishna Kumar Hariprasannan, David Shao Peng Tan, Tuan Zea Tan, Jason J. Pitt, Anand D. Jeyasekharan, Vaibhav Rajan. (2024)<br><strong>Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information</strong><br><button class=copy-to-clipboard title="Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-QM<br>Keyword Score: 33<br>Keywords: Benchmarking, Recommendation, Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10551v1.pdf filename=2402.10551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cancer remains a global challenge due to its growing clinical and economic burden. Its uniquely personal manifestation, which makes treatment difficult, has fuelled the quest for personalized treatment strategies. Thus, genomic profiling is increasingly becoming part of clinical diagnostic panels. Effective use of such panels requires accurate drug response prediction (DRP) models, which are challenging to build due to limited labelled patient data. Previous methods to address this problem have used various forms of <b>transfer</b> <b>learning.</b> However, they do not explicitly model the variable length sequential structure of the list of mutations in such diagnostic panels. Further, they do not utilize auxiliary information (like patient survival) for model training. We address these limitations through a novel <b>transformer</b> based method, which surpasses the performance of state-of-the-art DRP models on <b>benchmark</b> data. We also present the design of a treatment <b>recommendation</b> system (TRS), which is currently deployed at the National University Hospital, Singapore and is being evaluated in a clinical trial.</p></p class="citation"></blockquote><h3 id=1742--85204-parametric-augmentation-for-time-series-contrastive-learning-xu-zheng-et-al-2024>(17/42 | 85/204) Parametric Augmentation for Time Series Contrastive Learning (Xu Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Zheng, Tianchun Wang, Wei Cheng, Aitian Ma, Haifeng Chen, Mo Sha, Dongsheng Luo. (2024)<br><strong>Parametric Augmentation for Time Series Contrastive Learning</strong><br><button class=copy-to-clipboard title="Parametric Augmentation for Time Series Contrastive Learning" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Contrastive Learning, Data Augmentation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10434v1.pdf filename=2402.10434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern techniques like <b>contrastive</b> <b>learning</b> have been effectively used in many areas, including computer vision, natural language processing, and <b>graph-structured</b> <b>data.</b> <b>Creating</b> positive examples that assist the model in learning robust and discriminative representations is a crucial stage in <b>contrastive</b> <b>learning</b> approaches. Usually, preset human intuition directs the selection of relevant <b>data</b> <b>augmentations.</b> Due to patterns that are easily recognized by humans, this rule of thumb works well in the vision and language domains. However, it is impractical to visually inspect the temporal structures in time series. The diversity of time series augmentations at both the dataset and instance levels makes it difficult to choose meaningful augmentations on the fly. In this study, we address this gap by analyzing time series <b>data</b> <b>augmentation</b> using information theory and summarizing the most commonly adopted augmentations in a unified format. We then propose a <b>contrastive</b> <b>learning</b> framework with parametric augmentation, AutoTCL, which can be adaptively employed to support time series representation learning. The proposed approach is encoder-agnostic, allowing it to be seamlessly integrated with different backbone encoders. Experiments on univariate forecasting tasks demonstrate the highly competitive results of our method, with an average 6.5% reduction in MSE and 4.7% in MAE over the leading baselines. In classification tasks, AutoTCL achieves a $1.2%$ increase in average accuracy.</p></p class="citation"></blockquote><h3 id=1842--86204-double-duality-variational-primal-dual-policy-optimization-for-constrained-reinforcement-learning-zihao-li-et-al-2024>(18/42 | 86/204) Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning (Zihao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Li, Boyi Liu, Zhuoran Yang, Zhaoran Wang, Mengdi Wang. (2024)<br><strong>Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10810v1.pdf filename=2402.10810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the Constrained Convex <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP), where the goal is to minimize a convex functional of the visitation measure, subject to a convex constraint. Designing algorithms for a constrained convex MDP faces several challenges, including (1) handling the large state space, (2) managing the exploration/exploitation tradeoff, and (3) solving the constrained optimization where the objective and the constraint are both nonlinear functions of the visitation measure. In this work, we present a model-based algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which Lagrangian and Fenchel duality are implemented to reformulate the original constrained problem into an unconstrained primal-dual optimization. Moreover, the primal variables are updated by model-based value iteration following the principle of Optimism in the Face of Uncertainty (OFU), while the dual variables are updated by gradient ascent. Moreover, by embedding the visitation measure into a finite-dimensional space, we can handle large state spaces by incorporating function approximation. Two notable examples are (1) Kernelized Nonlinear Regulators and (2) Low-rank <b>MDPs.</b> We prove that with an optimistic planning oracle, our algorithm achieves sublinear regret and constraint violation in both cases and can attain the globally optimal policy of the original constrained problem.</p></p class="citation"></blockquote><h3 id=1942--87204-multitask-kernel-based-learning-with-logic-constraints-michelangelo-diligenti-et-al-2024>(19/42 | 87/204) Multitask Kernel-based Learning with Logic Constraints (Michelangelo Diligenti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michelangelo Diligenti, Marco Gori, Marco Maggini, Leonardo Rigutini. (2024)<br><strong>Multitask Kernel-based Learning with Logic Constraints</strong><br><button class=copy-to-clipboard title="Multitask Kernel-based Learning with Logic Constraints" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Semi-Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10617v1.pdf filename=2402.10617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a general framework to integrate prior knowledge in the form of logic constraints among a set of task functions into kernel machines. The logic propositions provide a partial representation of the environment, in which the learner operates, that is exploited by the learning algorithm together with the information available in the <b>supervised</b> examples. In particular, we consider a multi-task learning scheme, where multiple unary predicates on the feature space are to be learned by kernel machines and a higher level abstract representation consists of logic clauses on these predicates, known to hold for any input. A general approach is presented to convert the logic clauses into a continuous implementation, that processes the outputs computed by the kernel-based predicates. The learning task is formulated as a primal optimization problem of a loss function that combines a term measuring the fitting of the <b>supervised</b> examples, a regularization term, and a penalty term that enforces the constraints on both <b>supervised</b> and <b>unsupervised</b> examples. The proposed <b>semi-supervised</b> <b>learning</b> framework is particularly suited for learning in high dimensionality feature spaces, where the <b>supervised</b> training examples tend to be sparse and generalization difficult. Unlike for standard kernel machines, the cost function to optimize is not generally guaranteed to be convex. However, the experimental results show that it is still possible to find good solutions using a two stage learning schema, in which first the <b>supervised</b> examples are learned until convergence and then the logic constraints are forced. Some promising experimental results on artificial multi-task learning tasks are reported, showing how the classification accuracy can be effectively improved by exploiting the a priori rules and the <b>unsupervised</b> examples.</p></p class="citation"></blockquote><h3 id=2042--88204-timeseriesbench-an-industrial-grade-benchmark-for-time-series-anomaly-detection-models-haotian-si-et-al-2024>(20/42 | 88/204) TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models (Haotian Si et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Si, Changhua Pei, Hang Cui, Jingwen Yang, Yongqian Sun, Shenglin Zhang, Jingjing Li, Haiming Zhang, Jing Han, Dan Pei, Jianhui Li, Gaogang Xie. (2024)<br><strong>TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models</strong><br><button class=copy-to-clipboard title="TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Benchmarking, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10802v1.pdf filename=2402.10802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Driven by the proliferation of real-world application scenarios and scales, time series <b>anomaly</b> <b>detection</b> (TSAD) has attracted considerable scholarly and industrial interest. However, existing algorithms exhibit a gap in terms of training paradigm, online detection paradigm, and evaluation criteria when compared to the actual needs of real-world industrial systems. Firstly, current algorithms typically train a specific model for each individual time series. In a large-scale online system with tens of thousands of curves, maintaining such a multitude of models is impractical. The performance of using merely one single unified model to detect anomalies remains unknown. Secondly, most TSAD models are trained on the historical part of a time series and are tested on its future segment. In distributed systems, however, there are frequent system deployments and upgrades, with new, previously unseen time series emerging daily. The performance of testing newly incoming unseen time series on current TSAD algorithms remains unknown. Lastly, although some papers have conducted detailed surveys, the absence of an online evaluation platform prevents answering questions like &ldquo;Who is the best at <b>anomaly</b> <b>detection</b> at the current stage?&rdquo; In this paper, we propose TimeSeriesBench, an industrial-grade <b>benchmark</b> that we continuously maintain as a leaderboard. On this leaderboard, we assess the performance of existing algorithms across more than 168 evaluation settings combining different training and testing paradigms, evaluation metrics and datasets. Through our comprehensive analysis of the results, we provide <b>recommendations</b> for the future design of <b>anomaly</b> <b>detection</b> algorithms. To address known issues with existing public datasets, we release an industrial dataset to the public together with TimeSeriesBench. All code, data, and the online leaderboard have been made publicly available.</p></p class="citation"></blockquote><h3 id=2142--89204-fully-differentiable-lagrangian-convolutional-neural-network-for-continuity-consistent-physics-informed-precipitation-nowcasting-peter-pavlík-et-al-2024>(21/42 | 89/204) Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting (Peter Pavlík et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Pavlík, Martin Výboh, Anna Bou Ezzeddine, Viera Rozinajová. (2024)<br><strong>Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting</strong><br><button class=copy-to-clipboard title="Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-1; J-2, cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10747v1.pdf filename=2402.10747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a <b>convolutional</b> <b>neural</b> <b>network</b> model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods and implements the Lagrangian coordinate system transformation of the data in a fully differentiable and GPU-accelerated manner to allow for real-time end-to-end training and inference. Based on our evaluation, LUPIN matches and exceeds the performance of the chosen <b>benchmark,</b> opening the door for other Lagrangian machine learning models.</p></p class="citation"></blockquote><h3 id=2242--90204-unlink-to-unlearn-simplifying-edge-unlearning-in-gnns-jiajun-tan-et-al-2024>(22/42 | 90/204) Unlink to Unlearn: Simplifying Edge Unlearning in GNNs (Jiajun Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiajun Tan, Fei Sun, Ruichen Qiu, Du Su, Huawei Shen. (2024)<br><strong>Unlink to Unlearn: Simplifying Edge Unlearning in GNNs</strong><br><button class=copy-to-clipboard title="Unlink to Unlearn: Simplifying Edge Unlearning in GNNs" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10695v1.pdf filename=2402.10695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As concerns over data privacy intensify, unlearning in <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained <b>GNNs</b> upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. Furthermore, our analysis also suggests that loss functions may not be essential for effective edge unlearning. Building on these insights, we have simplified GNNDelete to develop Unlink-to-Unlearn (UtU), a novel method that facilitates unlearning exclusively through unlinking the forget edges from <b>graph</b> <b>structure.</b> <b>Our</b> extensive experiments demonstrate that UtU delivers privacy protection on par with that of a retrained model while preserving high accuracy in downstream tasks. Specifically, UtU upholds over 97.3% of the retrained model&rsquo;s privacy protection capabilities and 99.8% of its link prediction accuracy. Meanwhile, UtU requires only constant computational demands, underscoring its advantage as a highly lightweight and practical edge unlearning solution.</p></p class="citation"></blockquote><h3 id=2342--91204-logelectra-self-supervised-anomaly-detection-for-unstructured-logs-yuuki-yamanaka-et-al-2024>(23/42 | 91/204) LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs (Yuuki Yamanaka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuuki Yamanaka, Tomokatsu Takahashi, Takuya Minami, Yoshiaki Nakajima. (2024)<br><strong>LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs</strong><br><button class=copy-to-clipboard title="LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SE, cs.LG<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Benchmarking, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10397v1.pdf filename=2402.10397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>System logs are some of the most important information for the maintenance of software systems, which have become larger and more complex in recent years. The goal of log-based <b>anomaly</b> <b>detection</b> is to automatically detect system anomalies by analyzing the large number of logs generated in a short period of time, which is a critical challenge in the real world. Previous studies have used a log parser to extract templates from unstructured log data and detect anomalies on the basis of patterns of the template occurrences. These methods have limitations for logs with unknown templates. Furthermore, since most log anomalies are known to be point anomalies rather than contextual anomalies, detection methods based on occurrence patterns can cause unnecessary delays in detection. In this paper, we propose LogELECTRA, a new log <b>anomaly</b> <b>detection</b> model that analyzes a single line of log messages more deeply on the basis of <b>self-supervised</b> <b>anomaly</b> <b>detection.</b> LogELECTRA specializes in detecting log anomalies as point anomalies by applying ELECTRA, a natural language processing model, to analyze the semantics of a single line of log messages. LogELECTRA outperformed existing state-of-the-art methods in experiments on the public <b>benchmark</b> log datasets BGL, Sprit, and Thunderbird.</p></p class="citation"></blockquote><h3 id=2442--92204-differential-private-federated-transfer-learning-for-mental-health-monitoring-in-everyday-settings-a-case-study-on-stress-detection-ziyu-wang-et-al-2024>(24/42 | 92/204) Differential Private Federated Transfer Learning for Mental Health Monitoring in Everyday Settings: A Case Study on Stress Detection (Ziyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyu Wang, Zhongqi Yang, Iman Azimi, Amir M. Rahmani. (2024)<br><strong>Differential Private Federated Transfer Learning for Mental Health Monitoring in Everyday Settings: A Case Study on Stress Detection</strong><br><button class=copy-to-clipboard title="Differential Private Federated Transfer Learning for Mental Health Monitoring in Everyday Settings: A Case Study on Stress Detection" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10862v1.pdf filename=2402.10862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mental health conditions, prevalent across various demographics, necessitate efficient monitoring to mitigate their adverse impacts on life quality. The surge in data-driven methodologies for mental health monitoring has underscored the importance of privacy-preserving techniques in handling sensitive health data. Despite strides in <b>federated</b> <b>learning</b> for mental health monitoring, existing approaches struggle with vulnerabilities to certain cyber-attacks and data insufficiency in real-world applications. In this paper, we introduce a differential private <b>federated</b> <b>transfer</b> <b>learning</b> framework for mental health monitoring to enhance data privacy and enrich data sufficiency. To accomplish this, we integrate <b>federated</b> <b>learning</b> with two pivotal elements: (1) differential privacy, achieved by introducing noise into the updates, and (2) <b>transfer</b> <b>learning,</b> employing a pre-trained universal model to adeptly address issues of data imbalance and insufficiency. We evaluate the framework by a case study on stress detection, employing a dataset of physiological and contextual data from a longitudinal study. Our finding show that the proposed approach can attain a 10% boost in accuracy and a 21% enhancement in recall, while ensuring privacy protection.</p></p class="citation"></blockquote><h3 id=2542--93204-goal-conditioned-offline-reinforcement-learning-via-metric-learning-alfredo-reichlin-et-al-2024>(25/42 | 93/204) Goal-Conditioned Offline Reinforcement Learning via Metric Learning (Alfredo Reichlin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alfredo Reichlin, Miguel Vasco, Hang Yin, Danica Kragic. (2024)<br><strong>Goal-Conditioned Offline Reinforcement Learning via Metric Learning</strong><br><button class=copy-to-clipboard title="Goal-Conditioned Offline Reinforcement Learning via Metric Learning" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10820v1.pdf filename=2402.10820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we address the problem of learning optimal behavior from sub-optimal datasets in the context of goal-conditioned <b>offline</b> <b>reinforcement</b> <b>learning.</b> To do so, we propose a novel way of approximating the optimal value function for goal-conditioned <b>offline</b> <b>RL</b> <b>problems</b> under sparse rewards, symmetric and deterministic actions. We study a property for representations to recover optimality and propose a new optimization objective that leads to such property. We use the learned value function to guide the learning of a policy in an actor-critic fashion, a method we name MetricRL. Experimentally, we show how our method consistently outperforms other <b>offline</b> <b>RL</b> <b>baselines</b> in learning from sub-optimal <b>offline</b> <b>datasets.</b> <b>Moreover,</b> we show the effectiveness of our method in dealing with high-dimensional observations and in multi-goal tasks.</p></p class="citation"></blockquote><h3 id=2642--94204-physics-informed-meshgraphnets-pi-mgns-neural-finite-element-solvers-for-non-stationary-and-nonlinear-simulations-on-arbitrary-meshes-tobias-würth-et-al-2024>(26/42 | 94/204) Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes (Tobias Würth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Würth, Niklas Freymuth, Clemens Zimmerling, Gerhard Neumann, Luise Kärger. (2024)<br><strong>Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes</strong><br><button class=copy-to-clipboard title="Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CE, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10681v1.pdf filename=2402.10681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Engineering components must meet increasing technological demands in ever shorter development cycles. To face these challenges, a holistic approach is essential that allows for the concurrent development of part design, material system and manufacturing process. Current approaches employ numerical <b>simulations,</b> which however quickly becomes computation-intensive, especially for iterative optimization. Data-driven machine learning methods can be used to replace time- and resource-intensive numerical <b>simulations.</b> In particular, MeshGraphNets (MGNs) have shown promising results. They enable fast and accurate predictions on unseen mesh geometries while being fully differentiable for optimization. However, these models rely on large amounts of expensive training data, such as numerical <b>simulations.</b> Physics-informed neural networks (PINNs) offer an opportunity to train neural networks with partial differential equations instead of labeled data, but have not been extended yet to handle time-dependent <b>simulations</b> of arbitrary meshes. This work introduces PI-MGNs, a hybrid approach that combines PINNs and MGNs to quickly and accurately solve non-stationary and nonlinear partial differential equations (PDEs) on arbitrary meshes. The method is exemplified for thermal process <b>simulations</b> of unseen parts with inhomogeneous material distribution. Further results show that the model scales well to large and complex meshes, although it is trained on small generic meshes only.</p></p class="citation"></blockquote><h3 id=2742--95204-selective-prediction-for-semantic-segmentation-using-post-hoc-confidence-estimation-and-its-performance-under-distribution-shift-bruno-laboissiere-camargos-borges-et-al-2024>(27/42 | 95/204) Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift (Bruno Laboissiere Camargos Borges et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bruno Laboissiere Camargos Borges, Bruno Machado Pacheco, Danilo Silva. (2024)<br><strong>Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift</strong><br><button class=copy-to-clipboard title="Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Low-Resource, Selective Prediction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10665v1.pdf filename=2402.10665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic segmentation plays a crucial role in various computer vision applications, yet its efficacy is often hindered by the lack of high-quality labeled data. To address this challenge, a common strategy is to leverage models trained on data from different populations, such as publicly available datasets. This approach, however, leads to the distribution shift problem, presenting a reduced performance on the population of interest. In scenarios where model errors can have significant consequences, <b>selective</b> <b>prediction</b> methods offer a means to mitigate risks and reduce reliance on expert supervision. This paper investigates <b>selective</b> <b>prediction</b> for semantic segmentation in <b>low-resource</b> settings, thus focusing on post-hoc confidence estimators applied to pre-trained models operating under distribution shift. We propose a novel image-level confidence measure tailored for semantic segmentation and demonstrate its effectiveness through experiments on three medical imaging tasks. Our findings show that post-hoc confidence estimators offer a cost-effective approach to reducing the impacts of distribution shift.</p></p class="citation"></blockquote><h3 id=2842--96204-contiformer-continuous-time-transformer-for-irregular-time-series-modeling-yuqi-chen-et-al-2024>(28/42 | 96/204) ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling (Yuqi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Chen, Kan Ren, Yansen Wang, Yuchen Fang, Weiwei Sun, Dongsheng Li. (2024)<br><strong>ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling</strong><br><button class=copy-to-clipboard title="ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10635v1.pdf filename=2402.10635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling continuous-time dynamics on irregular time series is critical to account for data evolution and correlations that occur continuously. Traditional methods including <b>recurrent</b> <b>neural</b> <b>networks</b> or <b>Transformer</b> models leverage inductive bias via powerful neural architectures to capture complex patterns. However, due to their discrete characteristic, they have limitations in generalizing to continuous-time data paradigms. Though neural ordinary differential equations (Neural ODEs) and their variants have shown promising results in dealing with irregular time series, they often fail to capture the intricate correlations within these sequences. It is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. To tackle this problem, we propose ContiFormer that extends the relation modeling of vanilla <b>Transformer</b> to the continuous-time domain, which explicitly incorporates the modeling abilities of continuous dynamics of Neural ODEs with the attention mechanism of <b>Transformers.</b> We mathematically characterize the expressive power of ContiFormer and illustrate that, by curated designs of function hypothesis, many <b>Transformer</b> variants specialized in irregular time series modeling can be covered as a special case of ContiFormer. A wide range of experiments on both synthetic and real-world datasets have illustrated the superior modeling capacities and prediction performance of ContiFormer on irregular time series data. The project link is <a href=https://seqml.github.io/contiformer/>https://seqml.github.io/contiformer/</a>.</p></p class="citation"></blockquote><h3 id=2942--97204-understanding-self-distillation-and-partial-label-learning-in-multi-class-classification-with-label-noise-hyeonsu-jeong-et-al-2024>(29/42 | 97/204) Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise (Hyeonsu Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyeonsu Jeong, Hye Won Chung. (2024)<br><strong>Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise</strong><br><button class=copy-to-clipboard title="Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Self-Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10482v1.pdf filename=2402.10482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-distillation</b> (SD) is the process of training a student model using the outputs of a teacher model, with both models sharing the same architecture. Our study theoretically examines SD in multi-class classification with cross-entropy loss, exploring both multi-round SD and SD with refined teacher outputs, inspired by partial label learning (PLL). By deriving a closed-form solution for the student model&rsquo;s outputs, we discover that SD essentially functions as label averaging among instances with high feature correlations. Initially beneficial, this averaging helps the model focus on feature clusters correlated with a given instance for predicting the label. However, it leads to diminishing performance with increasing <b>distillation</b> rounds. Additionally, we demonstrate SD&rsquo;s effectiveness in label noise scenarios and identify the label corruption condition and minimum number of <b>distillation</b> rounds needed to achieve 100% classification accuracy. Our study also reveals that one-step <b>distillation</b> with refined teacher outputs surpasses the efficacy of multi-step SD using the teacher&rsquo;s direct output in high noise rate regimes.</p></p class="citation"></blockquote><h3 id=3042--98204-towards-cohesion-fairness-harmony-contrastive-regularization-in-individual-fair-graph-clustering-siamak-ghodsi-et-al-2024>(30/42 | 98/204) Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering (Siamak Ghodsi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siamak Ghodsi, Seyed Amjad Seyedi, Eirini Ntoutsi. (2024)<br><strong>Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering</strong><br><button class=copy-to-clipboard title="Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IT, cs-LG, cs-SI, cs.LG, math-IT<br>Keyword Score: 16<br>Keywords: Graph, Clustering, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10756v1.pdf filename=2402.10756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional fair <b>graph</b> <b>clustering</b> methods face two primary challenges: i) They prioritize balanced clusters at the expense of cluster cohesion by imposing rigid constraints, ii) Existing methods of both individual and group-level <b>fairness</b> in <b>graph</b> partitioning mostly rely on eigen decompositions and thus, generally lack interpretability. To address these issues, we propose iFairNMTF, an individual <b>Fairness</b> Nonnegative Matrix Tri-Factorization model with contrastive <b>fairness</b> regularization that achieves balanced and cohesive clusters. By introducing <b>fairness</b> regularization, our model allows for customizable accuracy-fairness trade-offs, thereby enhancing user autonomy without compromising the interpretability provided by nonnegative matrix tri-factorization. Experimental evaluations on real and synthetic datasets demonstrate the superior flexibility of iFairNMTF in achieving <b>fairness</b> and <b>clustering</b> performance.</p></p class="citation"></blockquote><h3 id=3142--99204-graph-based-forecasting-with-missing-data-through-spatiotemporal-downsampling-ivan-marisca-et-al-2024>(31/42 | 99/204) Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling (Ivan Marisca et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ivan Marisca, Cesare Alippi, Filippo Maria Bianchi. (2024)<br><strong>Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling</strong><br><button class=copy-to-clipboard title="Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Graph, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10634v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10634v1.pdf filename=2402.10634v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal <b>graph</b> <b>neural</b> <b>networks</b> achieve striking results by representing the relationships across time series as a <b>graph.</b> <b>Nonetheless,</b> <b>most</b> existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate the forecasts. Our approach outperforms state-of-the-art methods on synthetic and real-world <b>benchmarks</b> under different missing data distributions, particularly in the presence of contiguous blocks of missing values.</p></p class="citation"></blockquote><h3 id=3242--100204-policy-learning-for-off-dynamics-rl-with-deficient-support-linh-le-pham-van-et-al-2024>(32/42 | 100/204) Policy Learning for Off-Dynamics RL with Deficient Support (Linh Le Pham Van et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linh Le Pham Van, Hung The Tran, Sunil Gupta. (2024)<br><strong>Policy Learning for Off-Dynamics RL with Deficient Support</strong><br><button class=copy-to-clipboard title="Policy Learning for Off-Dynamics RL with Deficient Support" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10765v1.pdf filename=2402.10765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> (RL) can effectively learn complex policies. However, learning these policies often demands extensive trial-and-error interactions with the environment. In many real-world scenarios, this approach is not practical due to the high costs of data collection and safety concerns. As a result, a common strategy is to transfer a policy trained in a low-cost, rapid source simulator to a real-world target environment. However, this process poses challenges. Simulators, no matter how advanced, cannot perfectly replicate the intricacies of the real world, leading to dynamics discrepancies between the source and target environments. Past research posited that the source domain must encompass all possible target transitions, a condition we term full support. However, expecting full support is often unrealistic, especially in scenarios where significant dynamics discrepancies arise. In this paper, our emphasis shifts to addressing large dynamics mismatch adaptation. We move away from the stringent full support condition of earlier research, focusing instead on crafting an effective policy for the target domain. Our proposed approach is simple but effective. It is anchored in the central concepts of the skewing and extension of source support towards target support to mitigate support deficiencies. Through comprehensive testing on a varied set of <b>benchmarks,</b> our method&rsquo;s efficacy stands out, showcasing notable improvements over previous techniques.</p></p class="citation"></blockquote><h3 id=3342--101204-best-of-three-worlds-adaptive-experimentation-for-digital-marketing-in-practice-tanner-fiez-et-al-2024>(33/42 | 101/204) Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice (Tanner Fiez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanner Fiez, Houssam Nassif, Arick Chen, Sergio Gamez, Lalit Jain. (2024)<br><strong>Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice</strong><br><button class=copy-to-clipboard title="Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10870v1.pdf filename=2402.10870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adaptive experimental design (AED) methods are increasingly being used in industry as a tool to boost testing throughput or reduce experimentation cost relative to traditional A/B/N testing methods. However, the behavior and guarantees of such methods are not well-understood beyond idealized stationary settings. This paper shares lessons learned regarding the challenges of naively using AED systems in industrial settings where non-stationarity is prevalent, while also providing perspectives on the proper objectives and system specifications in such settings. We developed an AED framework for <b>counterfactual</b> inference based on these experiences, and tested it in a commercial environment.</p></p class="citation"></blockquote><h3 id=3442--102204-trading-off-consistency-and-dimensionality-of-convex-surrogates-for-the-mode-enrique-nueve-et-al-2024>(34/42 | 102/204) Trading off Consistency and Dimensionality of Convex Surrogates for the Mode (Enrique Nueve et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enrique Nueve, Bo Waggoner, Dhamma Kimpara, Jessie Finocchiaro. (2024)<br><strong>Trading off Consistency and Dimensionality of Convex Surrogates for the Mode</strong><br><button class=copy-to-clipboard title="Trading off Consistency and Dimensionality of Convex Surrogates for the Mode" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10818v1.pdf filename=2402.10818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In multiclass classification over $n$ outcomes, the outcomes must be embedded into the reals with dimension at least $n-1$ in order to design a consistent surrogate loss that leads to the &ldquo;correct&rdquo; classification, regardless of the data distribution. For large $n$, such as in <b>information</b> <b>retrieval</b> and structured prediction tasks, optimizing a surrogate in $n-1$ dimensions is often intractable. We investigate ways to trade off surrogate loss dimension, the number of problem instances, and restricting the region of consistency in the simplex for multiclass classification. Following past work, we examine an intuitive embedding procedure that maps outcomes into the vertices of convex polytopes in a low-dimensional surrogate space. We show that full-dimensional subsets of the simplex exist around each point mass distribution for which consistency holds, but also, with less than $n-1$ dimensions, there exist distributions for which a phenomenon called hallucination occurs, which is when the optimal report under the surrogate loss is an outcome with zero probability. Looking towards application, we derive a result to check if consistency holds under a given polytope embedding and low-noise assumption, providing insight into when to use a particular embedding. We provide examples of embedding $n = 2^{d}$ outcomes into the $d$-dimensional unit cube and $n = d!$ outcomes into the $d$-dimensional permutahedron under low-noise assumptions. Finally, we demonstrate that with multiple problem instances, we can learn the mode with $\frac{n}{2}$ dimensions over the whole simplex.</p></p class="citation"></blockquote><h3 id=3542--103204-associative-memories-in-the-feature-space-tommaso-salvatori-et-al-2024>(35/42 | 103/204) Associative Memories in the Feature Space (Tommaso Salvatori et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tommaso Salvatori, Beren Millidge, Yuhang Song, Rafal Bogacz, Thomas Lukasiewicz. (2024)<br><strong>Associative Memories in the Feature Space</strong><br><button class=copy-to-clipboard title="Associative Memories in the Feature Space" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10814v1.pdf filename=2402.10814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An autoassociative memory model is a function that, given a set of data points, takes as input an arbitrary vector and outputs the most similar data point from the memorized set. However, popular memory models fail to retrieve images even when the corruption is mild and easy to detect for a human evaluator. This is because similarities are evaluated in the raw pixel space, which does not contain any semantic information about the images. This problem can be easily solved by computing \emph{similarities} in an embedding space instead of the pixel space. We show that an effective way of computing such embeddings is via a network pretrained with a contrastive loss. As the dimension of embedding spaces is often significantly smaller than the pixel space, we also have a faster computation of similarity scores. We test this method on complex datasets such as CIFAR10 and STL10. An additional drawback of current models is the need of storing the whole dataset in the pixel space, which is often extremely large. We relax this condition and propose a class of memory models that only stores low-dimensional semantic embeddings, and uses them to retrieve similar, but not identical, memories. We demonstrate a proof of concept of this method on a simple task on the <b>MNIST</b> dataset.</p></p class="citation"></blockquote><h3 id=3642--104204-diversified-ensembling-an-experiment-in-crowdsourced-machine-learning-ira-globus-harris-et-al-2024>(36/42 | 104/204) Diversified Ensembling: An Experiment in Crowdsourced Machine Learning (Ira Globus-Harris et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ira Globus-Harris, Declan Harrison, Michael Kearns, Pietro Perona, Aaron Roth. (2024)<br><strong>Diversified Ensembling: An Experiment in Crowdsourced Machine Learning</strong><br><button class=copy-to-clipboard title="Diversified Ensembling: An Experiment in Crowdsourced Machine Learning" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-HC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10795v1.pdf filename=2402.10795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crowdsourced machine learning on competition platforms such as Kaggle is a popular and often effective method for generating accurate models. Typically, teams vie for the most accurate model, as measured by overall error on a holdout set, and it is common towards the end of such competitions for teams at the top of the leaderboard to ensemble or average their models outside the platform mechanism to get the final, best global model. In arXiv:2201.10408, the authors developed an alternative crowdsourcing framework in the context of fair machine learning, in order to integrate community feedback into models when subgroup unfairness is present and identifiable. There, unlike in classical crowdsourced ML, participants deliberately specialize their efforts by working on subproblems, such as demographic subgroups in the service of <b>fairness.</b> Here, we take a broader perspective on this work: we note that within this framework, participants may both specialize in the service of <b>fairness</b> and simply to cater to their particular expertise (e.g., focusing on identifying bird species in an image classification task). Unlike traditional crowdsourcing, this allows for the diversification of participants&rsquo; efforts and may provide a participation mechanism to a larger range of individuals (e.g. a machine learning novice who has insight into a specific <b>fairness</b> concern). We present the first medium-scale experimental evaluation of this framework, with 46 participating teams attempting to generate models to predict income from American Community Survey data. We provide an empirical analysis of teams&rsquo; approaches, and discuss the novel system architecture we developed. From here, we give concrete guidance for how best to deploy such a framework.</p></p class="citation"></blockquote><h3 id=3742--105204-error-feedback-reloaded-from-quadratic-to-arithmetic-mean-of-smoothness-constants-peter-richtárik-et-al-2024>(37/42 | 105/204) Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants (Peter Richtárik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Richtárik, Elnur Gasanov, Konstantin Burlachenko. (2024)<br><strong>Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants</strong><br><button class=copy-to-clipboard title="Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 90C26, 74Pxx, G-1-6; I-2-11; I-2-m, cs-AI, cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10774v1.pdf filename=2402.10774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Error Feedback (EF) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed GD or <b>SGD)</b> when these are enhanced with greedy communication compression techniques such as TopK. While EF was proposed almost a decade ago (Seide et al., 2014), and despite concentrated effort by the community to advance the theoretical understanding of this mechanism, there is still a lot to explore. In this work we study a modern form of error feedback called EF21 (Richtarik et al., 2021) which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice. In particular, while the theoretical communication complexity of EF21 depends on the quadratic mean of certain smoothness parameters, we improve this dependence to their arithmetic mean, which is always smaller, and can be substantially smaller, especially in heterogeneous data regimes. We take the reader on a journey of our discovery process. Starting with the idea of applying EF21 to an equivalent reformulation of the underlying problem which (unfortunately) requires (often impractical) machine cloning, we continue to the discovery of a new weighted version of EF21 which can (fortunately) be executed without any cloning, and finally circle back to an improved analysis of the original EF21 method. While this development applies to the simplest form of EF21, our approach naturally extends to more elaborate variants involving stochastic gradients and partial participation. Further, our technique improves the best-known theory of EF21 in the rare features regime (Richtarik et al., 2023). Finally, we validate our theoretical findings with suitable experiments.</p></p class="citation"></blockquote><h3 id=3842--106204-understanding-likelihood-of-normalizing-flow-and-image-complexity-through-the-lens-of-out-of-distribution-detection-genki-osada-et-al-2024>(38/42 | 106/204) Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection (Genki Osada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Genki Osada, Tsubasa Takahashi, Takashi Nishide. (2024)<br><strong>Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection</strong><br><button class=copy-to-clipboard title="Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10477v1.pdf filename=2402.10477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> (OOD) detection is crucial to safety-critical machine learning applications and has been extensively studied. While recent studies have predominantly focused on classifier-based methods, research on deep generative model (DGM)-based methods have lagged relatively. This disparity may be attributed to a perplexing phenomenon: DGMs often assign higher likelihoods to unknown OOD inputs than to their known training data. This paper focuses on explaining the underlying mechanism of this phenomenon. We propose a hypothesis that less complex images concentrate in high-density regions in the latent space, resulting in a higher likelihood assignment in the Normalizing Flow (NF). We experimentally demonstrate its validity for five NF architectures, concluding that their likelihood is untrustworthy. Additionally, we show that this problem can be alleviated by treating image complexity as an independent variable. Finally, we provide evidence of the potential applicability of our hypothesis in another DGM, PixelCNN++.</p></p class="citation"></blockquote><h3 id=3942--107204-one-bit-quantization-and-sparsification-for-multiclass-linear-classification-via-regularized-regression-reza-ghane-et-al-2024>(39/42 | 107/204) One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression (Reza Ghane et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reza Ghane, Danil Akhtiamov, Babak Hassibi. (2024)<br><strong>One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression</strong><br><button class=copy-to-clipboard title="One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10474v1.pdf filename=2402.10474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the use of linear regression for multiclass classification in the over-parametrized regime where some of the training data is mislabeled. In such scenarios it is necessary to add an explicit regularization term, $\lambda f(w)$, for some convex function $f(\cdot)$, to avoid overfitting the mislabeled data. In our analysis, we assume that the data is sampled from a Gaussian Mixture Model with equal class sizes, and that a proportion $c$ of the training labels is corrupted for each class. Under these assumptions, we prove that the best classification performance is achieved when $f(\cdot) = |\cdot|^2_2$ and $\lambda \to \infty$. We then proceed to analyze the classification errors for $f(\cdot) = |\cdot|<em>1$ and $f(\cdot) = |\cdot|</em>\infty$ in the large $\lambda$ regime and notice that it is often possible to find sparse and one-bit solutions, respectively, that perform almost as well as the one corresponding to $f(\cdot) = |\cdot|_2^2$.</p></p class="citation"></blockquote><h3 id=4042--108204-privacy-for-fairness-information-obfuscation-for-fair-representation-learning-with-local-differential-privacy-songjie-xie-et-al-2024>(40/42 | 108/204) Privacy for Fairness: Information Obfuscation for Fair Representation Learning with Local Differential Privacy (Songjie Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Songjie Xie, Youlong Wu, Jiaxuan Li, Ming Ding, Khaled B. Letaief. (2024)<br><strong>Privacy for Fairness: Information Obfuscation for Fair Representation Learning with Local Differential Privacy</strong><br><button class=copy-to-clipboard title="Privacy for Fairness: Information Obfuscation for Fair Representation Learning with Local Differential Privacy" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10473v1.pdf filename=2402.10473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As machine learning (ML) becomes more prevalent in human-centric applications, there is a growing emphasis on algorithmic <b>fairness</b> and privacy protection. While previous research has explored these areas as separate objectives, there is a growing recognition of the complex relationship between privacy and <b>fairness.</b> However, previous works have primarily focused on examining the interplay between privacy and <b>fairness</b> through empirical investigations, with limited attention given to theoretical exploration. This study aims to bridge this gap by introducing a theoretical framework that enables a comprehensive examination of their interrelation. We shall develop and analyze an information bottleneck (IB) based information obfuscation method with local differential privacy (LDP) for fair representation learning. In contrast to many empirical studies on <b>fairness</b> in ML, we show that the incorporation of LDP randomizers during the encoding process can enhance the <b>fairness</b> of the learned representation. Our analysis will demonstrate that the disclosure of sensitive information is constrained by the privacy budget of the LDP randomizer, thereby enabling the optimization process within the IB framework to effectively suppress sensitive information while preserving the desired utility through obfuscation. Based on the proposed method, we further develop a variational representation encoding approach that simultaneously achieves <b>fairness</b> and LDP. Our variational encoding approach offers practical advantages. It is trained using a non-adversarial method and does not require the introduction of any variational prior. Extensive experiments will be presented to validate our theoretical results and demonstrate the ability of our proposed approach to achieve both LDP and <b>fairness</b> while preserving adequate utility.</p></p class="citation"></blockquote><h3 id=4142--109204-fedkit-enabling-cross-platform-federated-learning-for-android-and-ios-sichang-he-et-al-2024>(41/42 | 109/204) FedKit: Enabling Cross-Platform Federated Learning for Android and iOS (Sichang He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sichang He, Beilong Tang, Boyan Zhang, Jiaoqi Shao, Xiaomin Ouyang, Daniel Nata Nugraha, Bing Luo. (2024)<br><strong>FedKit: Enabling Cross-Platform Federated Learning for Android and iOS</strong><br><button class=copy-to-clipboard title="FedKit: Enabling Cross-Platform Federated Learning for Android and iOS" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10464v1.pdf filename=2402.10464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present FedKit, a <b>federated</b> <b>learning</b> (FL) system tailored for cross-platform FL research on Android and iOS devices. FedKit pipelines cross-platform FL development by enabling model conversion, hardware-accelerated training, and cross-platform model aggregation. Our FL workflow supports flexible machine learning operations (MLOps) in production, facilitating continuous model delivery and training. We have deployed FedKit in a real-world use case for health data analysis on university campuses, demonstrating its effectiveness. FedKit is open-source at <a href=https://github.com/FedCampus/FedKit>https://github.com/FedCampus/FedKit</a>.</p></p class="citation"></blockquote><h3 id=4242--110204-random-projection-layers-for-multidimensional-time-sires-forecasting-chin-chia-michael-yeh-et-al-2024>(42/42 | 110/204) Random Projection Layers for Multidimensional Time Sires Forecasting (Chin-Chia Michael Yeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chin-Chia Michael Yeh, Yujie Fan, Xin Dai, Vivian Lai, Prince Osei Aboagye, Junpeng Wang, Huiyuan Chen, Yan Zheng, Zhongfang Zhuang, Liang Wang, Wei Zhang. (2024)<br><strong>Random Projection Layers for Multidimensional Time Sires Forecasting</strong><br><button class=copy-to-clipboard title="Random Projection Layers for Multidimensional Time Sires Forecasting" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10487v1.pdf filename=2402.10487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks&rsquo; outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting <b>benchmark</b> datasets demonstrate that our proposed method outperforms alternative methods, including both spatial-temporal <b>graph</b> models and general forecasting models.</p></p class="citation"></blockquote><h2 id=cscv-19>cs.CV (19)</h2><h3 id=119--111204-question-instructed-visual-descriptions-for-zero-shot-video-question-answering-david-romero-et-al-2024>(1/19 | 111/204) Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering (David Romero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Romero, Thamar Solorio. (2024)<br><strong>Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering</strong><br><button class=copy-to-clipboard title="Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 93<br>Keywords: Benchmarking, Zero-shot, GPT, Question Answering, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10698v1.pdf filename=2402.10698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Q-ViD, a simple approach for video <b>question</b> <b>answering</b> (video <b>QA),</b> that unlike prior methods, which are based on complex architectures, computationally expensive pipelines or use closed models like <b>GPTs,</b> Q-ViD relies on a single instruction-aware open <b>vision-language</b> model (InstructBLIP) to tackle videoQA using frame descriptions. Specifically, we create captioning instruction <b>prompts</b> that rely on the target <b>questions</b> <b>about</b> the videos and leverage InstructBLIP to obtain video frame captions that are useful to the task at hand. Subsequently, we form descriptions of the whole video using the <b>question-dependent</b> <b>frame</b> captions, and feed that information, along with a <b>question-answering</b> <b>prompt,</b> to a <b>large</b> <b>language</b> <b>model</b> <b>(LLM).</b> The <b>LLM</b> is our <b>reasoning</b> module, and performs the final step of multiple-choice <b>QA.</b> Our simple Q-ViD framework achieves competitive or even higher performances than current state of the art models on a diverse range of videoQA <b>benchmarks,</b> including NExT-QA, STAR, How2QA, TVQA and IntentQA.</p></p class="citation"></blockquote><h3 id=219--112204-palm2-vadapter-progressively-aligned-language-model-makes-a-strong-vision-language-adapter-junfei-xiao-et-al-2024>(2/19 | 112/204) PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter (Junfei Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junfei Xiao, Zheng Xu, Alan Yuille, Shen Yan, Boyu Wang. (2024)<br><strong>PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter</strong><br><button class=copy-to-clipboard title="PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 73<br>Keywords: Multi-modal, Question Answering, Reasoning, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10896v1.pdf filename=2402.10896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper demonstrates that a progressively aligned language model can effectively bridge frozen vision encoders and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> While the fundamental architecture and pre-training methods of vision encoders and <b>LLMs</b> have been extensively studied, the architecture and training strategy of <b>vision-language</b> adapters vary significantly across recent works. Our research undertakes a thorough exploration of the state-of-the-art perceiver resampler architecture and builds a strong baseline. However, we observe that the <b>vision-language</b> alignment with perceiver resampler exhibits slow convergence and limited scalability with a lack of direct supervision. To address this issue, we propose PaLM2-VAdapter, employing a progressively aligned language model as the <b>vision-language</b> adapter. Compared to the strong baseline with perceiver resampler, our method empirically shows faster convergence, higher performance, and stronger scalability. Extensive experiments across various <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> and captioning tasks on both images and videos demonstrate that our model exhibits state-of-the-art <b>visual</b> <b>understanding</b> <b>and</b> <b>multi-modal</b> <b>reasoning</b> capabilities. Notably, our method achieves these advancements with 30~70% fewer parameters than the state-of-the-art <b>large</b> <b>vision-language</b> <b>models,</b> marking a significant efficiency improvement.</p></p class="citation"></blockquote><h3 id=319--113204-universal-prompt-optimizer-for-safe-text-to-image-generation-zongyu-wu-et-al-2024>(3/19 | 113/204) Universal Prompt Optimizer for Safe Text-to-Image Generation (Zongyu Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zongyu Wu, Hongcheng Gao, Yueze Wang, Xiang Zhang, Suhang Wang. (2024)<br><strong>Universal Prompt Optimizer for Safe Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Universal Prompt Optimizer for Safe Text-to-Image Generation" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Fine-tuning, GPT, GPT-3, GPT-3.5, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10882v1.pdf filename=2402.10882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-Image</b> (T2I) models have shown great performance in generating images based on textual <b>prompts.</b> However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model <b>fine-tuning</b> and embedding blocking are impractical in real-world applications. Hence, \textit{we propose the first universal <b>prompt</b> optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean <b>prompt</b> pairs by <b>GPT-3.5</b> Turbo. To guide the optimizer to have the ability of converting toxic <b>prompt</b> to clean <b>prompt</b> while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating inappropriate images, with no significant impact on text alignment. It is also flexible to be combined with methods to achieve better performance.</p></p class="citation"></blockquote><h3 id=419--114204-biofusionnet-deep-learning-based-survival-risk-stratification-in-er-breast-cancer-through-multifeature-and-multimodal-data-fusion-raktim-kumar-mondol-et-al-2024>(4/19 | 114/204) BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion (Raktim Kumar Mondol et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raktim Kumar Mondol, Ewan K. A. Millar, Arcot Sowmya, Erik Meijering. (2024)<br><strong>BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion</strong><br><button class=copy-to-clipboard title="BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Autoencoder, Multi-modal, Multi-modal, Self-supervised Learning, Variational Autoencoder, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10717v1.pdf filename=2402.10717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients. We employ multiple <b>self-supervised</b> feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features. We then utilise a <b>variational</b> <b>autoencoder</b> (VAE) to fuse these features, and harness the latent space of the VAE to feed into a <b>self-attention</b> network, generating patient-level features. Next, we develop a co-dual-cross-attention mechanism to combine the histopathological features with genetic data, enabling the model to capture the interplay between them. Additionally, clinical data is incorporated using a feed-forward network (FFN), further enhancing predictive performance and achieving comprehensive <b>multimodal</b> feature integration. Furthermore, we introduce a weighted Cox loss function, specifically designed to handle imbalanced survival data, which is a common challenge in the field. The proposed model achieves a mean concordance index (C-index) of 0.77 and a time-dependent area under the curve (AUC) of 0.84, outperforming state-of-the-art methods. It predicts risk (high versus low) with prognostic significance for overall survival (OS) in univariate analysis (HR=2.99, 95% CI: 1.88&ndash;4.78, p&lt;0.005), and maintains independent significance in multivariate analysis incorporating standard clinicopathological variables (HR=2.91, 95% CI: 1.80&ndash;4.68, p&lt;0.005). The proposed method not only improves model performance but also addresses a critical gap in handling imbalanced data.</p></p class="citation"></blockquote><h3 id=519--115204-using-left-and-right-brains-together-towards-vision-and-language-planning-jun-cen-et-al-2024>(5/19 | 115/204) Using Left and Right Brains Together: Towards Vision and Language Planning (Jun Cen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Cen, Chenfei Wu, Xiao Liu, Shengming Yin, Yixuan Pei, Jinglong Yang, Qifeng Chen, Nan Duan, Jianguo Zhang. (2024)<br><strong>Using Left and Right Brains Together: Towards Vision and Language Planning</strong><br><button class=copy-to-clipboard title="Using Left and Right Brains Together: Towards Vision and Language Planning" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Large Language Model, Large Language Model, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10534v1.pdf filename=2402.10534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>Large</b> <b>Multi-modality</b> <b>Models</b> (LMMs) have demonstrated remarkable decision masking capabilities on a variety of tasks. However, they inherently operate planning within the language space, lacking the vision and spatial imagination ability. In contrast, humans utilize both left and right hemispheres of the brain for language and visual planning during the thinking process. Therefore, we introduce a novel <b>vision-language</b> planning framework in this work to perform concurrent visual and language planning for tasks with inputs of any form. Our framework incorporates visual planning to capture intricate environmental details, while language planning enhances the logical coherence of the overall system. We evaluate the effectiveness of our framework across <b>vision-language</b> tasks, vision-only tasks, and language-only tasks. The results demonstrate the superior performance of our approach, indicating that the integration of visual and language planning yields better contextually aware task execution.</p></p class="citation"></blockquote><h3 id=619--116204-control-color-multimodal-diffusion-based-interactive-image-colorization-zhexin-liang-et-al-2024>(6/19 | 116/204) Control Color: Multimodal Diffusion-based Interactive Image Colorization (Zhexin Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhexin Liang, Zhaochen Li, Shangchen Zhou, Chongyi Li, Chen Change Loy. (2024)<br><strong>Control Color: Multimodal Diffusion-based Interactive Image Colorization</strong><br><button class=copy-to-clipboard title="Control Color: Multimodal Diffusion-based Interactive Image Colorization" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Autoencoder, Multi-modal, Multi-modal, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10855v1.pdf filename=2402.10855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the existence of numerous colorization methods, several limitations still exist, such as lack of user interaction, inflexibility in local colorization, unnatural color rendering, insufficient color variation, and color overflow. To solve these issues, we introduce Control Color (CtrlColor), a <b>multi-modal</b> colorization method that leverages the pre-trained Stable Diffusion (SD) model, offering promising capabilities in highly controllable interactive image colorization. While several diffusion-based methods have been proposed, supporting colorization in multiple modalities remains non-trivial. In this study, we aim to tackle both unconditional and conditional image colorization (text <b>prompts,</b> strokes, exemplars) and address color overflow and incorrect color within a unified framework. Specifically, we present an effective way to encode user strokes to enable precise local color manipulation and employ a practical way to constrain the color distribution similar to exemplars. Apart from accepting text <b>prompts</b> as conditions, these designs add versatility to our approach. We also introduce a novel module based on <b>self-attention</b> and a content-guided deformable <b>autoencoder</b> to address the long-standing issues of color overflow and inaccurate coloring. Extensive comparisons show that our model outperforms state-of-the-art image colorization methods both qualitatively and quantitatively.</p></p class="citation"></blockquote><h3 id=719--117204-vatr-choose-your-words-wisely-for-handwritten-text-generation-bram-vanherle-et-al-2024>(7/19 | 117/204) VATr++: Choose Your Words Wisely for Handwritten Text Generation (Bram Vanherle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bram Vanherle, Vittorio Pippi, Silvia Cascianelli, Nick Michiels, Frank Van Reeth, Rita Cucchiara. (2024)<br><strong>VATr++: Choose Your Words Wisely for Handwritten Text Generation</strong><br><button class=copy-to-clipboard title="VATr++: Choose Your Words Wisely for Handwritten Text Generation" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Generative Adversarial Network, Transformer, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10798v1.pdf filename=2402.10798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Styled Handwritten <b>Text</b> <b>Generation</b> (HTG) has received significant attention in recent years, propelled by the success of learning-based solutions employing <b>GANs,</b> <b>Transformers,</b> and, preliminarily, Diffusion Models. Despite this surge in interest, there remains a critical yet understudied aspect - the impact of the input, both visual and textual, on the HTG model training and its subsequent influence on performance. This study delves deeper into a cutting-edge Styled-HTG approach, proposing strategies for input preparation and training regularization that allow the model to achieve better performance and generalize better. These aspects are validated through extensive analysis on several different settings and datasets. Moreover, in this work, we go beyond performance optimization and address a significant hurdle in HTG research - the lack of a standardized evaluation protocol. In particular, we propose a standardization of the evaluation protocol for HTG and conduct a comprehensive <b>benchmarking</b> of existing approaches. By doing so, we aim to establish a foundation for fair and meaningful comparisons between HTG strategies, fostering progress in the field.</p></p class="citation"></blockquote><h3 id=819--118204-fusion-of-diffusion-weighted-mri-and-clinical-data-for-predicting-functional-outcome-after-acute-ischemic-stroke-with-deep-contrastive-learning-chia-ling-tsai-et-al-2024>(8/19 | 118/204) Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning (Chia-Ling Tsai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chia-Ling Tsai, Hui-Yun Su, Shen-Feng Sung, Wei-Yang Lin, Ying-Ying Su, Tzu-Hsien Yang, Man-Lin Mai. (2024)<br><strong>Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning</strong><br><button class=copy-to-clipboard title="Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10894v1.pdf filename=2402.10894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stroke is a common disabling neurological condition that affects about one-quarter of the adult population over age 25; more than half of patients still have poor outcomes, such as permanent functional dependence or even death, after the onset of acute stroke. The aim of this study is to investigate the efficacy of diffusion-weighted MRI modalities combining with structured health profile on predicting the functional outcome to facilitate early intervention. A deep fusion learning network is proposed with two-stage training: the first stage focuses on cross-modality representation learning and the second stage on classification. <b>Supervised</b> <b>contrastive</b> <b>learning</b> is exploited to learn discriminative features that separate the two classes of patients from embeddings of individual modalities and from the fused <b>multimodal</b> embedding. The network takes as the input DWI and ADC images, and structured health profile data. The outcome is the prediction of the patient needing long-term care at 3 months after the onset of stroke. Trained and evaluated with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80 and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing models that consolidate both imaging and structured data in the medical domain. If trained with comprehensive clinical variables, including NIHSS and comorbidities, the gain from images on making accurate prediction is not considered substantial, but significant. However, diffusion-weighted MRI can replace NIHSS to achieve comparable level of accuracy combining with other readily available clinical variables for better generalization.</p></p class="citation"></blockquote><h3 id=919--119204-dynamic-patch-aware-enrichment-transformer-for-occluded-person-re-identification-xin-zhang-et-al-2024>(9/19 | 119/204) Dynamic Patch-aware Enrichment Transformer for Occluded Person Re-Identification (Xin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zhang, Keren Fu, Qijun Zhao. (2024)<br><strong>Dynamic Patch-aware Enrichment Transformer for Occluded Person Re-Identification</strong><br><button class=copy-to-clipboard title="Dynamic Patch-aware Enrichment Transformer for Occluded Person Re-Identification" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Contrastive Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10435v1.pdf filename=2402.10435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Person re-identification (re-ID) continues to pose a significant challenge, particularly in scenarios involving occlusions. Prior approaches aimed at tackling occlusions have predominantly focused on aligning physical body features through the utilization of external semantic cues. However, these methods tend to be intricate and susceptible to noise. To address the aforementioned challenges, we present an innovative end-to-end solution known as the Dynamic Patch-aware Enrichment <b>Transformer</b> (DPEFormer). This model effectively distinguishes human body information from occlusions automatically and dynamically, eliminating the need for external detectors or precise image alignment. Specifically, we introduce a dynamic patch token selection module (DPSM). DPSM utilizes a label-guided proxy token as an intermediary to identify informative occlusion-free tokens. These tokens are then selected for deriving subsequent local part features. To facilitate the seamless integration of global classification features with the finely detailed local features selected by DPSM, we introduce a novel feature blending module (FBM). FBM enhances feature representation through the complementary nature of information and the exploitation of part diversity. Furthermore, to ensure that DPSM and the entire DPEFormer can effectively learn with only identity labels, we also propose a Realistic Occlusion Augmentation (ROA) strategy. This strategy leverages the recent advances in the Segment Anything Model (SAM). As a result, it generates occlusion images that closely resemble real-world occlusions, greatly enhancing the subsequent <b>contrastive</b> <b>learning</b> process. Experiments on occluded and holistic re-ID <b>benchmarks</b> signify a substantial advancement of DPEFormer over existing state-of-the-art approaches. The code will be made publicly available.</p></p class="citation"></blockquote><h3 id=1019--120204-codamal-contrastive-domain-adaptation-for-malaria-detection-in-low-cost-microscopes-ishan-rajendrakumar-dave-et-al-2024>(10/19 | 120/204) CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes (Ishan Rajendrakumar Dave et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ishan Rajendrakumar Dave, Tristan de Blegiers, Chen Chen, Mubarak Shah. (2024)<br><strong>CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes</strong><br><button class=copy-to-clipboard title="CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10478v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10478v1.pdf filename=2402.10478v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Malaria is a major health issue worldwide, and its diagnosis requires scalable solutions that can work effectively with low-cost microscopes (LCM). Deep learning-based methods have shown success in computer-aided diagnosis from microscopic images. However, these methods need annotated images that show cells affected by malaria parasites and their life stages. Annotating images from LCM significantly increases the burden on medical experts compared to annotating images from high-cost microscopes (HCM). For this reason, a practical solution would be trained on HCM images which should generalize well on LCM images during testing. While earlier methods adopted a multi-stage learning process, they did not offer an end-to-end approach. In this work, we present an end-to-end learning framework, named CodaMal (Contrastive <b>Domain</b> <b>Adpation</b> for Malaria). In order to bridge the gap between HCM (training) and LCM (testing), we propose a <b>domain</b> <b>adaptive</b> contrastive loss. It reduces the <b>domain</b> <b>shift</b> by promoting similarity between the representations of HCM and its corresponding LCM image, without imposing an additional annotation burden. In addition, the training objective includes <b>object</b> <b>detection</b> objectives with carefully designed augmentations, ensuring the accurate detection of malaria parasites. On the publicly available large-scale M5-dataset, our proposed method shows a significant improvement of 16% over the state-of-the-art methods in terms of the mean average precision metric (mAP), provides 21x speed up during inference, and requires only half learnable parameters than the prior methods. Our code is publicly available.</p></p class="citation"></blockquote><h3 id=1119--121204-stf-spatio-temporal-fusion-module-for-improving-video-object-detection-noreen-anwar-et-al-2024>(11/19 | 121/204) STF: Spatio-Temporal Fusion Module for Improving Video Object Detection (Noreen Anwar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Noreen Anwar, Guillaume-Alexandre Bilodeau, Wassim Bouachir. (2024)<br><strong>STF: Spatio-Temporal Fusion Module for Improving Video Object Detection</strong><br><button class=copy-to-clipboard title="STF: Spatio-Temporal Fusion Module for Improving Video Object Detection" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10752v1.pdf filename=2402.10752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Consecutive frames in a video contain redundancy, but they may also contain relevant complementary information for the detection task. The objective of our work is to leverage this complementary information to improve detection. Therefore, we propose a spatio-temporal fusion framework (STF). We first introduce multi-frame and single-frame attention modules that allow a neural network to share feature maps between nearby frames to obtain more robust <b>object</b> <b>representations.</b> Second, we introduce a dual-frame fusion module that merges feature maps in a learnable manner to improve them. Our evaluation is conducted on three different <b>benchmarks</b> including video sequences of moving road users. The performed experiments demonstrate that the proposed spatio-temporal fusion module leads to improved detection performance compared to baseline <b>object</b> <b>detectors.</b> Code is available at <a href=https://github.com/noreenanwar/STF-module>https://github.com/noreenanwar/STF-module</a></p></p class="citation"></blockquote><h3 id=1219--122204-efficient-multi-task-uncertainties-for-joint-semantic-segmentation-and-monocular-depth-estimation-steven-landgraf-et-al-2024>(12/19 | 122/204) Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation (Steven Landgraf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Steven Landgraf, Markus Hillemann, Theodor Kapler, Markus Ulrich. (2024)<br><strong>Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation</strong><br><button class=copy-to-clipboard title="Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Knowledge Distillation, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10580v1.pdf filename=2402.10580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantifying the predictive uncertainty emerged as a possible solution to common challenges like overconfidence or lack of explainability and robustness of deep neural networks, albeit one that is often computationally expensive. Many real-world applications are <b>multi-modal</b> in nature and hence benefit from multi-task learning. In autonomous driving, for example, the joint solution of semantic segmentation and monocular depth estimation has proven to be valuable. In this work, we first combine different uncertainty quantification methods with joint semantic segmentation and monocular depth estimation and evaluate how they perform in comparison to each other. Additionally, we reveal the benefits of multi-task learning with regard to the uncertainty quality compared to solving both tasks separately. Based on these insights, we introduce EMUFormer, a novel student-teacher <b>distillation</b> approach for joint semantic segmentation and monocular depth estimation as well as efficient multi-task uncertainty quantification. By implicitly leveraging the predictive uncertainties of the teacher, EMUFormer achieves new state-of-the-art results on Cityscapes and NYUv2 and additionally estimates high-quality predictive uncertainties for both tasks that are comparable or superior to a Deep Ensemble despite being an order of magnitude more efficient.</p></p class="citation"></blockquote><h3 id=1319--123204-enhancement-driven-pretraining-for-robust-fingerprint-representation-learning-ekta-gavas-et-al-2024>(13/19 | 123/204) Enhancement-Driven Pretraining for Robust Fingerprint Representation Learning (Ekta Gavas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ekta Gavas, Kaustubh Olpadkar, Anoop Namboodiri. (2024)<br><strong>Enhancement-Driven Pretraining for Robust Fingerprint Representation Learning</strong><br><button class=copy-to-clipboard title="Enhancement-Driven Pretraining for Robust Fingerprint Representation Learning" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10847v1.pdf filename=2402.10847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fingerprint recognition stands as a pivotal component of biometric technology, with diverse applications from identity verification to advanced search tools. In this paper, we propose a unique method for deriving robust fingerprint representations by leveraging enhancement-based pre-training. Building on the achievements of U-Net-based fingerprint enhancement, our method employs a specialized encoder to derive representations from fingerprint images in a <b>self-supervised</b> manner. We further refine these representations, aiming to enhance the verification capabilities. Our experimental results, tested on publicly available fingerprint datasets, reveal a marked improvement in verification performance against established <b>self-supervised</b> training techniques. Our findings not only highlight the effectiveness of our method but also pave the way for potential advancements. Crucially, our research indicates that it is feasible to extract meaningful fingerprint representations from degraded images without relying on enhanced samples.</p></p class="citation"></blockquote><h3 id=1419--124204-training-class-imbalanced-diffusion-model-via-overlap-optimization-divin-yan-et-al-2024>(14/19 | 124/204) Training Class-Imbalanced Diffusion Model Via Overlap Optimization (Divin Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Divin Yan, Lu Qi, Vincent Tao Hu, Ming-Hsuan Yang, Meng Tang. (2024)<br><strong>Training Class-Imbalanced Diffusion Model Via Overlap Optimization</strong><br><button class=copy-to-clipboard title="Training Class-Imbalanced Diffusion Model Via Overlap Optimization" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10821v1.pdf filename=2402.10821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion models have made significant advances recently in high-quality image synthesis and related tasks. However, diffusion models trained on real-world datasets, which often follow long-tailed distributions, yield inferior fidelity for tail classes. Deep generative models, including diffusion models, are biased towards classes with abundant training images. To address the observed appearance overlap between synthesized images of rare classes and tail classes, we propose a method based on <b>contrastive</b> <b>learning</b> to minimize the overlap between distributions of synthetic images for different classes. We show variants of our probabilistic <b>contrastive</b> <b>learning</b> method can be applied to any class conditional diffusion model. We show significant improvement in image synthesis using our loss for multiple datasets with long-tailed distribution. Extensive experimental results demonstrate that the proposed method can effectively handle imbalanced data for diffusion-based generation and classification models. Our code and datasets will be publicly available at <a href=https://github.com/yanliang3612/DiffROP>https://github.com/yanliang3612/DiffROP</a>.</p></p class="citation"></blockquote><h3 id=1519--125204-pointmamba-a-simple-state-space-model-for-point-cloud-analysis-dingkang-liang-et-al-2024>(15/19 | 125/204) PointMamba: A Simple State Space Model for Point Cloud Analysis (Dingkang Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingkang Liang, Xin Zhou, Xinyu Wang, Xingkui Zhu, Wei Xu, Zhikang Zou, Xiaoqing Ye, Xiang Bai. (2024)<br><strong>PointMamba: A Simple State Space Model for Point Cloud Analysis</strong><br><button class=copy-to-clipboard title="PointMamba: A Simple State Space Model for Point Cloud Analysis" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10739v1.pdf filename=2402.10739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity and is difficult to extend to long sequence modeling due to limited computational resources and so on. Recently, state space models (SSM), a new family of deep sequence models, have presented great potential for sequence modeling in NLP tasks. In this paper, taking inspiration from the success of SSM in NLP, we propose PointMamba, a framework with global modeling and linear complexity. Specifically, by taking embedded point patches as input, we proposed a reordering strategy to enhance SSM&rsquo;s global modeling ability by providing a more logical geometric scanning order. The reordered point tokens are then sent to a series of Mamba blocks to causally capture the point cloud structure. Experimental results show our proposed PointMamba outperforms the <b>transformer-based</b> counterparts on different point cloud analysis datasets, while significantly saving about 44.3% parameters and 25% FLOPs, demonstrating the potential option for constructing foundational 3D vision models. We hope our PointMamba can provide a new perspective for point cloud analysis. The code is available at <a href=https://github.com/LMD0311/PointMamba>https://github.com/LMD0311/PointMamba</a>.</p></p class="citation"></blockquote><h3 id=1619--126204-pegasus-personalized-generative-3d-avatars-with-composable-attributes-hyunsoo-cha-et-al-2024>(16/19 | 126/204) PEGASUS: Personalized Generative 3D Avatars with Composable Attributes (Hyunsoo Cha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyunsoo Cha, Byungjun Kim, Hanbyul Joo. (2024)<br><strong>PEGASUS: Personalized Generative 3D Avatars with Composable Attributes</strong><br><button class=copy-to-clipboard title="PEGASUS: Personalized Generative 3D Avatars with Composable Attributes" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10636v1.pdf filename=2402.10636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present, PEGASUS, a method for constructing personalized generative 3D face avatars from monocular video sources. As a compositional generative model, our model enables disentangled controls to selectively alter the facial attributes (e.g., hair or nose) of the target individual, while preserving the identity. We present two key approaches to achieve this goal. First, we present a method to construct a person-specific generative 3D avatar by building a synthetic video collection of the target identity with varying facial attributes, where the videos are synthesized by borrowing parts from diverse individuals from other monocular videos. Through several experiments, we demonstrate the superior performance of our approach by generating unseen attributes with high realism. Subsequently, we introduce a <b>zero-shot</b> approach to achieve the same generative modeling more efficiently by leveraging a previously constructed personalized generative model.</p></p class="citation"></blockquote><h3 id=1719--127204-compact-and-de-biased-negative-instance-embedding-for-multi-instance-learning-on-whole-slide-image-classification-joohyung-lee-et-al-2024>(17/19 | 127/204) Compact and De-biased Negative Instance Embedding for Multi-Instance Learning on Whole-Slide Image Classification (Joohyung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joohyung Lee, Heejeong Nam, Kwanhyung Lee, Sangchul Hahn. (2024)<br><strong>Compact and De-biased Negative Instance Embedding for Multi-Instance Learning on Whole-Slide Image Classification</strong><br><button class=copy-to-clipboard title="Compact and De-biased Negative Instance Embedding for Multi-Instance Learning on Whole-Slide Image Classification" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Multiple Instance Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10595v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10595v1.pdf filename=2402.10595v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Whole-slide image (WSI) classification is a challenging task because 1) patches from WSI lack annotation, and 2) WSI possesses unnecessary variability, e.g., stain protocol. Recently, <b>Multiple-Instance</b> <b>Learning</b> <b>(MIL)</b> has made significant progress, allowing for classification based on slide-level, rather than patch-level, annotations. However, existing MIL methods ignore that all patches from normal slides are normal. Using this free annotation, we introduce a semi-supervision signal to de-bias the inter-slide variability and to capture the common factors of variation within normal patches. Because our method is orthogonal to the MIL algorithm, we evaluate our method on top of the recently proposed MIL algorithms and also compare the performance with other semi-supervised approaches. We evaluate our method on two public WSI datasets including Camelyon-16 and TCGA lung cancer and demonstrate that our approach significantly improves the predictive performance of existing MIL algorithms and outperforms other semi-supervised algorithms. We release our code at <a href=https://github.com/AITRICS/pathology_mil>https://github.com/AITRICS/pathology_mil</a>.</p></p class="citation"></blockquote><h3 id=1819--128204-make-a-cheap-scaling-a-self-cascade-diffusion-model-for-higher-resolution-adaptation-lanqing-guo-et-al-2024>(18/19 | 128/204) Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation (Lanqing Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xintao Wang, Qifeng Chen, Ying Shan, Bihan Wen. (2024)<br><strong>Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation</strong><br><button class=copy-to-clipboard title="Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10491v1.pdf filename=2402.10491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion models have proven to be highly effective in image and video generation; however, they still face composition challenges when generating images of varying sizes due to single-scale training data. Adapting large pre-trained diffusion models for higher resolution demands substantial computational and optimization resources, yet achieving a generation capability comparable to low-resolution models remains elusive. This paper proposes a novel self-cascade diffusion model that leverages the rich knowledge gained from a well-trained low-resolution model for rapid adaptation to higher-resolution image and video generation, employing either tuning-free or cheap upsampler tuning paradigms. Integrating a sequence of multi-scale upsampler modules, the self-cascade diffusion model can efficiently adapt to a higher resolution, preserving the original composition and generation capabilities. We further propose a pivot-guided noise re-schedule strategy to speed up the inference process and improve local structural details. Compared to full <b>fine-tuning,</b> our approach achieves a 5X training speed-up and requires only an additional 0.002M tuning parameters. Extensive experiments demonstrate that our approach can quickly adapt to higher resolution image and video synthesis by <b>fine-tuning</b> for just 10k steps, with virtually no additional inference time.</p></p class="citation"></blockquote><h3 id=1919--129204-optimizing-skin-lesion-classification-via-multimodal-data-and-auxiliary-task-integration-mahapara-khurshid-et-al-2024>(19/19 | 129/204) Optimizing Skin Lesion Classification via Multimodal Data and Auxiliary Task Integration (Mahapara Khurshid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahapara Khurshid, Mayank Vatsa, Richa Singh. (2024)<br><strong>Optimizing Skin Lesion Classification via Multimodal Data and Auxiliary Task Integration</strong><br><button class=copy-to-clipboard title="Optimizing Skin Lesion Classification via Multimodal Data and Auxiliary Task Integration" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10454v1.pdf filename=2402.10454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rising global prevalence of skin conditions, some of which can escalate to life-threatening stages if not timely diagnosed and treated, presents a significant healthcare challenge. This issue is particularly acute in remote areas where limited access to healthcare often results in delayed treatment, allowing skin diseases to advance to more critical stages. One of the primary challenges in diagnosing skin diseases is their low inter-class variations, as many exhibit similar visual characteristics, making accurate classification challenging. This research introduces a novel <b>multimodal</b> method for classifying skin lesions, integrating smartphone-captured images with essential clinical and demographic information. This approach mimics the diagnostic process employed by medical professionals. A distinctive aspect of this method is the integration of an auxiliary task focused on super-resolution image prediction. This component plays a crucial role in refining visual details and enhancing feature extraction, leading to improved differentiation between classes and, consequently, elevating the overall effectiveness of the model. The experimental evaluations have been conducted using the PAD-UFES20 dataset, applying various deep-learning architectures. The results of these experiments not only demonstrate the effectiveness of the proposed method but also its potential applicability under-resourced healthcare environments.</p></p class="citation"></blockquote><h2 id=eessiv-6>eess.IV (6)</h2><h3 id=16--130204-weak-mamba-unet-visual-mamba-makes-cnn-and-vit-work-better-for-scribble-based-medical-image-segmentation-ziyang-wang-et-al-2024>(1/6 | 130/204) Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation (Ziyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyang Wang, Chao Ma. (2024)<br><strong>Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 90<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Weakly-supervised Learning, Weakly-supervised Learning, Transformer, Vision Transformer, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10887v1.pdf filename=2402.10887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs. This paper introduces Weak-Mamba-UNet, an innovative <b>weakly-supervised</b> <b>learning</b> <b>(WSL)</b> framework that leverages the capabilities of <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN),</b> <b>Vision</b> <b>Transformer</b> (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations. The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a <b>CNN-based</b> UNet for detailed local feature extraction, a Swin <b>Transformer-based</b> SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling. The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet. This highlights its potential in scenarios with sparse or imprecise annotations. The source code is made publicly accessible.</p></p class="citation"></blockquote><h3 id=26--131204-u2mrpd-unsupervised-undersampled-mri-reconstruction-by-prompting-a-large-latent-diffusion-model-ziqi-gao-et-al-2024>(2/6 | 131/204) U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model (Ziqi Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqi Gao, S. Kevin Zhou. (2024)<br><strong>U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model</strong><br><button class=copy-to-clipboard title="U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Out-of-domain, Supervised Learning, Unsupervised Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10609v1.pdf filename=2402.10609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit visual knowledge in a large latent diffusion model (LLDM) pre-trained on natural images is rich and hypothetically universal to natural and medical images. To test this hypothesis, we introduce a novel framework for <b>Unsupervised</b> Undersampled MRI Reconstruction by <b>Prompting</b> a pre-trained large latent Diffusion model ( U$^2$MRPD). Existing data-driven, <b>supervised</b> undersampled MRI reconstruction networks are typically of limited generalizability and adaptability toward diverse data acquisition scenarios; yet U$^2$MRPD supports image-specific MRI reconstruction by <b>prompting</b> an LLDM with an MRSampler tailored for complex-valued MRI images. With any single-source or diverse-source MRI dataset, U$^2$MRPD&rsquo;s performance is further boosted by an MRAdapter while keeping the generative image priors intact. Experiments on multiple datasets show that U$^2$MRPD achieves comparable or better performance than <b>supervised</b> and MRI diffusion methods on in-domain datasets while demonstrating the best generalizability on <b>out-of-domain</b> datasets. To the best of our knowledge, U$^2$MRPD is the {\bf first} <b>unsupervised</b> method that demonstrates the universal prowess of a LLDM, %trained on magnitude-only natural images in medical imaging, attaining the best adaptability for both MRI database-free and database-available scenarios and generalizability towards <b>out-of-domain</b> data.</p></p class="citation"></blockquote><h3 id=36--132204-gan-driven-electromagnetic-imaging-of-2-d-dielectric-scatterers-ehtasham-naseer-et-al-2024>(3/6 | 132/204) GAN-driven Electromagnetic Imaging of 2-D Dielectric Scatterers (Ehtasham Naseer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehtasham Naseer, Ali Imran Sandhu, Muhammad Adnan Siddique, Waqas W. Ahmed, Mohamed Farhat, Ying Wu. (2024)<br><strong>GAN-driven Electromagnetic Imaging of 2-D Dielectric Scatterers</strong><br><button class=copy-to-clipboard title="GAN-driven Electromagnetic Imaging of 2-D Dielectric Scatterers" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CE, cs-LG, eess-IV, eess-SP, eess.IV<br>Keyword Score: 30<br>Keywords: Autoencoder, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10831v1.pdf filename=2402.10831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inverse scattering problems are inherently challenging, given the fact they are ill-posed and nonlinear. This paper presents a powerful deep learning-based approach that relies on <b>generative</b> <b>adversarial</b> <b>networks</b> to accurately and efficiently reconstruct randomly-shaped two-dimensional dielectric objects from amplitudes of multi-frequency scattered electric fields. An adversarial <b>autoencoder</b> (AAE) is trained to learn to generate the scatterer&rsquo;s geometry from a lower-dimensional latent representation constrained to adhere to the Gaussian distribution. A cohesive inverse neural network (INN) framework is set up comprising a sequence of appropriately designed dense layers, the already-trained generator as well as a separately trained forward neural network. The images reconstructed at the output of the inverse network are validated through comparison with outputs from the forward neural network, addressing the non-uniqueness challenge inherent to electromagnetic (EM) imaging problems. The trained INN demonstrates an enhanced robustness, evidenced by a mean binary cross-entropy (BCE) loss of $0.13$ and a structure similarity index (SSI) of $0.90$. The study not only demonstrates a significant reduction in computational load, but also marks a substantial improvement over traditional objective-function-based methods. It contributes both to the fields of machine learning and EM imaging by offering a real-time quantitative imaging approach. The results obtained with the simulated data, for both training and testing, yield promising results and may open new avenues for radio-frequency inverse imaging.</p></p class="citation"></blockquote><h3 id=46--133204-histosegcap-capsules-for-weakly-supervised-semantic-segmentation-of-histological-tissue-type-in-whole-slide-images-mobina-mansoori-et-al-2024>(4/6 | 133/204) HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images (Mobina Mansoori et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mobina Mansoori, Sajjad Shahabodini, Jamshid Abouei, Arash Mohammadi, Konstantinos N. Plataniotis. (2024)<br><strong>HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images</strong><br><button class=copy-to-clipboard title="HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10851v1.pdf filename=2402.10851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digital pathology involves converting physical tissue slides into high-resolution Whole Slide Images (WSIs), which pathologists analyze for disease-affected tissues. However, large histology slides with numerous microscopic fields pose challenges for visual search. To aid pathologists, Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently examining WSIs and identifying diagnostically relevant regions. This paper presents a novel histopathological image analysis method employing Weakly <b>Supervised</b> Semantic Segmentation (WSSS) based on Capsule Networks, the first such application. The proposed model is evaluated using the Atlas of Digital Pathology (ADP) dataset and its performance is compared with other histopathological semantic segmentation methodologies. The findings underscore the potential of Capsule Networks in enhancing the precision and efficiency of histopathological image analysis. Experimental results show that the proposed model outperforms traditional methods in terms of accuracy and the mean Intersection-over-Union (mIoU) metric.</p></p class="citation"></blockquote><h3 id=56--134204-semi-weakly-supervised-neural-network-training-for-medical-image-registration-yiwen-li-et-al-2024>(5/6 | 134/204) Semi-weakly-supervised neural network training for medical image registration (Yiwen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwen Li, Yunguan Fu, Iani J. M. B. Gayo, Qianye Yang, Zhe Min, Shaheer U. Saeed, Wen Yan, Yipei Wang, J. Alison Noble, Mark Emberton, Matthew J. Clarkson, Dean C. Barratt, Victor A. Prisacariu, Yipeng Hu. (2024)<br><strong>Semi-weakly-supervised neural network training for medical image registration</strong><br><button class=copy-to-clipboard title="Semi-weakly-supervised neural network training for medical image registration" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10728v1.pdf filename=2402.10728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For training registration networks, <b>weak</b> <b>supervision</b> from segmented corresponding regions-of-interest (ROIs) have been proven effective for (a) supplementing <b>unsupervised</b> methods, and (b) being used independently in registration tasks in which <b>unsupervised</b> losses are unavailable or ineffective. This correspondence-informing supervision entails cost in annotation that requires significant specialised effort. This paper describes a semi-weakly-supervised registration pipeline that improves the model performance, when only a small corresponding-ROI-labelled dataset is available, by exploiting unlabelled image pairs. We examine two types of augmentation methods by perturbation on network weights and image resampling, such that consistency-based <b>unsupervised</b> losses can be applied on unlabelled data. The novel WarpDDF and RegCut approaches are proposed to allow commutative perturbation between an image pair and the predicted spatial transformation (i.e. respective input and output of registration networks), distinct from existing perturbation methods for classification or segmentation. Experiments using 589 male pelvic MR images, labelled with eight anatomical ROIs, show the improvement in registration performance and the ablated contributions from the individual strategies. Furthermore, this study attempts to construct one of the first computational atlases for pelvic structures, enabled by registering inter-subject MRs, and quantifies the significant differences due to the proposed semi-weak supervision with a discussion on the potential clinical use of example atlas-derived statistics.</p></p class="citation"></blockquote><h3 id=66--135204-dabs-ls-deep-atlas-based-segmentation-using-regional-level-set-self-supervision-hannah-g-mason-et-al-2024>(6/6 | 135/204) DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set Self-Supervision (Hannah G. Mason et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hannah G. Mason, Jack H. Noble. (2024)<br><strong>DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set Self-Supervision</strong><br><button class=copy-to-clipboard title="DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set Self-Supervision" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10425v1.pdf filename=2402.10425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cochlear implants (CIs) are neural prosthetics used to treat patients with severe-to-profound hearing loss. Patient-specific modeling of CI stimulation of the auditory nerve fiber (ANFs) can help audiologists improve the CI programming. These models require localization of the ANFs relative to surrounding anatomy and the CI. Localization is challenging because the ANFs are so small they are not directly visible in clinical imaging. In this work, we hypothesize the position of the ANFs can be accurately inferred from the location of the internal auditory canal (IAC), which has high contrast in CT, since the ANFs pass through this canal between the cochlea and the brain. Inspired by VoxelMorph, in this paper we propose a deep atlas-based IAC segmentation network. We create a single atlas in which the IAC and ANFs are pre-localized. Our network is trained to produce deformation fields (DFs) mapping coordinates from the atlas to new target volumes and that accurately segment the IAC. We hypothesize that DFs that accurately segment the IAC in target images will also facilitate accurate atlas-based localization of the ANFs. As opposed to VoxelMorph, which aims to produce DFs that accurately register the entire volume, our novel contribution is an entirely <b>self-supervised</b> training scheme that aims to produce DFs that accurately segment the target structure. This self-supervision is facilitated using a regional level set (LS) inspired loss function. We call our method Deep Atlas Based Segmentation using Level Sets (DABS-LS). Results show that DABS-LS outperforms VoxelMorph for IAC segmentation. Tests with publicly available datasets for trachea and kidney segmentation also show significant improvement in segmentation accuracy, demonstrating the generalizability of the method.</p></p class="citation"></blockquote><h2 id=csro-4>cs.RO (4)</h2><h3 id=14--136204-rag-driver-generalisable-driving-explanations-with-retrieval-augmented-in-context-learning-in-multi-modal-large-language-model-jianhao-yuan-et-al-2024>(1/4 | 136/204) RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model (Jianhao Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, Matthew Gadd. (2024)<br><strong>RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model</strong><br><button class=copy-to-clipboard title="RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 73<br>Keywords: Multi-modal, Retrieval-Augmented Generation, Zero-shot, Grounding, Natural Language Explanation, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10828v1.pdf filename=2402.10828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robots powered by &lsquo;blackbox&rsquo; models need to provide human-understandable explanations which we can trust. Hence, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in <b>Multi-Modal</b> <b>Large</b> <b>Language</b> <b>models</b> (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with <b>natural</b> <b>language</b> <b>explanations.</b> However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment. To address these challenges, we present <b>RAG-Driver,</b> a novel retrieval-augmented <b>multi-modal</b> <b>large</b> <b>language</b> <b>model</b> that leverages <b>in-context</b> <b>learning</b> for high-performance, explainable, and generalisable autonomous driving. By <b>grounding</b> in retrieved expert demonstration, we empirically validate that <b>RAG-Driver</b> achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional <b>zero-shot</b> generalisation capabilities to unseen environments without further training endeavours.</p></p class="citation"></blockquote><h3 id=24--137204-autogptp-affordance-based-task-planning-with-large-language-models-timo-birr-et-al-2024>(2/4 | 137/204) AutoGPT+P: Affordance-based Task Planning with Large Language Models (Timo Birr et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timo Birr, Christoph Pohl, Abdelrahman Younes, Tamim Asfour. (2024)<br><strong>AutoGPT+P: Affordance-based Task Planning with Large Language Models</strong><br><button class=copy-to-clipboard title="AutoGPT+P: Affordance-based Task Planning with Large Language Models" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-2, cs-AI, cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Object Detection, ChatGPT, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10778v1.pdf filename=2402.10778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in task planning leverage <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in <b>reasoning</b> capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances encompass the action possibilities of an agent on the environment and <b>objects</b> <b>present</b> in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary <b>objects.</b> <b>AutoGPT+P</b> leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with incomplete information, e. g., tasks with missing <b>objects</b> <b>by</b> exploring the scene, suggesting alternatives, or providing a partial plan. The affordance-based scene representation combines <b>object</b> <b>detection</b> with an automatically generated <b>object-affordance-mapping</b> <b>using</b> <b>ChatGPT.</b> The core planning tool extends existing work by automatically correcting semantic and syntactic errors. Our approach achieves a success rate of 98%, surpassing the current 81% success rate of the current state-of-the-art <b>LLM-based</b> planning method SayCan on the SayCan instruction set. Furthermore, we evaluated our approach on our newly created dataset with 150 scenarios covering a wide range of complex tasks with missing <b>objects,</b> <b>achieving</b> a success rate of 79% on our dataset. The dataset and the code are publicly available at <a href=https://git.h2t.iar.kit.edu/birr/autogpt-p-standalone>https://git.h2t.iar.kit.edu/birr/autogpt-p-standalone</a>.</p></p class="citation"></blockquote><h3 id=34--138204-pedipulate-enabling-manipulation-skills-using-a-quadruped-robots-leg-philip-arm-et-al-2024>(3/4 | 138/204) Pedipulate: Enabling Manipulation Skills using a Quadruped Robot&rsquo;s Leg (Philip Arm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Arm, Mayank Mittal, Hendrik Kolvenbach, Marco Hutter. (2024)<br><strong>Pedipulate: Enabling Manipulation Skills using a Quadruped Robot&rsquo;s Leg</strong><br><button class=copy-to-clipboard title="Pedipulate: Enabling Manipulation Skills using a Quadruped Robot's Leg" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 15<br>Keywords: Knowledge Graph, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10837v1.pdf filename=2402.10837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios. In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots. In this work, we explore pedipulation - using the legs of a legged robot for manipulation. By training a <b>reinforcement</b> <b>learning</b> policy that tracks position targets for one foot, we enable a dedicated pedipulation controller that is robust to disturbances, has a large workspace through whole-body behaviors, and can reach far-away targets with gait emergence, enabling loco-pedipulation. By deploying our controller on a quadrupedal robot using teleoperation, we demonstrate various real-world tasks such as door opening, sample collection, and pushing obstacles. We demonstrate load carrying of more than 2.0 <b>kg</b> at the foot. Additionally, the controller is robust to interaction forces at the foot, disturbances at the base, and slippery contact surfaces. Videos of the experiments are available at <a href=https://sites.google.com/leggedrobotics.com/pedipulate>https://sites.google.com/leggedrobotics.com/pedipulate</a>.</p></p class="citation"></blockquote><h3 id=44--139204-3d-diffuser-actor-policy-diffusion-with-3d-scene-representations-tsung-wei-ke-et-al-2024>(4/4 | 139/204) 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations (Tsung-Wei Ke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tsung-Wei Ke, Nikolaos Gkanatsios, Katerina Fragkiadaki. (2024)<br><strong>3D Diffuser Actor: Policy Diffusion with 3D Scene Representations</strong><br><button class=copy-to-clipboard title="3D Diffuser Actor: Policy Diffusion with 3D Scene Representations" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Benchmarking, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10885v1.pdf filename=2402.10885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot&rsquo;s end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts the 3D translation and rotation error for each of them, by featurizing them using 3D relative attention to other 3D visual and language tokens. 3D Diffuser Actor sets a new state-of-the-art on RLBench with an absolute performance gain of 16.3% over the current SOTA on a multi-view setup and an absolute gain of 13.1% on a single-view setup. On the CALVIN <b>benchmark,</b> it outperforms the current SOTA in the setting of <b>zero-shot</b> unseen scene generalization by being able to successfully run 0.2 more tasks, a 7% relative increase. It also works in the real world from a handful of demonstrations. We ablate our model&rsquo;s architectural design choices, such as 3D scene featurization and 3D relative attentions, and show they all help generalization. Our results suggest that 3D scene representations and powerful generative modeling are keys to efficient robot learning from demonstrations.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--140204-apcodec-a-neural-audio-codec-with-parallel-amplitude-and-phase-spectrum-encoding-and-decoding-yang-ai-et-al-2024>(1/2 | 140/204) APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding (Yang Ai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Ai, Xiao-Hang Jiang, Ye-Xin Lu, Hui-Peng Du, Zhen-Hua Ling. (2024)<br><strong>APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding</strong><br><button class=copy-to-clipboard title="APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 70<br>Keywords: Convolution, Generative Adversarial Network, Generative Adversarial Network, Knowledge Distillation, Knowledge Distillation, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10533v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10533v1.pdf filename=2402.10533v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel neural audio codec targeting high waveform sampling rates and low bitrates named APCodec, which seamlessly integrates the strengths of parametric codecs and waveform codecs. The APCodec revolutionizes the process of audio encoding and decoding by concurrently handling the amplitude and phase spectra as audio parametric characteristics like parametric codecs. It is composed of an encoder and a decoder with the modified ConvNeXt v2 network as the backbone, connected by a quantizer based on the residual vector <b>quantization</b> (RVQ) mechanism. The encoder compresses the audio amplitude and phase spectra in parallel, amalgamating them into a continuous latent code at a reduced temporal resolution. This code is subsequently <b>quantized</b> by the quantizer. Ultimately, the decoder reconstructs the audio amplitude and phase spectra in parallel, and the decoded waveform is obtained by inverse short-time Fourier transform. To ensure the fidelity of decoded audio like waveform codecs, spectral-level loss, <b>quantization</b> loss, and <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> based loss are collectively employed for training the APCodec. To support low-latency streamable inference, we employ feed-forward layers and causal <b>convolutional</b> layers in APCodec, incorporating a <b>knowledge</b> <b>distillation</b> training strategy to enhance the quality of decoded audio. Experimental results confirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6 kbps, with no significant degradation in the quality of the decoded audio. At the same bitrate, our proposed APCodec also demonstrates superior decoded audio quality and faster generation speed compared to well-known codecs, such as SoundStream, Encodec, HiFi-Codec and AudioDec.</p></p class="citation"></blockquote><h3 id=22--141204-learning-disentangled-audio-representations-through-controlled-synthesis-yusuf-brima-et-al-2024>(2/2 | 141/204) Learning Disentangled Audio Representations through Controlled Synthesis (Yusuf Brima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusuf Brima, Ulf Krumnack, Simone Pika, Gunther Heidemann. (2024)<br><strong>Learning Disentangled Audio Representations through Controlled Synthesis</strong><br><button class=copy-to-clipboard title="Learning Disentangled Audio Representations through Controlled Synthesis" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10547v1.pdf filename=2402.10547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper tackles the scarcity of <b>benchmarking</b> data in disentangled auditory representation learning. We introduce SynTone, a synthetic dataset with explicit ground truth explanatory factors for evaluating disentanglement techniques. <b>Benchmarking</b> state-of-the-art methods on SynTone highlights its utility for method evaluation. Our results underscore strengths and limitations in audio disentanglement, motivating future research.</p></p class="citation"></blockquote><h2 id=q-fincp-2>q-fin.CP (2)</h2><h3 id=12--142204-emoji-driven-crypto-assets-market-reactions-xiaorui-zuo-et-al-2024>(1/2 | 142/204) Emoji Driven Crypto Assets Market Reactions (Xiaorui Zuo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaorui Zuo, Yao-Tsung Chen, Wolfgang Karl Härdle. (2024)<br><strong>Emoji Driven Crypto Assets Market Reactions</strong><br><button class=copy-to-clipboard title="Emoji Driven Crypto Assets Market Reactions" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.CP<br>Categories: cs-AI, cs-CL, cs-LG, q-fin-CP, q-fin-ST, q-fin.CP<br>Keyword Score: 66<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, BERT, GPT, GPT-4, Transformer, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10481v1.pdf filename=2402.10481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor <b>sentiments.</b> <b>In</b> our study, we leverage <b>GPT-4</b> and a <b>fine-tuned</b> <b>transformer-based</b> <b>BERT</b> model for a <b>multimodal</b> <b>sentiment</b> <b>analysis,</b> focusing on the impact of emoji <b>sentiment</b> <b>on</b> cryptocurrency markets. By translating emojis into quantifiable <b>sentiment</b> <b>data,</b> we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji <b>sentiment</b> <b>can</b> facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuanced perspective on the interplay between digital communication and market dynamics in an academic context.</p></p class="citation"></blockquote><h3 id=22--143204-modelling-crypto-markets-by-multi-agent-reinforcement-learning-johann-lussange-et-al-2024>(2/2 | 143/204) Modelling crypto markets by multi-agent reinforcement learning (Johann Lussange et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johann Lussange, Stefano Vrizzi, Stefano Palminteri, Boris Gutkin. (2024)<br><strong>Modelling crypto markets by multi-agent reinforcement learning</strong><br><button class=copy-to-clipboard title="Modelling crypto markets by multi-agent reinforcement learning" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.CP<br>Categories: cs-AI, cs-GT, cs-MA, q-fin-CP, q-fin.CP<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10803v1.pdf filename=2402.10803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Building on a previous foundation work (Lussange et al. 2020), this study introduces a multi-agent <b>reinforcement</b> <b>learning</b> (MARL) model simulating crypto markets, which is calibrated to the Binance&rsquo;s daily closing prices of $153$ cryptocurrencies that were continuously traded between 2018 and 2022. Unlike previous agent-based models (ABM) or multi-agent systems (MAS) which relied on zero-intelligence agents or single autonomous agent methodologies, our approach relies on endowing agents with <b>reinforcement</b> <b>learning</b> (RL) techniques in order to model crypto markets. This integration is designed to emulate, with a bottom-up approach to complexity inference, both individual and collective agents, ensuring robustness in the recent volatile conditions of such markets and during the COVID-19 era. A key feature of our model also lies in the fact that its autonomous agents perform asset price valuation based on two sources of information: the market prices themselves, and the approximation of the crypto assets fundamental values beyond what those market prices are. Our MAS calibration against real market data allows for an accurate emulation of crypto markets microstructure and probing key market behaviors, in both the bearish and bullish regimes of that particular time period.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--144204-when-dataflow-analysis-meets-large-language-models-chengpeng-wang-et-al-2024>(1/1 | 144/204) When Dataflow Analysis Meets Large Language Models (Chengpeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengpeng Wang, Wuqi Zhang, Zian Su, Xiangzhe Xu, Xiaoheng Xie, Xiangyu Zhang. (2024)<br><strong>When Dataflow Analysis Meets Large Language Models</strong><br><button class=copy-to-clipboard title="When Dataflow Analysis Meets Large Language Models" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: 68N30, 68T01, D-3-0; D-2-4; I-2-5; I-2-6, cs-LG, cs-PL, cs-SE, cs.PL<br>Keyword Score: 63<br>Keywords: Benchmarking, Few-shot, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10754v1.pdf filename=2402.10754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dataflow analysis is a powerful code analysis technique that reasons dependencies between program values, offering support for code optimization, program comprehension, and bug detection. Existing approaches require the successful compilation of the subject program and customizations for downstream applications. This paper introduces LLMDFA, an <b>LLM-powered</b> dataflow analysis framework that analyzes arbitrary code snippets without requiring a compilation infrastructure and automatically synthesizes downstream applications. Inspired by summary-based dataflow analysis, LLMDFA decomposes the problem into three sub-problems, which are effectively resolved by several essential strategies, including <b>few-shot</b> <b>chain-of-thought</b> <b>prompting</b> and tool synthesis. Our evaluation has shown that the design can mitigate the hallucination and improve the <b>reasoning</b> ability, obtaining high precision and recall in detecting dataflow-related bugs upon <b>benchmark</b> programs, outperforming state-of-the-art (classic) tools, including a very recent industrial analyzer.</p></p class="citation"></blockquote><h2 id=csir-5>cs.IR (5)</h2><h3 id=15--145204-spar-personalized-content-based-recommendation-via-long-engagement-attention-chiyu-zhang-et-al-2024>(1/5 | 145/204) SPAR: Personalized Content-Based Recommendation via Long Engagement Attention (Chiyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chiyu Zhang, Yifei Sun, Jun Chen, Jie Lei, Muhammad Abdul-Mageed, Sinong Wang, Rong Jin, Sem Park, Ning Yao, Bo Long. (2024)<br><strong>SPAR: Personalized Content-Based Recommendation via Long Engagement Attention</strong><br><button class=copy-to-clipboard title="SPAR: Personalized Content-Based Recommendation via Long Engagement Attention" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 53<br>Keywords: Benchmarking, Recommendation, Large Language Model, Large Language Model, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10555v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10555v1.pdf filename=2402.10555v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leveraging users&rsquo; long engagement histories is essential for personalized content <b>recommendations.</b> The success of <b>pretrained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> in NLP has led to their use in encoding user histories and candidate items, framing content <b>recommendations</b> as textual semantic matching tasks. However, existing works still struggle with processing very long user historical text and insufficient user-item interaction. In this paper, we introduce a content-based <b>recommendation</b> framework, SPAR, which effectively tackles the challenges of holistic user interest extraction from the long user engagement history. It achieves so by leveraging <b>PLM,</b> poly-attention layers and attention sparsity mechanisms to encode user&rsquo;s history in a session-based manner. The user and item side features are sufficiently fused for engagement prediction while maintaining standalone representations for both sides, which is efficient for practical model deployment. Moreover, we enhance user profiling by exploiting <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to extract global interests from user engagement history. Extensive experiments on two <b>benchmark</b> datasets demonstrate that our framework outperforms existing state-of-the-art (SoTA) methods.</p></p class="citation"></blockquote><h3 id=25--146204-umair-fps-user-aware-multi-modal-animation-illustration-recommendation-fusion-with-painting-style-yan-kang-et-al-2024>(2/5 | 146/204) UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style (Yan Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Kang, Hao Lin, Mingjian Yang, Shin-Jye Lee. (2024)<br><strong>UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style</strong><br><button class=copy-to-clipboard title="UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 33<br>Keywords: Fine-tuning, Multi-modal, Recommendation, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10381v1.pdf filename=2402.10381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of high-quality image generation models based on AI has generated a deluge of anime illustrations. Recommending illustrations to users within massive data has become a challenging and popular task. However, existing anime <b>recommendation</b> systems have focused on <b>text</b> <b>features</b> but still need to integrate image features. In addition, most <b>multi-modal</b> <b>recommendation</b> research is constrained by tightly coupled datasets, limiting its applicability to anime illustrations. We propose the User-aware <b>Multi-modal</b> Animation Illustration <b>Recommendation</b> Fusion with Painting Style (UMAIR-FPS) to tackle these gaps. In the feature extract phase, for image features, we are the first to combine image painting style features with semantic features to construct a dual-output image encoder for enhancing representation. For <b>text</b> <b>features,</b> we obtain <b>text</b> <b>embeddings</b> based on <b>fine-tuning</b> Sentence-Transformers by incorporating domain knowledge that composes a variety of domain <b>text</b> <b>pairs</b> from multilingual mappings, entity relationships, and term explanation perspectives, respectively. In the <b>multi-modal</b> fusion phase, we novelly propose a user-aware <b>multi-modal</b> contribution measurement mechanism to weight <b>multi-modal</b> features dynamically according to user features at the interaction level and employ the DCN-V2 module to model bounded-degree <b>multi-modal</b> crosses effectively. UMAIR-FPS surpasses the stat-of-the-art baselines on large real-world datasets, demonstrating substantial performance enhancements.</p></p class="citation"></blockquote><h3 id=35--147204-fairsync-ensuring-amortized-group-exposure-in-distributed-recommendation-retrieval-chen-xu-et-al-2024>(3/5 | 147/204) FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval (Chen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Xu, Jun Xu, Yiming Ding, Xiao Zhang, Qi Qi. (2024)<br><strong>FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval</strong><br><button class=copy-to-clipboard title="FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Fairness, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10628v1.pdf filename=2402.10628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In pursuit of <b>fairness</b> and balanced development, <b>recommender</b> <b>systems</b> (RS) often prioritize group <b>fairness,</b> ensuring that specific groups maintain a minimum level of exposure over a given period. For example, RS platforms aim to ensure adequate exposure for new providers or specific categories of items according to their needs. Modern industry RS usually adopts a two-stage pipeline: stage-1 (retrieval stage) retrieves hundreds of candidates from millions of items distributed across various servers, and stage-2 (ranking stage) focuses on presenting a small-size but accurate selection from items chosen in stage-1. Existing efforts for ensuring amortized group exposures focus on stage-2, however, stage-1 is also critical for the task. Without a high-quality set of candidates, the stage-2 ranker cannot ensure the required exposure of groups. Previous <b>fairness-aware</b> works designed for stage-2 typically require accessing and traversing all items. In stage-1, however, millions of items are distributively stored in servers, making it infeasible to traverse all of them. How to ensure group exposures in the distributed retrieval process is a challenging question. To address this issue, we introduce a model named FairSync, which transforms the problem into a constrained distributed optimization problem. Specifically, FairSync resolves the issue by moving it to the dual space, where a central node aggregates historical <b>fairness</b> data into a vector and distributes it to all servers. To trade off the efficiency and accuracy, the gradient descent technique is used to periodically update the parameter of the dual vector. The experiment results on two public <b>recommender</b> <b>retrieval</b> datasets showcased that FairSync outperformed all the baselines, achieving the desired minimum level of exposures while maintaining a high level of retrieval accuracy.</p></p class="citation"></blockquote><h3 id=45--148204-cognitive-personalized-search-integrating-large-language-models-with-an-efficient-memory-mechanism-yujia-zhou-et-al-2024>(4/5 | 148/204) Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism (Yujia Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujia Zhou, Qiannan Zhu, Jiajie Jin, Zhicheng Dou. (2024)<br><strong>Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism</strong><br><button class=copy-to-clipboard title="Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10548v1.pdf filename=2402.10548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional search engines usually provide identical search results for all users, overlooking individual preferences. To counter this limitation, personalized search has been developed to re-rank results based on user preferences derived from query logs. Deep learning-based personalized search methods have shown promise, but they rely heavily on abundant training data, making them susceptible to data sparsity challenges. This paper proposes a Cognitive Personalized Search (CoPS) model, which integrates <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with a cognitive memory mechanism inspired by human cognition. CoPS employs <b>LLMs</b> to enhance user modeling and user search experience. The cognitive memory mechanism comprises sensory memory for quick sensory responses, working memory for sophisticated cognitive responses, and long-term memory for storing historical interactions. CoPS handles new queries using a three-step approach: identifying re-finding behaviors, constructing user profiles with relevant historical information, and ranking documents based on personalized query intent. Experiments show that CoPS outperforms baseline models in <b>zero-shot</b> scenarios.</p></p class="citation"></blockquote><h3 id=55--149204-are-id-embeddings-necessary-whitening-pre-trained-text-embeddings-for-effective-sequential-recommendation-lingzi-zhang-et-al-2024>(5/5 | 149/204) Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for Effective Sequential Recommendation (Lingzi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingzi Zhang, Xin Zhou, Zhiwei Zeng, Zhiqi Shen. (2024)<br><strong>Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for Effective Sequential Recommendation</strong><br><button class=copy-to-clipboard title="Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for Effective Sequential Recommendation" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Benchmarking, Recommendation, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10602v1.pdf filename=2402.10602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent sequential <b>recommendation</b> models have combined pre-trained <b>text</b> <b>embeddings</b> of items with item ID embeddings to achieve superior <b>recommendation</b> performance. Despite their effectiveness, the expressive power of <b>text</b> <b>features</b> in these models remains largely unexplored. While most existing models emphasize the importance of ID embeddings in <b>recommendations,</b> our study takes a step further by studying sequential <b>recommendation</b> models that only rely on <b>text</b> <b>features</b> and do not necessitate ID embeddings. Upon examining pretrained <b>text</b> <b>embeddings</b> experimentally, we discover that they reside in an anisotropic semantic space, with an average cosine similarity of over 0.8 between items. We also demonstrate that this anisotropic nature hinders <b>recommendation</b> models from effectively differentiating between item representations and leads to degenerated performance. To address this issue, we propose to employ a pre-processing step known as whitening transformation, which transforms the anisotropic <b>text</b> <b>feature</b> distribution into an isotropic Gaussian distribution. Our experiments show that whitening pre-trained <b>text</b> <b>embeddings</b> in the sequential model can significantly improve <b>recommendation</b> performance. However, the full whitening operation might break the potential manifold of items with similar <b>text</b> <b>semantics.</b> To preserve the original semantics while benefiting from the isotropy of the whitened <b>text</b> <b>features,</b> we introduce WhitenRec+, an ensemble approach that leverages both fully whitened and relaxed whitened item representations for effective <b>recommendations.</b> We further discuss and analyze the benefits of our design through experiments and proofs. Experimental results on three public <b>benchmark</b> datasets demonstrate that WhitenRec+ outperforms state-of-the-art methods for sequential <b>recommendation.</b></p></p class="citation"></blockquote><h2 id=q-biobm-3>q-bio.BM (3)</h2><h3 id=13--150204-fusing-neural-and-physical-augment-protein-conformation-sampling-with-tractable-simulations-jiarui-lu-et-al-2024>(1/3 | 150/204) Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations (Jiarui Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiarui Lu, Zuobai Zhang, Bozitao Zhong, Chence Shi, Jian Tang. (2024)<br><strong>Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations</strong><br><button class=copy-to-clipboard title="Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio-QM, q-bio.BM<br>Keyword Score: 50<br>Keywords: Few-shot, Fine-tuning, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10433v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10433v1.pdf filename=2402.10433v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The protein dynamics are common and important for their biological functions and properties, the study of which usually involves time-consuming molecular dynamics (MD) <b>simulations</b> in silico. Recently, generative models has been leveraged as a surrogate sampler to obtain conformation ensembles with orders of magnitude faster and without requiring any <b>simulation</b> data (a <b>&ldquo;zero-shot&rdquo;</b> inference). However, being agnostic of the underlying energy landscape, the accuracy of such generative model may still be limited. In this work, we explore the <b>few-shot</b> setting of such pre-trained generative sampler which incorporates MD <b>simulations</b> in a tractable manner. Specifically, given a target protein of interest, we first acquire some seeding conformations from the pre-trained sampler followed by a number of physical <b>simulations</b> in parallel starting from these seeding samples. Then we <b>fine-tuned</b> the generative model using the <b>simulation</b> trajectories above to become a target-specific sampler. Experimental results demonstrated the superior performance of such <b>few-shot</b> conformation sampler at a tractable computational cost.</p></p class="citation"></blockquote><h3 id=23--151204-generative-ai-for-controllable-protein-sequence-design-a-survey-yiheng-zhu-et-al-2024>(2/3 | 151/204) Generative AI for Controllable Protein Sequence Design: A Survey (Yiheng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiheng Zhu, Zitai Kong, Jialu Wu, Weize Liu, Yuqiang Han, Mingze Yin, Hongxia Xu, Chang-Yu Hsieh, Tingjun Hou. (2024)<br><strong>Generative AI for Controllable Protein Sequence Design: A Survey</strong><br><button class=copy-to-clipboard title="Generative AI for Controllable Protein Sequence Design: A Survey" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-AI, cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10516v1.pdf filename=2402.10516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The design of novel protein sequences with targeted functionalities underpins a central theme in protein engineering, impacting diverse fields such as drug discovery and enzymatic engineering. However, navigating this vast combinatorial search space remains a severe challenge due to time and financial constraints. This scenario is rapidly evolving as the transformative advancements in AI, particularly in the realm of <b>generative</b> <b>models</b> and optimization algorithms, have been propelling the protein design field towards an unprecedented revolution. In this survey, we systematically review recent advances in <b>generative</b> <b>AI</b> for controllable protein sequence design. To set the stage, we first outline the foundational tasks in protein sequence design in terms of the constraints involved and present key <b>generative</b> <b>models</b> and optimization algorithms. We then offer in-depth reviews of each design task and discuss the pertinent applications. Finally, we identify the unresolved challenges and highlight research opportunities that merit deeper exploration.</p></p class="citation"></blockquote><h3 id=33--152204-mfbind-a-multi-fidelity-approach-for-evaluating-drug-compounds-in-practical-generative-modeling-peter-eckmann-et-al-2024>(3/3 | 152/204) MFBind: a Multi-Fidelity Approach for Evaluating Drug Compounds in Practical Generative Modeling (Peter Eckmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Eckmann, Dongxia Wu, Germano Heinzelmann, Michael K Gilson, Rose Yu. (2024)<br><strong>MFBind: a Multi-Fidelity Approach for Evaluating Drug Compounds in Practical Generative Modeling</strong><br><button class=copy-to-clipboard title="MFBind: a Multi-Fidelity Approach for Evaluating Drug Compounds in Practical Generative Modeling" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10387v1.pdf filename=2402.10387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current generative models for drug discovery primarily use molecular docking to evaluate the quality of generated compounds. However, such models are often not useful in practice because even compounds with high docking scores do not consistently show experimental activity. More accurate methods for activity prediction exist, such as molecular dynamics based binding free energy calculations, but they are too computationally expensive to use in a generative model. We propose a multi-fidelity approach, Multi-Fidelity Bind (MFBind), to achieve the optimal trade-off between accuracy and computational cost. MFBind integrates docking and binding free energy simulators to train a multi-fidelity deep surrogate model with <b>active</b> <b>learning.</b> Our deep surrogate model utilizes a pretraining technique and linear prediction heads to efficiently fit small amounts of high-fidelity data. We perform extensive experiments and show that MFBind (1) outperforms other state-of-the-art single and multi-fidelity baselines in surrogate modeling, and (2) boosts the performance of generative models with markedly higher quality compounds.</p></p class="citation"></blockquote><h2 id=csse-2>cs.SE (2)</h2><h3 id=12--153204-prompt-learning-for-multi-label-code-smell-detection-a-promising-approach-haiyang-liu-et-al-2024>(1/2 | 153/204) Prompt Learning for Multi-Label Code Smell Detection: A Promising Approach (Haiyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiyang Liu, Yang Zhang, Vidya Saikrishna, Quanquan Tian, Kun Zheng. (2024)<br><strong>Prompt Learning for Multi-Label Code Smell Detection: A Promising Approach</strong><br><button class=copy-to-clipboard title="Prompt Learning for Multi-Label Code Smell Detection: A Promising Approach" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Large Language Model, Large Language Model, Pre-trained Language Model, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10398v1.pdf filename=2402.10398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Code smells indicate the potential problems of software quality so that developers can identify refactoring opportunities by detecting code smells. State-of-the-art approaches leverage heuristics, machine learning, and deep learning to detect code smells. However, existing approaches have not fully explored the potential of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> In this paper, we propose \textit{PromptSmell}, a novel approach based on <b>prompt</b> <b>learning</b> for detecting multi-label code smell. Firstly, code snippets are acquired by traversing abstract syntax trees. Combined code snippets with natural language <b>prompts</b> <b>and</b> mask tokens, \textit{PromptSmell} constructs the input of <b>LLMs.</b> Secondly, to detect multi-label code smell, we leverage a label combination approach by converting a multi-label problem into a multi-classification problem. A customized answer space is added to the word list of <b>pre-trained</b> <b>language</b> <b>models,</b> and the probability distribution of intermediate answers is obtained by predicting the words at the mask positions. Finally, the intermediate answers are mapped to the target class labels by a verbalizer as the final classification result. We evaluate the effectiveness of \textit{PromptSmell} by answering six research questions. The experimental results demonstrate that \textit{PromptSmell} obtains an improvement of 11.17% in $precision_{w}$ and 7.4% in $F1_{w}$ compared to existing approaches.</p></p class="citation"></blockquote><h3 id=22--154204-language-driven-engineering-an-interdisciplinary-software-development-paradigm-bernhard-steffen-et-al-2024>(2/2 | 154/204) Language-Driven Engineering An Interdisciplinary Software Development Paradigm (Bernhard Steffen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bernhard Steffen, Tiziana Margaria, Alexander Bainczyk, Steve Boßelmann, Daniel Busch, Marc Driessen, Markus Frohme, Falk Howar, Sven Jörges, Marvin Krause, Marco Krumrey, Anna-Lena Lamprecht, Michael Lybecait, Alnis Murtovi, Stefan Naujokat, Johannes Neubauer, Alexander Schieweck, Jonas Schürmann, Steven Smyth, Barbara Steffen, Fabian Storek, Tim Tegeler, Sebastian Teumert, Dominic Wirkner, Philip Zweihoff. (2024)<br><strong>Language-Driven Engineering An Interdisciplinary Software Development Paradigm</strong><br><button class=copy-to-clipboard title="Language-Driven Engineering An Interdisciplinary Software Development Paradigm" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-PL, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10684v1.pdf filename=2402.10684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We illustrate how purpose-specific, graphical modeling enables application experts with different levels of expertise to collaboratively design and then produce complex applications using their individual, purpose-specific modeling language. Our illustration includes seven graphical Integrated Modeling Environments (IMEs) that support full <b>code</b> <b>generation,</b> as well as four browser-based applications that were modeled and then fully automatically generated and produced using DIME, our most complex graphical IME. While the seven IMEs were chosen to illustrate the types of languages we support with our Language-Driven Engineering (LDE) approach, the four DIME products were chosen to give an impression of the power of our LDE-generated IMEs. In fact, Equinocs, Springer Nature&rsquo;s future editorial system for proceedings, is also being fully automatically generated and then deployed at their Dordrecht site using a deployment pipeline generated with Rig, one of the IMEs presented. Our technology is open source and the products presented are currently in use.</p></p class="citation"></blockquote><h2 id=csai-7>cs.AI (7)</h2><h3 id=17--155204-autosat-automatically-optimize-sat-solvers-via-large-language-models-yiwen-sun-et-al-2024>(1/7 | 155/204) AutoSAT: Automatically Optimize SAT Solvers via Large Language Models (Yiwen Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwen Sun, Xianyin Zhang, Shiyu Huang, Shaowei Cai, Bing-Zhen Zhang, Ke Wei. (2024)<br><strong>AutoSAT: Automatically Optimize SAT Solvers via Large Language Models</strong><br><button class=copy-to-clipboard title="AutoSAT: Automatically Optimize SAT Solvers via Large Language Models" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Human Intervention, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10705v1.pdf filename=2402.10705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Heuristics are crucial in SAT solvers, while no heuristic rules are suitable for all problem instances. Therefore, it typically requires to refine specific solvers for specific problem instances. In this context, we present AutoSAT, a novel framework for automatically optimizing heuristics in SAT solvers. AutoSAT is based on <b>Large</b> <b>Large</b> <b>Models</b> <b>(LLMs)</b> which is able to autonomously generate code, conduct evaluation, then utilize the feedback to further optimize heuristics, thereby reducing <b>human</b> <b>intervention</b> and enhancing solver capabilities. AutoSAT operates on a plug-and-play basis, eliminating the need for extensive preliminary setup and model training, and fosters a Chain of Thought collaborative process with fault-tolerance, ensuring robust heuristic optimization. Extensive experiments on a Conflict-Driven Clause Learning (CDCL) solver demonstrates the overall superior performance of AutoSAT, especially in solving some specific SAT problem instances.</p></p class="citation"></blockquote><h3 id=27--156204-robust-agents-learn-causal-world-models-jonathan-richens-et-al-2024>(2/7 | 156/204) Robust agents learn causal world models (Jonathan Richens et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Richens, Tom Everitt. (2024)<br><strong>Robust agents learn causal world models</strong><br><button class=copy-to-clipboard title="Robust agents learn causal world models" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Transfer Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10877v1.pdf filename=2402.10877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It has long been hypothesised that causal <b>reasoning</b> plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including <b>transfer</b> <b>learning</b> and causal inference.</p></p class="citation"></blockquote><h3 id=37--157204-grounding-language-about-belief-in-a-bayesian-theory-of-mind-lance-ying-et-al-2024>(3/7 | 157/204) Grounding Language about Belief in a Bayesian Theory-of-Mind (Lance Ying et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lance Ying, Tan Zhi-Xuan, Lionel Wong, Vikash Mansinghka, Joshua Tenenbaum. (2024)<br><strong>Grounding Language about Belief in a Bayesian Theory-of-Mind</strong><br><button class=copy-to-clipboard title="Grounding Language about Belief in a Bayesian Theory-of-Mind" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: Grounding, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10416v1.pdf filename=2402.10416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the fact that beliefs are mental states that cannot be directly observed, humans talk about each others&rsquo; beliefs on a regular basis, often using rich compositional language to describe what others think and know. What explains this capacity to interpret the hidden epistemic content of other minds? In this paper, we take a step towards an answer by <b>grounding</b> the semantics of belief statements in a Bayesian theory-of-mind: By modeling how humans jointly infer coherent sets of goals, beliefs, and plans that explain an agent&rsquo;s actions, then evaluating statements about the agent&rsquo;s beliefs against these inferences via epistemic logic, our framework provides a conceptual role semantics for belief, explaining the gradedness and compositionality of human belief attributions, as well as their intimate connection with goals and plans. We evaluate this framework by studying how humans attribute goals and beliefs while watching an agent solve a doors-and-keys gridworld puzzle that requires instrumental <b>reasoning</b> about hidden objects. In contrast to pure logical deduction, non-mentalizing baselines, and mentalizing that ignores the role of instrumental plans, our model provides a much better fit to human goal and belief attributions, demonstrating the importance of theory-of-mind for a semantics of belief.</p></p class="citation"></blockquote><h3 id=47--158204-explainability-for-machine-learning-models-from-data-adaptability-to-user-perception-julien-delaunay-2024>(4/7 | 158/204) Explainability for Machine Learning Models: From Data Adaptability to User Perception (julien Delaunay, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>julien Delaunay. (2024)<br><strong>Explainability for Machine Learning Models: From Data Adaptability to User Perception</strong><br><button class=copy-to-clipboard title="Explainability for Machine Learning Models: From Data Adaptability to User Perception" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10888v1.pdf filename=2402.10888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This thesis explores the generation of local explanations for already deployed machine learning models, aiming to identify optimal conditions for producing meaningful explanations considering both data and user requirements. The primary goal is to develop methods for generating explanations for any model while ensuring that these explanations remain faithful to the underlying model and comprehensible to the users. The thesis is divided into two parts. The first enhances a widely used rule-based explanation method. It then introduces a novel approach for evaluating the suitability of linear explanations to approximate a model. Additionally, it conducts a comparative experiment between two families of <b>counterfactual</b> explanation methods to analyze the advantages of one over the other. The second part focuses on user experiments to assess the impact of three explanation methods and two distinct representations. These experiments measure how users perceive their interaction with the model in terms of understanding and trust, depending on the explanations and representations. This research contributes to a better explanation generation, with potential implications for enhancing the transparency, trustworthiness, and usability of deployed AI systems.</p></p class="citation"></blockquote><h3 id=57--159204-on-explaining-unfairness-an-overview-christos-fragkathoulas-et-al-2024>(5/7 | 159/204) On Explaining Unfairness: An Overview (Christos Fragkathoulas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christos Fragkathoulas, Vasiliki Papanikou, Danae Pla Karidi, Evaggelia Pitoura. (2024)<br><strong>On Explaining Unfairness: An Overview</strong><br><button class=copy-to-clipboard title="On Explaining Unfairness: An Overview" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10762v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10762v1.pdf filename=2402.10762v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Algorithmic <b>fairness</b> and explainability are foundational elements for achieving responsible AI. In this paper, we focus on their interplay, a research area that is recently receiving increasing attention. To this end, we first present two comprehensive taxonomies, each representing one of the two complementary fields of study: <b>fairness</b> and explanations. Then, we categorize explanations for <b>fairness</b> into three types: (a) Explanations to enhance <b>fairness</b> metrics, (b) Explanations to help us understand the causes of (un)fairness, and (c) Explanations to assist us in designing methods for mitigating unfairness. Finally, based on our <b>fairness</b> and explanation taxonomies, we present undiscovered literature paths revealing gaps that can serve as valuable insights for future research.</p></p class="citation"></blockquote><h3 id=67--160204-cloud-kitchen-using-planning-based-composite-ai-to-optimize-food-delivery-process-slavomír-švancár-et-al-2024>(6/7 | 160/204) Cloud Kitchen: Using Planning-based Composite AI to Optimize Food Delivery Process (Slavomír Švancár et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Slavomír Švancár, Lukáš Chrpa, Filip Dvořák, Tomáš Balyo. (2024)<br><strong>Cloud Kitchen: Using Planning-based Composite AI to Optimize Food Delivery Process</strong><br><button class=copy-to-clipboard title="Cloud Kitchen: Using Planning-based Composite AI to Optimize Food Delivery Process" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LO, cs.AI<br>Keyword Score: 10<br>Keywords: Planning Domain Descrition Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10725v1.pdf filename=2402.10725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The global food delivery market provides many opportunities for AI-based services that can improve the efficiency of feeding the world. This paper presents the Cloud Kitchen platform as a decision-making tool for restaurants with food delivery and a simulator to evaluate the impact of the decisions. The platform consists of a Technology-Specific Bridge (TSB) that provides an interface for communicating with restaurants or the simulator. TSB uses a <b>PDDL</b> model to represent decisions embedded in the Unified Planning Framework (UPF). Decision-making, which concerns allocating customers&rsquo; orders to vehicles and deciding in which order the customers will be served (for each vehicle), is done via a Vehicle Routing Problem with Time Windows (VRPTW), an efficient tool for this problem. We show that decisions made by our platform can improve customer satisfaction by reducing the number of delayed deliveries using a real-world historical dataset.</p></p class="citation"></blockquote><h3 id=77--161204-learning-planning-action-models-from-state-traces-tomáš-balyo-et-al-2024>(7/7 | 161/204) Learning Planning Action Models from State Traces (Tomáš Balyo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomáš Balyo, Martin Suda, Lukáš Chrpa, Dominik Šafránek, Filip Dvořák, Roman Barták, G. Michael Youngblood. (2024)<br><strong>Learning Planning Action Models from State Traces</strong><br><button class=copy-to-clipboard title="Learning Planning Action Models from State Traces" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10726v1.pdf filename=2402.10726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous STRIPS domain model acquisition approaches that learn from state traces start with the names and parameters of the actions to be learned. Therefore their only task is to deduce the preconditions and effects of the given actions. In this work, we explore learning in situations when the parameters of learned actions are not provided. We define two levels of trace quality based on which information is provided and present an algorithm for each. In one level (L1), the states in the traces are labeled with action names, so we can deduce the number and names of the actions, but we still need to work out the number and types of parameters. In the other level (L2), the states are additionally labeled with objects that constitute the parameters of the corresponding grounded actions. Here we still need to deduce the types of the parameters in the learned actions. We experimentally evaluate the proposed algorithms and compare them with the state-of-the-art learning tool FAMA on a large collection of IPC <b>benchmarks.</b> The evaluation shows that our new algorithms are faster, can handle larger inputs and provide better results in terms of learning action models more similar to reference models.</p></p class="citation"></blockquote><h2 id=eesssp-3>eess.SP (3)</h2><h3 id=13--162204-power-efficient-indoor-localization-using-adaptive-channel-aware-ultra-wideband-dl-tdoa-sagnik-bhattacharya-et-al-2024>(1/3 | 162/204) Power-Efficient Indoor Localization Using Adaptive Channel-aware Ultra-wideband DL-TDOA (Sagnik Bhattacharya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sagnik Bhattacharya, Junyoung Choi, Joohyun Lee. (2024)<br><strong>Power-Efficient Indoor Localization Using Adaptive Channel-aware Ultra-wideband DL-TDOA</strong><br><button class=copy-to-clipboard title="Power-Efficient Indoor Localization Using Adaptive Channel-aware Ultra-wideband DL-TDOA" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-AI, eess-SP, eess.SP<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10515v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10515v1.pdf filename=2402.10515v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Among the various Ultra-wideband (UWB) ranging methods, the absence of uplink communication or centralized computation makes downlink time-difference-of-arrival (DL-TDOA) localization the most suitable for large-scale industrial deployments. However, temporary or permanent obstacles in the deployment region often lead to non-line-of-sight (NLOS) channel path and signal outage effects, which result in localization errors. Prior research has addressed this problem by increasing the ranging frequency, which leads to a heavy increase in the user device power consumption. It also does not contribute to any increase in localization accuracy under line-of-sight (LOS) conditions. In this paper, we propose and implement a novel low-power channel-aware dynamic frequency DL-TDOA ranging algorithm. It comprises NLOS probability predictor based on a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN),</b> a dynamic ranging frequency control module, and an IMU sensor-based ranging filter. Based on the conducted experiments, we show that the proposed algorithm achieves 50% higher accuracy in NLOS conditions while having 46% lower power consumption in LOS conditions compared to baseline methods from prior research.</p></p class="citation"></blockquote><h3 id=23--163204-beamforming-optimization-for-active-ris-aided-multiuser-communications-with-hardware-impairments-zhangjie-peng-et-al-2024>(2/3 | 163/204) Beamforming Optimization for Active RIS-Aided Multiuser Communications With Hardware Impairments (Zhangjie Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhangjie Peng, Zhibo Zhang, Cunhua Pan, Marco Di Renzo, Octavia A. Dobre, Jiangzhou Wang. (2024)<br><strong>Beamforming Optimization for Active RIS-Aided Multiuser Communications With Hardware Impairments</strong><br><button class=copy-to-clipboard title="Beamforming Optimization for Active RIS-Aided Multiuser Communications With Hardware Impairments" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10687v1.pdf filename=2402.10687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider an active reconfigurable intelligent surface (RIS) to assist the multiuser downlink transmission in the presence of practical hardware impairments (HWIs), including the HWIs at the transceivers and the phase noise at the active RIS. The active RIS is deployed to amplify the incident signals to alleviate the multiplicative fading effect, which is a limitation in the conventional passive RIS-aided wireless systems. We aim to maximize the sum rate through jointly designing the transmit beamforming at the base station (BS), the amplification factors and the phase shifts at the active RIS. To tackle this challenging optimization problem effectively, we decouple it into two tractable subproblems. Subsequently, each subproblem is transformed into a second order cone programming problem. The block coordinate descent framework is applied to tackle them, where the transmit beamforming and the reflection coefficients are alternately designed. In addition, another efficient algorithm is presented to reduce the computational complexity. Specifically, by exploiting the majorization-minimization approach, each subproblem is reformulated into a tractable surrogate problem, whose closed-form solutions are obtained by Lagrange dual decomposition approach and element-wise alternating sequential optimization method. <b>Simulation</b> results validate the effectiveness of our developed algorithms, and reveal that the HWIs significantly limit the system performance of active RIS-empowered wireless communications. Furthermore, the active RIS noticeably boosts the sum rate under the same total power budget, compared with the passive RIS.</p></p class="citation"></blockquote><h3 id=33--164204-a-noisy-beat-is-worth-16-words-a-tiny-transformer-for-low-power-arrhythmia-classification-on-microcontrollers-paola-busia-et-al-2024>(3/3 | 164/204) A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers (Paola Busia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paola Busia, Matteo Antonio Scrugli, Victor Jean-Baptiste Jung, Luca Benini, Paolo Meloni. (2024)<br><strong>A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers</strong><br><button class=copy-to-clipboard title="A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-HC, cs-LG, eess-SP, eess.SP<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10748v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10748v1.pdf filename=2402.10748v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wearable systems for the long-term monitoring of cardiovascular diseases are becoming widespread and valuable assets in diagnosis and therapy. A promising approach for real-time analysis of the electrocardiographic (ECG) signal and the detection of heart conditions, such as arrhythmia, is represented by the <b>transformer</b> machine learning model. <b>Transformers</b> are powerful models for the classification of time series, although efficient implementation in the wearable domain raises significant design challenges, to combine adequate accuracy and a suitable complexity. In this work, we present a tiny <b>transformer</b> model for the analysis of the ECG signal, requiring only 6k parameters and reaching 98.97% accuracy in the recognition of the 5 most common arrhythmia classes from the MIT-BIH Arrhythmia database, assessed considering 8-bit integer inference as required for efficient execution on low-power microcontroller-based devices. We explored an augmentation-based training approach for improving the robustness against electrode motion artifacts noise, resulting in a worst-case post-deployment performance assessment of 98.36% accuracy. Suitability for wearable monitoring solutions is finally demonstrated through efficient deployment on the parallel ultra-low-power GAP9 processor, where inference execution requires 4.28ms and 0.09mJ.</p></p class="citation"></blockquote><h2 id=statml-6>stat.ML (6)</h2><h3 id=16--165204-fixed-confidence-best-arm-identification-in-the-bayesian-setting-kyoungseok-jang-et-al-2024>(1/6 | 165/204) Fixed Confidence Best Arm Identification in the Bayesian Setting (Kyoungseok Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyoungseok Jang, Junpei Komiyama, Kazutoshi Yamazaki. (2024)<br><strong>Fixed Confidence Best Arm Identification in the Bayesian Setting</strong><br><button class=copy-to-clipboard title="Fixed Confidence Best Arm Identification in the Bayesian Setting" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10429v1.pdf filename=2402.10429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the fixed-confidence best arm identification (FC-BAI) problem in the Bayesian Setting. This problem aims to find the arm of the largest mean with a fixed confidence level when the <b>bandit</b> model has been sampled from the known prior. Most studies on the FC-BAI problem have been conducted in the frequentist setting, where the <b>bandit</b> model is predetermined before the game starts. We show that the traditional FC-BAI algorithms studied in the frequentist setting, such as track-and-stop and top-two algorithms, result in arbitrary suboptimal performances in the Bayesian setting. We also prove a lower bound of the expected number of samples in the Bayesian setting and introduce a variant of successive elimination that has a matching performance with the lower bound up to a logarithmic factor. <b>Simulations</b> verify the theoretical results.</p></p class="citation"></blockquote><h3 id=26--166204-stochastic-localization-via-iterative-posterior-sampling-louis-grenioux-et-al-2024>(2/6 | 166/204) Stochastic Localization via Iterative Posterior Sampling (Louis Grenioux et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Louis Grenioux, Maxence Noble, Marylou Gabrié, Alain Oliviero Durmus. (2024)<br><strong>Stochastic Localization via Iterative Posterior Sampling</strong><br><button class=copy-to-clipboard title="Stochastic Localization via Iterative Posterior Sampling" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-CO, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Benchmarking, Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10758v1.pdf filename=2402.10758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, $\textit{Stochastic Localization via Iterative Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimation of the denoiser and comes with detailed practical guidelines. We illustrate the benefits and applicability of SLIPS on several <b>benchmarks,</b> including Gaussian mixtures in increasing dimensions, Bayesian <b>logistic</b> <b>regression</b> and a high-dimensional field system from statistical-mechanics.</p></p class="citation"></blockquote><h3 id=36--167204-generative-modeling-for-tabular-data-via-penalized-optimal-transport-network-wenhui-sophia-lu-et-al-2024>(3/6 | 167/204) Generative Modeling for Tabular Data via Penalized Optimal Transport Network (Wenhui Sophia Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhui Sophia Lu, Chenyang Zhong, Wing Hung Wong. (2024)<br><strong>Generative Modeling for Tabular Data via Penalized Optimal Transport Network</strong><br><button class=copy-to-clipboard title="Generative Modeling for Tabular Data via Penalized Optimal Transport Network" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-AP, stat-ME, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Benchmarking, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10456v1.pdf filename=2402.10456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of precisely learning the probability distribution of rows within tabular data and producing authentic synthetic samples is both crucial and non-trivial. Wasserstein <b>generative</b> <b>adversarial</b> <b>network</b> (WGAN) marks a notable improvement in <b>generative</b> <b>modeling,</b> <b>addressing</b> the challenges faced by its predecessor, <b>generative</b> <b>adversarial</b> <b>network.</b> However, due to the mixed data types and multimodalities prevalent in tabular data, the delicate equilibrium between the generator and discriminator, as well as the inherent instability of Wasserstein distance in high dimensions, WGAN often fails to produce high-fidelity samples. To this end, we propose POTNet (Penalized Optimal Transport Network), a <b>generative</b> <b>deep</b> <b>neural</b> network based on a novel, robust, and interpretable marginally-penalized Wasserstein (MPW) loss. POTNet can effectively model tabular data containing both categorical and continuous features. Moreover, it offers the flexibility to condition on a subset of features. We provide theoretical justifications for the motivation behind the MPW loss. We also empirically demonstrate the effectiveness of our proposed method on four different <b>benchmarks</b> across a variety of real-world and simulated datasets. Our proposed model achieves orders of magnitude speedup during the sampling stage compared to state-of-the-art <b>generative</b> <b>models</b> <b>for</b> tabular data, thereby enabling efficient large-scale synthetic data generation.</p></p class="citation"></blockquote><h3 id=46--168204-predictive-uncertainty-quantification-via-risk-decompositions-for-strictly-proper-scoring-rules-nikita-kotelevskii-et-al-2024>(4/6 | 168/204) Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules (Nikita Kotelevskii et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikita Kotelevskii, Maxim Panov. (2024)<br><strong>Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules</strong><br><button class=copy-to-clipboard title="Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10727v1.pdf filename=2402.10727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distinguishing sources of predictive uncertainty is of crucial importance in the application of forecasting models across various domains. Despite the presence of a great variety of proposed uncertainty measures, there are no strict definitions to disentangle them. Furthermore, the relationship between different measures of uncertainty quantification remains somewhat unclear. In this work, we introduce a general framework, rooted in statistical <b>reasoning,</b> which not only allows the creation of new uncertainty measures but also clarifies their interrelations. Our approach leverages statistical risk to distinguish aleatoric and epistemic uncertainty components and utilizes proper scoring rules to quantify them. To make it practically tractable, we propose an idea to incorporate Bayesian <b>reasoning</b> into this framework and discuss the properties of the proposed approximation.</p></p class="citation"></blockquote><h3 id=56--169204-conformalized-credal-set-predictors-alireza-javanmardi-et-al-2024>(5/6 | 169/204) Conformalized Credal Set Predictors (Alireza Javanmardi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alireza Javanmardi, David Stutz, Eyke Hüllermeier. (2024)<br><strong>Conformalized Credal Set Predictors</strong><br><button class=copy-to-clipboard title="Conformalized Credal Set Predictors" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Natural Language Inference<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10723v1.pdf filename=2402.10723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution. In machine learning, they have recently attracted attention as an appealing formalism for uncertainty representation, in particular due to their ability to represent both the aleatoric and epistemic uncertainty in a prediction. However, the design of methods for learning credal set predictors remains a challenging problem. In this paper, we make use of conformal prediction for this purpose. More specifically, we propose a method for predicting credal sets in the classification task, given training data labeled by probability distributions. Since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution). We demonstrate the applicability of our method to <b>natural</b> <b>language</b> <b>inference,</b> a highly ambiguous <b>natural</b> <b>language</b> <b>task</b> where it is common to obtain multiple annotations per example.</p></p class="citation"></blockquote><h3 id=66--170204-performance-gaps-in-multi-view-clustering-under-the-nested-matrix-tensor-model-hugo-lebeau-et-al-2024>(6/6 | 170/204) Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model (Hugo Lebeau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugo Lebeau, Mohamed El Amine Seddik, José Henrique de Morais Goulart. (2024)<br><strong>Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model</strong><br><button class=copy-to-clipboard title="Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-PR, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10677v1.pdf filename=2402.10677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the estimation of a planted signal hidden in a recently introduced nested matrix-tensor model, which is an extension of the classical spiked rank-one tensor model, motivated by multi-view <b>clustering.</b> Prior work has theoretically examined the performance of a tensor-based approach, which relies on finding a best rank-one approximation, a problem known to be computationally hard. A tractable alternative approach consists in computing instead the best rank-one (matrix) approximation of an unfolding of the observed tensor data, but its performance was hitherto unknown. We quantify here the performance gap between these two approaches, in particular by deriving the precise algorithmic threshold of the unfolding approach and demonstrating that it exhibits a BBP-type transition behavior. This work is therefore in line with recent contributions which deepen our understanding of why tensor-based methods surpass matrix-based methods in handling structured tensor data.</p></p class="citation"></blockquote><h2 id=eesssy-2>eess.SY (2)</h2><h3 id=12--171204-data-driven-abstractions-for-control-systems-rudi-coppola-et-al-2024>(1/2 | 171/204) Data-Driven Abstractions for Control Systems (Rudi Coppola et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rudi Coppola, Andrea Peruffo, Manuel Mazo Jr. (2024)<br><strong>Data-Driven Abstractions for Control Systems</strong><br><button class=copy-to-clipboard title="Data-Driven Abstractions for Control Systems" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-FL, cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10668v1.pdf filename=2402.10668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>At the intersection of dynamical systems, control theory, and formal methods lies the construction of symbolic abstractions: these typically represent simpler, finite-state models whose behaviour mimics the one of an underlying concrete system but are easier to analyse. Building an abstraction usually requires an accurate knowledge of the underlying model: this knowledge may be costly to gather, especially in real-life applications. We aim to bridge this gap by building abstractions based on sampling finite length trajectories. Adding the controller degrees of freedom, we newly define the notion of probabilistic alternating <b>simulation,</b> and provide probably approximately correct (PAC) guarantees that the constructed abstraction includes all behaviours of the concrete system and that it is suitable for control design, for arbitrarily long time horizons, leveraging the scenario theory. Our method is then tested on several numerical <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=22--172204-autonomous-emergency-braking-with-driver-in-the-loop-torque-vectoring-for-active-learning-benjamin-sullivan-et-al-2024>(2/2 | 172/204) Autonomous Emergency Braking With Driver-In-The-Loop: Torque Vectoring for Active Learning (Benjamin Sullivan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Sullivan, Jingjing Jiang, Georgios Mavros, Wen-Hua Chen. (2024)<br><strong>Autonomous Emergency Braking With Driver-In-The-Loop: Torque Vectoring for Active Learning</strong><br><button class=copy-to-clipboard title="Autonomous Emergency Braking With Driver-In-The-Loop: Torque Vectoring for Active Learning" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10761v1.pdf filename=2402.10761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous Emergency Braking (AEB) potentially brings significant improvements in automotive safety due to its ability to autonomously prevent collisions in situations where the driver may not be able to do so. Driven by the poor performance of the state of the art in recent testing, this work provides an online solution to identify critical parameters such as the current and maximum friction coefficients. The method introduced here, namely Torque Vectoring for <b>Active</b> <b>Learning</b> (TVAL), can perform state and parameter estimation whilst following the driver&rsquo;s input. Importantly with less power requirements than normal driving. Our method is designed with a crucial focus on ensuring minimal disruption to the driver, allowing them to maintain full control of the vehicle. Additionally, we exploit a rain/light sensor to drive the observer resampling to maintain estimation certainty across prolonged operation. Then a scheme to modulate TVAL is introduced that considers powertrain efficiency, safety, and availability in an online fashion. Using a high-fidelity vehicle model and drive cycle we demonstrate the functionality of TVAL controller across changing road surfaces where we successfully identify the road surface whenever possible.</p></p class="citation"></blockquote><h2 id=cscr-3>cs.CR (3)</h2><h3 id=13--173204-proving-membership-in-llm-pretraining-data-via-data-watermarks-johnny-tian-zheng-wei-et-al-2024>(1/3 | 173/204) Proving membership in LLM pretraining data via data watermarks (Johnny Tian-Zheng Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johnny Tian-Zheng Wei, Ryan Yixiang Wang, Robin Jia. (2024)<br><strong>Proving membership in LLM pretraining data via data watermarks</strong><br><button class=copy-to-clipboard title="Proving membership in LLM pretraining data via data watermarks" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: BLOOM, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10892v1.pdf filename=2402.10892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting whether copyright holders&rsquo; works were used in <b>LLM</b> pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design &ndash; watermark length, number of duplications, and interference &ndash; affect the power of the hypothesis test. Next, we study how a watermark&rsquo;s detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks remain strong if the model size also increases. Finally, we view SHA hashes as natural watermarks and show that we can robustly detect hashes from <b>BLOOM-176B&rsquo;s</b> training data, as long as they occurred at least 90 times. Together, our results point towards a promising future for data watermarks in real world use.</p></p class="citation"></blockquote><h3 id=23--174204-enabling-zero-trust-security-in-iomt-edge-network-maha-ali-allouzi-et-al-2024>(2/3 | 174/204) Enabling Zero Trust Security in IoMT Edge Network (Maha Ali Allouzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maha Ali Allouzi, Javed Khan. (2024)<br><strong>Enabling Zero Trust Security in IoMT Edge Network</strong><br><button class=copy-to-clipboard title="Enabling Zero Trust Security in IoMT Edge Network" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CY, cs.CR<br>Keyword Score: 10<br>Keywords: Zero Trust<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10389v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10389v1.pdf filename=2402.10389v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Internet of Medical Things (IoMT) deals with a patient-data-rich segment, which makes security and privacy a severe concern for patients. Therefore, access control is a significant aspect of ensuring trust in the IoMT. However, deploying existing authentication and authorization solutions to the Internet of Medical Things (IoMT) is not straightforward because of highly dynamic and possibly unprotected environments and untrusted supply chain for the IoT devices. In this article, we propose Soter, a <b>Zero-Trust</b> <b>based</b> authentication system for the IoMT. Soter Incorporates trust negotiation mechanisms within the <b>Zero</b> <b>Trust</b> framework to enable dynamic trust establishment. When a user or device seeks access to a resource, initiate a trust negotiation process. During this process, credentials, attributes, and contextual information are exchanged between the requester and the resource owner. Soter defines access rules based on various factors, including user identity, device health, and location. Access is granted or denied based on these conditions.</p></p class="citation"></blockquote><h3 id=33--175204-aim-automated-input-set-minimization-for-metamorphic-security-testing-nazanin-bayati-chaleshtari-et-al-2024>(3/3 | 175/204) AIM: Automated Input Set Minimization for Metamorphic Security Testing (Nazanin Bayati Chaleshtari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nazanin Bayati Chaleshtari, Yoann Marquer, Fabrizio Pastore, Lionel C. Briand. (2024)<br><strong>AIM: Automated Input Set Minimization for Metamorphic Security Testing</strong><br><button class=copy-to-clipboard title="AIM: Automated Input Set Minimization for Metamorphic Security Testing" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SE, cs.CR<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10773v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10773v1.pdf filename=2402.10773v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For Web systems, which are accessible to any machine connected to internet, security is a critical concern. Although security testing can be automated by generating crafted inputs as an attacker would do, solutions to automate the test oracle, i.e., distinguishing correct from incorrect outputs for a given input, remain preliminary. Specifically, previous work has demonstrated the potential of metamorphic testing; indeed, security failures can be determined by metamorphic relations that turn valid inputs into malicious inputs and compare their outputs. However, without further guidance, metamorphic relations should be executed on a very large set of valid inputs, which is time consuming and makes metamorphic testing impractical. Hence, in this study, we propose AIM, an approach that automatically selects inputs to reduce testing costs while preserving vulnerability detection capabilities. AIM includes a <b>clustering-based</b> black box approach, identifying similar inputs based on their security properties. It also presents a novel genetic algorithm able to efficiently select diverse inputs while minimizing their total cost. Further, it contains a problem reduction component to reduce the search space and speed up the minimization process. We evaluated the effectiveness of AIM on two well-known web systems, Jenkins and Joomla. We compared AIM&rsquo;s results with four baselines in security testing. Overall, AIM reduced MRs execution time by 84 percent for Jenkins and 82 percent for Joomla while preserving full vulnerability detection. Furthermore, AIM outperformed all the considered baselines regarding vulnerability coverage. Although it has been tuned to work with Web system inputs, AIM could be applied to minimize metamorphic testing cost in other contexts.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--176204-design-of-2d-skyrmionic-metamaterial-through-controlled-assembly-qichen-xu-et-al-2024>(1/1 | 176/204) Design of 2D Skyrmionic Metamaterial Through Controlled Assembly (Qichen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qichen Xu, Zhuanglin Shen, Alexander Edström, I. P. Miranda, Zhiwei Lu, Anders Bergman, Danny Thonig, Wanjian Yin, Olle Eriksson, Anna Delin. (2024)<br><strong>Design of 2D Skyrmionic Metamaterial Through Controlled Assembly</strong><br><button class=copy-to-clipboard title="Design of 2D Skyrmionic Metamaterial Through Controlled Assembly" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-LG, physics-comp-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10874v1.pdf filename=2402.10874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite extensive research on magnetic skyrmions and antiskyrmions, a significant challenge remains in crafting nontrivial high-order skyrmionic textures with varying, or even tailor-made, topologies. We address this challenge, by focusing on a construction pathway of skyrmionics metamaterial within a monolayer thin film and suggest several promising lattice-like, flakes-like, and cell-like skyrmionic metamaterials that are surprisingly stable. Central to our approach is the concept of &lsquo;simulated controlled assembly&rsquo;, in short, a protocol inspired by &lsquo;click chemistry&rsquo; that allows for positioning topological magnetic structures where one likes, and then allowing for energy minimization to elucidate the stability. Utilizing high-throughput atomistic-spin-dynamic (ASD) <b>simulations</b> alongside state-of-the-art AI-driven tools, we have isolated skyrmions (topological charge Q=1), antiskyrmions (Q=-1), and skyrmionium (Q=0). These entities serve as foundational &lsquo;skyrmionic building blocks&rsquo; to forming reported intricate textures. In this work, two key contributions are introduced to the field of skyrmionic systems. First, we present a novel method for integrating control assembly protocols for the stabilization and investigation of topological magnets, which marks a significant advancement in the ability to explore new skyrmionic textures. Second, we report on the discovery of skyrmionic metamaterials, which shows a plethora of complex topologies that are possible to investigate theoretically and experimentally.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=13--177204-probabilistic-on-demand-charging-scheduling-for-isac-assisted-wrsns-with-multiple-mobile-charging-vehicles-muhammad-umar-farooq-qaisar-et-al-2024>(1/3 | 177/204) Probabilistic On-Demand Charging Scheduling for ISAC-Assisted WRSNs with Multiple Mobile Charging Vehicles (Muhammad Umar Farooq Qaisar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Umar Farooq Qaisar, Weijie Yuan, Paolo Bellavista, Guangjie Han, Rabiu Sale Zakariyya, Adeel Ahmed. (2024)<br><strong>Probabilistic On-Demand Charging Scheduling for ISAC-Assisted WRSNs with Multiple Mobile Charging Vehicles</strong><br><button class=copy-to-clipboard title="Probabilistic On-Demand Charging Scheduling for ISAC-Assisted WRSNs with Multiple Mobile Charging Vehicles" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10873v1.pdf filename=2402.10873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The internet of things (IoT) based wireless sensor networks (WSNs) face an energy shortage challenge that could be overcome by the novel wireless power transfer (WPT) technology. The combination of WSNs and WPT is known as wireless rechargeable sensor networks (WRSNs), with the charging efficiency and charging scheduling being the primary concerns. Therefore, this paper proposes a probabilistic on-demand charging scheduling for integrated sensing and communication (ISAC)-assisted WRSNs with multiple mobile charging vehicles (MCVs) that addresses three parts. First, it considers the four attributes with their probability distributions to balance the charging load on each MCV. The distributions are residual energy of charging node, distance from MCV to charging node, degree of charging node, and charging node betweenness centrality. Second, it considers the efficient charging factor strategy to partially charge network nodes. Finally, it employs the ISAC concept to efficiently utilize the wireless resources to reduce the traveling cost of each MCV and to avoid the charging conflicts between them. The <b>simulation</b> results show that the proposed protocol outperforms cutting-edge protocols in terms of energy usage efficiency, charging delay, survival rate, and travel distance.</p></p class="citation"></blockquote><h3 id=23--178204-qkdnetsim-improvement-of-the-quantum-network-simulator-for-ns-3-david-soler-et-al-2024>(2/3 | 178/204) QKDNetSim+: Improvement of the Quantum Network Simulator for NS-3 (David Soler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Soler, Iván Cillero, Carlos Dafonte, Manuel Fernández-Veiga, Ana Fernández-Vilas, Francisco J. Nóvoa. (2024)<br><strong>QKDNetSim+: Improvement of the Quantum Network Simulator for NS-3</strong><br><button class=copy-to-clipboard title="QKDNetSim+: Improvement of the Quantum Network Simulator for NS-3" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-ET, cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10822v1.pdf filename=2402.10822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The first Quantum Key Distribution (QKD) networks are currently being deployed, but the implementation cost is still prohibitive for most researchers. As such, there is a need for realistic QKD network simulators. The \textit{QKDNetSim} module for the network simulator NS-3 focuses on the representation of packets and the management of key material in a QKD network at the application layer. Although QKDNetSim&rsquo;s representation of a QKD network is insightful, some its components lack the depth that would allow the simulator to faithfully represent the behaviour of a real quantum network. In this work, we analyse QKDNetSim&rsquo;s architecture to identify its limitations, and we present an enhanced version of QKDNetSim in which some of its components have been modified to provide a more realistic <b>simulation</b> environment.</p></p class="citation"></blockquote><h3 id=33--179204-does-twinning-vehicular-networks-enhance-their-performance-in-dense-areas-sarah-al-shareeda-et-al-2024>(3/3 | 179/204) Does Twinning Vehicular Networks Enhance Their Performance in Dense Areas? (Sarah Al-Shareeda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarah Al-Shareeda, Sema F. Oktug, Yusuf Yaslan, Gokhan Yurdakul, Berk Canberk. (2024)<br><strong>Does Twinning Vehicular Networks Enhance Their Performance in Dense Areas?</strong><br><button class=copy-to-clipboard title="Does Twinning Vehicular Networks Enhance Their Performance in Dense Areas?" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-NI, cs.NI<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10701v1.pdf filename=2402.10701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the potential of Digital Twins (DTs) to enhance network performance in densely populated urban areas, specifically focusing on vehicular networks. The study comprises two phases. In Phase I, we utilize traffic data and AI <b>clustering</b> to identify critical locations, particularly in crowded urban areas with high accident rates. In Phase II, we evaluate the advantages of twinning vehicular networks through three deployment scenarios: edge-based twin, cloud-based twin, and hybrid-based twin. Our analysis demonstrates that twinning significantly reduces network delays, with virtual twins outperforming physical networks. Virtual twins maintain low delays even with increased vehicle density, such as 15.05 seconds for 300 vehicles. Moreover, they exhibit faster computational speeds, with cloud-based twins being 1.7 times faster than edge twins in certain scenarios. These findings provide insights for efficient vehicular communication and underscore the potential of virtual twins in enhancing vehicular networks in crowded areas while emphasizing the importance of considering real-world factors when making deployment decisions.</p></p class="citation"></blockquote><h2 id=statap-1>stat.AP (1)</h2><h3 id=11--180204-agent-based-simulation-evaluation-of-cbd-tolling-a-case-study-from-new-york-city-qingnan-liang-et-al-2024>(1/1 | 180/204) Agent-based Simulation Evaluation of CBD Tolling: A Case Study from New York City (Qingnan Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingnan Liang, Ruili Yao, Ruixuan Zhang, Zhibin Chen, Guoyuan Wu. (2024)<br><strong>Agent-based Simulation Evaluation of CBD Tolling: A Case Study from New York City</strong><br><button class=copy-to-clipboard title="Agent-based Simulation Evaluation of CBD Tolling: A Case Study from New York City" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.AP<br>Categories: cs-CY, stat-AP, stat.AP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10834v1.pdf filename=2402.10834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Congestion tollings have been widely developed and adopted as an effective tool to mitigate urban traffic congestion and enhance transportation system sustainability. Nevertheless, these tolling schemes are often tailored on a city-by-city or even area-by-area basis, and the cost of conducting field experiments often makes the design and evaluation process challenging. In this work, we leverage MATSim, a <b>simulation</b> platform that provides microscopic behaviors at the agent level, to evaluate performance on tolling schemes. Specifically, we conduct a case study of the Manhattan Central Business District (CBD) in New York City (NYC) using a fine-granularity traffic network model in the large-scale agent behavior setting. The flexibility of MATSim enables the implementation of a customized tolling policy proposed yet not deployed by the NYC agency while providing detailed interpretations. The quantitative and qualitative results indicate that the tested tolling program can regulate the personal vehicle volume in the CBD area and encourage the usage of public transportation, which proves to be a practical move towards sustainable transportation systems. More importantly, our work demonstrates that agent-based <b>simulation</b> helps better understand the travel pattern change subject to tollings in dense and complex urban environments, and it has the potential to facilitate efficient decision-making for the devotion to sustainable traffic management.</p></p class="citation"></blockquote><h2 id=mathna-3>math.NA (3)</h2><h3 id=13--181204-a-lattice-boltzmann-method-for-non-newtonian-blood-flow-in-coiled-intracranial-aneurysms-medeea-horvat-et-al-2024>(1/3 | 181/204) A lattice Boltzmann method for non-Newtonian blood flow in coiled intracranial aneurysms (Medeea Horvat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Medeea Horvat, Stephan B. Lunowa, Dmytro Sytnyk, Barbara Wohlmuth. (2024)<br><strong>A lattice Boltzmann method for non-Newtonian blood flow in coiled intracranial aneurysms</strong><br><button class=copy-to-clipboard title="A lattice Boltzmann method for non-Newtonian blood flow in coiled intracranial aneurysms" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10809v1.pdf filename=2402.10809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intracranial aneurysms are the leading cause of stroke. One of the established treatment approaches is the embolization induced by coil insertion. However, the prediction of treatment and subsequent changed flow characteristics in the aneurysm, is still an open problem. In this work, we present an approach based on patient specific geometry and parameters including a coil representation as inhomogeneous porous medium. The model consists of the volume-averaged Navier-Stokes equations including the non-Newtonian blood rheology. We solve these equations using a problem-adapted lattice Boltzmann method and present a comparison between fully-resolved and volume-averaged <b>simulations.</b> The results indicate the validity of the model. Overall, this workflow allows for patient specific assessment of the flow due to potential treatment.</p></p class="citation"></blockquote><h3 id=23--182204-hermite-neural-network-simulation-for-solving-the-2d-schrodinger-equation-kourosh-parand-et-al-2024>(2/3 | 182/204) Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation (Kourosh Parand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kourosh Parand, Aida Pakniyat. (2024)<br><strong>Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation</strong><br><button class=copy-to-clipboard title="Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10649v1.pdf filename=2402.10649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Schrodinger equation is a mathematical equation describing the wave function&rsquo;s behavior in a quantum-mechanical system. It is a partial differential equation that provides valuable insights into the fundamental principles of quantum mechanics. In this paper, the aim was to solve the Schrodinger equation with sufficient accuracy by using a mixture of neural networks with the collocation method base Hermite functions. Initially, the Hermite functions roots were employed as collocation points, enhancing the efficiency of the solution. The Schrodinger equation is defined in an infinite domain, the use of Hermite functions as activation functions resulted in excellent precision. Finally, the proposed method was simulated using MATLAB&rsquo;s Simulink tool. The results were then compared with those obtained using Physics-informed neural networks and the presented method.</p></p class="citation"></blockquote><h3 id=33--183204-a-predictive-surrogate-model-for-heat-transfer-of-an-impinging-jet-on-a-concave-surface-sajad-salavatidezfouli-et-al-2024>(3/3 | 183/204) A Predictive Surrogate Model for Heat Transfer of an Impinging Jet on a Concave Surface (Sajad Salavatidezfouli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sajad Salavatidezfouli, Saeid Rakhsha, Armin Sheidani, Giovanni Stabile, Gianluigi Rozza. (2024)<br><strong>A Predictive Surrogate Model for Heat Transfer of an Impinging Jet on a Concave Surface</strong><br><button class=copy-to-clipboard title="A Predictive Surrogate Model for Heat Transfer of an Impinging Jet on a Concave Surface" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-CE, cs-LG, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10641v1.pdf filename=2402.10641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper aims to comprehensively investigate the efficacy of various Model Order Reduction (MOR) and deep learning techniques in predicting heat transfer in a pulsed jet impinging on a concave surface. Expanding on the previous experimental and numerical research involving pulsed circular jets, this investigation extends to evaluate Predictive Surrogate Models (PSM) for heat transfer across various jet characteristics. To this end, this work introduces two predictive approaches, one employing a Fast Fourier Transformation augmented Artificial Neural Network (FFT-ANN) for predicting the average Nusselt number under constant-frequency scenarios. Moreover, the investigation introduces the Proper Orthogonal Decomposition and <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(POD-LSTM)</b> approach for random-frequency impingement jets. The POD-LSTM method proves to be a robust solution for predicting the local heat transfer rate under random-frequency impingement scenarios, capturing both the trend and value of temporal modes. The comparison of these approaches highlights the versatility and efficacy of advanced machine learning techniques in modelling complex heat transfer phenomena.</p></p class="citation"></blockquote><h2 id=q-finst-1>q-fin.ST (1)</h2><h3 id=11--184204-ragic-risk-aware-generative-adversarial-model-for-stock-interval-construction-jingyi-gu-et-al-2024>(1/1 | 184/204) RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction (Jingyi Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyi Gu, Wenlu Du, Guiling Wang. (2024)<br><strong>RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction</strong><br><button class=copy-to-clipboard title="RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.ST<br>Categories: cs-LG, q-fin-ST, q-fin.ST<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10760v1.pdf filename=2402.10760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efforts to predict stock market outcomes have yielded limited success due to the inherently stochastic nature of the market, influenced by numerous unpredictable factors. Many existing prediction approaches focus on single-point predictions, lacking the depth needed for effective decision-making and often overlooking market risk. To bridge this gap, we propose a novel model, RAGIC, which introduces sequence generation for stock interval prediction to quantify uncertainty more effectively. Our approach leverages a <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN)</b> to produce future price sequences infused with randomness inherent in financial markets. RAGIC&rsquo;s generator includes a risk module, capturing the risk perception of informed investors, and a temporal module, accounting for historical price trends and seasonality. This multi-faceted generator informs the creation of risk-sensitive intervals through statistical inference, incorporating horizon-wise insights. The interval&rsquo;s width is carefully adjusted to reflect market volatility. Importantly, our approach relies solely on publicly available data and incurs only low computational overhead. RAGIC&rsquo;s evaluation across globally recognized broad-based indices demonstrates its balanced performance, offering both accuracy and informativeness. Achieving a consistent 95% coverage, RAGIC maintains a narrow interval width. This promising outcome suggests that our approach effectively addresses the challenges of stock market prediction while incorporating vital risk considerations.</p></p class="citation"></blockquote><h2 id=csit-4>cs.IT (4)</h2><h3 id=14--185204-uncertainty-calibration-and-membership-inference-attacks-an-information-theoretic-perspective-meiyi-zhu-et-al-2024>(1/4 | 185/204) Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective (Meiyi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meiyi Zhu, Caili Guo, Chunyan Feng, Osvaldo Simeone. (2024)<br><strong>Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective</strong><br><button class=copy-to-clipboard title="Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-CR, cs-IT, cs-LG, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10686v1.pdf filename=2402.10686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in which an adaptive prediction set is produced as in conformal prediction. We derive bounds on the advantage of an MIA adversary with the aim of offering insights into the impact of uncertainty and calibration on the effectiveness of MIAs. <b>Simulation</b> results demonstrate that the derived analytical bounds predict well the effectiveness of MIAs.</p></p class="citation"></blockquote><h3 id=24--186204-towards-6g-evolution-three-enhancements-three-innovations-and-three-major-challenges-rohit-singh-et-al-2024>(2/4 | 186/204) Towards 6G Evolution: Three Enhancements, Three Innovations, and Three Major Challenges (Rohit Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohit Singh, Aryan Kaushik, Wonjae Shin, Marco Di Renzo, Vincenzo Sciancalepore, Doohwan Lee, Hirofumi Sasaki, Arman Shojaeifard, Octavia A. Dobre. (2024)<br><strong>Towards 6G Evolution: Three Enhancements, Three Innovations, and Three Major Challenges</strong><br><button class=copy-to-clipboard title="Towards 6G Evolution: Three Enhancements, Three Innovations, and Three Major Challenges" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10781v1.pdf filename=2402.10781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past few decades, wireless communication has witnessed remarkable growth, experiencing several transformative changes. This article aims to provide a comprehensive overview of wireless communication technologies, from the foundations to the recent wireless advances. Specifically, we take a neutral look at the state-of-the-art technologies for 5G and the ongoing evolutions towards 6G, reviewing the <b>recommendations</b> of the International Mobile Communication vision for 2030 (IMT-2030). We first highlight specific features of IMT 2030, including three IMT-2020 extensions (URLLC+, eMBB+, and mMTC+) and three new innovations (Ubiquitous connectivity and integrating the new capabilities of sensing & AI with communication functionality). Then, we delve into three major challenges in implementing 6G, along with global standardization efforts. Besides, a proof of concept is provided by demonstrating terahertz (THz) signal transmission using Orbital Angular Momentum (OAM) multiplexing, which is one of the potential candidates for 6G and beyond. To inspire further potential research, we conclude by identifying research opportunities and future visions on IMT-2030 <b>recommendations.</b></p></p class="citation"></blockquote><h3 id=34--187204-robust-beamforming-for-ris-aided-communications-gradient-based-manifold-meta-learning-fenghao-zhu-et-al-2024>(3/4 | 187/204) Robust Beamforming for RIS-aided Communications: Gradient-based Manifold Meta Learning (Fenghao Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fenghao Zhu, Xinquan Wang, Chongwen Huang, Zhaohui Yang, Xiaoming Chen, Ahmed Alhammadi, Zhaoyang Zhang, Chau Yuen, Mérouane Debbah. (2024)<br><strong>Robust Beamforming for RIS-aided Communications: Gradient-based Manifold Meta Learning</strong><br><button class=copy-to-clipboard title="Robust Beamforming for RIS-aided Communications: Gradient-based Manifold Meta Learning" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10626v1.pdf filename=2402.10626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surface (RIS) has become a promising technology to realize the programmable wireless environment via steering the incident signal in fully customizable ways. However, a major challenge in RIS-aided communication systems is the simultaneous design of the precoding matrix at the base station (BS) and the phase shifting matrix of the RIS elements. This is mainly attributed to the highly non-convex optimization space of variables at both the BS and the RIS, and the diversity of communication environments. Generally, traditional optimization methods for this problem suffer from the high complexity, while existing deep learning based methods are lack of robustness in various scenarios. To address these issues, we introduce a gradient-based manifold <b>meta</b> <b>learning</b> method (GMML), which works without pre-training and has strong robustness for RIS-aided communications. Specifically, the proposed method fuses <b>meta</b> <b>learning</b> and manifold learning to improve the overall spectral efficiency, and reduce the overhead of the high-dimensional signal process. Unlike traditional deep learning based methods which directly take channel state information as input, GMML feeds the gradients of the precoding matrix and phase shifting matrix into neural networks. Coherently, we design a differential regulator to constrain the phase shifting matrix of the RIS. Numerical results show that the proposed GMML can improve the spectral efficiency by up to 7.31%, and speed up the convergence by 23 times faster compared to traditional approaches. Moreover, they also demonstrate remarkable robustness and adaptability in dynamic settings.</p></p class="citation"></blockquote><h3 id=44--188204-bayesian-learning-for-double-ris-aided-isac-systems-with-superimposed-pilots-and-data-xu-gan-et-al-2024>(4/4 | 188/204) Bayesian Learning for Double-RIS Aided ISAC Systems with Superimposed Pilots and Data (Xu Gan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Gan, Chongwen Huang, Zhaohui Yang, Caijun Zhong, Xiaoming Chen, Zhaoyang Zhang, Qinghua Guo, Chau Yuen, Merouane Debbah. (2024)<br><strong>Bayesian Learning for Double-RIS Aided ISAC Systems with Superimposed Pilots and Data</strong><br><button class=copy-to-clipboard title="Bayesian Learning for Double-RIS Aided ISAC Systems with Superimposed Pilots and Data" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10593v1.pdf filename=2402.10593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surface (RIS) has great potential to improve the performance of integrated sensing and communication (ISAC) systems, especially in scenarios where line-of-sight paths between the base station and users are blocked. However, the spectral efficiency (SE) of RIS-aided ISAC uplink transmissions may be drastically reduced by the heavy burden of pilot overhead for realizing sensing capabilities. In this paper, we tackle this bottleneck by proposing a superimposed symbol scheme, which superimposes sensing pilots onto data symbols over the same time-frequency resources. Specifically, we develop a structure-aware sparse Bayesian learning framework, where decoded data symbols serve as side information to enhance sensing performance and increase SE. To meet the low-latency requirements of emerging ISAC applications, we further propose a low-complexity simultaneous communication and localization algorithm for multiple users. This algorithm employs the unitary approximate message passing in the Bayesian learning framework for initial angle estimate, followed by iterative refinements through reduced-dimension matrix calculations. Moreover, the sparse code multiple access technology is incorporated into this iterative framework for accurate data detection which also facilitates localization. Numerical results show that the proposed superimposed symbol-based scheme empowered by the developed algorithm can achieve centimeter-level localization while attaining up to $96%$ of the SE of conventional communications without sensing capabilities. Moreover, compared to other typical ISAC schemes, the proposed superimposed symbol scheme can provide an effective throughput improvement over $133%$.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--189204-generative-ai-and-attentive-user-interfaces-five-strategies-to-enhance-take-over-quality-in-automated-driving-patrick-ebel-2024>(1/3 | 189/204) Generative AI and Attentive User Interfaces: Five Strategies to Enhance Take-Over Quality in Automated Driving (Patrick Ebel, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Ebel. (2024)<br><strong>Generative AI and Attentive User Interfaces: Five Strategies to Enhance Take-Over Quality in Automated Driving</strong><br><button class=copy-to-clipboard title="Generative AI and Attentive User Interfaces: Five Strategies to Enhance Take-Over Quality in Automated Driving" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Generative AI, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10664v1.pdf filename=2402.10664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the automotive world moves toward higher levels of driving automation, Level 3 automated driving represents a critical juncture. In Level 3 driving, vehicles can drive alone under limited conditions, but drivers are expected to be ready to take over when the system requests. Assisting the driver to maintain an appropriate level of Situation Awareness (SA) in such contexts becomes a critical task. This position paper explores the potential of Attentive User Interfaces (AUIs) powered by <b>generative</b> <b>Artificial</b> Intelligence (AI) to address this need. Rather than relying on overt notifications, we argue that AUIs based on novel AI technologies such as <b>large</b> <b>language</b> <b>models</b> or diffusion models can be used to improve SA in an unconscious and subtle way without negative effects on drivers overall workload. Accordingly, we propose 5 strategies how <b>generative</b> <b>AI</b> s can be used to improve the quality of takeovers and, ultimately, road safety.</p></p class="citation"></blockquote><h3 id=23--190204-how-people-prompt-to-create-interactive-vr-scenes-setareh-aghel-manesh-et-al-2024>(2/3 | 190/204) How People Prompt to Create Interactive VR Scenes (Setareh Aghel Manesh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Setareh Aghel Manesh, Tianyi Zhang, Yuki Onishi, Kotaro Hara, Scott Bateman, Jiannan Li, Anthony Tang. (2024)<br><strong>How People Prompt to Create Interactive VR Scenes</strong><br><button class=copy-to-clipboard title="How People Prompt to Create Interactive VR Scenes" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Generative AI, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10525v1.pdf filename=2402.10525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> tools can provide people with the ability to create virtual environments and scenes with natural language <b>prompts.</b> Yet, how people will formulate such <b>prompts</b> is unclear &ndash; particularly when they inhabit the environment that they are designing. For instance, it is likely that a person might say, &ldquo;Put a chair here&rdquo;, while pointing at a location. If such linguistic features are common to people&rsquo;s <b>prompts,</b> we need to tune models to accommodate them. In this work, we present a wizard-of-oz elicitation study with 22 participants, where we studied people&rsquo;s implicit expectations when verbally <b>prompting</b> such programming agents to create interactive VR scenes. Our findings show that people <b>prompt</b> with several implicit expectations: (1) that agents have an embodied knowledge of the environment; (2) that agents understand embodied <b>prompts</b> by users; (3) that the agents can recall previous states of the scene and the conversation, and that (4) agents have a commonsense understanding of objects in the scene. Further, we found that participants <b>prompt</b> differently when they are <b>prompting</b> in situ (i.e. within the VR environment) versus ex situ (i.e. viewing the VR environment from the outside). To explore how our could be applied, we designed and built Oastaad, a conversational programming agent that allows non-programmers to design interactive VR experiences that they inhabit. Based on these explorations, we outline new opportunities and challenges for conversational programming agents that create VR environments.</p></p class="citation"></blockquote><h3 id=33--191204-llm-comparator-visual-analytics-for-side-by-side-evaluation-of-large-language-models-minsuk-kahng-et-al-2024>(3/3 | 191/204) LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models (Minsuk Kahng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minsuk Kahng, Ian Tenney, Mahima Pushkarna, Michael Xieyang Liu, James Wexler, Emily Reif, Krystal Kallarackal, Minsuk Chang, Michael Terry, Lucas Dixon. (2024)<br><strong>LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models</strong><br><button class=copy-to-clipboard title="LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-HC, cs-LG, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10524v1.pdf filename=2402.10524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present <b>LLM</b> Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a <b>large</b> <b>technology</b> <b>company.</b> This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--192204-network-formation-and-dynamics-among-multi-llms-marios-papachristou-et-al-2024>(1/1 | 192/204) Network Formation and Dynamics Among Multi-LLMs (Marios Papachristou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marios Papachristou, Yuan Yuan. (2024)<br><strong>Network Formation and Dynamics Among Multi-LLMs</strong><br><button class=copy-to-clipboard title="Network Formation and Dynamics Among Multi-LLMs" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-CL, cs-MA, cs-SI, cs.SI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10659v1.pdf filename=2402.10659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple <b>LLMs</b> align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that <b>LLMs</b> demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furthermore, we investigate <b>LLMs&rsquo;</b> decision-making based on real-world networks to compare the strengths of these principles. Our results reveal that triadic closure and homophily have a stronger influence than preferential attachment and that <b>LLMs</b> substantially exceed random guessing in the task of network formation predictions. Overall, our study contributes to the development of socially aware <b>LLMs</b> by shedding light on <b>LLMs&rsquo;</b> network formation behaviors and exploring their impacts on social dynamics and norms.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--193204-an-energy-based-material-model-for-the-simulation-of-shape-memory-alloys-under-complex-boundary-value-problems-c-erdogan-et-al-2024>(1/1 | 193/204) An energy-based material model for the simulation of shape memory alloys under complex boundary value problems (C. Erdogan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>C. Erdogan, T. Bode, P. Junker. (2024)<br><strong>An energy-based material model for the simulation of shape memory alloys under complex boundary value problems</strong><br><button class=copy-to-clipboard title="An energy-based material model for the simulation of shape memory alloys under complex boundary value problems" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10655v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10655v1.pdf filename=2402.10655v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Shape memory alloys are remarkable &lsquo;smart&rsquo; materials used in a broad spectrum of applications, ranging from aerospace to robotics, thanks to their unique thermomechanical coupling capabilities. Given the complex properties of shape memory alloys, which are largely influenced by thermal and mechanical loads, as well as their loading history, predicting their behavior can be challenging. Consequently, there exists a pronounced demand for an efficient material model to simulate the behavior of these alloys. This paper introduces a material model rooted in Hamilton&rsquo;s principle. The key advantages of the presented material model encompass a more accurate depiction of the internal variable evolution and heightened robustness. As such, the proposed material model signifies an advancement in the realistic and efficient <b>simulation</b> of shape memory alloys.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=11--194204-generative-cross-modal-retrieval-memorizing-images-in-multimodal-language-models-for-retrieval-and-beyond-yongqi-li-et-al-2024>(1/1 | 194/204) Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond (Yongqi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongqi Li, Wenjie Wang, Leigang Qu, Liqiang Nie, Wenjie Li, Tat-Seng Chua. (2024)<br><strong>Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond</strong><br><button class=copy-to-clipboard title="Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-AI, cs-CL, cs-CV, cs-IR, cs-MM, cs.MM<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10805v1.pdf filename=2402.10805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to &ldquo;recall&rdquo; the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter step teaches the MLLM to generate the corresponding identifier of the target image, given the textual query input. By memorizing images in MLLMs, we introduce a new paradigm to cross-modal retrieval, distinct from previous discriminative approaches. The experiments demonstrate that the generative paradigm performs effectively and efficiently even with <b>large-scale</b> <b>image</b> <b>candidate</b> sets.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--195204-accelerating-sparse-dnns-based-on-tiled-gemm-cong-guo-et-al-2024>(1/1 | 195/204) Accelerating Sparse DNNs Based on Tiled GEMM (Cong Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cong Guo, Fengchen Xue, Jingwen Leng, Yuxian Qiu, Yue Guan, Weihao Cui, Quan Chen, Minyi Guo. (2024)<br><strong>Accelerating Sparse DNNs Based on Tiled GEMM</strong><br><button class=copy-to-clipboard title="Accelerating Sparse DNNs Based on Tiled GEMM" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10876v1.pdf filename=2402.10876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Network <b>pruning</b> can reduce the computation cost of deep neural network (DNN) models. However, sparse models often produce randomly-distributed weights to maintain accuracy, leading to irregular computations. Consequently, unstructured sparse models cannot achieve meaningful speedup on commodity hardware built for dense matrix computations. Accelerators are usually modified or designed with structured sparsity-optimized architectures for exploiting sparsity. For example, the Ampere architecture introduces a sparse tensor core, which adopts the 2:4 sparsity pattern. We propose a <b>pruning</b> method that builds upon the insight that matrix multiplication generally breaks the large matrix into multiple smaller tiles for parallel execution. We present the tile-wise sparsity pattern, which maintains a structured sparsity pattern at the tile level for efficient execution but allows for irregular <b>pruning</b> at the global scale to maintain high accuracy. In addition, the tile-wise sparsity is implemented at the global memory level, and the 2:4 sparsity executes at the register level inside the sparse tensor core. We can combine these two patterns into a tile-vector-wise (TVW) sparsity pattern to explore more fine-grained sparsity and further accelerate the sparse DNN models. We evaluate the TVW on the GPU, achieving averages of $1.85\times$, $2.75\times$, and $22.18\times$ speedups over the dense model, block sparsity, and unstructured sparsity.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--196204-error-checking-for-sparse-systolic-tensor-arrays-christodoulos-peltekis-et-al-2024>(1/1 | 196/204) Error Checking for Sparse Systolic Tensor Arrays (Christodoulos Peltekis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christodoulos Peltekis, Dionysios Filippas, Giorgos Dimitrakopoulos. (2024)<br><strong>Error Checking for Sparse Systolic Tensor Arrays</strong><br><button class=copy-to-clipboard title="Error Checking for Sparse Systolic Tensor Arrays" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10850v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10850v1.pdf filename=2402.10850v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structured sparsity is an efficient way to prune the complexity of modern Machine Learning (ML) applications and to simplify the handling of sparse data in hardware. In such cases, the acceleration of structured-sparse ML models is handled by sparse systolic tensor arrays. The increasing prevalence of ML in safety-critical systems requires enhancing the sparse tensor arrays with online error detection for managing random hardware failures. Algorithm-based fault tolerance has been proposed as a low-cost mechanism to check online the result of computations against random hardware failures. In this work, we address a key architectural challenge with structured-sparse tensor arrays: how to provide online error checking for a range of structured sparsity levels while maintaining high utilization of the hardware. Experimental results highlight the minimum hardware overhead incurred by the proposed checking logic and its error detection properties after injecting random hardware faults on sparse tensor arrays that execute layers of ResNet50 <b>CNN.</b></p></p class="citation"></blockquote><h2 id=eessas-1>eess.AS (1)</h2><h3 id=11--197204-speaking-in-wavelet-domain-a-simple-and-efficient-approach-to-speed-up-speech-diffusion-model-xiangyu-zhang-et-al-2024>(1/1 | 197/204) Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model (Xiangyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyu Zhang, Daijiao Liu, Hexin Liu, Qiquan Zhang, Hanyu Meng, Leibny Paola Garcia, Eng Siong Chng, Lina Yao. (2024)<br><strong>Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model</strong><br><button class=copy-to-clipboard title="Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, eess-AS, eess.AS<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10642v1.pdf filename=2402.10642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, Denoising Diffusion <b>Probabilistic</b> <b>Models</b> (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their long training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training a key factor in the costs associated with adding or customizing voices often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves comparable or superior performance to the original model in speech synthesis tasks but also demonstrates its versatility. By investigating and utilizing different wavelet bases, our approach proves effective not just in speech synthesis, but also in speech enhancement.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--198204-competitive-equilibrium-for-chores-from-dual-eisenberg-gale-to-a-fast-greedy-lp-based-algorithm-bhaskar-ray-chaudhury-et-al-2024>(1/1 | 198/204) Competitive Equilibrium for Chores: from Dual Eisenberg-Gale to a Fast, Greedy, LP-based Algorithm (Bhaskar Ray Chaudhury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhaskar Ray Chaudhury, Christian Kroer, Ruta Mehta, Tianlong Nan. (2024)<br><strong>Competitive Equilibrium for Chores: from Dual Eisenberg-Gale to a Fast, Greedy, LP-based Algorithm</strong><br><button class=copy-to-clipboard title="Competitive Equilibrium for Chores: from Dual Eisenberg-Gale to a Fast, Greedy, LP-based Algorithm" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT, math-OC<br>Keyword Score: 10<br>Keywords: Karush-Kuhn-Tucker<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10439v1.pdf filename=2402.10439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the computation of competitive equilibrium for Fisher markets with $n$ agents and $m$ divisible chores. Prior work showed that competitive equilibria correspond to the nonzero <b>KKT</b> points of a non-convex analogue of the Eisenberg-Gale convex program. We introduce an analogue of the Eisenberg-Gale dual for chores: we show that all <b>KKT</b> points of this dual correspond to competitive equilibria, and while it is not a dual of the non-convex primal program in a formal sense, the objectives touch at all <b>KKT</b> points. Similar to the primal, the dual has problems from an optimization perspective: there are many feasible directions where the objective tends to positive infinity. We then derive a new constraint for the dual, which restricts optimization to a hyperplane that avoids all these directions. We show that restriction to this hyperplane retains all <b>KKT</b> points, and surprisingly, does not introduce any new ones. This allows, for the first time ever, application of iterative optimization methods over a convex region for computing competitive equilibria for chores. We next introduce a greedy Frank-Wolfe algorithm for optimization over our program and show a state-of-the-art convergence rate to competitive equilibrium. In the case of equal incomes, we show a $\mathcal{\tilde O}(n/\epsilon^2)$ rate of convergence, which improves over the two prior state-of-the-art rates of $\mathcal{\tilde O}(n^3/\epsilon^2)$ for an exterior-point method and $\mathcal{\tilde O}(nm/\epsilon^2)$ for a combinatorial method. Moreover, our method is significantly simpler: each iteration of our method only requires solving a simple linear program. We show through numerical experiments on simulated data and a paper review bidding dataset that our method is extremely practical. This is the first highly practical method for solving competitive equilibrium for Fisher markets with chores.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--199204-towards-benchmarking-of-solidity-verification-tools-massimo-bartoletti-et-al-2024>(1/1 | 199/204) Towards benchmarking of Solidity verification tools (Massimo Bartoletti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Massimo Bartoletti, Fabio Fioravanti, Giulia Matricardi, Roberto Pettinau, Franco Sainas. (2024)<br><strong>Towards benchmarking of Solidity verification tools</strong><br><button class=copy-to-clipboard title="Towards benchmarking of Solidity verification tools" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10750v1.pdf filename=2402.10750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Formal verification of smart contracts has become a hot topic in academic and industrial research, given the growing value of assets managed by decentralized applications and the consequent incentive for adversaries to tamper with them. Most of the current research on the verification of contracts revolves around Solidity, the main high-level language supported by Ethereum and other leading blockchains. Although bug detection tools for Solidity have been proliferating almost since the inception of Ethereum, only in the last few years we have seen verification tools capable of proving that a contract respects some desirable properties. An open issue is how to evaluate and compare the effectiveness of these tools: indeed, the existing <b>benchmarks</b> for general-purpose programming languages cannot be adapted to Solidity, given substantial differences in the programming model and in the desirable properties. We address this problem by proposing an open <b>benchmark</b> for Solidity verification tools. By exploiting our <b>benchmark,</b> we compare two leading tools, SolCMC and Certora, discussing their completeness, soundness and expressiveness limitations.</p></p class="citation"></blockquote><h2 id=csds-3>cs.DS (3)</h2><h3 id=13--200204-hypergraph-connectivity-augmentation-in-strongly-polynomial-time-kristóf-bérczi-et-al-2024>(1/3 | 200/204) Hypergraph Connectivity Augmentation in Strongly Polynomial Time (Kristóf Bérczi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kristóf Bérczi, Karthekeyan Chandrasekaran, Tamás Király, Shubhang Kulkarni. (2024)<br><strong>Hypergraph Connectivity Augmentation in Strongly Polynomial Time</strong><br><button class=copy-to-clipboard title="Hypergraph Connectivity Augmentation in Strongly Polynomial Time" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10861v1.pdf filename=2402.10861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider hypergraph network design problems where the goal is to construct a hypergraph that satisfies certain connectivity requirements. For <b>graph</b> network design problems where the goal is to construct a <b>graph</b> that satisfies certain connectivity requirements, the number of edges in every feasible solution is at most quadratic in the number of vertices. In contrast, for hypergraph network design problems, we might have feasible solutions in which the number of hyperedges is exponential in the number of vertices. This presents an additional technical challenge in hypergraph network design problems compared to <b>graph</b> network design problems: in order to solve the problem in polynomial time, we first need to show that there exists a feasible solution in which the number of hyperedges is polynomial in the input size. The central theme of this work is to show that certain hypergraph network design problems admit solutions in which the number of hyperedges is polynomial in the number of vertices and moreover, can be solved in strongly polynomial time. Our work improves on the previous fastest pseudo-polynomial run-time for these problems. In addition, we develop strongly polynomial time algorithms that return near-uniform hypergraphs as solutions (i.e., every pair of hyperedges differ in size by at most one). As applications of our results, we derive the first strongly polynomial time algorithms for (i) degree-specified hypergraph connectivity augmentation using hyperedges, (ii) degree-specified hypergraph node-to-area connectivity augmentation using hyperedges, and (iii) degree-constrained mixed-hypergraph connectivity augmentation using hyperedges.</p></p class="citation"></blockquote><h3 id=23--201204-core-stability-in-additively-separable-hedonic-games-of-low-treewidth-tesshu-hanaka-et-al-2024>(2/3 | 201/204) Core Stability in Additively Separable Hedonic Games of Low Treewidth (Tesshu Hanaka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tesshu Hanaka, Noleen Köhler, Michael Lampis. (2024)<br><strong>Core Stability in Additively Separable Hedonic Games of Low Treewidth</strong><br><button class=copy-to-clipboard title="Core Stability in Additively Separable Hedonic Games of Low Treewidth" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CC, cs-DS, cs-GT, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10815v1.pdf filename=2402.10815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Additively Separable Hedonic Game (ASHG) are coalition-formation games where we are given a <b>graph</b> whose vertices represent $n$ selfish agents and the weight of each edge $uv$ denotes how much agent $u$ gains (or loses) when she is placed in the same coalition as agent $v$. We revisit the computational complexity of the well-known notion of core stability of ASHGs, where the goal is to construct a partition of the agents into coalitions such that no group of agents would prefer to diverge from the given partition and form a new (blocking) coalition. Since both finding a core stable partition and verifying that a given partition is core stable are intractable problems ($\Sigma_2^p$-complete and coNP-complete respectively) we study their complexity from the point of view of structural parameterized complexity, using standard <b>graph-theoretic</b> parameters, such as treewidth.</p></p class="citation"></blockquote><h3 id=33--202204-streaming-algorithms-for-connectivity-augmentation-ce-jin-et-al-2024>(3/3 | 202/204) Streaming Algorithms for Connectivity Augmentation (Ce Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ce Jin, Michael Kapralov, Sepideh Mahabadi, Ali Vakilian. (2024)<br><strong>Streaming Algorithms for Connectivity Augmentation</strong><br><button class=copy-to-clipboard title="Streaming Algorithms for Connectivity Augmentation" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10806v1.pdf filename=2402.10806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the $k$-connectivity augmentation problem ($k$-CAP) in the single-pass streaming model. Given a $(k-1)$-edge connected <b>graph</b> $G=(V,E)$ that is stored in memory, and a stream of weighted edges $L$ with weights in ${0,1,\dots,W}$, the goal is to choose a minimum weight subset $L&rsquo;\subseteq L$ such that $G&rsquo;=(V,E\cup L&rsquo;)$ is $k$-edge connected. We give a $(2+\epsilon)$-approximation algorithm for this problem which requires to store $O(\epsilon^{-1} n\log n)$ words. Moreover, we show our result is tight: Any algorithm with better than $2$-approximation for the problem requires $\Omega(n^2)$ bits of space even when $k=2$. This establishes a gap between the optimal approximation factor one can obtain in the streaming vs the offline setting for $k$-CAP. We further consider a natural generalization to the fully streaming model where both $E$ and $L$ arrive in the stream in an arbitrary order. We show that this problem has a space lower bound that matches the best possible size of a spanner of the same approximation ratio. Following this, we give improved results for spanners on weighted <b>graphs:</b> We show a streaming algorithm that finds a $(2t-1+\epsilon)$-approximate weighted spanner of size at most $O(\epsilon^{-1} n^{1+1/t}\log n)$ for integer $t$, whereas the best prior streaming algorithm for spanner on weighted <b>graphs</b> had size depending on $\log W$. Using our spanner result, we provide an optimal $O(t)$-approximation for $k$-CAP in the fully streaming model with $O(nk + n^{1+1/t})$ words of space. Finally we apply our results to network design problems such as Steiner tree augmentation problem (STAP), $k$-edge connected spanning subgraph ($k$-ECSS), and the general Survivable Network Design problem (SNDP). In particular, we show a single-pass $O(t\log k)$-approximation for SNDP using $O(kn^{1+1/t})$ words of space, where $k$ is the maximum connectivity requirement.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--203204-covering-a-graph-with-minimal-local-sets-nathan-claudet-et-al-2024>(1/1 | 203/204) Covering a Graph with Minimal Local Sets (Nathan Claudet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathan Claudet, Simon Perdrix. (2024)<br><strong>Covering a Graph with Minimal Local Sets</strong><br><button class=copy-to-clipboard title="Covering a Graph with Minimal Local Sets" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DM, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10678v1.pdf filename=2402.10678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Local sets, a <b>graph</b> structure invariant under local complementation, have been originally introduced in the context of quantum computing for the study of quantum entanglement within the so-called <b>graph</b> state formalism. A local set in a <b>graph</b> is made of a non-empty set of vertices together with its odd neighborhood. We show that any <b>graph</b> can be covered by minimal local sets, i.e. that every vertex is contained in at least one local set that is minimal by inclusion. More precisely, we introduce an algorithm for finding a minimal local set cover in polynomial time. This result is proved by exploring the link between local sets and cut-rank. We prove some additional results on minimal local sets: we give tight bounds on their size, and we show that there can be exponentially many of them in a <b>graph.</b> Finally, we provide an extension of our definitions and our main result to $q$-multigraphs, the graphical counterpart of quantum qudit <b>graph</b> states.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--204204-alphabet-reduction-for-reconfiguration-problems-naoto-ohsaka-2024>(1/1 | 204/204) Alphabet Reduction for Reconfiguration Problems (Naoto Ohsaka, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naoto Ohsaka. (2024)<br><strong>Alphabet Reduction for Reconfiguration Problems</strong><br><button class=copy-to-clipboard title="Alphabet Reduction for Reconfiguration Problems" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DM, cs-DS, cs.CC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10627v1.pdf filename=2402.10627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a reconfiguration analogue of alphabet reduction `a la Dinur (J. ACM, 2007) and its applications. Given a binary constraint <b>graph</b> $G$ and its two satisfying assignments $\psi^\mathsf{ini}$ and $\psi^\mathsf{tar}$, the Maxmin Binary CSP Reconfiguration problem requests to transform $\psi^\mathsf{ini}$ into $\psi^\mathsf{tar}$ by repeatedly changing the value of a single vertex so that the minimum fraction of satisfied edges is maximized. We demonstrate a polynomial-time reduction from Maxmin Binary CSP Reconfiguration with arbitrarily large alphabet size $W \in \mathbb{N}$ to itself with universal alphabet size $W_0 \in \mathbb{N}$ such that 1. the perfect completeness is preserved, and 2. if any reconfiguration for the former violates $\varepsilon$-fraction of edges, then $\Omega(\varepsilon)$-fraction of edges must be unsatisfied during any reconfiguration for the latter. The crux of its construction is the reconfigurability of Hadamard codes, which enables to reconfigure between a pair of codewords, while avoiding getting too close to the other codewords. Combining this alphabet reduction with gap amplification due to Ohsaka (SODA 2024), we are able to amplify the $1$ vs. $1-\varepsilon$ gap for arbitrarily small $\varepsilon \in (0,1)$ up to the $1$ vs. $1-\varepsilon_0$ for some universal $\varepsilon_0 \in (0,1)$ without blowing up the alphabet size. In particular, a $1$ vs. $1-\varepsilon_0$ gap version of Maxmin Binary CSP Reconfiguration with alphabet size $W_0$ is PSPACE-hard only assuming the Reconfiguration Inapproximability Hypothesis posed by Ohsaka (STACS 2023), whose gap parameter can be arbitrarily small. This may not be achieved only by gap amplification of Ohsaka, which makes the alphabet size gigantic depending on the gap value of the hypothesis.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.17</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Bandit Algorithm Basic</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-68>cs.CL (68)</a><ul><li><a href=#168--1204-linkner-linking-local-named-entity-recognition-models-to-large-language-models-using-uncertainty-zhen-zhang-et-al-2024>(1/68 | 1/204) LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty (Zhen Zhang et al., 2024)</a></li><li><a href=#268--2204-large-language-models-as-zero-shot-dialogue-state-tracker-through-function-calling-zekun-li-et-al-2024>(2/68 | 2/204) Large Language Models as Zero-shot Dialogue State Tracker through Function Calling (Zekun Li et al., 2024)</a></li><li><a href=#368--3204-multi-modal-preference-alignment-remedies-regression-of-visual-instruction-tuning-on-language-model-shengzhi-li-et-al-2024>(3/68 | 3/204) Multi-modal preference alignment remedies regression of visual instruction tuning on language model (Shengzhi Li et al., 2024)</a></li><li><a href=#468--4204-multi-cultural-commonsense-knowledge-distillation-tuan-phong-nguyen-et-al-2024>(4/68 | 4/204) Multi-Cultural Commonsense Knowledge Distillation (Tuan-Phong Nguyen et al., 2024)</a></li><li><a href=#568--5204-openfmnav-towards-open-set-zero-shot-object-navigation-via-vision-language-foundation-models-yuxuan-kuang-et-al-2024>(5/68 | 5/204) OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models (Yuxuan Kuang et al., 2024)</a></li><li><a href=#668--6204-bitdistiller-unleashing-the-potential-of-sub-4-bit-llms-via-self-distillation-dayou-du-et-al-2024>(6/68 | 6/204) BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation (Dayou Du et al., 2024)</a></li><li><a href=#768--7204-lets-learn-step-by-step-enhancing-in-context-learning-ability-with-curriculum-learning-yinpeng-liu-et-al-2024>(7/68 | 7/204) Let&rsquo;s Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning (Yinpeng Liu et al., 2024)</a></li><li><a href=#868--8204-assessing-the-reasoning-abilities-of-chatgpt-in-the-context-of-claim-verification-john-dougrez-lewis-et-al-2024>(8/68 | 8/204) Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification (John Dougrez-Lewis et al., 2024)</a></li><li><a href=#968--9204-can-separators-improve-chain-of-thought-prompting-yoonjeong-park-et-al-2024>(9/68 | 9/204) Can Separators Improve Chain-of-Thought Prompting? (Yoonjeong Park et al., 2024)</a></li><li><a href=#1068--10204-how-reliable-are-automatic-evaluation-methods-for-instruction-tuned-llms-ehsan-doostmohammadi-et-al-2024>(10/68 | 10/204) How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs? (Ehsan Doostmohammadi et al., 2024)</a></li><li><a href=#1168--11204-inference-to-the-best-explanation-in-large-language-models-dhairya-dalal-et-al-2024>(11/68 | 11/204) Inference to the Best Explanation in Large Language Models (Dhairya Dalal et al., 2024)</a></li><li><a href=#1268--12204-in-search-of-needles-in-a-10m-haystack-recurrent-memory-finds-what-llms-miss-yuri-kuratov-et-al-2024>(12/68 | 12/204) In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss (Yuri Kuratov et al., 2024)</a></li><li><a href=#1368--13204-ecorank-budget-constrained-text-re-ranking-using-large-language-models-muhammad-shihab-rashid-et-al-2024>(13/68 | 13/204) EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models (Muhammad Shihab Rashid et al., 2024)</a></li><li><a href=#1468--14204-decomposition-for-enhancing-attention-improving-llm-based-text-to-sql-through-workflow-paradigm-yuanzhen-xie-et-al-2024>(14/68 | 14/204) Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm (Yuanzhen Xie et al., 2024)</a></li><li><a href=#1568--15204-jailbreaking-proprietary-large-language-models-using-word-substitution-cipher-divij-handa-et-al-2024>(15/68 | 15/204) Jailbreaking Proprietary Large Language Models using Word Substitution Cipher (Divij Handa et al., 2024)</a></li><li><a href=#1668--16204-a-condensed-transition-graph-framework-for-zero-shot-link-prediction-with-large-language-models-mingchen-li-et-al-2024>(16/68 | 16/204) A Condensed Transition Graph Framework for Zero-shot Link Prediction with Large Language Models (Mingchen Li et al., 2024)</a></li><li><a href=#1768--17204-exploring-precision-and-recall-to-assess-the-quality-and-diversity-of-llms-le-bronnec-florian-et-al-2024>(17/68 | 17/204) Exploring Precision and Recall to assess the quality and diversity of LLMs (Le Bronnec Florian et al., 2024)</a></li><li><a href=#1868--18204-fine-tuning-named-entity-extraction-models-for-the-fantasy-domain-aravinth-sivaganeshan-et-al-2024>(18/68 | 18/204) Fine Tuning Named Entity Extraction Models for the Fantasy Domain (Aravinth Sivaganeshan et al., 2024)</a></li><li><a href=#1968--19204-comparing-hallucination-detection-metrics-for-multilingual-generation-haoqiang-kang-et-al-2024>(19/68 | 19/204) Comparing Hallucination Detection Metrics for Multilingual Generation (Haoqiang Kang et al., 2024)</a></li><li><a href=#2068--20204-construction-of-a-syntactic-analysis-map-for-yi-shui-school-through-text-mining-and-natural-language-processing-research-hanqing-zhao-et-al-2024>(20/68 | 20/204) Construction of a Syntactic Analysis Map for Yi Shui School through Text Mining and Natural Language Processing Research (Hanqing Zhao et al., 2024)</a></li><li><a href=#2168--21204-exploring-hybrid-question-answering-via-program-based-prompting-qi-shi-et-al-2024>(21/68 | 21/204) Exploring Hybrid Question Answering via Program-based Prompting (Qi Shi et al., 2024)</a></li><li><a href=#2268--22204-can-we-verify-step-by-step-for-incorrect-answer-detection-xin-xu-et-al-2024>(22/68 | 22/204) Can We Verify Step by Step for Incorrect Answer Detection? (Xin Xu et al., 2024)</a></li><li><a href=#2368--23204-measuring-and-reducing-llm-hallucination-without-gold-standard-answers-via-expertise-weighting-jiaheng-wei-et-al-2024>(23/68 | 23/204) Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting (Jiaheng Wei et al., 2024)</a></li><li><a href=#2468--24204-understanding-survey-paper-taxonomy-about-large-language-models-via-graph-representation-learning-jun-zhuang-et-al-2024>(24/68 | 24/204) Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning (Jun Zhuang et al., 2024)</a></li><li><a href=#2568--25204-instruction-diversity-drives-generalization-to-unseen-tasks-dylan-zhang-et-al-2024>(25/68 | 25/204) Instruction Diversity Drives Generalization To Unseen Tasks (Dylan Zhang et al., 2024)</a></li><li><a href=#2668--26204-quantifying-the-persona-effect-in-llm-simulations-tiancheng-hu-et-al-2024>(26/68 | 26/204) Quantifying the Persona Effect in LLM Simulations (Tiancheng Hu et al., 2024)</a></li><li><a href=#2768--27204-rethinking-human-like-translation-strategy-integrating-drift-diffusion-model-with-large-language-models-for-machine-translation-hongbin-na-et-al-2024>(27/68 | 27/204) Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion Model with Large Language Models for Machine Translation (Hongbin Na et al., 2024)</a></li><li><a href=#2868--28204-multipot-multilingual-program-of-thoughts-harnesses-multiple-programming-languages-xianzhen-luo-et-al-2024>(28/68 | 28/204) MultiPoT: Multilingual Program of Thoughts Harnesses Multiple Programming Languages (Xianzhen Luo et al., 2024)</a></li><li><a href=#2968--29204-german-text-simplification-finetuning-large-language-models-with-semi-synthetic-data-lars-klöser-et-al-2024>(29/68 | 29/204) German Text Simplification: Finetuning Large Language Models with Semi-Synthetic Data (Lars Klöser et al., 2024)</a></li><li><a href=#3068--30204-generalizability-of-mixture-of-domain-specific-adapters-from-the-lens-of-signed-weight-directions-and-its-application-to-effective-model-pruning-tuc-nguyen-et-al-2024>(30/68 | 30/204) Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning (Tuc Nguyen et al., 2024)</a></li><li><a href=#3168--31204-can-llms-speak-for-diverse-people-tuning-llms-via-debate-to-generate-controllable-controversial-statements-ming-li-et-al-2024>(31/68 | 31/204) Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements (Ming Li et al., 2024)</a></li><li><a href=#3268--32204-insaaf-incorporating-safety-through-accuracy-and-fairness--are-llms-ready-for-the-indian-legal-domain-yogesh-tripathi-et-al-2024>(32/68 | 32/204) InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain? (Yogesh Tripathi et al., 2024)</a></li><li><a href=#3368--33204-pushing-the-limits-of-zero-shot-end-to-end-speech-translation-ioannis-tsiamas-et-al-2024>(33/68 | 33/204) Pushing the Limits of Zero-shot End-to-End Speech Translation (Ioannis Tsiamas et al., 2024)</a></li><li><a href=#3468--34204-genres-rethinking-evaluation-for-generative-relation-extraction-in-the-era-of-large-language-models-pengcheng-jiang-et-al-2024>(34/68 | 34/204) GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models (Pengcheng Jiang et al., 2024)</a></li><li><a href=#3568--35204-disordered-dabs-a-benchmark-for-dynamic-aspect-based-summarization-in-disordered-texts-xiaobo-guo-et-al-2024>(35/68 | 35/204) Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts (Xiaobo Guo et al., 2024)</a></li><li><a href=#3668--36204-when-is-tree-search-useful-for-llm-planning-it-depends-on-the-discriminator-ziru-chen-et-al-2024>(36/68 | 36/204) When is Tree Search Useful for LLM Planning? It Depends on the Discriminator (Ziru Chen et al., 2024)</a></li><li><a href=#3768--37204-toolsword-unveiling-safety-issues-of-large-language-models-in-tool-learning-across-three-stages-junjie-ye-et-al-2024>(37/68 | 37/204) ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages (Junjie Ye et al., 2024)</a></li><li><a href=#3868--38204-longheads-multi-head-attention-is-secretly-a-long-context-processor-yi-lu-et-al-2024>(38/68 | 38/204) LongHeads: Multi-Head Attention is Secretly a Long Context Processor (Yi Lu et al., 2024)</a></li><li><a href=#3968--39204-improving-demonstration-diversity-by-human-free-fusing-for-text-to-sql-dingzirui-wang-et-al-2024>(39/68 | 39/204) Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL (Dingzirui Wang et al., 2024)</a></li><li><a href=#4068--40204-enhancing-numerical-reasoning-with-the-guidance-of-reliable-reasoning-processes-dingzirui-wang-et-al-2024>(40/68 | 40/204) Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes (Dingzirui Wang et al., 2024)</a></li><li><a href=#4168--41204-retrieve-only-when-it-needs-adaptive-retrieval-augmentation-for-hallucination-mitigation-in-large-language-models-hanxing-ding-et-al-2024>(41/68 | 41/204) Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models (Hanxing Ding et al., 2024)</a></li><li><a href=#4268--42204-efficiency-at-scale-investigating-the-performance-of-diminutive-language-models-in-clinical-tasks-niall-taylor-et-al-2024>(42/68 | 42/204) Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks (Niall Taylor et al., 2024)</a></li><li><a href=#4368--43204-direct-preference-optimization-with-an-offset-afra-amini-et-al-2024>(43/68 | 43/204) Direct Preference Optimization with an Offset (Afra Amini et al., 2024)</a></li><li><a href=#4468--44204-zero-shot-sampling-of-adversarial-entities-in-biomedical-question-answering-r-patrick-xian-et-al-2024>(44/68 | 44/204) Zero-shot sampling of adversarial entities in biomedical question answering (R. Patrick Xian et al., 2024)</a></li><li><a href=#4568--45204-steering-conversational-large-language-models-for-long-emotional-support-conversations-navid-madani-et-al-2024>(45/68 | 45/204) Steering Conversational Large Language Models for Long Emotional Support Conversations (Navid Madani et al., 2024)</a></li><li><a href=#4668--46204-i-am-not-them-fluid-identities-and-persistent-out-group-bias-in-large-language-models-wenchao-dong-et-al-2024>(46/68 | 46/204) I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models (Wenchao Dong et al., 2024)</a></li><li><a href=#4768--47204-understanding-in-context-learning-with-a-pelican-soup-framework-ting-rui-chiang-et-al-2024>(47/68 | 47/204) Understanding In-Context Learning with a Pelican Soup Framework (Ting-Rui Chiang et al., 2024)</a></li><li><a href=#4868--48204-datadreamer-a-tool-for-synthetic-data-generation-and-reproducible-llm-workflows-ajay-patel-et-al-2024>(48/68 | 48/204) DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows (Ajay Patel et al., 2024)</a></li><li><a href=#4968--49204-enhancing-role-playing-systems-through-aggressive-queries-evaluation-and-improvement-yihong-tang-et-al-2024>(49/68 | 49/204) Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement (Yihong Tang et al., 2024)</a></li><li><a href=#5068--50204-conversational-simulmt-efficient-simultaneous-translation-with-large-language-models-minghan-wang-et-al-2024>(50/68 | 50/204) Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models (Minghan Wang et al., 2024)</a></li><li><a href=#5168--51204-chain-of-logic-rule-based-reasoning-with-large-language-models-sergio-servantez-et-al-2024>(51/68 | 51/204) Chain of Logic: Rule-Based Reasoning with Large Language Models (Sergio Servantez et al., 2024)</a></li><li><a href=#5268--52204-time-series-forecasting-with-llms-understanding-and-enhancing-model-capabilities-mingyu-jin-et-al-2024>(52/68 | 52/204) Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities (Mingyu Jin et al., 2024)</a></li><li><a href=#5368--53204-distillation-enhanced-generative-retrieval-yongqi-li-et-al-2024>(53/68 | 53/204) Distillation Enhanced Generative Retrieval (Yongqi Li et al., 2024)</a></li><li><a href=#5468--54204-an-empirical-study-on-cross-lingual-vocabulary-adaptation-for-efficient-generative-llm-inference-atsuki-yamaguchi-et-al-2024>(54/68 | 54/204) An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference (Atsuki Yamaguchi et al., 2024)</a></li><li><a href=#5568--55204-opening-the-black-box-of-large-language-models-two-views-on-holistic-interpretability-haiyan-zhao-et-al-2024>(55/68 | 55/204) Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability (Haiyan Zhao et al., 2024)</a></li><li><a href=#5668--56204-humans-or-llms-as-the-judge-a-study-on-judgement-biases-guiming-hardy-chen-et-al-2024>(56/68 | 56/204) Humans or LLMs as the Judge? A Study on Judgement Biases (Guiming Hardy Chen et al., 2024)</a></li><li><a href=#5768--57204-absinstruct-eliciting-abstraction-ability-from-llms-through-explanation-tuning-with-plausibility-estimation-zhaowei-wang-et-al-2024>(57/68 | 57/204) AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation (Zhaowei Wang et al., 2024)</a></li><li><a href=#5868--58204-do-llamas-work-in-english-on-the-latent-language-of-multilingual-transformers-chris-wendler-et-al-2024>(58/68 | 58/204) Do Llamas Work in English? On the Latent Language of Multilingual Transformers (Chris Wendler et al., 2024)</a></li><li><a href=#5968--59204-threads-of-subtlety-detecting-machine-generated-texts-through-discourse-motifs-zae-myung-kim-et-al-2024>(59/68 | 59/204) Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs (Zae Myung Kim et al., 2024)</a></li><li><a href=#6068--60204-neural-paraphrasing-by-automatically-crawled-and-aligned-sentence-pairs-achille-globo-et-al-2024>(60/68 | 60/204) Neural paraphrasing by automatically crawled and aligned sentence pairs (Achille Globo et al., 2024)</a></li><li><a href=#6168--61204-strong-hallucinations-from-negation-and-how-to-fix-them-nicholas-asher-et-al-2024>(61/68 | 61/204) Strong hallucinations from negation and how to fix them (Nicholas Asher et al., 2024)</a></li><li><a href=#6268--62204-properties-and-challenges-of-llm-generated-explanations-jenny-kunz-et-al-2024>(62/68 | 62/204) Properties and Challenges of LLM-Generated Explanations (Jenny Kunz et al., 2024)</a></li><li><a href=#6368--63204-evaluating-and-improving-continual-learning-in-spoken-language-understanding-muqiao-yang-et-al-2024>(63/68 | 63/204) Evaluating and Improving Continual Learning in Spoken Language Understanding (Muqiao Yang et al., 2024)</a></li><li><a href=#6468--64204-reviewer2-optimizing-review-generation-through-prompt-generation-zhaolin-gao-et-al-2024>(64/68 | 64/204) Reviewer2: Optimizing Review Generation Through Prompt Generation (Zhaolin Gao et al., 2024)</a></li><li><a href=#6568--65204-incremental-sequence-labeling-a-tale-of-two-shifts-shengjie-qiu-et-al-2024>(65/68 | 65/204) Incremental Sequence Labeling: A Tale of Two Shifts (Shengjie Qiu et al., 2024)</a></li><li><a href=#6668--66204-smaller-language-models-are-capable-of-selecting-instruction-tuning-training-data-for-larger-language-models-dheeraj-mekala-et-al-2024>(66/68 | 66/204) Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models (Dheeraj Mekala et al., 2024)</a></li><li><a href=#6768--67204-dell-generating-reactions-and-explanations-for-llm-based-misinformation-detection-herun-wan-et-al-2024>(67/68 | 67/204) DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection (Herun Wan et al., 2024)</a></li><li><a href=#6868--68204-enhancing-esg-impact-type-identification-through-early-fusion-and-multilingual-models-hariram-veeramani-et-al-2024>(68/68 | 68/204) Enhancing ESG Impact Type Identification through Early Fusion and Multilingual Models (Hariram Veeramani et al., 2024)</a></li></ul></li><li><a href=#cslg-42>cs.LG (42)</a><ul><li><a href=#142--69204-rlvf-learning-from-verbal-feedback-without-overgeneralization-moritz-stephan-et-al-2024>(1/42 | 69/204) RLVF: Learning from Verbal Feedback without Overgeneralization (Moritz Stephan et al., 2024)</a></li><li><a href=#242--70204-symbolic-autoencoding-for-self-supervised-sequence-learning-mohammad-hossein-amani-et-al-2024>(2/42 | 70/204) Symbolic Autoencoding for Self-Supervised Sequence Learning (Mohammad Hossein Amani et al., 2024)</a></li><li><a href=#342--71204-provably-sample-efficient-rlhf-via-active-preference-optimization-nirjhar-das-et-al-2024>(3/42 | 71/204) Provably Sample Efficient RLHF via Active Preference Optimization (Nirjhar Das et al., 2024)</a></li><li><a href=#442--72204-qdylora-quantized-dynamic-low-rank-adaptation-for-efficient-large-language-model-tuning-hossein-rajabzadeh-et-al-2024>(4/42 | 72/204) QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning (Hossein Rajabzadeh et al., 2024)</a></li><li><a href=#542--73204-fedd2s-personalized-data-free-federated-knowledge-distillation-kawa-atapour-et-al-2024>(5/42 | 73/204) FedD2S: Personalized Data-Free Federated Knowledge Distillation (Kawa Atapour et al., 2024)</a></li><li><a href=#642--74204-subgraph-level-universal-prompt-tuning-junhyun-lee-et-al-2024>(6/42 | 74/204) Subgraph-level Universal Prompt Tuning (Junhyun Lee et al., 2024)</a></li><li><a href=#742--75204-edgeqat-entropy-and-distribution-guided-quantization-aware-training-for-the-acceleration-of-lightweight-llms-on-the-edge-xuan-shen-et-al-2024>(7/42 | 75/204) EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge (Xuan Shen et al., 2024)</a></li><li><a href=#842--76204-machine-learning-based-prediction-of-ditching-loads-henning-schwarz-et-al-2024>(8/42 | 76/204) Machine Learning based Prediction of Ditching Loads (Henning Schwarz et al., 2024)</a></li><li><a href=#942--77204-linear-transformers-with-learnable-kernel-functions-are-better-in-context-models-yaroslav-aksenov-et-al-2024>(9/42 | 77/204) Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Yaroslav Aksenov et al., 2024)</a></li><li><a href=#1042--78204-adversarial-curriculum-graph-contrastive-learning-with-pair-wise-augmentation-xinjian-zhao-et-al-2024>(10/42 | 78/204) Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation (Xinjian Zhao et al., 2024)</a></li><li><a href=#1142--79204-masked-attention-is-all-you-need-for-graphs-david-buterez-et-al-2024>(11/42 | 79/204) Masked Attention is All You Need for Graphs (David Buterez et al., 2024)</a></li><li><a href=#1242--80204-pretext-training-algorithms-for-event-sequence-data-yimu-wang-et-al-2024>(12/42 | 80/204) Pretext Training Algorithms for Event Sequence Data (Yimu Wang et al., 2024)</a></li><li><a href=#1342--81204-any-precision-llm-low-cost-deployment-of-multiple-different-sized-llms-yeonhong-park-et-al-2024>(13/42 | 81/204) Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs (Yeonhong Park et al., 2024)</a></li><li><a href=#1442--82204-can-transformers-predict-vibrations-fusataka-kuniyoshi-et-al-2024>(14/42 | 82/204) Can Transformers Predict Vibrations? (Fusataka Kuniyoshi et al., 2024)</a></li><li><a href=#1542--83204-prise-learning-temporal-action-abstractions-as-a-sequence-compression-problem-ruijie-zheng-et-al-2024>(15/42 | 83/204) PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem (Ruijie Zheng et al., 2024)</a></li><li><a href=#1642--84204-personalised-drug-identifier-for-cancer-treatment-with-transformers-using-auxiliary-information-aishwarya-jayagopal-et-al-2024>(16/42 | 84/204) Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information (Aishwarya Jayagopal et al., 2024)</a></li><li><a href=#1742--85204-parametric-augmentation-for-time-series-contrastive-learning-xu-zheng-et-al-2024>(17/42 | 85/204) Parametric Augmentation for Time Series Contrastive Learning (Xu Zheng et al., 2024)</a></li><li><a href=#1842--86204-double-duality-variational-primal-dual-policy-optimization-for-constrained-reinforcement-learning-zihao-li-et-al-2024>(18/42 | 86/204) Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning (Zihao Li et al., 2024)</a></li><li><a href=#1942--87204-multitask-kernel-based-learning-with-logic-constraints-michelangelo-diligenti-et-al-2024>(19/42 | 87/204) Multitask Kernel-based Learning with Logic Constraints (Michelangelo Diligenti et al., 2024)</a></li><li><a href=#2042--88204-timeseriesbench-an-industrial-grade-benchmark-for-time-series-anomaly-detection-models-haotian-si-et-al-2024>(20/42 | 88/204) TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models (Haotian Si et al., 2024)</a></li><li><a href=#2142--89204-fully-differentiable-lagrangian-convolutional-neural-network-for-continuity-consistent-physics-informed-precipitation-nowcasting-peter-pavlík-et-al-2024>(21/42 | 89/204) Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting (Peter Pavlík et al., 2024)</a></li><li><a href=#2242--90204-unlink-to-unlearn-simplifying-edge-unlearning-in-gnns-jiajun-tan-et-al-2024>(22/42 | 90/204) Unlink to Unlearn: Simplifying Edge Unlearning in GNNs (Jiajun Tan et al., 2024)</a></li><li><a href=#2342--91204-logelectra-self-supervised-anomaly-detection-for-unstructured-logs-yuuki-yamanaka-et-al-2024>(23/42 | 91/204) LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs (Yuuki Yamanaka et al., 2024)</a></li><li><a href=#2442--92204-differential-private-federated-transfer-learning-for-mental-health-monitoring-in-everyday-settings-a-case-study-on-stress-detection-ziyu-wang-et-al-2024>(24/42 | 92/204) Differential Private Federated Transfer Learning for Mental Health Monitoring in Everyday Settings: A Case Study on Stress Detection (Ziyu Wang et al., 2024)</a></li><li><a href=#2542--93204-goal-conditioned-offline-reinforcement-learning-via-metric-learning-alfredo-reichlin-et-al-2024>(25/42 | 93/204) Goal-Conditioned Offline Reinforcement Learning via Metric Learning (Alfredo Reichlin et al., 2024)</a></li><li><a href=#2642--94204-physics-informed-meshgraphnets-pi-mgns-neural-finite-element-solvers-for-non-stationary-and-nonlinear-simulations-on-arbitrary-meshes-tobias-würth-et-al-2024>(26/42 | 94/204) Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes (Tobias Würth et al., 2024)</a></li><li><a href=#2742--95204-selective-prediction-for-semantic-segmentation-using-post-hoc-confidence-estimation-and-its-performance-under-distribution-shift-bruno-laboissiere-camargos-borges-et-al-2024>(27/42 | 95/204) Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift (Bruno Laboissiere Camargos Borges et al., 2024)</a></li><li><a href=#2842--96204-contiformer-continuous-time-transformer-for-irregular-time-series-modeling-yuqi-chen-et-al-2024>(28/42 | 96/204) ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling (Yuqi Chen et al., 2024)</a></li><li><a href=#2942--97204-understanding-self-distillation-and-partial-label-learning-in-multi-class-classification-with-label-noise-hyeonsu-jeong-et-al-2024>(29/42 | 97/204) Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise (Hyeonsu Jeong et al., 2024)</a></li><li><a href=#3042--98204-towards-cohesion-fairness-harmony-contrastive-regularization-in-individual-fair-graph-clustering-siamak-ghodsi-et-al-2024>(30/42 | 98/204) Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering (Siamak Ghodsi et al., 2024)</a></li><li><a href=#3142--99204-graph-based-forecasting-with-missing-data-through-spatiotemporal-downsampling-ivan-marisca-et-al-2024>(31/42 | 99/204) Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling (Ivan Marisca et al., 2024)</a></li><li><a href=#3242--100204-policy-learning-for-off-dynamics-rl-with-deficient-support-linh-le-pham-van-et-al-2024>(32/42 | 100/204) Policy Learning for Off-Dynamics RL with Deficient Support (Linh Le Pham Van et al., 2024)</a></li><li><a href=#3342--101204-best-of-three-worlds-adaptive-experimentation-for-digital-marketing-in-practice-tanner-fiez-et-al-2024>(33/42 | 101/204) Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice (Tanner Fiez et al., 2024)</a></li><li><a href=#3442--102204-trading-off-consistency-and-dimensionality-of-convex-surrogates-for-the-mode-enrique-nueve-et-al-2024>(34/42 | 102/204) Trading off Consistency and Dimensionality of Convex Surrogates for the Mode (Enrique Nueve et al., 2024)</a></li><li><a href=#3542--103204-associative-memories-in-the-feature-space-tommaso-salvatori-et-al-2024>(35/42 | 103/204) Associative Memories in the Feature Space (Tommaso Salvatori et al., 2024)</a></li><li><a href=#3642--104204-diversified-ensembling-an-experiment-in-crowdsourced-machine-learning-ira-globus-harris-et-al-2024>(36/42 | 104/204) Diversified Ensembling: An Experiment in Crowdsourced Machine Learning (Ira Globus-Harris et al., 2024)</a></li><li><a href=#3742--105204-error-feedback-reloaded-from-quadratic-to-arithmetic-mean-of-smoothness-constants-peter-richtárik-et-al-2024>(37/42 | 105/204) Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants (Peter Richtárik et al., 2024)</a></li><li><a href=#3842--106204-understanding-likelihood-of-normalizing-flow-and-image-complexity-through-the-lens-of-out-of-distribution-detection-genki-osada-et-al-2024>(38/42 | 106/204) Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection (Genki Osada et al., 2024)</a></li><li><a href=#3942--107204-one-bit-quantization-and-sparsification-for-multiclass-linear-classification-via-regularized-regression-reza-ghane-et-al-2024>(39/42 | 107/204) One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression (Reza Ghane et al., 2024)</a></li><li><a href=#4042--108204-privacy-for-fairness-information-obfuscation-for-fair-representation-learning-with-local-differential-privacy-songjie-xie-et-al-2024>(40/42 | 108/204) Privacy for Fairness: Information Obfuscation for Fair Representation Learning with Local Differential Privacy (Songjie Xie et al., 2024)</a></li><li><a href=#4142--109204-fedkit-enabling-cross-platform-federated-learning-for-android-and-ios-sichang-he-et-al-2024>(41/42 | 109/204) FedKit: Enabling Cross-Platform Federated Learning for Android and iOS (Sichang He et al., 2024)</a></li><li><a href=#4242--110204-random-projection-layers-for-multidimensional-time-sires-forecasting-chin-chia-michael-yeh-et-al-2024>(42/42 | 110/204) Random Projection Layers for Multidimensional Time Sires Forecasting (Chin-Chia Michael Yeh et al., 2024)</a></li></ul></li><li><a href=#cscv-19>cs.CV (19)</a><ul><li><a href=#119--111204-question-instructed-visual-descriptions-for-zero-shot-video-question-answering-david-romero-et-al-2024>(1/19 | 111/204) Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering (David Romero et al., 2024)</a></li><li><a href=#219--112204-palm2-vadapter-progressively-aligned-language-model-makes-a-strong-vision-language-adapter-junfei-xiao-et-al-2024>(2/19 | 112/204) PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter (Junfei Xiao et al., 2024)</a></li><li><a href=#319--113204-universal-prompt-optimizer-for-safe-text-to-image-generation-zongyu-wu-et-al-2024>(3/19 | 113/204) Universal Prompt Optimizer for Safe Text-to-Image Generation (Zongyu Wu et al., 2024)</a></li><li><a href=#419--114204-biofusionnet-deep-learning-based-survival-risk-stratification-in-er-breast-cancer-through-multifeature-and-multimodal-data-fusion-raktim-kumar-mondol-et-al-2024>(4/19 | 114/204) BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion (Raktim Kumar Mondol et al., 2024)</a></li><li><a href=#519--115204-using-left-and-right-brains-together-towards-vision-and-language-planning-jun-cen-et-al-2024>(5/19 | 115/204) Using Left and Right Brains Together: Towards Vision and Language Planning (Jun Cen et al., 2024)</a></li><li><a href=#619--116204-control-color-multimodal-diffusion-based-interactive-image-colorization-zhexin-liang-et-al-2024>(6/19 | 116/204) Control Color: Multimodal Diffusion-based Interactive Image Colorization (Zhexin Liang et al., 2024)</a></li><li><a href=#719--117204-vatr-choose-your-words-wisely-for-handwritten-text-generation-bram-vanherle-et-al-2024>(7/19 | 117/204) VATr++: Choose Your Words Wisely for Handwritten Text Generation (Bram Vanherle et al., 2024)</a></li><li><a href=#819--118204-fusion-of-diffusion-weighted-mri-and-clinical-data-for-predicting-functional-outcome-after-acute-ischemic-stroke-with-deep-contrastive-learning-chia-ling-tsai-et-al-2024>(8/19 | 118/204) Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning (Chia-Ling Tsai et al., 2024)</a></li><li><a href=#919--119204-dynamic-patch-aware-enrichment-transformer-for-occluded-person-re-identification-xin-zhang-et-al-2024>(9/19 | 119/204) Dynamic Patch-aware Enrichment Transformer for Occluded Person Re-Identification (Xin Zhang et al., 2024)</a></li><li><a href=#1019--120204-codamal-contrastive-domain-adaptation-for-malaria-detection-in-low-cost-microscopes-ishan-rajendrakumar-dave-et-al-2024>(10/19 | 120/204) CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes (Ishan Rajendrakumar Dave et al., 2024)</a></li><li><a href=#1119--121204-stf-spatio-temporal-fusion-module-for-improving-video-object-detection-noreen-anwar-et-al-2024>(11/19 | 121/204) STF: Spatio-Temporal Fusion Module for Improving Video Object Detection (Noreen Anwar et al., 2024)</a></li><li><a href=#1219--122204-efficient-multi-task-uncertainties-for-joint-semantic-segmentation-and-monocular-depth-estimation-steven-landgraf-et-al-2024>(12/19 | 122/204) Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation (Steven Landgraf et al., 2024)</a></li><li><a href=#1319--123204-enhancement-driven-pretraining-for-robust-fingerprint-representation-learning-ekta-gavas-et-al-2024>(13/19 | 123/204) Enhancement-Driven Pretraining for Robust Fingerprint Representation Learning (Ekta Gavas et al., 2024)</a></li><li><a href=#1419--124204-training-class-imbalanced-diffusion-model-via-overlap-optimization-divin-yan-et-al-2024>(14/19 | 124/204) Training Class-Imbalanced Diffusion Model Via Overlap Optimization (Divin Yan et al., 2024)</a></li><li><a href=#1519--125204-pointmamba-a-simple-state-space-model-for-point-cloud-analysis-dingkang-liang-et-al-2024>(15/19 | 125/204) PointMamba: A Simple State Space Model for Point Cloud Analysis (Dingkang Liang et al., 2024)</a></li><li><a href=#1619--126204-pegasus-personalized-generative-3d-avatars-with-composable-attributes-hyunsoo-cha-et-al-2024>(16/19 | 126/204) PEGASUS: Personalized Generative 3D Avatars with Composable Attributes (Hyunsoo Cha et al., 2024)</a></li><li><a href=#1719--127204-compact-and-de-biased-negative-instance-embedding-for-multi-instance-learning-on-whole-slide-image-classification-joohyung-lee-et-al-2024>(17/19 | 127/204) Compact and De-biased Negative Instance Embedding for Multi-Instance Learning on Whole-Slide Image Classification (Joohyung Lee et al., 2024)</a></li><li><a href=#1819--128204-make-a-cheap-scaling-a-self-cascade-diffusion-model-for-higher-resolution-adaptation-lanqing-guo-et-al-2024>(18/19 | 128/204) Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation (Lanqing Guo et al., 2024)</a></li><li><a href=#1919--129204-optimizing-skin-lesion-classification-via-multimodal-data-and-auxiliary-task-integration-mahapara-khurshid-et-al-2024>(19/19 | 129/204) Optimizing Skin Lesion Classification via Multimodal Data and Auxiliary Task Integration (Mahapara Khurshid et al., 2024)</a></li></ul></li><li><a href=#eessiv-6>eess.IV (6)</a><ul><li><a href=#16--130204-weak-mamba-unet-visual-mamba-makes-cnn-and-vit-work-better-for-scribble-based-medical-image-segmentation-ziyang-wang-et-al-2024>(1/6 | 130/204) Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation (Ziyang Wang et al., 2024)</a></li><li><a href=#26--131204-u2mrpd-unsupervised-undersampled-mri-reconstruction-by-prompting-a-large-latent-diffusion-model-ziqi-gao-et-al-2024>(2/6 | 131/204) U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model (Ziqi Gao et al., 2024)</a></li><li><a href=#36--132204-gan-driven-electromagnetic-imaging-of-2-d-dielectric-scatterers-ehtasham-naseer-et-al-2024>(3/6 | 132/204) GAN-driven Electromagnetic Imaging of 2-D Dielectric Scatterers (Ehtasham Naseer et al., 2024)</a></li><li><a href=#46--133204-histosegcap-capsules-for-weakly-supervised-semantic-segmentation-of-histological-tissue-type-in-whole-slide-images-mobina-mansoori-et-al-2024>(4/6 | 133/204) HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images (Mobina Mansoori et al., 2024)</a></li><li><a href=#56--134204-semi-weakly-supervised-neural-network-training-for-medical-image-registration-yiwen-li-et-al-2024>(5/6 | 134/204) Semi-weakly-supervised neural network training for medical image registration (Yiwen Li et al., 2024)</a></li><li><a href=#66--135204-dabs-ls-deep-atlas-based-segmentation-using-regional-level-set-self-supervision-hannah-g-mason-et-al-2024>(6/6 | 135/204) DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set Self-Supervision (Hannah G. Mason et al., 2024)</a></li></ul></li><li><a href=#csro-4>cs.RO (4)</a><ul><li><a href=#14--136204-rag-driver-generalisable-driving-explanations-with-retrieval-augmented-in-context-learning-in-multi-modal-large-language-model-jianhao-yuan-et-al-2024>(1/4 | 136/204) RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model (Jianhao Yuan et al., 2024)</a></li><li><a href=#24--137204-autogptp-affordance-based-task-planning-with-large-language-models-timo-birr-et-al-2024>(2/4 | 137/204) AutoGPT+P: Affordance-based Task Planning with Large Language Models (Timo Birr et al., 2024)</a></li><li><a href=#34--138204-pedipulate-enabling-manipulation-skills-using-a-quadruped-robots-leg-philip-arm-et-al-2024>(3/4 | 138/204) Pedipulate: Enabling Manipulation Skills using a Quadruped Robot&rsquo;s Leg (Philip Arm et al., 2024)</a></li><li><a href=#44--139204-3d-diffuser-actor-policy-diffusion-with-3d-scene-representations-tsung-wei-ke-et-al-2024>(4/4 | 139/204) 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations (Tsung-Wei Ke et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--140204-apcodec-a-neural-audio-codec-with-parallel-amplitude-and-phase-spectrum-encoding-and-decoding-yang-ai-et-al-2024>(1/2 | 140/204) APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding (Yang Ai et al., 2024)</a></li><li><a href=#22--141204-learning-disentangled-audio-representations-through-controlled-synthesis-yusuf-brima-et-al-2024>(2/2 | 141/204) Learning Disentangled Audio Representations through Controlled Synthesis (Yusuf Brima et al., 2024)</a></li></ul></li><li><a href=#q-fincp-2>q-fin.CP (2)</a><ul><li><a href=#12--142204-emoji-driven-crypto-assets-market-reactions-xiaorui-zuo-et-al-2024>(1/2 | 142/204) Emoji Driven Crypto Assets Market Reactions (Xiaorui Zuo et al., 2024)</a></li><li><a href=#22--143204-modelling-crypto-markets-by-multi-agent-reinforcement-learning-johann-lussange-et-al-2024>(2/2 | 143/204) Modelling crypto markets by multi-agent reinforcement learning (Johann Lussange et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--144204-when-dataflow-analysis-meets-large-language-models-chengpeng-wang-et-al-2024>(1/1 | 144/204) When Dataflow Analysis Meets Large Language Models (Chengpeng Wang et al., 2024)</a></li></ul></li><li><a href=#csir-5>cs.IR (5)</a><ul><li><a href=#15--145204-spar-personalized-content-based-recommendation-via-long-engagement-attention-chiyu-zhang-et-al-2024>(1/5 | 145/204) SPAR: Personalized Content-Based Recommendation via Long Engagement Attention (Chiyu Zhang et al., 2024)</a></li><li><a href=#25--146204-umair-fps-user-aware-multi-modal-animation-illustration-recommendation-fusion-with-painting-style-yan-kang-et-al-2024>(2/5 | 146/204) UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation Fusion with Painting Style (Yan Kang et al., 2024)</a></li><li><a href=#35--147204-fairsync-ensuring-amortized-group-exposure-in-distributed-recommendation-retrieval-chen-xu-et-al-2024>(3/5 | 147/204) FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval (Chen Xu et al., 2024)</a></li><li><a href=#45--148204-cognitive-personalized-search-integrating-large-language-models-with-an-efficient-memory-mechanism-yujia-zhou-et-al-2024>(4/5 | 148/204) Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism (Yujia Zhou et al., 2024)</a></li><li><a href=#55--149204-are-id-embeddings-necessary-whitening-pre-trained-text-embeddings-for-effective-sequential-recommendation-lingzi-zhang-et-al-2024>(5/5 | 149/204) Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for Effective Sequential Recommendation (Lingzi Zhang et al., 2024)</a></li></ul></li><li><a href=#q-biobm-3>q-bio.BM (3)</a><ul><li><a href=#13--150204-fusing-neural-and-physical-augment-protein-conformation-sampling-with-tractable-simulations-jiarui-lu-et-al-2024>(1/3 | 150/204) Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations (Jiarui Lu et al., 2024)</a></li><li><a href=#23--151204-generative-ai-for-controllable-protein-sequence-design-a-survey-yiheng-zhu-et-al-2024>(2/3 | 151/204) Generative AI for Controllable Protein Sequence Design: A Survey (Yiheng Zhu et al., 2024)</a></li><li><a href=#33--152204-mfbind-a-multi-fidelity-approach-for-evaluating-drug-compounds-in-practical-generative-modeling-peter-eckmann-et-al-2024>(3/3 | 152/204) MFBind: a Multi-Fidelity Approach for Evaluating Drug Compounds in Practical Generative Modeling (Peter Eckmann et al., 2024)</a></li></ul></li><li><a href=#csse-2>cs.SE (2)</a><ul><li><a href=#12--153204-prompt-learning-for-multi-label-code-smell-detection-a-promising-approach-haiyang-liu-et-al-2024>(1/2 | 153/204) Prompt Learning for Multi-Label Code Smell Detection: A Promising Approach (Haiyang Liu et al., 2024)</a></li><li><a href=#22--154204-language-driven-engineering-an-interdisciplinary-software-development-paradigm-bernhard-steffen-et-al-2024>(2/2 | 154/204) Language-Driven Engineering An Interdisciplinary Software Development Paradigm (Bernhard Steffen et al., 2024)</a></li></ul></li><li><a href=#csai-7>cs.AI (7)</a><ul><li><a href=#17--155204-autosat-automatically-optimize-sat-solvers-via-large-language-models-yiwen-sun-et-al-2024>(1/7 | 155/204) AutoSAT: Automatically Optimize SAT Solvers via Large Language Models (Yiwen Sun et al., 2024)</a></li><li><a href=#27--156204-robust-agents-learn-causal-world-models-jonathan-richens-et-al-2024>(2/7 | 156/204) Robust agents learn causal world models (Jonathan Richens et al., 2024)</a></li><li><a href=#37--157204-grounding-language-about-belief-in-a-bayesian-theory-of-mind-lance-ying-et-al-2024>(3/7 | 157/204) Grounding Language about Belief in a Bayesian Theory-of-Mind (Lance Ying et al., 2024)</a></li><li><a href=#47--158204-explainability-for-machine-learning-models-from-data-adaptability-to-user-perception-julien-delaunay-2024>(4/7 | 158/204) Explainability for Machine Learning Models: From Data Adaptability to User Perception (julien Delaunay, 2024)</a></li><li><a href=#57--159204-on-explaining-unfairness-an-overview-christos-fragkathoulas-et-al-2024>(5/7 | 159/204) On Explaining Unfairness: An Overview (Christos Fragkathoulas et al., 2024)</a></li><li><a href=#67--160204-cloud-kitchen-using-planning-based-composite-ai-to-optimize-food-delivery-process-slavomír-švancár-et-al-2024>(6/7 | 160/204) Cloud Kitchen: Using Planning-based Composite AI to Optimize Food Delivery Process (Slavomír Švancár et al., 2024)</a></li><li><a href=#77--161204-learning-planning-action-models-from-state-traces-tomáš-balyo-et-al-2024>(7/7 | 161/204) Learning Planning Action Models from State Traces (Tomáš Balyo et al., 2024)</a></li></ul></li><li><a href=#eesssp-3>eess.SP (3)</a><ul><li><a href=#13--162204-power-efficient-indoor-localization-using-adaptive-channel-aware-ultra-wideband-dl-tdoa-sagnik-bhattacharya-et-al-2024>(1/3 | 162/204) Power-Efficient Indoor Localization Using Adaptive Channel-aware Ultra-wideband DL-TDOA (Sagnik Bhattacharya et al., 2024)</a></li><li><a href=#23--163204-beamforming-optimization-for-active-ris-aided-multiuser-communications-with-hardware-impairments-zhangjie-peng-et-al-2024>(2/3 | 163/204) Beamforming Optimization for Active RIS-Aided Multiuser Communications With Hardware Impairments (Zhangjie Peng et al., 2024)</a></li><li><a href=#33--164204-a-noisy-beat-is-worth-16-words-a-tiny-transformer-for-low-power-arrhythmia-classification-on-microcontrollers-paola-busia-et-al-2024>(3/3 | 164/204) A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers (Paola Busia et al., 2024)</a></li></ul></li><li><a href=#statml-6>stat.ML (6)</a><ul><li><a href=#16--165204-fixed-confidence-best-arm-identification-in-the-bayesian-setting-kyoungseok-jang-et-al-2024>(1/6 | 165/204) Fixed Confidence Best Arm Identification in the Bayesian Setting (Kyoungseok Jang et al., 2024)</a></li><li><a href=#26--166204-stochastic-localization-via-iterative-posterior-sampling-louis-grenioux-et-al-2024>(2/6 | 166/204) Stochastic Localization via Iterative Posterior Sampling (Louis Grenioux et al., 2024)</a></li><li><a href=#36--167204-generative-modeling-for-tabular-data-via-penalized-optimal-transport-network-wenhui-sophia-lu-et-al-2024>(3/6 | 167/204) Generative Modeling for Tabular Data via Penalized Optimal Transport Network (Wenhui Sophia Lu et al., 2024)</a></li><li><a href=#46--168204-predictive-uncertainty-quantification-via-risk-decompositions-for-strictly-proper-scoring-rules-nikita-kotelevskii-et-al-2024>(4/6 | 168/204) Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules (Nikita Kotelevskii et al., 2024)</a></li><li><a href=#56--169204-conformalized-credal-set-predictors-alireza-javanmardi-et-al-2024>(5/6 | 169/204) Conformalized Credal Set Predictors (Alireza Javanmardi et al., 2024)</a></li><li><a href=#66--170204-performance-gaps-in-multi-view-clustering-under-the-nested-matrix-tensor-model-hugo-lebeau-et-al-2024>(6/6 | 170/204) Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model (Hugo Lebeau et al., 2024)</a></li></ul></li><li><a href=#eesssy-2>eess.SY (2)</a><ul><li><a href=#12--171204-data-driven-abstractions-for-control-systems-rudi-coppola-et-al-2024>(1/2 | 171/204) Data-Driven Abstractions for Control Systems (Rudi Coppola et al., 2024)</a></li><li><a href=#22--172204-autonomous-emergency-braking-with-driver-in-the-loop-torque-vectoring-for-active-learning-benjamin-sullivan-et-al-2024>(2/2 | 172/204) Autonomous Emergency Braking With Driver-In-The-Loop: Torque Vectoring for Active Learning (Benjamin Sullivan et al., 2024)</a></li></ul></li><li><a href=#cscr-3>cs.CR (3)</a><ul><li><a href=#13--173204-proving-membership-in-llm-pretraining-data-via-data-watermarks-johnny-tian-zheng-wei-et-al-2024>(1/3 | 173/204) Proving membership in LLM pretraining data via data watermarks (Johnny Tian-Zheng Wei et al., 2024)</a></li><li><a href=#23--174204-enabling-zero-trust-security-in-iomt-edge-network-maha-ali-allouzi-et-al-2024>(2/3 | 174/204) Enabling Zero Trust Security in IoMT Edge Network (Maha Ali Allouzi et al., 2024)</a></li><li><a href=#33--175204-aim-automated-input-set-minimization-for-metamorphic-security-testing-nazanin-bayati-chaleshtari-et-al-2024>(3/3 | 175/204) AIM: Automated Input Set Minimization for Metamorphic Security Testing (Nazanin Bayati Chaleshtari et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--176204-design-of-2d-skyrmionic-metamaterial-through-controlled-assembly-qichen-xu-et-al-2024>(1/1 | 176/204) Design of 2D Skyrmionic Metamaterial Through Controlled Assembly (Qichen Xu et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#13--177204-probabilistic-on-demand-charging-scheduling-for-isac-assisted-wrsns-with-multiple-mobile-charging-vehicles-muhammad-umar-farooq-qaisar-et-al-2024>(1/3 | 177/204) Probabilistic On-Demand Charging Scheduling for ISAC-Assisted WRSNs with Multiple Mobile Charging Vehicles (Muhammad Umar Farooq Qaisar et al., 2024)</a></li><li><a href=#23--178204-qkdnetsim-improvement-of-the-quantum-network-simulator-for-ns-3-david-soler-et-al-2024>(2/3 | 178/204) QKDNetSim+: Improvement of the Quantum Network Simulator for NS-3 (David Soler et al., 2024)</a></li><li><a href=#33--179204-does-twinning-vehicular-networks-enhance-their-performance-in-dense-areas-sarah-al-shareeda-et-al-2024>(3/3 | 179/204) Does Twinning Vehicular Networks Enhance Their Performance in Dense Areas? (Sarah Al-Shareeda et al., 2024)</a></li></ul></li><li><a href=#statap-1>stat.AP (1)</a><ul><li><a href=#11--180204-agent-based-simulation-evaluation-of-cbd-tolling-a-case-study-from-new-york-city-qingnan-liang-et-al-2024>(1/1 | 180/204) Agent-based Simulation Evaluation of CBD Tolling: A Case Study from New York City (Qingnan Liang et al., 2024)</a></li></ul></li><li><a href=#mathna-3>math.NA (3)</a><ul><li><a href=#13--181204-a-lattice-boltzmann-method-for-non-newtonian-blood-flow-in-coiled-intracranial-aneurysms-medeea-horvat-et-al-2024>(1/3 | 181/204) A lattice Boltzmann method for non-Newtonian blood flow in coiled intracranial aneurysms (Medeea Horvat et al., 2024)</a></li><li><a href=#23--182204-hermite-neural-network-simulation-for-solving-the-2d-schrodinger-equation-kourosh-parand-et-al-2024>(2/3 | 182/204) Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation (Kourosh Parand et al., 2024)</a></li><li><a href=#33--183204-a-predictive-surrogate-model-for-heat-transfer-of-an-impinging-jet-on-a-concave-surface-sajad-salavatidezfouli-et-al-2024>(3/3 | 183/204) A Predictive Surrogate Model for Heat Transfer of an Impinging Jet on a Concave Surface (Sajad Salavatidezfouli et al., 2024)</a></li></ul></li><li><a href=#q-finst-1>q-fin.ST (1)</a><ul><li><a href=#11--184204-ragic-risk-aware-generative-adversarial-model-for-stock-interval-construction-jingyi-gu-et-al-2024>(1/1 | 184/204) RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction (Jingyi Gu et al., 2024)</a></li></ul></li><li><a href=#csit-4>cs.IT (4)</a><ul><li><a href=#14--185204-uncertainty-calibration-and-membership-inference-attacks-an-information-theoretic-perspective-meiyi-zhu-et-al-2024>(1/4 | 185/204) Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective (Meiyi Zhu et al., 2024)</a></li><li><a href=#24--186204-towards-6g-evolution-three-enhancements-three-innovations-and-three-major-challenges-rohit-singh-et-al-2024>(2/4 | 186/204) Towards 6G Evolution: Three Enhancements, Three Innovations, and Three Major Challenges (Rohit Singh et al., 2024)</a></li><li><a href=#34--187204-robust-beamforming-for-ris-aided-communications-gradient-based-manifold-meta-learning-fenghao-zhu-et-al-2024>(3/4 | 187/204) Robust Beamforming for RIS-aided Communications: Gradient-based Manifold Meta Learning (Fenghao Zhu et al., 2024)</a></li><li><a href=#44--188204-bayesian-learning-for-double-ris-aided-isac-systems-with-superimposed-pilots-and-data-xu-gan-et-al-2024>(4/4 | 188/204) Bayesian Learning for Double-RIS Aided ISAC Systems with Superimposed Pilots and Data (Xu Gan et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--189204-generative-ai-and-attentive-user-interfaces-five-strategies-to-enhance-take-over-quality-in-automated-driving-patrick-ebel-2024>(1/3 | 189/204) Generative AI and Attentive User Interfaces: Five Strategies to Enhance Take-Over Quality in Automated Driving (Patrick Ebel, 2024)</a></li><li><a href=#23--190204-how-people-prompt-to-create-interactive-vr-scenes-setareh-aghel-manesh-et-al-2024>(2/3 | 190/204) How People Prompt to Create Interactive VR Scenes (Setareh Aghel Manesh et al., 2024)</a></li><li><a href=#33--191204-llm-comparator-visual-analytics-for-side-by-side-evaluation-of-large-language-models-minsuk-kahng-et-al-2024>(3/3 | 191/204) LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models (Minsuk Kahng et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--192204-network-formation-and-dynamics-among-multi-llms-marios-papachristou-et-al-2024>(1/1 | 192/204) Network Formation and Dynamics Among Multi-LLMs (Marios Papachristou et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--193204-an-energy-based-material-model-for-the-simulation-of-shape-memory-alloys-under-complex-boundary-value-problems-c-erdogan-et-al-2024>(1/1 | 193/204) An energy-based material model for the simulation of shape memory alloys under complex boundary value problems (C. Erdogan et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#11--194204-generative-cross-modal-retrieval-memorizing-images-in-multimodal-language-models-for-retrieval-and-beyond-yongqi-li-et-al-2024>(1/1 | 194/204) Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond (Yongqi Li et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--195204-accelerating-sparse-dnns-based-on-tiled-gemm-cong-guo-et-al-2024>(1/1 | 195/204) Accelerating Sparse DNNs Based on Tiled GEMM (Cong Guo et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--196204-error-checking-for-sparse-systolic-tensor-arrays-christodoulos-peltekis-et-al-2024>(1/1 | 196/204) Error Checking for Sparse Systolic Tensor Arrays (Christodoulos Peltekis et al., 2024)</a></li></ul></li><li><a href=#eessas-1>eess.AS (1)</a><ul><li><a href=#11--197204-speaking-in-wavelet-domain-a-simple-and-efficient-approach-to-speed-up-speech-diffusion-model-xiangyu-zhang-et-al-2024>(1/1 | 197/204) Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model (Xiangyu Zhang et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--198204-competitive-equilibrium-for-chores-from-dual-eisenberg-gale-to-a-fast-greedy-lp-based-algorithm-bhaskar-ray-chaudhury-et-al-2024>(1/1 | 198/204) Competitive Equilibrium for Chores: from Dual Eisenberg-Gale to a Fast, Greedy, LP-based Algorithm (Bhaskar Ray Chaudhury et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--199204-towards-benchmarking-of-solidity-verification-tools-massimo-bartoletti-et-al-2024>(1/1 | 199/204) Towards benchmarking of Solidity verification tools (Massimo Bartoletti et al., 2024)</a></li></ul></li><li><a href=#csds-3>cs.DS (3)</a><ul><li><a href=#13--200204-hypergraph-connectivity-augmentation-in-strongly-polynomial-time-kristóf-bérczi-et-al-2024>(1/3 | 200/204) Hypergraph Connectivity Augmentation in Strongly Polynomial Time (Kristóf Bérczi et al., 2024)</a></li><li><a href=#23--201204-core-stability-in-additively-separable-hedonic-games-of-low-treewidth-tesshu-hanaka-et-al-2024>(2/3 | 201/204) Core Stability in Additively Separable Hedonic Games of Low Treewidth (Tesshu Hanaka et al., 2024)</a></li><li><a href=#33--202204-streaming-algorithms-for-connectivity-augmentation-ce-jin-et-al-2024>(3/3 | 202/204) Streaming Algorithms for Connectivity Augmentation (Ce Jin et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--203204-covering-a-graph-with-minimal-local-sets-nathan-claudet-et-al-2024>(1/1 | 203/204) Covering a Graph with Minimal Local Sets (Nathan Claudet et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--204204-alphabet-reduction-for-reconfiguration-problems-naoto-ohsaka-2024>(1/1 | 204/204) Alphabet Reduction for Reconfiguration Problems (Naoto Ohsaka, 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>