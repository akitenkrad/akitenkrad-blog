<!doctype html><html><head><title>arXiv @ 2024.02.12</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.12"><meta property="og:description" content="Primary Categories cs.AI (5) cs.CE (1) cs.CL (14) cs.CR (4) cs.CV (7) cs.HC (1) cs.IR (1) cs.IT (2) cs.LG (25) cs.MM (1) cs.NE (1) cs.NI (3) cs.RO (2) cs.SD (2) eess.AS (2) eess.IV (2) eess.SY (2) q-bio.BM (1) q-bio.NC (1) q-bio.QM (1) quant-ph (1) stat.ML (2) Keywords keyword cs.CL cs.LG Automatic Speech Recognition 1 BERT 1 Bandit Algorithm 3 Benchmarking 4 6 Data Augmentation 2 Document Classification 1 Federated Learning 4 Few-shot 1 Fine-tuning 3 1 Foundation Model 1 GLUE 1 GPT 1 1 GPT-4 1 1 Gemini 1 Graph Attention Networks 2 Graph Classification 1 Graph Neural Network 2 Hallucination Detection 1 In-context Learning 2 Information Retrieval 1 Instruction Following 1 Instruction Tuning 1 Keyword Extraction 1 Knowledge Distillation 2 Label Smoothing 1 Language Generation 1 Large Language Model 11 8 Machine Unlearning 1 Markov Game 1 Message-Passing 1 Meta Learning 1 Multi-modal 2 2 Neural Machine Translation 1 Out-of-distribution 1 Pre-trained Language Model 2 Prompt 1 1 Pruning 2 Quantization 1 1 Question Answering 1 Reasoning 2 1 Recommendation 1 Recurrent Neural Network 2 Reinforcement Learning 3 Reinforcement Learning from Human Feedback 1 Scaling Law 1 Self-supervised Learning 1 1 Semi-Supervised Learning 1 Simulation 1 Simulator 1 Stochastic Gradient Descent 2 Summarization 4 1 Supervised Learning 4 Text Generation 1 1 Text Summarization 2 Transfer Learning 1 Transformer 2 3 Unsupervised Learning 3 2 Visual Question Answering 2 Word Embedding 2 Word2vec 1 cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240212000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-12T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-12T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.12"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11">arXiv @ 2024.04.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12">arXiv @ 2024.04.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/ title="arXiv @ 2024.04.13">arXiv @ 2024.04.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240414000000/ title="arXiv @ 2024.04.14">arXiv @ 2024.04.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/ title="arXiv @ 2024.04.15">arXiv @ 2024.04.15</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240212000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Monday, Feb 12, 2024</p></div><div class=title><h1>arXiv @ 2024.02.12</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#csai-5>cs.AI (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#cscl-14>cs.CL (14)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#cscr-4>cs.CR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#cscv-7>cs.CV (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#cshc-1>cs.HC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#csir-1>cs.IR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#cslg-25>cs.LG (25)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#csro-2>cs.RO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#eessiv-2>eess.IV (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#eesssy-2>eess.SY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/#statml-2>stat.ML (2)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.LG</th></tr></thead><tbody><tr><td>Automatic Speech Recognition</td><td></td><td>1</td></tr><tr><td>BERT</td><td></td><td>1</td></tr><tr><td>Bandit Algorithm</td><td></td><td>3</td></tr><tr><td>Benchmarking</td><td>4</td><td>6</td></tr><tr><td>Data Augmentation</td><td></td><td>2</td></tr><tr><td>Document Classification</td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td>4</td></tr><tr><td>Few-shot</td><td></td><td>1</td></tr><tr><td>Fine-tuning</td><td>3</td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td>1</td></tr><tr><td>GLUE</td><td>1</td><td></td></tr><tr><td>GPT</td><td>1</td><td>1</td></tr><tr><td>GPT-4</td><td>1</td><td>1</td></tr><tr><td>Gemini</td><td>1</td><td></td></tr><tr><td>Graph Attention Networks</td><td></td><td>2</td></tr><tr><td>Graph Classification</td><td></td><td>1</td></tr><tr><td>Graph Neural Network</td><td></td><td>2</td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td></tr><tr><td>In-context Learning</td><td></td><td>2</td></tr><tr><td>Information Retrieval</td><td>1</td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td></tr><tr><td>Instruction Tuning</td><td></td><td>1</td></tr><tr><td>Keyword Extraction</td><td>1</td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>2</td></tr><tr><td>Label Smoothing</td><td></td><td>1</td></tr><tr><td>Language Generation</td><td>1</td><td></td></tr><tr><td>Large Language Model</td><td>11</td><td>8</td></tr><tr><td>Machine Unlearning</td><td></td><td>1</td></tr><tr><td>Markov Game</td><td></td><td>1</td></tr><tr><td>Message-Passing</td><td></td><td>1</td></tr><tr><td>Meta Learning</td><td></td><td>1</td></tr><tr><td>Multi-modal</td><td>2</td><td>2</td></tr><tr><td>Neural Machine Translation</td><td>1</td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td></td></tr><tr><td>Pre-trained Language Model</td><td>2</td><td></td></tr><tr><td>Prompt</td><td>1</td><td>1</td></tr><tr><td>Pruning</td><td></td><td>2</td></tr><tr><td>Quantization</td><td>1</td><td>1</td></tr><tr><td>Question Answering</td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>2</td><td>1</td></tr><tr><td>Recommendation</td><td></td><td>1</td></tr><tr><td>Recurrent Neural Network</td><td></td><td>2</td></tr><tr><td>Reinforcement Learning</td><td></td><td>3</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td>1</td></tr><tr><td>Scaling Law</td><td></td><td>1</td></tr><tr><td>Self-supervised Learning</td><td>1</td><td>1</td></tr><tr><td>Semi-Supervised Learning</td><td>1</td><td></td></tr><tr><td>Simulation</td><td></td><td>1</td></tr><tr><td>Simulator</td><td></td><td>1</td></tr><tr><td>Stochastic Gradient Descent</td><td>2</td><td></td></tr><tr><td>Summarization</td><td>4</td><td>1</td></tr><tr><td>Supervised Learning</td><td></td><td>4</td></tr><tr><td>Text Generation</td><td>1</td><td>1</td></tr><tr><td>Text Summarization</td><td>2</td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td></tr><tr><td>Transformer</td><td>2</td><td>3</td></tr><tr><td>Unsupervised Learning</td><td>3</td><td>2</td></tr><tr><td>Visual Question Answering</td><td>2</td><td></td></tr><tr><td>Word Embedding</td><td>2</td><td></td></tr><tr><td>Word2vec</td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-14>cs.CL (14)</h2><h3 id=114--181-gemini-goes-to-med-school-exploring-the-capabilities-of-multimodal-large-language-models-on-medical-challenge-problems--hallucinations-ankit-pal-et-al-2024>(1/14 | 1/81) Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations (Ankit Pal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankit Pal, Malaikannan Sankarasubbu. (2024)<br><strong>Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations</strong><br><button class=copy-to-clipboard title="Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-HC, cs-LG, cs.CL<br>Keyword Score: 116<br>Keywords: Multi-modal, Multi-modal, GPT, GPT-4, Gemini, Hallucination Detection, Question Answering, Reasoning, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07023v1.pdf filename=2402.07023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> have the potential to be valuable in the healthcare industry, but it&rsquo;s crucial to verify their safety and effectiveness through rigorous evaluation. For this purpose, we comprehensively evaluated both open-source <b>LLMs</b> and Google&rsquo;s new <b>multimodal</b> <b>LLM</b> called <b>Gemini</b> across Medical <b>reasoning,</b> <b>hallucination</b> <b>detection,</b> and Medical <b>Visual</b> <b>Question</b> <b>Answering</b> tasks. While <b>Gemini</b> showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and <b>GPT-4</b> in diagnostic accuracy. Additionally, <b>Gemini</b> achieved an accuracy of 61.45% on the medical <b>VQA</b> dataset, significantly lower than <b>GPT-4V&rsquo;s</b> score of 88%. Our analysis revealed that <b>Gemini</b> is highly susceptible to <b>hallucinations,</b> <b>overconfidence,</b> and knowledge gaps, which indicate risks if deployed uncritically. We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians. To mitigate risks, we applied <b>prompting</b> strategies that improved performance. Additionally, we facilitated future research and development by releasing a Python module for medical <b>LLM</b> evaluation and establishing a dedicated leaderboard on Hugging Face for medical domain <b>LLMs.</b> Python module can be found at <a href=https://github.com/promptslab/RosettaEval>https://github.com/promptslab/RosettaEval</a></p></p class="citation"></blockquote><h3 id=214--281-should-i-try-multiple-optimizers-when-fine-tuning-pre-trained-transformers-for-nlp-tasks-should-i-tune-their-hyperparameters-nefeli-gkouti-et-al-2024>(2/14 | 2/81) Should I try multiple optimizers when fine-tuning pre-trained Transformers for NLP tasks? Should I tune their hyperparameters? (Nefeli Gkouti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nefeli Gkouti, Prodromos Malakasiotis, Stavros Toumpis, Ion Androutsopoulos. (2024)<br><strong>Should I try multiple optimizers when fine-tuning pre-trained Transformers for NLP tasks? Should I tune their hyperparameters?</strong><br><button class=copy-to-clipboard title="Should I try multiple optimizers when fine-tuning pre-trained Transformers for NLP tasks? Should I tune their hyperparameters?" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Stochastic Gradient Descent, Stochastic Gradient Descent, Transfer Learning, Transformer, GLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06948v1.pdf filename=2402.06948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>NLP research has explored different neural model architectures and sizes, datasets, training objectives, and <b>transfer</b> <b>learning</b> techniques. However, the choice of optimizer during training has not been explored as extensively. Typically, some variant of <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> <b>(SGD)</b> is employed, selected among numerous variants, using unclear criteria, often with minimal or no tuning of the optimizer&rsquo;s hyperparameters. Experimenting with five <b>GLUE</b> datasets, two models (DistilBERT and DistilRoBERTa), and seven popular optimizers <b>(SGD,</b> <b>SGD</b> with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound), we find that when the hyperparameters of the optimizers are tuned, there is no substantial difference in test performance across the five more elaborate (adaptive) optimizers, despite differences in training loss. Furthermore, tuning just the learning rate is in most cases as good as tuning all the hyperparameters. Hence, we recommend picking any of the best-behaved adaptive optimizers (e.g., Adam) and tuning only its learning rate. When no hyperparameter can be tuned, <b>SGD</b> with Momentum is the best choice.</p></p class="citation"></blockquote><h3 id=314--381-gentranslate-large-language-models-are-generative-multilingual-speech-and-machine-translators-yuchen-hu-et-al-2024>(3/14 | 3/81) GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators (Yuchen Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen, Eng Siong Chng. (2024)<br><strong>GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators</strong><br><button class=copy-to-clipboard title="GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs-SD, cs.CL, eess-AS<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Neural Machine Translation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06894v1.pdf filename=2402.06894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have stepped forward the development of multilingual speech and <b>machine</b> <b>translation</b> by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely &ldquo;GenTranslate&rdquo;, which builds upon <b>LLMs</b> to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong <b>reasoning</b> abilities of <b>LLMs,</b> our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support <b>LLM</b> <b>finetuning,</b> we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and <b>machine</b> <b>translation</b> <b>benchmarks</b> (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model.</p></p class="citation"></blockquote><h3 id=414--481-nlp-for-knowledge-discovery-and-information-extraction-from-energetics-corpora-francis-g-vangessel-et-al-2024>(4/14 | 4/81) NLP for Knowledge Discovery and Information Extraction from Energetics Corpora (Francis G. VanGessel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francis G. VanGessel, Efrem Perry, Salil Mohan, Oliver M. Barham, Mark Cavolowsky. (2024)<br><strong>NLP for Knowledge Discovery and Information Extraction from Energetics Corpora</strong><br><button class=copy-to-clipboard title="NLP for Knowledge Discovery and Information Extraction from Energetics Corpora" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cond-mat-mtrl-sci, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Unsupervised Learning, Transformer, Word2vec, Document Classification, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06964v1.pdf filename=2402.06964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a demonstration of the utility of NLP for aiding research into energetic materials and associated systems. The NLP method enables machine understanding of textual data, offering an automated route to knowledge discovery and <b>information</b> <b>extraction</b> from energetics text. We apply three established <b>unsupervised</b> NLP models: Latent Dirichlet Allocation, <b>Word2Vec,</b> and the <b>Transformer</b> to a large curated dataset of energetics-related scientific articles. We demonstrate that each NLP algorithm is capable of identifying energetic topics and concepts, generating a language model which aligns with Subject Matter Expert knowledge. Furthermore, we present a <b>document</b> <b>classification</b> pipeline for energetics text. Our classification pipeline achieves 59-76% accuracy depending on the NLP model used, with the highest performing <b>Transformer</b> model rivaling inter-annotator agreement metrics. The NLP approaches studied in this work can identify concepts germane to energetics and therefore hold promise as a tool for accelerating energetics research efforts and energetics material development.</p></p class="citation"></blockquote><h3 id=514--581-semi-supervised-learning-for-bilingual-lexicon-induction-paul-garnier-et-al-2024>(5/14 | 5/81) Semi-Supervised Learning for Bilingual Lexicon Induction (Paul Garnier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Garnier, Gauthier Guinet. (2024)<br><strong>Semi-Supervised Learning for Bilingual Lexicon Induction</strong><br><button class=copy-to-clipboard title="Semi-Supervised Learning for Bilingual Lexicon Induction" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Semi-Supervised Learning, Unsupervised Learning, Unsupervised Learning, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07028v1.pdf filename=2402.07028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of aligning two sets of continuous <b>word</b> <b>representations,</b> corresponding to languages, to a common space in order to infer a bilingual lexicon. It was recently shown that it is possible to infer such lexicon, without using any parallel data, by aligning <b>word</b> <b>embeddings</b> trained on monolingual data. Such line of work is called <b>unsupervised</b> <b>bilingual</b> induction. By wondering whether it was possible to gain experience in the progressive learning of several languages, we asked ourselves to what extent we could integrate the knowledge of a given set of languages when learning a new one, without having parallel data for the latter. In other <b>words,</b> <b>while</b> keeping the core problem of <b>unsupervised</b> <b>learning</b> in the latest step, we allowed the access to other corpora of idioms, hence the name <b>semi-supervised.</b> <b>This</b> led us to propose a novel formulation, considering the lexicon induction as a ranking problem for which we used recent tools of this machine learning field. Our experiments on standard <b>benchmarks,</b> inferring dictionary from English to more than 20 languages, show that our approach consistently outperforms existing state of the art <b>benchmark.</b> In addition, we deduce from this new scenario several relevant conclusions allowing a better understanding of the alignment phenomenon.</p></p class="citation"></blockquote><h3 id=614--681-investigating-consistency-in-query-based-meeting-summarization-a-comparative-study-of-different-embedding-methods-chen-jia-chen-et-al-2024>(6/14 | 6/81) Investigating Consistency in Query-Based Meeting Summarization: A Comparative Study of Different Embedding Methods (Chen Jia-Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Jia-Chen, Guillem Senabre, Allane Caron. (2024)<br><strong>Investigating Consistency in Query-Based Meeting Summarization: A Comparative Study of Different Embedding Methods</strong><br><button class=copy-to-clipboard title="Investigating Consistency in Query-Based Meeting Summarization: A Comparative Study of Different Embedding Methods" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Text Summarization, Summarization, Summarization, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06907v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06907v1.pdf filename=2402.06907v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With more and more advanced data analysis techniques emerging, people will expect these techniques to be applied in more complex tasks and solve problems in our daily lives. <b>Text</b> <b>Summarization</b> is one of famous applications in Natural Language Processing (NLP) field. It aims to automatically generate summary with important information based on a given context, which is important when you have to deal with piles of documents. <b>Summarization</b> techniques can help capture key points in a short time and bring convenience in works. One of applicable situation is meeting <b>summarization,</b> especially for important meeting that tend to be long, complicated, multi-topic and multi-person. Therefore, when people want to review specific content from a meeting, it will be hard and time-consuming to find the related spans in the meeting transcript. However, most of previous works focus on doing <b>summarization</b> for newsletters, scientific articles&mldr;etc, which have a clear document structure and an official format. For the documents with complex structure like transcripts, we think those works are not quite suitable for meeting <b>summarization.</b> Besides, the consistency of summary is another issue common to be discussed in NLP field. To conquer challenges of meeting <b>summarization,</b> we are inspired by &ldquo;QMSum: A New <b>Benchmark</b> for Query-based Multi-domain Meeting <b>Summarization&rdquo;</b> proposed by Microsoft and we also propose our Locater model designed to extract relevant spans based on given transcript and query, which are then <b>summarized</b> by Summarizer model. Furthermore, we perform a comparative study by applying different <b>word</b> <b>embedding</b> techniques to improve summary consistency.</p></p class="citation"></blockquote><h3 id=714--781-instruct-once-chat-consistently-in-multiple-rounds-an-efficient-tuning-framework-for-dialogue-jian-wang-et-al-2024>(7/14 | 7/81) Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue (Jian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Wang, Chak Tou Leong, Jiashuo Wang, Dongding Lin, Wenjie Li, Xiao-Yong Wei. (2024)<br><strong>Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue</strong><br><button class=copy-to-clipboard title="Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Language Generation, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06967v1.pdf filename=2402.06967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tuning <b>pretrained</b> <b>language</b> <b>models</b> for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other <b>language</b> <b>generation</b> tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner leads to unsatisfactory chat consistency of the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. We propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon <b>large</b> <b>language</b> <b>models,</b> where they utilize utterances round by round in alternating order and are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our framework performs superior to traditional <b>fine-tuning</b> and harbors the tremendous potential for improving dialogue consistency.</p></p class="citation"></blockquote><h3 id=814--881-a-thorough-examination-of-decoding-methods-in-the-era-of-llms-chufan-shi-et-al-2024>(8/14 | 8/81) A Thorough Examination of Decoding Methods in the Era of LLMs (Chufan Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, Wai Lam. (2024)<br><strong>A Thorough Examination of Decoding Methods in the Era of LLMs</strong><br><button class=copy-to-clipboard title="A Thorough Examination of Decoding Methods in the Era of LLMs" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06925v1.pdf filename=2402.06925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of <b>LLMs,</b> evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and <b>quantization.</b> Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts.</p></p class="citation"></blockquote><h3 id=914--981-can-llms-recognize-toxicity-structured-toxicity-investigation-framework-and-semantic-based-metric-hyukhun-koh-et-al-2024>(9/14 | 9/81) Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric (Hyukhun Koh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyukhun Koh, Dohyung Kim, Minwoo Lee, Kyomin Jung. (2024)<br><strong>Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric</strong><br><button class=copy-to-clipboard title="Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06900v1.pdf filename=2402.06900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the pursuit of developing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to <b>out-of-distribution</b> (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on <b>LLMs</b> to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of <b>LLMs</b> to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, <b>LLMs</b> As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an influence on downstream metrics.</p></p class="citation"></blockquote><h3 id=1014--1081-speechclip-self-supervised-multi-task-representation-learning-for-speech-via-clip-and-speech-image-data-hsuan-fu-wang-et-al-2024>(10/14 | 10/81) SpeechCLIP+: Self-supervised multi-task representation learning for speech via CLIP and speech-image data (Hsuan-Fu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hsuan-Fu Wang, Yi-Jen Shih, Heng-Jui Chang, Layne Berry, Puyuan Peng, Hung-yi Lee, Hsin-Min Wang, David Harwath. (2024)<br><strong>SpeechCLIP+: Self-supervised multi-task representation learning for speech via CLIP and speech-image data</strong><br><button class=copy-to-clipboard title="SpeechCLIP+: Self-supervised multi-task representation learning for speech via CLIP and speech-image data" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Keyword Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06959v1.pdf filename=2402.06959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recently proposed visually grounded speech model SpeechCLIP is an innovative framework that bridges speech and text through images via CLIP without relying on text transcription. On this basis, this paper introduces two extensions to SpeechCLIP. First, we apply the Continuous Integrate-and-Fire (CIF) module to replace a fixed number of CLS tokens in the cascaded architecture. Second, we propose a new hybrid architecture that merges the cascaded and parallel architectures of SpeechCLIP into a multi-task learning framework. Our experimental evaluation is performed on the Flickr8k and SpokenCOCO datasets. The results show that in the speech <b>keyword</b> <b>extraction</b> task, the CIF-based cascaded SpeechCLIP model outperforms the previous cascaded SpeechCLIP model using a fixed number of CLS tokens. Furthermore, through our hybrid architecture, cascaded task learning boosts the performance of the parallel branch in image-speech retrieval tasks.</p></p class="citation"></blockquote><h3 id=1114--1181-lifi-lightweight-controlled-text-generation-with-fine-grained-control-codes-chufan-shi-et-al-2024>(11/14 | 11/81) LiFi: Lightweight Controlled Text Generation with Fine-Grained Control Codes (Chufan Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chufan Shi, Deng Cai, Yujiu Yang. (2024)<br><strong>LiFi: Lightweight Controlled Text Generation with Fine-Grained Control Codes</strong><br><button class=copy-to-clipboard title="LiFi: Lightweight Controlled Text Generation with Fine-Grained Control Codes" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Text Generation, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06930v1.pdf filename=2402.06930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving field of <b>text</b> <b>generation,</b> the demand for more precise control mechanisms has become increasingly apparent. To address this need, we present a novel methodology, LIFI, which offers a lightweight approach with fine-grained control for controlled <b>text</b> <b>generation.</b> Unlike previous studies that train <b>pre-trained</b> <b>language</b> <b>models</b> to follow discrete, categorical, and exclusive control codes, LIFI learns controlled <b>text</b> <b>generation</b> under the guidance of continuous, relative, and nonexclusive control codes. These fine-grained codes are automatically derived from an attribute classifier, initially trained with a small amount of labeled data and subsequently employed to label abundant unlabeled data, thus garnering more extensive supervision signals. Moreover, to achieve efficient control, we incorporate the fine-grained control codes with adapters, a parameter- and compute-efficient way to steer a <b>pre-trained</b> <b>language</b> <b>model.</b> We evaluate LIFI on two conventional tasks &ndash; sentiment control and topic control &ndash; and one newly proposed task &ndash; stylistic novel writing. Comprehensive experimental results validate the effectiveness of our proposed methods, demonstrating substantial performance improvements over existing baselines.</p></p class="citation"></blockquote><h3 id=1214--1281-tldr-progress-multi-faceted-literature-exploration-in-text-summarization-shahbaz-syed-et-al-2024>(12/14 | 12/81) TL;DR Progress: Multi-faceted Literature Exploration in Text Summarization (Shahbaz Syed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shahbaz Syed, Khalid Al-Khatib, Martin Potthast. (2024)<br><strong>TL;DR Progress: Multi-faceted Literature Exploration in Text Summarization</strong><br><button class=copy-to-clipboard title="TL;DR Progress: Multi-faceted Literature Exploration in Text Summarization" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Text Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06913v1.pdf filename=2402.06913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents TL;DR Progress, a new tool for exploring the literature on neural <b>text</b> <b>summarization.</b> It organizes 514~papers based on a comprehensive annotation scheme for <b>text</b> <b>summarization</b> approaches and enables fine-grained, faceted search. Each paper was manually annotated to capture aspects such as evaluation metrics, quality dimensions, learning paradigms, challenges addressed, datasets, and document domains. In addition, a succinct indicative summary is provided for each paper, consisting of automatically extracted contextual factors, issues, and proposed solutions. The tool is available online at <a href=https://www.tldr-progress.de>https://www.tldr-progress.de</a>, a demo video at <a href=https://youtu.be/uCVRGFvXUj8>https://youtu.be/uCVRGFvXUj8</a></p></p class="citation"></blockquote><h3 id=1314--1381-history-development-and-principles-of-large-language-models-an-introductory-survey-zhibo-chu-et-al-2024>(13/14 | 13/81) History, Development, and Principles of Large Language Models-An Introductory Survey (Zhibo Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Chengming Li, Xiping Hu, Ruifeng Xu, Min Yang, Wenbin Zhang. (2024)<br><strong>History, Development, and Principles of Large Language Models-An Introductory Survey</strong><br><button class=copy-to-clipboard title="History, Development, and Principles of Large Language Models-An Introductory Survey" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06853v1.pdf filename=2402.06853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models serve as a cornerstone in natural language processing (NLP), utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models (SLMs) to the contemporary landscape of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Notably, the swift evolution of <b>LLMs</b> has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that <b>LLMs</b> offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most <b>LLMs</b> reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of <b>LLMs</b> to assist a broader audience. It strives to facilitate a comprehensive understanding by exploring the historical background of language models and tracing their evolution over time. The survey further investigates the factors influencing the development of <b>LLMs,</b> emphasizing key contributions. Additionally, it concentrates on elucidating the underlying principles of <b>LLMs,</b> equipping audiences with essential theoretical knowledge. The survey also highlights the limitations of existing work and points out promising future directions.</p></p class="citation"></blockquote><h3 id=1414--1481-event-keyed-summarization-william-gantt-et-al-2024>(14/14 | 14/81) Event-Keyed Summarization (William Gantt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Gantt, Alexander Martin, Pavlo Kuchmiichuk, Aaron Steven White. (2024)<br><strong>Event-Keyed Summarization</strong><br><button class=copy-to-clipboard title="Event-Keyed Summarization" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06973v1.pdf filename=2402.06973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce event-keyed <b>summarization</b> (EKS), a novel task that marries traditional <b>summarization</b> and document-level event extraction, with the goal of generating a contextualized summary for a specific event, given a document and an extracted event structure. We introduce a dataset for this task, MUCSUM, consisting of summaries of all events in the classic MUC-4 dataset, along with a set of baselines that comprises both pretrained LM standards in the <b>summarization</b> literature, as well as larger frontier models. We show that ablations that reduce EKS to traditional <b>summarization</b> or structure-to-text yield inferior summaries of target events and that MUCSUM is a robust <b>benchmark</b> for this task. Lastly, we conduct a human evaluation of both reference and model summaries, and provide some detailed analysis of the results.</p></p class="citation"></blockquote><h2 id=cslg-25>cs.LG (25)</h2><h3 id=125--1581-openfedllm-training-large-language-models-on-decentralized-private-data-via-federated-learning-rui-ye-et-al-2024>(1/25 | 15/81) OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning (Rui Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, Siheng Chen. (2024)<br><strong>OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning</strong><br><button class=copy-to-clipboard title="OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-DC, cs-LG, cs-MA, cs.LG<br>Keyword Score: 83<br>Keywords: Benchmarking, Federated Learning, Fine-tuning, GPT, GPT-4, Instruction Following, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06954v1.pdf filename=2402.06954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trained on massive publicly available data, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high-quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary <b>LLMs:</b> collaborative and privacy-preserving <b>LLM</b> training on the underutilized distributed private data via <b>federated</b> <b>learning</b> (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research-friendly framework/codebase, named OpenFedLLM. It covers <b>federated</b> <b>instruction</b> <b>tuning</b> for enhancing <b>instruction-following</b> <b>capability,</b> <b>federated</b> <b>value</b> alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cover 30+ evaluation metrics. Through extensive experiments, we observe that all FL algorithms outperform local training on training <b>LLMs,</b> demonstrating a clear performance improvement across a variety of settings. Notably, in a financial <b>benchmark,</b> Llama2-7B <b>fine-tuned</b> by applying any FL algorithm can outperform <b>GPT-4</b> by a significant margin while the model obtained through individual training cannot, demonstrating strong motivation for clients to participate in FL. The code is available at <a href=https://github.com/rui-ye/OpenFedLLM>https://github.com/rui-ye/OpenFedLLM</a>.</p></p class="citation"></blockquote><h3 id=225--1681-principled-penalty-based-methods-for-bilevel-reinforcement-learning-and-rlhf-han-shen-et-al-2024>(2/25 | 16/81) Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF (Han Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Shen, Zhuoran Yang, Tianyi Chen. (2024)<br><strong>Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF</strong><br><button class=copy-to-clipboard title="Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 70<br>Keywords: Markov Game, Reinforcement Learning, Reinforcement Learning from Human Feedback, Simulation, Simulator, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06886v1.pdf filename=2402.06886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bilevel optimization has been recently applied to many machine learning tasks. However, their applications have been restricted to the <b>supervised</b> <b>learning</b> setting, where static objective functions with benign structures are considered. But bilevel problems such as incentive design, inverse <b>reinforcement</b> <b>learning</b> (RL), and RL from human feedback <b>(RLHF)</b> are often modeled as dynamic objective functions that go beyond the simple static objective structures, which pose significant challenges of using existing bilevel solutions. To tackle this new class of bilevel problems, we introduce the first principled algorithmic framework for solving bilevel RL problems through the lens of penalty formulation. We provide theoretical studies of the problem landscape and its penalty-based (policy) gradient algorithms. We demonstrate the effectiveness of our algorithms via <b>simulations</b> in the Stackelberg <b>Markov</b> <b>game,</b> RL from human feedback and incentive design.</p></p class="citation"></blockquote><h3 id=325--1781-in-context-data-distillation-with-tabpfn-junwei-ma-et-al-2024>(3/25 | 17/81) In-Context Data Distillation with TabPFN (Junwei Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junwei Ma, Valentin Thomas, Guangwei Yu, Anthony Caterini. (2024)<br><strong>In-Context Data Distillation with TabPFN</strong><br><button class=copy-to-clipboard title="In-Context Data Distillation with TabPFN" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Foundation Model, Knowledge Distillation, Transformer, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06971v1.pdf filename=2402.06971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> have revolutionized tasks in computer vision and natural language processing. However, in the realm of tabular data, tree-based models like XGBoost continue to dominate. TabPFN, a <b>transformer</b> model tailored for tabular data, mirrors recent <b>foundation</b> <b>models</b> in its exceptional <b>in-context</b> <b>learning</b> capability, being competitive with XGBoost&rsquo;s performance without the need for task-specific training or hyperparameter tuning. Despite its promise, TabPFN&rsquo;s applicability is hindered by its data size constraint, limiting its use in real-world scenarios. To address this, we present <b>in-context</b> <b>data</b> <b>distillation</b> (ICD), a novel methodology that effectively eliminates these constraints by optimizing TabPFN&rsquo;s context. ICD efficiently enables TabPFN to handle significantly larger datasets with a fixed memory budget, improving TabPFN&rsquo;s quadratic memory complexity but at the cost of a linear number of tuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong performance against established tree-based models and modern deep learning methods on 48 large tabular datasets from OpenML.</p></p class="citation"></blockquote><h3 id=425--1881-generating-chain-of-thoughts-with-a-direct-pairwise-comparison-approach-to-searching-for-the-most-promising-intermediate-thought-zhen-yu-zhang-et-al-2024>(4/25 | 18/81) Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought (Zhen-Yu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen-Yu Zhang, Siwei Han, Huaxiu Yao, Gang Niu, Masashi Sugiyama. (2024)<br><strong>Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought</strong><br><button class=copy-to-clipboard title="Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Bandit Algorithm, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06918v1.pdf filename=2402.06918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To improve the ability of the <b>large</b> <b>language</b> <b>model</b> <b>(LLMs)</b> to handle complex <b>reasoning</b> problems, chain-of-thoughts (CoT) methods were proposed to guide <b>LLMs</b> to reason step-by-step, facilitating problem solving from simple to complex tasks. State-of-the-art approaches for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the <b>LLM,</b> guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the <b>LLM</b> is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik&rsquo;s principle, we propose a novel comparison-based CoT generation algorithm that directly identifies the most promising thoughts with the noisy feedback from the <b>LLM.</b> In each round, we randomly pair intermediate thoughts and directly <b>prompt</b> the <b>LLM</b> to select the more promising one from each pair, allowing us to identify the most promising thoughts through an iterative process. To further model the noise in the comparison, we resort to the techniques of ensemble and dueling <b>bandits</b> and propose two variants of the proposed algorithm. Experiments on three real-world mathematical and <b>reasoning</b> tasks demonstrate the effectiveness of our proposed algorithm and verify the rationale of the direct pairwise comparison.</p></p class="citation"></blockquote><h3 id=525--1981-lirank-industrial-large-scale-ranking-models-at-linkedin-fedor-borisyuk-et-al-2024>(5/25 | 19/81) LiRank: Industrial Large Scale Ranking Models at LinkedIn (Fedor Borisyuk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fedor Borisyuk, Mingzhou Zhou, Qingquan Song, Siyu Zhu, Birjodh Tiwana, Ganesh Parameswaran, Siddharth Dangi, Lars Hertel, Qiang Xiao, Xiaochen Hou, Yunbo Ouyang, Aman Gupta, Sheallika Singh, Dan Liu, Hailing Cheng, Lei Le, Jonathan Hung, Sathiya Keerthi, Ruoyan Wang, Fengyu Zhang, Mohit Kothari, Chen Zhu, Daqi Sun, Yun Dai, Xun Luan, Sirou Zhu, Zhiwei Wang, Neil Daftary, Qianqi Shen, Chengming Jiang, Haichao Wei, Maneesh Varshney, Amol Ghoting, Souvik Ghosh. (2024)<br><strong>LiRank: Industrial Large Scale Ranking Models at LinkedIn</strong><br><button class=copy-to-clipboard title="LiRank: Industrial Large Scale Ranking Models at LinkedIn" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: H-3-3, cs-AI, cs-IR, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Graph Attention Networks, Quantization, Recommendation, Transformer, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06859v1.pdf filename=2402.06859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense <b>Gating,</b> <b>Transformers</b> and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods. To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using <b>quantization</b> and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs <b>Recommendations,</b> and Ads click-through rate (CTR) prediction. We <b>summarize</b> our learnings from various A/B tests by elucidating the most effective technical approaches. These ideas have contributed to relative metrics improvements across the board at LinkedIn: +0.5% member sessions in the Feed, +1.76% qualified job applications for Jobs search and <b>recommendations,</b> and +4.3% for Ads CTR. We hope this work can provide practical insights and solutions for practitioners interested in leveraging large-scale deep ranking systems.</p></p class="citation"></blockquote><h3 id=625--2081-a-tale-of-tails-model-collapse-as-a-change-of-scaling-laws-elvis-dohmatob-et-al-2024>(6/25 | 20/81) A Tale of Tails: Model Collapse as a Change of Scaling Laws (Elvis Dohmatob et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, Julia Kempe. (2024)<br><strong>A Tale of Tails: Model Collapse as a Change of Scaling Laws</strong><br><button class=copy-to-clipboard title="A Tale of Tails: Model Collapse as a Change of Scaling Laws" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Transformer, Text Generation, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07043v1.pdf filename=2402.07043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As AI model size grows, neural <b>scaling</b> <b>laws</b> have become a crucial tool to predict the improvements of <b>large</b> <b>models</b> <b>when</b> increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and <b>text</b> <b>will</b> co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the <b>scaling</b> <b>laws</b> change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of <b>scaling</b> <b>laws.</b> We discover a wide range of decay phenomena, analyzing loss of <b>scaling,</b> <b>shifted</b> <b>scaling</b> <b>with</b> number of generations, the &lsquo;&lsquo;un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by <b>large-scale</b> <b>experiments</b> <b>with</b> a <b>transformer</b> on an arithmetic task and <b>text</b> <b>generation</b> using the <b>large</b> <b>language</b> <b>model</b> Llama2.</p></p class="citation"></blockquote><h3 id=725--2181-clustering-techniques-selection-for-a-hybrid-regression-model-a-case-study-based-on-a-solar-thermal-system-maría-teresa-garcía-ordás-et-al-2024>(7/25 | 21/81) Clustering Techniques Selection for a Hybrid Regression Model: A Case Study Based on a Solar Thermal System (María Teresa García-Ordás et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>María Teresa García-Ordás, Héctor Alaiz-Moretón, José-Luis Casteleiro-Roca, Esteban Jove, José Alberto Benítez-Andrades, Isaías García-Rodríguez, Héctor Quintián, José Luis Calvo-Rolle. (2024)<br><strong>Clustering Techniques Selection for a Hybrid Regression Model: A Case Study Based on a Solar Thermal System</strong><br><button class=copy-to-clipboard title="Clustering Techniques Selection for a Hybrid Regression Model: A Case Study Based on a Solar Thermal System" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-MA, cs.LG<br>Keyword Score: 40<br>Keywords: Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06921v1.pdf filename=2402.06921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work addresses the performance comparison between four clustering techniques with the objective of achieving strong hybrid models in <b>supervised</b> <b>learning</b> tasks. A real dataset from a bio-climatic house named Sotavento placed on experimental wind farm and located in Xermade (Lugo) in Galicia (Spain) has been collected. Authors have chosen the thermal solar generation system in order to study how works applying several cluster methods followed by a regression technique to predict the output temperature of the system. With the objective of defining the quality of each clustering method two possible solutions have been implemented. The first one is based on three <b>unsupervised</b> <b>learning</b> metrics (Silhouette, Calinski-Harabasz and Davies-Bouldin) while the second one, employs the most common error measurements for a regression algorithm such as Multi Layer Perceptron.</p></p class="citation"></blockquote><h3 id=825--2281-discriminative-adversarial-unlearning-rohan-sharma-et-al-2024>(8/25 | 22/81) Discriminative Adversarial Unlearning (Rohan Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohan Sharma, Shijie Zhou, Kaiyi Ji, Changyou Chen. (2024)<br><strong>Discriminative Adversarial Unlearning</strong><br><button class=copy-to-clipboard title="Discriminative Adversarial Unlearning" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Machine Unlearning, Pruning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06864v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06864v2.pdf filename=2402.06864v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel <b>machine</b> <b>unlearning</b> framework founded upon the established principles of the min-max optimization paradigm. We capitalize on the capabilities of strong Membership Inference Attacks (MIA) to facilitate the unlearning of specific samples from a trained model. We consider the scenario of two networks, the attacker $\mathbf{A}$ and the trained defender $\mathbf{D}$ pitted against each other in an adversarial objective, wherein the attacker aims at teasing out the information of the data to be unlearned in order to infer membership, and the defender unlearns to defend the network against the attack, whilst preserving its general performance. The algorithm can be trained end-to-end using backpropagation, following the well known iterative min-max approach in updating the attacker and the defender. We additionally incorporate a <b>self-supervised</b> objective effectively addressing the feature space discrepancies between the forget set and the validation set, enhancing unlearning performance. Our proposed algorithm closely approximates the ideal <b>benchmark</b> of retraining from scratch for both random sample forgetting and class-wise forgetting schemes on standard <b>machine-unlearning</b> <b>datasets.</b> Specifically, on the class unlearning scheme, the method demonstrates near-optimal performance and comprehensively overcomes known methods over the random sample forgetting scheme across all metrics and multiple network <b>pruning</b> strategies.</p></p class="citation"></blockquote><h3 id=925--2381-deepcover-advancing-rnn-test-coverage-and-online-error-prediction-using-state-machine-extraction-pouria-golshanrad-et-al-2024>(9/25 | 23/81) DeepCover: Advancing RNN Test Coverage and Online Error Prediction using State Machine Extraction (Pouria Golshanrad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pouria Golshanrad, Fathiyeh Faghih. (2024)<br><strong>DeepCover: Advancing RNN Test Coverage and Online Error Prediction using State Machine Extraction</strong><br><button class=copy-to-clipboard title="DeepCover: Advancing RNN Test Coverage and Online Error Prediction using State Machine Extraction" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Recurrent Neural Network, Recurrent Neural Network, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06966v1.pdf filename=2402.06966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recurrent</b> <b>neural</b> <b>networks</b> <b>(RNNs)</b> have emerged as powerful tools for processing sequential data in various fields, including natural language processing and <b>speech</b> <b>recognition.</b> However, the lack of explainability in <b>RNN</b> models has limited their interpretability, posing challenges in understanding their internal workings. To address this issue, this paper proposes a methodology for extracting a state machine (SM) from an <b>RNN-based</b> model to provide insights into its internal function. The proposed SM extraction algorithm was assessed using four newly proposed metrics: Purity, Richness, Goodness, and Scale. The proposed methodology along with its assessment metrics contribute to increasing explainability in <b>RNN</b> models by providing a clear representation of their internal decision making process through the extracted SM. In addition to improving the explainability of <b>RNNs,</b> the extracted SM can be used to advance testing and and monitoring of the primary <b>RNN-based</b> model. To enhance <b>RNN</b> testing, we introduce six model coverage criteria based on the extracted SM, serving as metrics for evaluating the effectiveness of test suites designed to analyze the primary model. We also propose a tree-based model to predict the error probability of the primary model for each input based on the extracted SM. We evaluated our proposed online error prediction approach using the MNIST dataset and Mini <b>Speech</b> <b>Commands</b> dataset, achieving an area under the curve (AUC) exceeding 80% for the receiver operating characteristic (ROC) chart.</p></p class="citation"></blockquote><h3 id=1025--2481-learning-attributed-graphlets-predictive-graph-mining-by-graphlets-with-trainable-attribute-tajima-shinji-et-al-2024>(10/25 | 24/81) Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute (Tajima Shinji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tajima Shinji, Ren Sugihara, Ryota Kitahara, Masayuki Karasuyama. (2024)<br><strong>Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute</strong><br><button class=copy-to-clipboard title="Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Graph Classification, Graph Neural Network, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06932v1.pdf filename=2402.06932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>graph</b> <b>classification</b> <b>problem</b> has been widely studied; however, achieving an interpretable model with high predictive performance remains a challenging issue. This paper proposes an interpretable classification algorithm for attributed <b>graph</b> <b>data,</b> <b>called</b> LAGRA (Learning Attributed GRAphlets). LAGRA learns importance weights for small attributed subgraphs, called attributed graphlets (AGs), while simultaneously optimizing their attribute vectors. This enables us to obtain a combination of subgraph structures and their attribute vectors that strongly contribute to discriminating different classes. A significant characteristics of LAGRA is that all the subgraph structures in the training dataset can be considered as a candidate structures of AGs. This approach can explore all the potentially important subgraphs exhaustively, but obviously, a naive implementation can require a large amount of computations. To mitigate this issue, we propose an efficient <b>pruning</b> strategy by combining the proximal gradient descent and a <b>graph</b> <b>mining</b> <b>tree</b> search. Our <b>pruning</b> strategy can ensure that the quality of the solution is maintained compared to the result without <b>pruning.</b> We empirically demonstrate that LAGRA has superior or comparable prediction performance to the standard existing algorithms including <b>graph</b> <b>neural</b> <b>networks,</b> while using only a small number of AGs in an interpretable manner.</p></p class="citation"></blockquote><h3 id=1125--2581-topological-neural-networks-mitigating-the-bottlenecks-of-graph-neural-networks-via-higher-order-interactions-lorenzo-giusti-2024>(11/25 | 25/81) Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions (Lorenzo Giusti, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Giusti. (2024)<br><strong>Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions</strong><br><button class=copy-to-clipboard title="Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Graph Attention Networks, Message-Passing, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06908v1.pdf filename=2402.06908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The irreducible complexity of natural phenomena has led <b>Graph</b> <b>Neural</b> <b>Networks</b> to be employed as a standard model to perform representation learning tasks on <b>graph-structured</b> <b>data.</b> <b>While</b> their capacity to capture local and global patterns is remarkable, the implications associated with long-range and higher-order dependencies pose considerable challenges to such models. This work starts with a theoretical framework to reveal the impact of network&rsquo;s width, depth, and <b>graph</b> <b>topology</b> <b>on</b> the over-squashing phenomena in <b>message-passing</b> neural networks. Then, the work drifts towards, higher-order interactions and multi-relational inductive biases via Topological Neural Networks. Such models propagate messages through higher-dimensional structures, providing shortcuts or additional routes for information flow. With this construction, the underlying computational <b>graph</b> <b>is</b> <b>no</b> longer coupled with the input <b>graph</b> <b>structure,</b> <b>thus</b> mitigating the aforementioned bottlenecks while accounting also for higher-order interactions. Inspired by <b>Graph</b> <b>Attention</b> <b>Networks,</b> two topological attention networks are proposed: Simplicial and Cell Attention Networks. The rationale behind these architecture is to leverage the extended notion of neighbourhoods provided by the arrangement of groups of nodes within a simplicial or cell complex to design anisotropic aggregations able to measure the importance of the information coming from different regions of the domain. By doing so, they capture dependencies that conventional <b>Graph</b> <b>Neural</b> <b>Networks</b> might miss. Finally, a multi-way communication scheme is introduced with Enhanced Cellular Isomorphism Networks, which augment topological message passing schemes to enable a direct interactions among groups of nodes arranged in ring-like structures.</p></p class="citation"></blockquote><h3 id=1225--2681-for-better-or-for-worse-learning-minimum-variance-features-with-label-augmentation-muthu-chidambaram-et-al-2024>(12/25 | 26/81) For Better or For Worse? Learning Minimum Variance Features With Label Augmentation (Muthu Chidambaram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muthu Chidambaram, Rong Ge. (2024)<br><strong>For Better or For Worse? Learning Minimum Variance Features With Label Augmentation</strong><br><button class=copy-to-clipboard title="For Better or For Worse? Learning Minimum Variance Features With Label Augmentation" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Data Augmentation, Label Smoothing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06855v1.pdf filename=2402.06855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>augmentation</b> has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of <b>data</b> <b>augmentation</b> techniques - which includes both <b>label</b> <b>smoothing</b> and Mixup - involves modifying not only the input <b>data</b> <b>but</b> also the input <b>label</b> <b>during</b> model training. In this work, we analyze the role played by the <b>label</b> <b>augmentation</b> aspect of such methods. We prove that linear models on linearly separable <b>data</b> <b>trained</b> with <b>label</b> <b>augmentation</b> learn only the minimum variance features in the <b>data,</b> <b>while</b> standard training (which includes weight decay) can learn higher variance features. An important consequence of our results is negative: <b>label</b> <b>smoothing</b> and Mixup can be less robust to adversarial perturbations of the training <b>data</b> <b>when</b> compared to standard training. We verify that our theory reflects practice via a range of experiments on synthetic <b>data</b> <b>and</b> image classification <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1325--2781-llm-learning-automata-from-examples-using-natural-language-oracles-marcell-vazquez-chanlatte-et-al-2024>(13/25 | 27/81) $L^*LM$: Learning Automata from Examples using Natural Language Oracles (Marcell Vazquez-Chanlatte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcell Vazquez-Chanlatte, Karim Elmaaroufi, Stefan J. Witwicki, Sanjit A. Seshia. (2024)<br>*<em>$L^<em>LM$: Learning Automata from Examples using Natural Language Oracles</em></em><br><button class=copy-to-clipboard title="$L^*LM$: Learning Automata from Examples using Natural Language Oracles" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-FL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Few-shot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07051v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07051v1.pdf filename=2402.07051v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Expert demonstrations have proven an easy way to indirectly specify complex tasks. Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations. Unfortunately, these techniques are generally not sample efficient. In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language. Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages <b>large</b> <b>language</b> <b>models</b> to answer membership queries about the underlying task. This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems. In our experiments, we observe the two modalities complement each other, yielding a powerful <b>few-shot</b> learner.</p></p class="citation"></blockquote><h3 id=1425--2881-distilling-symbolic-priors-for-concept-learning-into-neural-networks-ioana-marinescu-et-al-2024>(14/25 | 28/81) Distilling Symbolic Priors for Concept Learning into Neural Networks (Ioana Marinescu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ioana Marinescu, R. Thomas McCoy, Thomas L. Griffiths. (2024)<br><strong>Distilling Symbolic Priors for Concept Learning into Neural Networks</strong><br><button class=copy-to-clipboard title="Distilling Symbolic Priors for Concept Learning into Neural Networks" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07035v1.pdf filename=2402.07035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans can learn new concepts from a small number of examples by drawing on their inductive biases. These inductive biases have previously been captured by using Bayesian models defined over symbolic hypothesis spaces. Is it possible to create a neural network that displays the same inductive biases? We show that inductive biases that enable rapid concept learning can be instantiated in artificial neural networks by <b>distilling</b> a prior distribution from a symbolic Bayesian model via <b>meta-learning,</b> <b>an</b> approach for extracting the common structure from a set of tasks. By generating the set of tasks used in <b>meta-learning</b> <b>from</b> the prior distribution of a Bayesian model, we are able to transfer that prior into a neural network. We use this approach to create a neural network with an inductive bias towards concepts expressed as short logical formulas. Analyzing results from previous behavioral experiments in which people learned logical concepts from a few examples, we find that our <b>meta-trained</b> <b>models</b> are highly aligned with human performance.</p></p class="citation"></blockquote><h3 id=1525--2981-fiddler-cpu-gpu-orchestration-for-fast-inference-of-mixture-of-experts-models-keisuke-kamahori-et-al-2024>(15/25 | 29/81) Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models (Keisuke Kamahori et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keisuke Kamahori, Yile Gu, Kan Zhu, Baris Kasikci. (2024)<br><strong>Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models</strong><br><button class=copy-to-clipboard title="Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs-OS, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07033v1.pdf filename=2402.07033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}</p></p class="citation"></blockquote><h3 id=1625--3081-non-linear-fusion-in-federated-learning-a-hypernetwork-approach-to-federated-domain-generalization-marc-bartholet-et-al-2024>(16/25 | 30/81) Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization (Marc Bartholet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marc Bartholet, Taehyeon Kim, Ami Beuret, Se-Young Yun, Joachim M. Buhmann. (2024)<br><strong>Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization</strong><br><button class=copy-to-clipboard title="Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06974v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06974v2.pdf filename=2402.06974v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) has emerged as a promising paradigm in which multiple clients collaboratively train a shared global model while preserving data privacy. To create a robust and practicable FL framework, it is crucial to extend its ability to generalize well to unseen domains - a problem referred to as <b>federated</b> <b>Domain</b> Generalization (FDG), being still under-explored. We propose an innovative <b>federated</b> <b>algorithm,</b> termed hFedF for hypernetwork-based <b>Federated</b> <b>Fusion,</b> designed to bridge the performance gap between generalization and personalization, capable of addressing various degrees of domain shift. Essentially, the hypernetwork supports a non-linear fusion of client models enabling a comprehensive understanding of the underlying data distribution. We encompass an extensive discussion and provide novel insights into the tradeoff between personalization and generalization in FL. The proposed algorithm outperforms strong <b>benchmarks</b> on three widely-used data sets for DG in an exceeding number of cases.</p></p class="citation"></blockquote><h3 id=1725--3181-tree-ensembles-for-contextual-bandits-hannes-nilsson-et-al-2024>(17/25 | 31/81) Tree Ensembles for Contextual Bandits (Hannes Nilsson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hannes Nilsson, Rikard Johansson, Niklas Åkerblom, Morteza Haghir Chehreghani. (2024)<br><strong>Tree Ensembles for Contextual Bandits</strong><br><button class=copy-to-clipboard title="Tree Ensembles for Contextual Bandits" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Bandit Algorithm, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06963v1.pdf filename=2402.06963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel framework for contextual multi-armed <b>bandits</b> based on tree ensembles. Our framework integrates two widely used <b>bandit</b> methods, Upper Confidence Bound and Thompson Sampling, for both standard and combinatorial settings. We demonstrate the effectiveness of our framework via several experimental studies, employing XGBoost, a popular tree ensemble method. Compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to <b>benchmark</b> datasets and the real-world application of navigation over road networks.</p></p class="citation"></blockquote><h3 id=1825--3281-solving-deep-reinforcement-learning-benchmarks-with-linear-policy-networks-annie-wong-et-al-2024>(18/25 | 32/81) Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks (Annie Wong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Annie Wong, Jacob de Nobel, Thomas Bäck, Aske Plaat, Anna V. Kononova. (2024)<br><strong>Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks</strong><br><button class=copy-to-clipboard title="Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06912v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06912v1.pdf filename=2402.06912v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although Deep <b>Reinforcement</b> <b>Learning</b> (DRL) methods can learn effective policies for challenging problems such as Atari games and robotics tasks, algorithms are complex and training times are often long. This study investigates how evolution strategies (ES) perform compared to gradient-based deep <b>reinforcement</b> <b>learning</b> methods. We use ES to optimize the weights of a neural network via neuroevolution, performing direct policy search. We <b>benchmark</b> both regular networks and policy networks consisting of a single linear layer from observations to actions; for three classical ES methods and for three gradient-based methods such as PPO. Our results reveal that ES can find effective linear policies for many RL <b>benchmark</b> tasks, in contrast to DRL methods that can only find successful policies using much larger networks, suggesting that current <b>benchmarks</b> are easier to solve than previously assumed. Interestingly, also for higher complexity tasks, ES achieves results comparable to gradient-based DRL algorithms. Furthermore, we find that by directly accessing the memory state of the game, ES are able to find successful policies in Atari, outperforming DQN. While gradient-based methods have dominated the field in recent years, ES offers an alternative that is easy to implement, parallelize, understand, and tune.</p></p class="citation"></blockquote><h3 id=1925--3381-fast-ucb-type-algorithms-for-stochastic-bandits-with-heavy-and-super-heavy-symmetric-noise-yuriy-dorn-et-al-2024>(19/25 | 33/81) Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise (Yuriy Dorn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuriy Dorn, Aleksandr Katrutsa, Ilgam Latypov, Andrey Pudovikov. (2024)<br><strong>Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise</strong><br><button class=copy-to-clipboard title="Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07062v1.pdf filename=2402.07062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we propose a new method for constructing UCB-type algorithms for stochastic multi-armed <b>bandits</b> based on general convex optimization methods with an inexact oracle. We derive the regret bounds corresponding to the convergence rates of the optimization methods. We propose a new algorithm Clipped-SGD-UCB and show, both theoretically and empirically, that in the case of symmetric noise in the reward, we can achieve an $O(\log T\sqrt{KT\log T})$ regret bound instead of $O\left (T^{\frac{1}{1+\alpha}} K^{\frac{\alpha}{1+\alpha}} \right)$ for the case when the reward distribution satisfies $\mathbb{E}_{X \in D}[|X|^{1+\alpha}] \leq \sigma^{1+\alpha}$ ($\alpha \in (0, 1])$, i.e. perform better than it is assumed by the general lower bound for <b>bandits</b> with heavy-tails. Moreover, the same bound holds even when the reward distribution does not have the expectation, that is, when $\alpha&lt;0$.</p></p class="citation"></blockquote><h3 id=2025--3481-understanding-the-training-speedup-from-sampling-with-approximate-losses-rudrajit-das-et-al-2024>(20/25 | 34/81) Understanding the Training Speedup from Sampling with Approximate Losses (Rudrajit Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rudrajit Das, Xi Chen, Bertram Ieong, Parikshit Bansal, Sujay Sanghavi. (2024)<br><strong>Understanding the Training Speedup from Sampling with Approximate Losses</strong><br><button class=copy-to-clipboard title="Understanding the Training Speedup from Sampling with Approximate Losses" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07052v1.pdf filename=2402.07052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is well known that selecting samples with large losses/gradients can significantly reduce the number of training steps. However, the selection overhead is often too high to yield any meaningful gains in terms of overall training time. In this work, we focus on the greedy approach of selecting samples with large \textit{approximate losses} instead of exact losses in order to reduce the selection overhead. For smooth convex losses, we show that such a greedy strategy can converge to a constant factor of the minimum value of the average loss in fewer iterations than the standard approach of random selection. We also theoretically quantify the effect of the approximation level. We then develop SIFT which uses early exiting to obtain approximate losses with an intermediate layer&rsquo;s representations for sample selection. We evaluate SIFT on the task of training a 110M parameter 12-layer <b>BERT</b> base model and show significant gains (in terms of training hours and number of backpropagation steps) without any optimized implementation over vanilla training. For e.g., to reach 64% validation accuracy, SIFT with exit at the first layer takes ~43 hours compared to ~57 hours of vanilla training.</p></p class="citation"></blockquote><h3 id=2125--3581-informativeness-of-reward-functions-in-reinforcement-learning-rati-devidze-et-al-2024>(21/25 | 35/81) Informativeness of Reward Functions in Reinforcement Learning (Rati Devidze et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rati Devidze, Parameswaran Kamalaruban, Adish Singla. (2024)<br><strong>Informativeness of Reward Functions in Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Informativeness of Reward Functions in Reinforcement Learning" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07019v1.pdf filename=2402.07019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reward functions are central in specifying the task we want a <b>reinforcement</b> <b>learning</b> agent to perform. Given a task and desired optimal behavior, we study the problem of designing informative reward functions so that the designed rewards speed up the agent&rsquo;s convergence. In particular, we consider expert-driven reward design settings where an expert or teacher seeks to provide informative and interpretable rewards to a learning agent. Existing works have considered several different reward design formulations; however, the key challenge is formulating a reward informativeness criterion that adapts w.r.t. the agent&rsquo;s current policy and can be optimized under specified structural constraints to obtain interpretable rewards. In this paper, we propose a novel reward informativeness criterion, a quantitative measure that captures how the agent&rsquo;s current policy will improve if it receives rewards from a specific reward function. We theoretically showcase the utility of the proposed informativeness criterion for adaptively designing rewards for an agent. Experimental results on two navigation tasks demonstrate the effectiveness of our adaptive reward informativeness criterion.</p></p class="citation"></blockquote><h3 id=2225--3681-fedimpro-measuring-and-improving-client-update-in-federated-learning-zhenheng-tang-et-al-2024>(22/25 | 36/81) FedImpro: Measuring and Improving Client Update in Federated Learning (Zhenheng Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenheng Tang, Yonggang Zhang, Shaohuai Shi, Xinmei Tian, Tongliang Liu, Bo Han, Xiaowen Chu. (2024)<br><strong>FedImpro: Measuring and Improving Client Update in Federated Learning</strong><br><button class=copy-to-clipboard title="FedImpro: Measuring and Improving Client Update in Federated Learning" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07011v1.pdf filename=2402.07011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Experimental results show that FedImpro can help FL defend against data heterogeneity and enhance the generalization performance of the model.</p></p class="citation"></blockquote><h3 id=2325--3781-clients-collaborate-flexible-differentially-private-federated-learning-with-guaranteed-improvement-of-utility-privacy-trade-off-yuecheng-li-et-al-2024>(23/25 | 37/81) Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off (Yuecheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuecheng Li, Tong Wang, Chuan Chen, Jian Lou, Bin Chen, Lei Yang, Zibin Zheng. (2024)<br><strong>Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off</strong><br><button class=copy-to-clipboard title="Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07002v1.pdf filename=2402.07002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To defend against privacy leakage of user data, differential privacy is widely used in <b>federated</b> <b>learning,</b> but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel <b>federated</b> <b>learning</b> framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients &lsquo;&lsquo;Collaborate with Each Other&rsquo;&rsquo;. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an order of $\sqrt{d}$, where $d$ is the input dimension. We illustrate our theoretical results with experiments on representative image datasets. It observes significant performance improvements and strict privacy guarantees under different privacy settings.</p></p class="citation"></blockquote><h3 id=2425--3881-understanding-test-time-augmentation-masanari-kimura-2024>(24/25 | 38/81) Understanding Test-Time Augmentation (Masanari Kimura, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masanari Kimura. (2024)<br><strong>Understanding Test-Time Augmentation</strong><br><button class=copy-to-clipboard title="Understanding Test-Time Augmentation" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06892v1.pdf filename=2402.06892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test-Time Augmentation (TTA) is a very powerful heuristic that takes advantage of <b>data</b> <b>augmentation</b> during testing to produce averaged output. Despite the experimental effectiveness of TTA, there is insufficient discussion of its theoretical aspects. In this paper, we aim to give theoretical guarantees for TTA and clarify its behavior.</p></p class="citation"></blockquote><h3 id=2525--3981-assessing-uncertainty-estimation-methods-for-3d-image-segmentation-under-distribution-shifts-masoumeh-javanbakhat-et-al-2024>(25/25 | 39/81) Assessing Uncertainty Estimation Methods for 3D Image Segmentation under Distribution Shifts (Masoumeh Javanbakhat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masoumeh Javanbakhat, Md Tasnimul Hasan, Cristoph Lippert. (2024)<br><strong>Assessing Uncertainty Estimation Methods for 3D Image Segmentation under Distribution Shifts</strong><br><button class=copy-to-clipboard title="Assessing Uncertainty Estimation Methods for 3D Image Segmentation under Distribution Shifts" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06937v1.pdf filename=2402.06937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, machine learning has witnessed extensive adoption across various sectors, yet its application in medical image-based disease detection and diagnosis remains challenging due to distribution shifts in real-world data. In practical settings, deployed models encounter samples that differ significantly from the training dataset, especially in the health domain, leading to potential performance issues. This limitation hinders the expressiveness and reliability of deep learning models in health applications. Thus, it becomes crucial to identify methods capable of producing reliable uncertainty estimation in the context of distribution shifts in the health sector. In this paper, we explore the feasibility of using cutting-edge Bayesian and non-Bayesian methods to detect distributionally shifted samples, aiming to achieve reliable and trustworthy diagnostic predictions in segmentation task. Specifically, we compare three distinct uncertainty estimation methods, each designed to capture either unimodal or <b>multimodal</b> aspects in the posterior distribution. Our findings demonstrate that methods capable of addressing <b>multimodal</b> characteristics in the posterior distribution, offer more dependable uncertainty estimates. This research contributes to enhancing the utility of deep learning in healthcare, making diagnostic predictions more robust and trustworthy.</p></p class="citation"></blockquote><h2 id=csai-5>cs.AI (5)</h2><h3 id=15--4081-urbankgent-a-unified-large-language-model-agent-framework-for-urban-knowledge-graph-construction-yansong-ning-et-al-2024>(1/5 | 40/81) UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction (Yansong Ning et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yansong Ning, Hao Liu. (2024)<br><strong>UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction</strong><br><button class=copy-to-clipboard title="UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 78<br>Keywords: Benchmarking, Fine-tuning, Knowledge Distillation, Knowledge Graph, GPT, GPT-4, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06861v1.pdf filename=2402.06861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Urban <b>knowledge</b> <b>graph</b> has recently worked as an emerging building block to <b>distill</b> critical <b>knowledge</b> <b>from</b> multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban <b>knowledge</b> <b>graph</b> construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified <b>large</b> <b>language</b> <b>model</b> agent framework, for urban <b>knowledge</b> <b>graph</b> construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and <b>knowledge</b> <b>graph</b> completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories <b>distilled</b> from <b>GPT-4.</b> Through hybrid instruction <b>fine-tuning</b> with augmented trajectories on <b>Llama-2-13B,</b> we obtain the UrbanKGC agent, UrbanKGent-13B. We perform a comprehensive evaluation on two real-world datasets using both human and <b>GPT-4</b> self-evaluation. The experimental results demonstrate that UrbanKGent-13B not only can significantly outperform 21 baselines in UrbanKGC tasks, but also surpass the state-of-the-art <b>LLM,</b> <b>GPT-4,</b> by more than 10% with approximately 20 times lower cost. We deploy UrbanKGent-13B to provide online services, which can construct an UrbanKG with thousands of times richer relationships using only one-fifth of the data compared with the existing <b>benchmark.</b> Our data, code, and opensource UrbanKGC agent are available at <a href=https://github.com/usail-hkust/UrbanKGent>https://github.com/usail-hkust/UrbanKGent</a>.</p></p class="citation"></blockquote><h3 id=25--4181-realm-rag-driven-enhancement-of-multimodal-electronic-health-records-analysis-via-large-language-models-yinghao-zhu-et-al-2024>(2/5 | 41/81) REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models (Yinghao Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, Chengwei Pan. (2024)<br><strong>REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models</strong><br><button class=copy-to-clipboard title="REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 76<br>Keywords: Knowledge Graph, Knowledge Graph, Multi-modal, Multi-modal, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07016v1.pdf filename=2402.07016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of <b>multimodal</b> Electronic Health Records (EHR) data has significantly improved clinical predictive capabilities. Leveraging clinical notes and multivariate time-series EHR, existing models often lack the medical context relevent to clinical tasks, <b>prompting</b> the incorporation of external <b>knowledge,</b> <b>particularly</b> from the <b>knowledge</b> <b>graph</b> <b>(KG).</b> Previous approaches with <b>KG</b> <b>knowledge</b> <b>have</b> primarily focused on structured <b>knowledge</b> <b>extraction,</b> neglecting unstructured data modalities and semantic high dimensional medical <b>knowledge.</b> <b>In</b> response, we propose REALM, a <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> driven framework to enhance <b>multimodal</b> EHR representations that address these limitations. Firstly, we apply <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to encode long context clinical notes and GRU model to encode time-series EHR data. Secondly, we <b>prompt</b> <b>LLM</b> to extract task-relevant medical entities and match entities in professionally labeled external <b>knowledge</b> <b>graph</b> (PrimeKG) with corresponding medical <b>knowledge.</b> <b>By</b> matching and aligning with clinical standards, our framework eliminates hallucinations and ensures consistency. Lastly, we propose an adaptive <b>multimodal</b> fusion network to integrate extracted <b>knowledge</b> <b>with</b> <b>multimodal</b> EHR data. Our extensive experiments on MIMIC-III mortality and readmission tasks showcase the superior performance of our REALM framework over baselines, emphasizing the effectiveness of each module. REALM framework contributes to refining the use of <b>multimodal</b> EHR data in healthcare and bridging the gap with nuanced medical context essential for informed clinical predictions.</p></p class="citation"></blockquote><h3 id=35--4281-chemllm-a-chemical-large-language-model-di-zhang-et-al-2024>(3/5 | 42/81) ChemLLM: A Chemical Large Language Model (Di Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, Shufei Zhang, Mao Su, Hansen Zhong, Yuqiang Li, Wanli Ouyang. (2024)<br><strong>ChemLLM: A Chemical Large Language Model</strong><br><button class=copy-to-clipboard title="ChemLLM: A Chemical Large Language Model" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 70<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Dialogue System, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06852v1.pdf filename=2402.06852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have made impressive progress in chemistry applications, including molecular property prediction, molecular generation, experimental protocol design, etc. However, the community lacks a <b>dialogue-based</b> <b>model</b> specifically designed for chemistry. The challenge arises from the fact that most chemical data and scientific knowledge are primarily stored in structured databases, and the direct use of these structured data compromises the model&rsquo;s ability to maintain coherent <b>dialogue.</b> <b>To</b> tackle this issue, we develop a novel template-based instruction construction method that transforms structured knowledge into plain <b>dialogue,</b> <b>making</b> it suitable for language model training. By leveraging this approach, we develop ChemLLM, the first <b>large</b> <b>language</b> <b>model</b> dedicated to chemistry, capable of performing various tasks across chemical disciplines with smooth <b>dialogue</b> <b>interaction.</b> ChemLLM beats <b>GPT-3.5</b> on all three principal tasks in chemistry, i.e., name conversion, molecular caption, and reaction prediction, and surpasses <b>GPT-4</b> on two of them. Remarkably, ChemLLM also shows exceptional adaptability to related mathematical and physical tasks despite being trained mainly on chemical-centric corpora. Furthermore, ChemLLM demonstrates proficiency in specialized NLP tasks within chemistry, such as literature translation and cheminformatic programming. ChemLLM opens up a new avenue for exploration within chemical studies, while our method of integrating structured chemical knowledge into <b>dialogue</b> <b>systems</b> sets a new frontier for developing <b>LLMs</b> across various scientific fields. Codes, Datasets, and Model weights are publicly accessible at hf.co/AI4Chem/ChemLLM-7B-Chat.</p></p class="citation"></blockquote><h3 id=45--4381-a-factor-graph-model-of-trust-for-a-collaborative-multi-agent-system-behzad-akbari-et-al-2024>(4/5 | 43/81) A Factor Graph Model of Trust for a Collaborative Multi-Agent System (Behzad Akbari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Behzad Akbari, Mingfeng Yuan, Hao Wang, Haibin Zhu, Jinjun Shan. (2024)<br><strong>A Factor Graph Model of Trust for a Collaborative Multi-Agent System</strong><br><button class=copy-to-clipboard title="A Factor Graph Model of Trust for a Collaborative Multi-Agent System" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07049v1.pdf filename=2402.07049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of Multi-Agent Systems (MAS), known for their openness, dynamism, and cooperative nature, the ability to trust the resources and services of other agents is crucial. Trust, in this setting, is the reliance and confidence an agent has in the information, behaviors, intentions, truthfulness, and capabilities of others within the system. Our paper introduces a new graphical approach that utilizes factor graphs to represent the interdependent behaviors and trustworthiness among agents. This includes modeling the behavior of robots as a trajectory of actions using a <b>Gaussian</b> <b>process</b> factor graph, which accounts for smoothness, obstacle avoidance, and trust-related factors. Our method for evaluating trust is decentralized and considers key interdependent sub-factors such as proximity safety, consistency, and cooperation. The overall system comprises a network of factor graphs that interact through trust-related factors and employs a Bayesian inference method to dynamically assess trust-based decisions with informed consent. The effectiveness of this method is validated via <b>simulations</b> and empirical tests with autonomous robots navigating unsignalized intersections.</p></p class="citation"></blockquote><h3 id=55--4481-making-a-prototype-of-seoul-historical-sites-chatbot-using-langchain-jae-young-suh-et-al-2024>(5/5 | 44/81) Making a prototype of Seoul historical sites chatbot using Langchain (Jae Young Suh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jae Young Suh, Minsoo Kwak, Soo Yong Kim, Hyoungseo Cho. (2024)<br><strong>Making a prototype of Seoul historical sites chatbot using Langchain</strong><br><button class=copy-to-clipboard title="Making a prototype of Seoul historical sites chatbot using Langchain" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Chatbot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06929v1.pdf filename=2402.06929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we are going to share a draft of the development of a conversational agent created to disseminate information about historical sites located in the Seoul. The primary objective of the agent is to increase awareness among visitors who are not familiar with Seoul, about the presence and precise locations of valuable cultural heritage sites. It aims to promote a basic understanding of Korea&rsquo;s rich and diverse cultural history. The agent is thoughtfully designed for accessibility in English and utilizes data generously provided by the Seoul Metropolitan Government. Despite the limited data volume, it consistently delivers reliable and accurate responses, seamlessly aligning with the available information. We have meticulously detailed the methodologies employed in creating this agent and provided a comprehensive overview of its underlying structure within the paper. Additionally, we delve into potential improvements to enhance this initial version of the system, with a primary emphasis on expanding the available data through our <b>prompting.</b> In conclusion, we provide an in-depth discussion of our expectations regarding the future impact of this agent in promoting and facilitating the sharing of historical sites.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--4581-cochceps-augment-a-novel-self-supervised-contrastive-learning-using-cochlear-cepstrum-based-masking-for-speech-emotion-recognition-ioannis-ziogas-et-al-2024>(1/2 | 45/81) CochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using Cochlear Cepstrum-based Masking for Speech Emotion Recognition (Ioannis Ziogas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ioannis Ziogas, Hessa Alfalahi, Ahsan H. Khandoker, Leontios J. Hadjileontiadis. (2024)<br><strong>CochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using Cochlear Cepstrum-based Masking for Speech Emotion Recognition</strong><br><button class=copy-to-clipboard title="CochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using Cochlear Cepstrum-based Masking for Speech Emotion Recognition" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-LG, cs-SD, eess-AS, eess-SP, eess.AS, stat-ML<br>Keyword Score: 73<br>Keywords: Benchmarking, Contrastive Learning, Fine-tuning, Self-supervised Learning, Self-supervised Learning, Unsupervised Learning, Automatic Speech Recognition, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06923v1.pdf filename=2402.06923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> (SSL) for automated <b>speech</b> <b>recognition</b> in terms of its <b>emotional</b> <b>content,</b> can be heavily degraded by the presence noise, affecting the efficiency of modeling the intricate temporal and spectral informative structures of <b>speech.</b> <b>Recently,</b> SSL on large <b>speech</b> <b>datasets,</b> as well as new audio-specific SSL proxy tasks, such as, temporal and frequency masking, have emerged, yielding superior performance compared to classic approaches drawn from the image augmentation domain. Our proposed contribution builds upon this successful paradigm by introducing CochCeps-Augment, a novel bio-inspired masking augmentation task for <b>self-supervised</b> <b>contrastive</b> <b>learning</b> of <b>speech</b> <b>representations.</b> Specifically, we utilize the newly introduced bio-inspired cochlear cepstrogram (CCGRAM) to derive noise robust representations of input <b>speech,</b> <b>that</b> are then further refined through a <b>self-supervised</b> <b>learning</b> scheme. The latter employs SimCLR to generate <b>contrastive</b> <b>views</b> of a CCGRAM through masking of its angle and quefrency dimensions. Our experimental approach and validations on the <b>emotion</b> <b>recognition</b> K-EmoCon <b>benchmark</b> dataset, for the first time via a speaker-independent approach, features <b>unsupervised</b> pre-training, linear probing and <b>fine-tuning.</b> Our results potentiate CochCeps-Augment to serve as a standard tool in <b>speech</b> <b>emotion</b> <b>recognition</b> analysis, showing the added value of incorporating bio-inspired masking as an informative augmentation task for self-supervision. Our code for implementing CochCeps-Augment will be made available at: <a href=https://github.com/GiannisZgs/CochCepsAugment>https://github.com/GiannisZgs/CochCepsAugment</a>.</p></p class="citation"></blockquote><h3 id=22--4681-analysis-of-self-supervised-speech-models-on-childrens-speech-and-infant-vocalizations-jialu-li-et-al-2024>(2/2 | 46/81) Analysis of Self-Supervised Speech Models on Children&rsquo;s Speech and Infant Vocalizations (Jialu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialu Li, Mark Hasegawa-Johnson, Nancy L. McElwain. (2024)<br><strong>Analysis of Self-Supervised Speech Models on Children&rsquo;s Speech and Infant Vocalizations</strong><br><button class=copy-to-clipboard title="Analysis of Self-Supervised Speech Models on Children's Speech and Infant Vocalizations" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06888v1.pdf filename=2402.06888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To understand why <b>self-supervised</b> <b>learning</b> (SSL) models have empirically achieved strong performances on several speech-processing downstream tasks, numerous studies have focused on analyzing the encoded information of the SSL layer representations in adult speech. Limited work has investigated how pre-training and <b>fine-tuning</b> affect SSL models encoding children&rsquo;s speech and vocalizations. In this study, we aim to bridge this gap by probing SSL models on two relevant downstream tasks: (1) phoneme recognition (PR) on the speech of adults, older children (8-10 years old), and younger children (1-4 years old), and (2) vocalization classification (VC) distinguishing cry, fuss, and babble for infants under 14 months old. For younger children&rsquo;s PR, the superiority of <b>fine-tuned</b> SSL models is largely due to their ability to learn features that represent older children&rsquo;s speech and then adapt those features to the speech of younger children. For infant VC, SSL models pre-trained on large-scale home recordings learn to leverage phonetic representations at middle layers, and thereby enhance the performance of this task.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=13--4781-near-perfect-coverage-manifold-estimation-in-cellular-networks-via-conditional-gan-washim-uddin-mondal-et-al-2024>(1/3 | 47/81) Near-perfect Coverage Manifold Estimation in Cellular Networks via conditional GAN (Washim Uddin Mondal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Washim Uddin Mondal, Veni Goyal, Satish V. Ukkusuri, Goutam Das, Di Wang, Mohamed-Slim Alouini, Vaneet Aggarwal. (2024)<br><strong>Near-perfect Coverage Manifold Estimation in Cellular Networks via conditional GAN</strong><br><button class=copy-to-clipboard title="Near-perfect Coverage Manifold Estimation in Cellular Networks via conditional GAN" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 70<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Generative Adversarial Network, Generative Adversarial Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06901v1.pdf filename=2402.06901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a conditional <b>generative</b> <b>adversarial</b> <b>network</b> (cGAN) that translates base station location (BSL) information of any Region-of-Interest (RoI) to location-dependent coverage probability values within a subset of that region, called the region-of-evaluation (RoE). We train our network utilizing the BSL data of India, the USA, Germany, and Brazil. In comparison to the state-of-the-art <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs),</b> our model improves the prediction error ($L_1$ difference between the coverage manifold generated by the network under consideration and that generated via <b>simulation)</b> by two orders of magnitude. Moreover, the cGAN-generated coverage manifolds appear to be almost visually indistinguishable from the ground truth.</p></p class="citation"></blockquote><h3 id=23--4881-orient-a-priority-aware-energy-efficient-approach-for-latency-sensitive-applications-in-6g-masoud-shokrnezhad-et-al-2024>(2/3 | 48/81) ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G (Masoud Shokrnezhad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masoud Shokrnezhad, Tarik Taleb. (2024)<br><strong>ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G</strong><br><button class=copy-to-clipboard title="ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-DC, cs-LG, cs-NI, cs.NI<br>Keyword Score: 40<br>Keywords: Graph Neural Network, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06931v1.pdf filename=2402.06931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Anticipation for 6G&rsquo;s arrival comes with growing concerns about increased energy consumption in computing and networking. The expected surge in connected devices and resource-demanding applications presents unprecedented challenges for energy resources. While sustainable resource allocation strategies have been discussed in the past, these efforts have primarily focused on single-domain orchestration or ignored the unique requirements posed by 6G. To address this gap, we investigate the joint problem of service instance placement and assignment, path selection, and request prioritization, dubbed PIRA. The objective function is to maximize the system&rsquo;s overall profit as a function of the number of concurrently supported requests while simultaneously minimizing energy consumption over an extended period of time. In addition, end-to-end latency requirements and resource capacity constraints are considered for computing and networking resources, where queuing theory is utilized to estimate the Age of Information (AoI) for requests. After formulating the problem in a non-linear fashion, we prove its NP-hardness and propose a method, denoted ORIENT. This method is based on the Double Dueling Deep Q-Learning (D3QL) mechanism and leverages <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> for state encoding. Extensive numerical <b>simulations</b> demonstrate that ORIENT yields near-optimal solutions for varying system sizes and request counts.</p></p class="citation"></blockquote><h3 id=33--4981-toward-scalable-generative-ai-via-mixture-of-experts-in-mobile-edge-networks-jiacheng-wang-et-al-2024>(3/3 | 49/81) Toward Scalable Generative AI via Mixture of Experts in Mobile Edge Networks (Jiacheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacheng Wang, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim, Khaled B. Letaief. (2024)<br><strong>Toward Scalable Generative AI via Mixture of Experts in Mobile Edge Networks</strong><br><button class=copy-to-clipboard title="Toward Scalable Generative AI via Mixture of Experts in Mobile Edge Networks" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Generative AI, Reinforcement Learning, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06942v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06942v1.pdf filename=2402.06942v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of <b>generative</b> <b>artificial</b> intelligence (GAI) has driven revolutionary applications like <b>ChatGPT.</b> The widespread of these applications relies on the mixture of experts (MoE), which contains multiple experts and selectively engages them for each task to lower operation costs while maintaining performance. Despite MoE, GAI faces challenges in resource consumption when deployed on user devices. This paper proposes mobile edge networks supported MoE-based GAI. We first review the MoE from traditional AI and GAI perspectives, including structure, principles, and applications. We then propose a framework that transfers subtasks to devices in mobile edge networks, aiding GAI model operation on user devices. We discuss challenges in this process and introduce a deep <b>reinforcement</b> <b>learning</b> based algorithm to select edge devices for subtask execution. Experimental results will show that our framework not only facilitates GAI&rsquo;s deployment on resource-limited devices but also generates higher-quality content compared to methods without edge network support.</p></p class="citation"></blockquote><h2 id=cscr-4>cs.CR (4)</h2><h3 id=14--5081-system-level-analysis-of-adversarial-attacks-and-defenses-on-intelligence-in-o-ran-based-cellular-networks-azuka-chiejina-et-al-2024>(1/4 | 50/81) System-level Analysis of Adversarial Attacks and Defenses on Intelligence in O-RAN based Cellular Networks (Azuka Chiejina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Azuka Chiejina, Brian Kim, Kaushik Chowhdury, Vijay K. Shah. (2024)<br><strong>System-level Analysis of Adversarial Attacks and Defenses on Intelligence in O-RAN based Cellular Networks</strong><br><button class=copy-to-clipboard title="System-level Analysis of Adversarial Attacks and Defenses on Intelligence in O-RAN based Cellular Networks" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SY, cs.CR, eess-SY<br>Keyword Score: 60<br>Keywords: Adversarial Learning, Convolution, Convolutional Neural Network, Convolutional Neural Network, Knowledge Distillation, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06846v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06846v2.pdf filename=2402.06846v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While the open architecture, open interfaces, and integration of intelligence within Open Radio Access Network technology hold the promise of transforming 5G and 6G networks, they also introduce cybersecurity vulnerabilities that hinder its widespread adoption. In this paper, we conduct a thorough system-level investigation of cyber threats, with a specific focus on machine learning (ML) intelligence components known as xApps within the O-RAN&rsquo;s near-real-time RAN Intelligent Controller (near-RT RIC) platform. Our study begins by developing a malicious xApp designed to execute <b>adversarial</b> <b>attacks</b> on two types of test data - spectrograms and key performance metrics (KPMs), stored in the RIC database within the near-RT RIC. To mitigate these threats, we utilize a <b>distillation</b> technique that involves training a teacher model at a high softmax temperature and transferring its knowledge to a student model trained at a lower softmax temperature, which is deployed as the robust ML model within xApp. We prototype an over-the-air LTE/5G O-RAN testbed to assess the impact of these attacks and the effectiveness of the <b>distillation</b> defense technique by leveraging an ML-based Interference Classification (InterClass) xApp as an example. We examine two versions of InterClass xApp under distinct scenarios, one based on <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and another based on Deep Neural Networks (DNNs) using spectrograms and KPMs as input data respectively. Our findings reveal up to 100% and 96.3% degradation in the accuracy of both the <b>CNN</b> and DNN models respectively resulting in a significant decline in network performance under considered <b>adversarial</b> <b>attacks.</b> Under the strict latency constraints of the near-RT RIC closed control loop, our analysis shows that the <b>distillation</b> technique outperforms classical <b>adversarial</b> <b>training</b> by achieving an accuracy of up to 98.3% for mitigating such attacks.</p></p class="citation"></blockquote><h3 id=24--5181-sentinels-of-the-stream-unleashing-large-language-models-for-dynamic-packet-classification-in-software-defined-networks----position-paper-shariq-murtuza-2024>(2/4 | 51/81) Sentinels of the Stream: Unleashing Large Language Models for Dynamic Packet Classification in Software Defined Networks &ndash; Position Paper (Shariq Murtuza, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shariq Murtuza. (2024)<br><strong>Sentinels of the Stream: Unleashing Large Language Models for Dynamic Packet Classification in Software Defined Networks &ndash; Position Paper</strong><br><button class=copy-to-clipboard title="Sentinels of the Stream: Unleashing Large Language Models for Dynamic Packet Classification in Software Defined Networks -- Position Paper" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs.CR<br>Keyword Score: 50<br>Keywords: ChatGPT, GPT, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07950v1.pdf filename=2402.07950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the release of OpenAI&rsquo;s <b>ChatGPT,</b> the field of <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> saw an increase of academic interest in <b>GPT</b> based chat assistants. In the next few months multiple accesible <b>large</b> <b>language</b> <b>models</b> were released that included Meta&rsquo;s <b>LLama</b> models and Mistral AI&rsquo;s Mistral and Mixtral MoE models. These models are available openly for a wide array of purposes with a wide spectrum of licenses. These <b>LLMs</b> have found their use in a different number of fields like code development, SQL generation etc. In this work we propose our plan to explore the applicability of <b>large</b> <b>language</b> <b>model</b> in the domain of network security. We plan to create Sentinel, a <b>LLM,</b> to analyse network packet contents and pass a judgment on it&rsquo;s threat level. This work is a preliminary report that will lay our plan for our future endeavors.</p></p class="citation"></blockquote><h3 id=34--5281-whispers-in-the-machine-confidentiality-in-llm-integrated-systems-jonathan-evertz-et-al-2024>(3/4 | 52/81) Whispers in the Machine: Confidentiality in LLM-integrated Systems (Jonathan Evertz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Evertz, Merlin Chlosta, Lea Schönherr, Thorsten Eisenhofer. (2024)<br><strong>Whispers in the Machine: Confidentiality in LLM-integrated Systems</strong><br><button class=copy-to-clipboard title="Whispers in the Machine: Confidentiality in LLM-integrated Systems" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06922v1.pdf filename=2402.06922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are increasingly integrated with external tools. While these integrations can significantly improve the functionality of <b>LLMs,</b> they also create a new attack surface where confidential data may be disclosed between different components. Specifically, malicious tools can exploit vulnerabilities in the <b>LLM</b> itself to manipulate the model and compromise the data of other services, raising the question of how private data can be protected in the context of <b>LLM</b> integrations. In this work, we provide a systematic way of evaluating confidentiality in <b>LLM-integrated</b> systems. For this, we formalize a &ldquo;secret key&rdquo; game that can capture the ability of a model to conceal private information. This enables us to compare the vulnerability of a model against confidentiality attacks and also the effectiveness of different defense strategies. In this framework, we evaluate eight previously published attacks and four defenses. We find that current defenses lack generalization across attack strategies. Building on this analysis, we propose a method for robustness <b>fine-tuning,</b> inspired by <b>adversarial</b> <b>training.</b> This approach is effective in lowering the success rate of attackers and in improving the system&rsquo;s resilience against unknown attacks.</p></p class="citation"></blockquote><h3 id=44--5381-benchmarking-frameworks-and-comparative-studies-of-controller-area-network-can-intrusion-detection-systems-a-review-shaila-sharmin-et-al-2024>(4/4 | 53/81) Benchmarking Frameworks and Comparative Studies of Controller Area Network (CAN) Intrusion Detection Systems: A Review (Shaila Sharmin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaila Sharmin, Hafizah Mansor, Andi Fitriah Abdul Kadir, Normaziah A. Aziz. (2024)<br><strong>Benchmarking Frameworks and Comparative Studies of Controller Area Network (CAN) Intrusion Detection Systems: A Review</strong><br><button class=copy-to-clipboard title="Benchmarking Frameworks and Comparative Studies of Controller Area Network (CAN) Intrusion Detection Systems: A Review" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06904v1.pdf filename=2402.06904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of intrusion detection systems (IDS) for the in-vehicle Controller Area Network (CAN) bus is one of the main efforts being taken to secure the in-vehicle network against various cyberattacks, which have the potential to cause vehicles to malfunction and result in dangerous accidents. These CAN IDS are evaluated in disparate experimental conditions that vary in terms of the workload used, the features used, the metrics reported, etc., which makes direct comparison difficult. Therefore, there have been several <b>benchmarking</b> frameworks and comparative studies designed to evaluate CAN IDS in similar experimental conditions to understand their relative performance and facilitate the selection of the best CAN IDS for implementation in automotive networks. This work provides a comprehensive survey of CAN IDS <b>benchmarking</b> frameworks and comparative studies in the current literature. A CAN IDS evaluation design space is also proposed in this work, which draws from the wider CAN IDS literature. This is not only expected to serve as a guide for designing CAN IDS evaluation experiments but is also used for categorizing current <b>benchmarking</b> efforts. The surveyed works have been discussed on the basis of the five aspects in the design space-namely IDS type, attack model, evaluation type, workload generation, and evaluation metrics-and <b>recommendations</b> for future work have been identified.</p></p class="citation"></blockquote><h2 id=cscv-7>cs.CV (7)</h2><h3 id=17--5481-domain-adaptable-fine-tune-distillation-framework-for-advancing-farm-surveillance-raza-imam-et-al-2024>(1/7 | 54/81) Domain Adaptable Fine-Tune Distillation Framework For Advancing Farm Surveillance (Raza Imam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raza Imam, Muhammad Huzaifa, Nabil Mansour, Shaher Bano Mirza, Fouad Lamghari. (2024)<br><strong>Domain Adaptable Fine-Tune Distillation Framework For Advancing Farm Surveillance</strong><br><button class=copy-to-clipboard title="Domain Adaptable Fine-Tune Distillation Framework For Advancing Farm Surveillance" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Object Detection, Fine-tuning, Fine-tuning, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07059v1.pdf filename=2402.07059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we propose an automated framework for camel farm monitoring, introducing two key contributions: the Unified Auto-Annotation framework and the <b>Fine-Tune</b> <b>Distillation</b> framework. The Unified Auto-Annotation approach combines two models, GroundingDINO (GD), and Segment-Anything-Model (SAM), to automatically annotate raw datasets extracted from surveillance videos. Building upon this foundation, the <b>Fine-Tune</b> <b>Distillation</b> framework conducts <b>fine-tuning</b> of student models using the auto-annotated dataset. This process involves transferring <b>knowledge</b> <b>from</b> a large teacher model to a student model, resembling a variant of <b>Knowledge</b> <b>Distillation.</b> The <b>Fine-Tune</b> <b>Distillation</b> framework aims to be adaptable to specific use cases, enabling the transfer of <b>knowledge</b> <b>from</b> the large models to the small models, making it suitable for domain-specific applications. By leveraging our raw dataset collected from Al-Marmoom Camel Farm in Dubai, UAE, and a pre-trained teacher model, GroundingDINO, the <b>Fine-Tune</b> <b>Distillation</b> framework produces a lightweight deployable model, YOLOv8. This framework demonstrates high performance and computational efficiency, facilitating efficient real-time <b>object</b> <b>detection.</b> Our code is available at \href{https://github.com/Razaimam45/Fine-Tune-Distillation}{https://github.com/Razaimam45/Fine-Tune-Distillation}</p></p class="citation"></blockquote><h3 id=27--5581-synthesizing-cta-image-data-for-type-b-aortic-dissection-using-stable-diffusion-models-ayman-abaid-et-al-2024>(2/7 | 55/81) Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable Diffusion Models (Ayman Abaid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayman Abaid, Muhammad Ali Farooq, Niamh Hynes, Peter Corcoran, Ihsan Ullah. (2024)<br><strong>Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable Diffusion Models</strong><br><button class=copy-to-clipboard title="Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable Diffusion Models" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Generative AI, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06969v1.pdf filename=2402.06969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stable Diffusion (SD) has gained a lot of attention in recent years in the field of <b>Generative</b> <b>AI</b> thus helping in synthesizing medical imaging data with distinct features. The aim is to contribute to the ongoing effort focused on overcoming the limitations of data scarcity and improving the capabilities of ML algorithms for cardiovascular image processing. Therefore, in this study, the possibility of generating synthetic cardiac CTA images was explored by <b>fine-tuning</b> stable diffusion models based on user defined text <b>prompts,</b> using only limited number of CTA images as input. A comprehensive evaluation of the synthetic data was conducted by incorporating both quantitative analysis and qualitative assessment, where a clinician assessed the quality of the generated data. It has been shown that Cardiac CTA images can be successfully generated using using Text to Image (T2I) stable diffusion model. The results demonstrate that the tuned T2I CTA diffusion model was able to generate images with features that are typically unique to acute type B aortic dissection (TBAD) medical conditions.</p></p class="citation"></blockquote><h3 id=37--5681-semantic-object-level-modeling-for-robust-visual-camera-relocalization-yifan-zhu-et-al-2024>(3/7 | 56/81) Semantic Object-level Modeling for Robust Visual Camera Relocalization (Yifan Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Zhu, Lingjuan Miao, Haitao Wu, Zhiqiang Zhou, Weiyi Chen, Longwen Wu. (2024)<br><strong>Semantic Object-level Modeling for Robust Visual Camera Relocalization</strong><br><button class=copy-to-clipboard title="Semantic Object-level Modeling for Robust Visual Camera Relocalization" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06951v1.pdf filename=2402.06951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual relocalization is crucial for autonomous visual localization and navigation of mobile robotics. Due to the improvement of <b>CNN-based</b> <b>object</b> <b>detection</b> algorithm, the robustness of visual relocalization is greatly enhanced especially in viewpoints where classical methods fail. However, ellipsoids (quadrics) generated by axis-aligned <b>object</b> <b>detection</b> may limit the accuracy of the <b>object-level</b> <b>representation</b> and degenerate the performance of visual relocalization system. In this paper, we propose a novel method of automatic <b>object-level</b> <b>voxel</b> modeling for accurate ellipsoidal representations of <b>objects.</b> <b>As</b> for visual relocalization, we design a better pose optimization strategy for camera pose recovery, to fully utilize the projection characteristics of 2D fitted ellipses and the 3D accurate ellipsoids. All of these modules are entirely intergrated into visual SLAM system. Experimental results show that our semantic <b>object-level</b> <b>mapping</b> and <b>object-based</b> <b>visual</b> relocalization methods significantly enhance the performance of visual relocalization in terms of robustness to new viewpoints.</p></p class="citation"></blockquote><h3 id=47--5781-latent-enhancing-autoencoder-for-occluded-image-classification-ketan-kotwal-et-al-2024>(4/7 | 57/81) Latent Enhancing AutoEncoder for Occluded Image Classification (Ketan Kotwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ketan Kotwal, Tanay Deshmukh, Preeti Gopal. (2024)<br><strong>Latent Enhancing AutoEncoder for Occluded Image Classification</strong><br><button class=copy-to-clipboard title="Latent Enhancing AutoEncoder for Occluded Image Classification" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Autoencoder, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06936v1.pdf filename=2402.06936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large occlusions result in a significant decline in image classification accuracy. During inference, diverse types of unseen occlusions introduce <b>out-of-distribution</b> data to the classification model, leading to accuracy dropping as low as 50%. As occlusions encompass spatially connected regions, conventional methods involving feature reconstruction are inadequate for enhancing classification performance. We introduce LEARN: Latent Enhancing feAture Reconstruction Network &ndash; An auto-encoder based network that can be incorporated into the classification model before its classifier head without modifying the weights of classification model. In addition to reconstruction and classification losses, training of LEARN effectively combines intra- and inter-class losses calculated over its latent space &ndash; which lead to improvement in recovering latent space of occluded data, while preserving its class-specific discriminative information. On the OccludedPASCAL3D+ dataset, the proposed LEARN outperforms standard classification models (VGG16 and ResNet-50) by a large margin and up to 2% over state-of-the-art methods. In cross-dataset testing, our method improves the average classification accuracy by more than 5% over the state-of-the-art methods. In every experiment, our model consistently maintains excellent accuracy on in-distribution data.</p></p class="citation"></blockquote><h3 id=57--5881-treatment-wise-glioblastoma-survival-inference-with-multi-parametric-preoperative-mri-xiaofeng-liu-et-al-2024>(5/7 | 58/81) Treatment-wise Glioblastoma Survival Inference with Multi-parametric Preoperative MRI (Xiaofeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaofeng Liu, Nadya Shusharina, Helen A Shih, C. -C. Jay Kuo, Georges El Fakhri, Jonghye Woo. (2024)<br><strong>Treatment-wise Glioblastoma Survival Inference with Multi-parametric Preoperative MRI</strong><br><button class=copy-to-clipboard title="Treatment-wise Glioblastoma Survival Inference with Multi-parametric Preoperative MRI" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, physics-med-ph<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06982v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06982v1.pdf filename=2402.06982v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we aim to predict the survival time (ST) of glioblastoma (GBM) patients undergoing different treatments based on preoperative magnetic resonance (MR) scans. The personalized and precise treatment planning can be achieved by comparing the ST of different treatments. It is well established that both the current status of the patient (as represented by the MR scans) and the choice of treatment are the cause of ST. While previous related MR-based glioblastoma ST studies have focused only on the direct mapping of MR scans to ST, they have not included the underlying causal relationship between treatments and ST. To address this limitation, we propose a treatment-conditioned regression model for glioblastoma ST that incorporates treatment information in addition to MR scans. Our approach allows us to effectively utilize the data from all of the treatments in a unified manner, rather than having to train separate models for each of the treatments. Furthermore, treatment can be effectively injected into each <b>convolutional</b> layer through the adaptive instance normalization we employ. We evaluate our framework on the BraTS20 ST prediction task. Three treatment options are considered: Gross Total Resection (GTR), Subtotal Resection (STR), and no resection. The evaluation results demonstrate the effectiveness of injecting the treatment for estimating GBM survival.</p></p class="citation"></blockquote><h3 id=67--5981-a-change-detection-reality-check-isaac-corley-et-al-2024>(6/7 | 59/81) A Change Detection Reality Check (Isaac Corley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Isaac Corley, Caleb Robinson, Anthony Ortiz. (2024)<br><strong>A Change Detection Reality Check</strong><br><button class=copy-to-clipboard title="A Change Detection Reality Check" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06994v1.pdf filename=2402.06994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been an explosion of proposed change detection deep learning architectures in the remote sensing literature. These approaches claim to offer state-of the-art performance on different standard <b>benchmark</b> datasets. However, has the field truly made significant progress? In this paper we perform experiments which conclude a simple U-Net segmentation baseline without training tricks or complicated architectural changes is still a top performer for the task of change detection.</p></p class="citation"></blockquote><h3 id=77--6081-ossar-towards-open-set-surgical-activity-recognition-in-robot-assisted-surgery-long-bai-et-al-2024>(7/7 | 60/81) OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted Surgery (Long Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long Bai, Guankun Wang, Jie Wang, Xiaoxiao Yang, Huxin Gao, Xin Liang, An Wang, Mobarakol Islam, Hongliang Ren. (2024)<br><strong>OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted Surgery</strong><br><button class=copy-to-clipboard title="OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted Surgery" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06985v1.pdf filename=2402.06985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of automated robotic surgery and computer-assisted interventions, understanding robotic surgical activities stands paramount. Existing algorithms dedicated to surgical activity recognition predominantly cater to pre-defined closed-set paradigms, ignoring the challenges of real-world open-set scenarios. Such algorithms often falter in the presence of test samples originating from classes unseen during training phases. To tackle this problem, we introduce an innovative Open-Set Surgical Activity Recognition (OSSAR) framework. Our solution leverages the hyperspherical reciprocal point strategy to enhance the distinction between known and unknown classes in the feature space. Additionally, we address the issue of over-confidence in the closed set by refining model calibration, avoiding misclassification of unknown classes as known ones. To support our assertions, we establish an open-set surgical activity <b>benchmark</b> utilizing the public JIGSAWS dataset. Besides, we also collect a novel dataset on endoscopic submucosal dissection for surgical activity tasks. Extensive comparisons and ablation experiments on these datasets demonstrate the significant outperformance of our method over existing state-of-the-art approaches. Our proposed solution can effectively address the challenges of real-world surgical scenarios. Our code is publicly accessible at <a href=https://github.com/longbai1006/OSSAR>https://github.com/longbai1006/OSSAR</a>.</p></p class="citation"></blockquote><h2 id=statml-2>stat.ML (2)</h2><h3 id=12--6181-generalization-error-of-graph-neural-networks-in-the-mean-field-regime-gholamali-aminian-et-al-2024>(1/2 | 61/81) Generalization Error of Graph Neural Networks in the Mean-field Regime (Gholamali Aminian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gholamali Aminian, Yixuan He, Gesine Reinert, Łukasz Szpruch, Samuel N. Cohen. (2024)<br><strong>Generalization Error of Graph Neural Networks in the Mean-field Regime</strong><br><button class=copy-to-clipboard title="Generalization Error of Graph Neural Networks in the Mean-field Regime" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: 62B10, 60F99, 49N80, 46N30, cs-IT, cs-LG, math-IT, stat-ML, stat.ML<br>Keyword Score: 50<br>Keywords: Message-Passing, Graph Classification, Graph Neural Network, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07025v1.pdf filename=2402.07025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work provides a theoretical framework for assessing the generalization error of <b>graph</b> <b>classification</b> <b>tasks</b> via <b>graph</b> <b>neural</b> <b>networks</b> in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. We explore two widely utilized types of <b>graph</b> <b>neural</b> <b>networks:</b> <b>graph</b> <b>convolutional</b> <b>neural</b> <b>networks</b> and message passing <b>graph</b> <b>neural</b> <b>networks.</b> Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these <b>graph</b> <b>neural</b> <b>networks.</b> We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of <b>graph</b> <b>samples.</b> <b>These</b> upper bounds offer a theoretical assurance of the networks&rsquo; performance on unseen data in the challenging over-parameterized regime and overall contribute to our understanding of their performance.</p></p class="citation"></blockquote><h3 id=22--6281-low-rank-approximation-of-structural-redundancy-for-self-supervised-learning-kang-du-et-al-2024>(2/2 | 62/81) Low-Rank Approximation of Structural Redundancy for Self-Supervised Learning (Kang Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kang Du, Yu Xiang. (2024)<br><strong>Low-Rank Approximation of Structural Redundancy for Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="Low-Rank Approximation of Structural Redundancy for Self-Supervised Learning" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 40<br>Keywords: Self-supervised Learning, Self-supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06884v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06884v1.pdf filename=2402.06884v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the data-generating mechanism for reconstructive SSL to shed light on its effectiveness. With an infinite amount of labeled samples, we provide a sufficient and necessary condition for perfect linear approximation. The condition reveals a full-rank component that preserves the label classes of Y, along with a redundant component. Motivated by the condition, we propose to approximate the redundant component by a low-rank factorization and measure the approximation quality by introducing a new quantity $\epsilon_s$, parameterized by the rank of factorization s. We incorporate $\epsilon_s$ into the excess risk analysis under both linear regression and ridge regression settings, where the latter regularization approach is to handle scenarios when the dimension of the learned features is much larger than the number of labeled samples n for downstream tasks. We design three stylized experiments to compare SSL with <b>supervised</b> <b>learning</b> under different settings to support our theoretical findings.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--6381-cacophony-an-improved-contrastive-audio-text-model-ge-zhu-et-al-2024>(1/2 | 63/81) Cacophony: An Improved Contrastive Audio-Text Model (Ge Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ge Zhu, Zhiyao Duan. (2024)<br><strong>Cacophony: An Improved Contrastive Audio-Text Model</strong><br><button class=copy-to-clipboard title="Cacophony: An Improved Contrastive Audio-Text Model" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 50<br>Keywords: Autoencoder, Zero-shot, Image2text, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06986v1.pdf filename=2402.06986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite recent improvements in audio-text modeling, audio-text contrastive models still lag behind their <b>image-text</b> counterparts in scale and performance. We propose a method to improve both the scale and the training of audio-text contrastive models. Specifically, we craft a <b>large-scale</b> <b>audio-text</b> <b>dataset</b> consisting of over 13,000 hours of text-labeled audio, aided by <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> processing and audio captioning. Further, we employ an masked <b>autoencoder</b> (MAE) pre-pretraining phase with random patch dropout, which allows us to both scale unlabeled audio datasets and train efficiently with variable length audio. After MAE pre-pretraining of our audio encoder, we train a contrastive model with an auxiliary captioning objective. Our final model, which we name Cacophony, achieves state-of-the-art performance on audio-text retrieval tasks, and exhibits competitive results on other downstream tasks such as <b>zero-shot</b> classification.</p></p class="citation"></blockquote><h3 id=22--6481-speech-motion-anomaly-detection-via-cross-modal-translation-of-4d-motion-fields-from-tagged-mri-xiaofeng-liu-et-al-2024>(2/2 | 64/81) Speech motion anomaly detection via cross-modal translation of 4D motion fields from tagged MRI (Xiaofeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaofeng Liu, Fangxu Xing, Jiachen Zhuo, Maureen Stone, Jerry L. Prince, Georges El Fakhri, Jonghye Woo. (2024)<br><strong>Speech motion anomaly detection via cross-modal translation of 4D motion fields from tagged MRI</strong><br><button class=copy-to-clipboard title="Speech motion anomaly detection via cross-modal translation of 4D motion fields from tagged MRI" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CV, cs-MM, cs-SD, cs.SD, eess-AS, eess-IV<br>Keyword Score: 40<br>Keywords: Anomaly Detection, Convolution, Out-of-distribution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06984v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06984v1.pdf filename=2402.06984v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the relationship between tongue motion patterns during speech and their resulting speech acoustic outcomes &ndash; i.e., articulatory-acoustic relation &ndash; is of great importance in assessing speech quality and developing innovative treatment and rehabilitative strategies. This is especially important when evaluating and detecting abnormal articulatory features in patients with speech-related disorders. In this work, we aim to develop a framework for detecting speech motion anomalies in conjunction with their corresponding speech acoustics. This is achieved through the use of a deep cross-modal translator trained on data from healthy individuals only, which bridges the gap between 4D motion fields obtained from tagged MRI and 2D spectrograms derived from speech acoustic data. The trained translator is used as an <b>anomaly</b> <b>detector,</b> by measuring the spectrogram reconstruction quality on healthy individuals or patients. In particular, the cross-modal translator is likely to yield limited generalization capabilities on patient data, which includes unseen <b>out-of-distribution</b> patterns and demonstrates subpar performance, when compared with healthy individuals.~A one-class SVM is then used to distinguish the spectrograms of healthy individuals from those of patients. To validate our framework, we collected a total of 39 paired tagged MRI and speech waveforms, consisting of data from 36 healthy individuals and 3 tongue cancer patients. We used both 3D <b>convolutional</b> and <b>transformer-based</b> deep translation models, training them on the healthy training set and then applying them to both the healthy and patient testing sets. Our framework demonstrates a capability to detect abnormal patient data, thereby illustrating its potential in enhancing the understanding of the articulatory-acoustic relation for both healthy individuals and patients.</p></p class="citation"></blockquote><h2 id=csro-2>cs.RO (2)</h2><h3 id=12--6581-bayesian-optimization-with-adaptive-kernels-for-robot-control-ruben-martinez-cantin-2024>(1/2 | 65/81) Bayesian Optimization with Adaptive Kernels for Robot Control (Ruben Martinez-Cantin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruben Martinez-Cantin. (2024)<br><strong>Bayesian Optimization with Adaptive Kernels for Robot Control</strong><br><button class=copy-to-clipboard title="Bayesian Optimization with Adaptive Kernels for Robot Control" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Gaussian Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07021v1.pdf filename=2402.07021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Active policy search combines the trial-and-error methodology from policy search with Bayesian optimization to actively find the optimal policy. First, policy search is a type of <b>reinforcement</b> <b>learning</b> which has become very popular for robot control, for its ability to deal with complex continuous state and action spaces. Second, Bayesian optimization is a sample efficient global optimization method that uses a surrogate model, like a <b>Gaussian</b> <b>process,</b> and optimal decision making to carefully select each sample during the optimization process. Sample efficiency is of paramount importance when each trial involves the real robot, expensive Monte Carlo runs, or a complex simulator. Black-box Bayesian optimization generally assumes a cost function from a stationary process, because nonstationary modeling is usually based on prior knowledge. However, many control problems are inherently nonstationary due to their failure conditions, terminal states and other abrupt effects. In this paper, we present a kernel function specially designed for Bayesian optimization, that allows nonstationary modeling without prior knowledge, using an adaptive local region. The new kernel results in an improved local search (exploitation), without penalizing the global search (exploration), as shown experimentally in well-known optimization <b>benchmarks</b> and robot control scenarios. We finally show its potential for the design of the wing shape of a UAV.</p></p class="citation"></blockquote><h3 id=22--6681-cahsor-competence-aware-high-speed-off-road-ground-navigation-in-se3-anuj-pokhrel-et-al-2024>(2/2 | 66/81) CAHSOR: Competence-Aware High-Speed Off-Road Ground Navigation in SE(3) (Anuj Pokhrel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anuj Pokhrel, Aniket Datar, Mohammad Nazeri, Xuesu Xiao. (2024)<br><strong>CAHSOR: Competence-Aware High-Speed Off-Road Ground Navigation in SE(3)</strong><br><button class=copy-to-clipboard title="CAHSOR: Competence-Aware High-Speed Off-Road Ground Navigation in SE(3)" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07065v1.pdf filename=2402.07065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While the workspace of traditional ground vehicles is usually assumed to be in a 2D plane, i.e., SE(2), such an assumption may not hold when they drive at high speeds on unstructured off-road terrain: High-speed sharp turns on high-friction surfaces may lead to vehicle rollover; Turning aggressively on loose gravel or grass may violate the non-holonomic constraint and cause significant lateral sliding; Driving quickly on rugged terrain will produce extensive vibration along the vertical axis. Therefore, most offroad vehicles are currently limited to drive only at low speeds to assure vehicle stability and safety. In this work, we aim at empowering high-speed off-road vehicles with competence awareness in SE(3) so that they can reason about the consequences of taking aggressive maneuvers on different terrain with a 6-DoF forward kinodynamic model. The model is learned from visual and inertial Terrain Representation for Off-road Navigation (TRON) using <b>multimodal,</b> <b>self-supervised</b> vehicle-terrain interactions. We demonstrate the efficacy of our Competence-Aware High-Speed Off-Road (CAHSOR) navigation approach on a physical ground robot in both an autonomous navigation and a human shared-control setup and show that CAHSOR can efficiently reduce vehicle instability by 62% while only compromising 8.6% average speed with the help of TRON.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--6781-nonlinear-electro-elastic-finite-element-analysis-with-neural-network-constitutive-models-dominik-k-klein-et-al-2024>(1/1 | 67/81) Nonlinear electro-elastic finite element analysis with neural network constitutive models (Dominik K. Klein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dominik K. Klein, Rogelio Ortigosa, Jesús Martínez-Frutos, Oliver Weeger. (2024)<br><strong>Nonlinear electro-elastic finite element analysis with neural network constitutive models</strong><br><button class=copy-to-clipboard title="Nonlinear electro-elastic finite element analysis with neural network constitutive models" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07007v1.pdf filename=2402.07007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the present work, the applicability of physics-augmented neural network (PANN) constitutive models for complex electro-elastic finite element analysis is demonstrated. For the investigations, PANN models for electro-elastic material behavior at finite deformations are calibrated to different synthetically generated datasets, including an analytical isotropic potential, a homogenised rank-one laminate, and a homogenised metamaterial with a spherical inclusion. Subsequently, boundary value problems inspired by engineering applications of composite electro-elastic materials are considered. Scenarios with large electrically induced deformations and instabilities are particularly challenging and thus necessitate extensive investigations of the PANN constitutive models in the context of finite element analyses. First of all, an excellent prediction quality of the model is required for very general load cases occurring in the <b>simulation.</b> Furthermore, <b>simulation</b> of large deformations and instabilities poses challenges on the stability of the numerical solver, which is closely related to the constitutive model. In all cases studied, the PANN models yield excellent prediction qualities and a stable numerical behavior even in highly nonlinear scenarios. This can be traced back to the PANN models excellent performance in learning both the first and second derivatives of the ground truth electro-elastic potentials, even though it is only calibrated on the first derivatives. Overall, this work demonstrates the applicability of PANN constitutive models for the efficient and robust <b>simulation</b> of engineering applications of composite electro-elastic materials.</p></p class="citation"></blockquote><h2 id=eesssy-2>eess.SY (2)</h2><h3 id=12--6881-high-performance-distributed-control-for-large-scale-linear-systems-a-partitioned-distributed-observer-approach-haotian-xu-et-al-2024>(1/2 | 68/81) High-Performance Distributed Control for Large-Scale Linear Systems: A Partitioned Distributed Observer Approach (Haotian Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Xu, Shuai Liu, Ling Shi. (2024)<br><strong>High-Performance Distributed Control for Large-Scale Linear Systems: A Partitioned Distributed Observer Approach</strong><br><button class=copy-to-clipboard title="High-Performance Distributed Control for Large-Scale Linear Systems: A Partitioned Distributed Observer Approach" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-DS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06903v1.pdf filename=2402.06903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the distributed-observer-based distributed control law has shown powerful ability to arbitrarily approximate the centralized control performance. However, the traditional distributed observer requires each local observer to reconstruct the state information of the whole system, which is unrealistic for large-scale scenarios. To fill this gap, this paper develops a greedy-idea-based large-scale system partition algorithm, which can significantly reduce the dimension of local observers. Then, the partitioned distributed observer for large-scale systems is proposed to overcome the problem that the system dynamics are difficult to estimate due to the coupling between partitions. Furthermore, the two-layer Lyapunov analysis method is adopted and the dynamic transformation lemma of compact errors is proven, which solves the problem of analyzing stability of the error dynamic of the partitioned distributed observer. Finally, it is proved that the distributed control law based on the partitioned distributed observer can also arbitrarily approximate the control performance of the centralized control law, and the dimension of the local observer is greatly reduced compared with the traditional method. The <b>simulation</b> results show that when the similarity between the physical network and the communication network is about 80%, the local observer dimension is greatly reduced by 90% and the relative error between the performance of the distributed control law and that of the centralized control law is less than 1%.</p></p class="citation"></blockquote><h3 id=22--6981-implementation-of-kalman-filter-approach-for-active-noise-control-by-using-matlab-dynamic-noise-cancellation-guo-yu-2024>(2/2 | 69/81) Implementation of Kalman Filter Approach for Active Noise Control by Using MATLAB: Dynamic Noise Cancellation (Guo Yu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guo Yu. (2024)<br><strong>Implementation of Kalman Filter Approach for Active Noise Control by Using MATLAB: Dynamic Noise Cancellation</strong><br><button class=copy-to-clipboard title="Implementation of Kalman Filter Approach for Active Noise Control by Using MATLAB: Dynamic Noise Cancellation" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-AS, eess-SP, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06896v1.pdf filename=2402.06896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article offers an elaborate description of a Kalman filter code employed in the active control system. Conventional active noise management methods usually employ an adaptive filter, such as the filtered reference least mean square (FxLMS) algorithm, to adjust to changes in the primary noise and acoustic environment. Nevertheless, the slow convergence characteristics of the FxLMS algorithm typically impact the effectiveness of reducing dynamic noise. Hence, this study suggests employing the Kalman filter in the active noise control (ANC) system to enhance the efficacy of noise reduction for dynamic noise. The ANC application effectively utilizes the Kalman filter with a novel dynamic ANC model. The numerical <b>simulation</b> revealed that the proposed Kalman filter exhibits superior convergence performance compared to the FxLMS algorithm for handling dynamic noise. The code is available on \href{https://github.com/ShiDongyuan/Kalman_Filter_for_ANC.git}{GitHub} and \href{https://www.mathworks.com/matlabcentral/fileexchange/159311-kalman-filter-for-active-noise-control}{MathWorks}.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--7081-multi-user-sr-ldpc-codes-via-coded-demixing-with-applications-to-cell-free-systems-jamison-r-ebert-et-al-2024>(1/2 | 70/81) Multi-User SR-LDPC Codes via Coded Demixing with Applications to Cell-Free Systems (Jamison R. Ebert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jamison R. Ebert, Jean-Francois Chamberland, Krishna R. Narayanan. (2024)<br><strong>Multi-User SR-LDPC Codes via Coded Demixing with Applications to Cell-Free Systems</strong><br><button class=copy-to-clipboard title="Multi-User SR-LDPC Codes via Coded Demixing with Applications to Cell-Free Systems" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06881v1.pdf filename=2402.06881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Novel sparse regression LDPC (SR-LDPC) codes exhibit excellent performance over additive white Gaussian noise (AWGN) channels in part due to their natural provision of shaping gains. Though SR-LDPC-like codes have been considered within the context of single-user error correction and massive random access, they are yet to be examined as candidates for coordinated multi-user communication scenarios. This article explores this gap in the literature and demonstrates that SR-LDPC codes, when combined with coded demixing techniques, offer a new framework for efficient non-orthogonal multiple access (NOMA) in the context of coordinated multi-user communication channels. The ensuing communication scheme is referred to as MU-SR-LDPC coding. Empirical evidence suggests that, for a fixed SNR, MU-SR-LDPC coding can achieve a target bit error rate (BER) at a higher sum rate than orthogonal multiple access (OMA) techniques such as time division multiple access (TDMA) and frequency division multiple access (FDMA). Importantly, MU-SR-LDPC codes enable a pragmatic solution path for user-centric cell-free communication systems with (local) joint decoding. Results are supported by numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=22--7181-treet-transfer-entropy-estimation-via-transformer-omer-luxembourg-et-al-2024>(2/2 | 71/81) TREET: TRansfer Entropy Estimation via Transformer (Omer Luxembourg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omer Luxembourg, Dor Tsur, Haim Permuter. (2024)<br><strong>TREET: TRansfer Entropy Estimation via Transformer</strong><br><button class=copy-to-clipboard title="TREET: TRansfer Entropy Estimation via Transformer" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06919v1.pdf filename=2402.06919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Transfer entropy (TE) is a measurement in information theory that reveals the directional flow of information between processes, providing valuable insights for a wide range of real-world applications. This work proposes Transfer Entropy Estimation via <b>Transformers</b> (TREET), a novel <b>transformer-based</b> approach for estimating the TE for stationary processes. The proposed approach employs Donsker-Vardhan (DV) representation to TE and leverages the attention mechanism for the task of neural estimation. We propose a detailed theoretical and empirical study of the TREET, comparing it to existing methods. To increase its applicability, we design an estimated TE optimization scheme that is motivated by the functional representation lemma. Afterwards, we take advantage of the joint optimization scheme to optimize the capacity of communication channels with memory, which is a canonical optimization problem in information theory, and show the memory capabilities of our estimator. Finally, we apply TREET to real-world feature analysis. Our work, applied with state-of-the-art deep learning methods, opens a new door for communication problems which are yet to be solved.</p></p class="citation"></blockquote><h2 id=csir-1>cs.IR (1)</h2><h3 id=11--7281-non-autoregressive-generative-models-for-reranking-recommendation-yuxin-ren-et-al-2024>(1/1 | 72/81) Non-autoregressive Generative Models for Reranking Recommendation (Yuxin Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxin Ren, Qiya Yang, Yichun Wu, Wei Xu, Yalong Wang, Zhiqiang Zhang. (2024)<br><strong>Non-autoregressive Generative Models for Reranking Recommendation</strong><br><button class=copy-to-clipboard title="Non-autoregressive Generative Models for Reranking Recommendation" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Rerank<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06871v1.pdf filename=2402.06871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a multi-stage <b>recommendation</b> system, <b>reranking</b> plays a crucial role by modeling the intra-list correlations among items.The key challenge of <b>reranking</b> lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. Generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Hence, we propose a Non-AutoRegressive generative model for <b>reranking</b> <b>Recommendation</b> (NAR4Rec) designed to enhance efficiency and effectiveness. To address challenges related to sparse training samples and dynamic candidates impacting model convergence, we introduce a matching model. Considering the diverse nature of user feedback, we propose a sequence-level unlikelihood training objective to distinguish feasible from unfeasible sequences. Additionally, to overcome the lack of dependency modeling in non-autoregressive models regarding target items, we introduce contrastive decoding to capture correlations among these items. Extensive offline experiments on publicly available datasets validate the superior performance of our proposed approach compared to the existing state-of-the-art <b>reranking</b> methods. Furthermore, our method has been fully deployed in a popular video app Kuaishou with over 300 million daily active users, significantly enhancing online <b>recommendation</b> quality, and demonstrating the effectiveness and efficiency of our approach.</p></p class="citation"></blockquote><h2 id=eessiv-2>eess.IV (2)</h2><h3 id=12--7381-an-optimization-framework-for-processing-and-transfer-learning-for-the-brain-tumor-segmentation-tianyi-ren-et-al-2024>(1/2 | 73/81) An Optimization Framework for Processing and Transfer Learning for the Brain Tumor Segmentation (Tianyi Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyi Ren, Ethan Honey, Harshitha Rebala, Abhishek Sharma, Agamdeep Chopra, Mehmet Kurt. (2024)<br><strong>An Optimization Framework for Processing and Transfer Learning for the Brain Tumor Segmentation</strong><br><button class=copy-to-clipboard title="An Optimization Framework for Processing and Transfer Learning for the Brain Tumor Segmentation" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 13<br>Keywords: Multi-modal, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07008v1.pdf filename=2402.07008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tumor segmentation from <b>multi-modal</b> brain MRI images is a challenging task due to the limited samples, high variance in shapes and uneven distribution of tumor morphology. The performance of automated medical image segmentation has been significant improvement by the recent advances in deep learning. However, the model predictions have not yet reached the desired level for clinical use in terms of accuracy and generalizability. In order to address the distinct problems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed an optimization framework based on a 3D U-Net model for brain tumor segmentation. This framework incorporates a range of techniques, including various pre-processing and post-processing techniques, and <b>transfer</b> <b>learning.</b> On the validation datasets, this multi-modality brain tumor segmentation framework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on Challenges 1, 2, 3 respectively.</p></p class="citation"></blockquote><h3 id=22--7481-disentangled-latent-energy-based-style-translation-an-image-level-structural-mri-harmonization-framework-mengqi-wu-et-al-2024>(2/2 | 74/81) Disentangled Latent Energy-Based Style Translation: An Image-Level Structural MRI Harmonization Framework (Mengqi Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengqi Wu, Lintao Zhang, Pew-Thian Yap, Hongtu Zhu, Mingxia Liu. (2024)<br><strong>Disentangled Latent Energy-Based Style Translation: An Image-Level Structural MRI Harmonization Framework</strong><br><button class=copy-to-clipboard title="Disentangled Latent Energy-Based Style Translation: An Image-Level Structural MRI Harmonization Framework" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06875v1.pdf filename=2402.06875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Brain magnetic resonance imaging (MRI) has been extensively employed across clinical and research fields, but often exhibits sensitivity to site effects arising from nonbiological variations such as differences in field strength and scanner vendors. Numerous retrospective MRI harmonization techniques have demonstrated encouraging outcomes in reducing the site effects at image level. However, existing methods generally suffer from high computational requirements and limited generalizability, restricting their applicability to unseen MRIs. In this paper, we design a novel disentangled latent energy-based style translation (DLEST) framework for unpaired image-level MRI harmonization, consisting of (1) site-invariant image generation (SIG), (2) site-specific style translation (SST), and (3) site-specific MRI synthesis (SMS). Specifically, the SIG employs a latent <b>autoencoder</b> to encode MRIs into a low-dimensional latent space and reconstruct MRIs from latent codes. The SST utilizes an energy-based model to comprehend the global latent distribution of a target domain and translate source latent codes toward the target domain, while SMS enables MRI synthesis with a target-specific style. By disentangling image generation and style translation in latent space, the DLEST can achieve efficient style translation. Our model was trained on T1-weighted MRIs from a public dataset (with 3,984 subjects across 58 acquisition sites/settings) and validated on an independent dataset (with 9 traveling subjects scanned in 11 sites/settings) in 4 tasks: (1) histogram and clustering comparison, (2) site classification, (3) brain tissue segmentation, and (4) site-specific MRI synthesis. Qualitative and quantitative results demonstrate the superiority of our method over several state-of-the-arts.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--7581-on-leaky-integrate-and-fire-as-spike-train-quantization-operator-on-dirac-superimposed-continuous-time-signals-bernhard-a-moser-et-al-2024>(1/1 | 75/81) On Leaky-Integrate-and Fire as Spike-Train-Quantization Operator on Dirac-Superimposed Continuous-Time Signals (Bernhard A. Moser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bernhard A. Moser, Michael Lunglmayr. (2024)<br><strong>On Leaky-Integrate-and Fire as Spike-Train-Quantization Operator on Dirac-Superimposed Continuous-Time Signals</strong><br><button class=copy-to-clipboard title="On Leaky-Integrate-and Fire as Spike-Train-Quantization Operator on Dirac-Superimposed Continuous-Time Signals" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: 82C32, 41A65, cs-NE, cs.NE, eess-SP<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07954v1.pdf filename=2402.07954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leaky-integrate-and-fire (LIF) is studied as a non-linear operator that maps an integrable signal $f$ to a sequence $\eta_f$ of discrete events, the spikes. In the case without any Dirac pulses in the input, it makes no difference whether to set the neuron&rsquo;s potential to zero or to subtract the threshold $\vartheta$ immediately after a spike triggering event. However, in the case of superimpose Dirac pulses the situation is different which raises the question of a mathematical justification of each of the proposed reset variants. In the limit case of zero refractory time the standard reset scheme based on threshold subtraction results in a modulo-based reset scheme which allows to characterize LIF as a <b>quantization</b> operator based on a weighted Alexiewicz norm $|.|<em>{A, \alpha}$ with leaky parameter $\alpha$. We prove the <b>quantization</b> formula $|\eta_f - f|</em>{A, \alpha} &lt; \vartheta$ under the general condition of local integrability, almost everywhere boundedness and locally finitely many superimposed weighted Dirac pulses which provides a much larger signal space and more flexible sparse signal representation than manageable by classical signal processing.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--7681-a-rational-analysis-of-the-speech-to-song-illusion-raja-marjieh-et-al-2024>(1/1 | 76/81) A Rational Analysis of the Speech-to-Song Illusion (Raja Marjieh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raja Marjieh, Pol van Rijn, Ilia Sucholutsky, Harin Lee, Thomas L. Griffiths, Nori Jacoby. (2024)<br><strong>A Rational Analysis of the Speech-to-Song Illusion</strong><br><button class=copy-to-clipboard title="A Rational Analysis of the Speech-to-Song Illusion" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-AI, cs-CL, q-bio-NC, q-bio.NC, stat-AP<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06992v1.pdf filename=2402.06992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The speech-to-song illusion is a robust psychological phenomenon whereby a spoken sentence sounds increasingly more musical as it is repeated. Despite decades of research, a complete formal account of this transformation is still lacking, and some of its nuanced characteristics, namely, that certain phrases appear to transform while others do not, is not well understood. Here we provide a formal account of this phenomenon, by recasting it as a statistical inference whereby a rational agent attempts to decide whether a sequence of utterances is more likely to have been produced in a song or speech. Using this approach and analyzing song and speech corpora, we further introduce a novel prose-to-lyrics illusion that is purely text-based. In this illusion, simply duplicating written sentences makes them appear more like song lyrics. We provide robust evidence for this new illusion in both human participants and <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=11--7781-evaluation-metrics-for-automated-typographic-poster-generation-sérgio-m-rebelo-et-al-2024>(1/1 | 77/81) Evaluation Metrics for Automated Typographic Poster Generation (Sérgio M. Rebelo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sérgio M. Rebelo, J. J. Merelo, João Bicker, Penousal Machado. (2024)<br><strong>Evaluation Metrics for Automated Typographic Poster Generation</strong><br><button class=copy-to-clipboard title="Evaluation Metrics for Automated Typographic Poster Generation" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: 68W50, I-2-1; I-7; J-7; J-5, cs-AI, cs-HC, cs-MM, cs.MM<br>Keyword Score: 10<br>Keywords: Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06945v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06945v1.pdf filename=2402.06945v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computational Design approaches facilitate the generation of typographic design, but evaluating these designs remains a challenging task. In this paper, we propose a set of heuristic metrics for typographic design evaluation, focusing on their legibility, which assesses the text visibility, aesthetics, which evaluates the visual quality of the design, and semantic features, which estimate how effectively the design conveys the content semantics. We experiment with a constrained evolutionary approach for generating typographic posters, incorporating the proposed evaluation metrics with varied setups, and treating the legibility metrics as constraints. We also integrate <b>emotion</b> <b>recognition</b> to identify text semantics automatically and analyse the performance of the approach and the visual characteristics outputs.</p></p class="citation"></blockquote><h2 id=cshc-1>cs.HC (1)</h2><h3 id=11--7881-dimvis-interpreting-visual-clusters-in-dimensionality-reduction-with-explainable-boosting-machine-parisa-salmanian-et-al-2024>(1/1 | 78/81) DimVis: Interpreting Visual Clusters in Dimensionality Reduction With Explainable Boosting Machine (Parisa Salmanian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parisa Salmanian, Angelos Chatzimparmpas, Ali Can Karaca, Rafael M. Martins. (2024)<br><strong>DimVis: Interpreting Visual Clusters in Dimensionality Reduction With Explainable Boosting Machine</strong><br><button class=copy-to-clipboard title="DimVis: Interpreting Visual Clusters in Dimensionality Reduction With Explainable Boosting Machine" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-LG, cs.HC, stat-CO<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06885v1.pdf filename=2402.06885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dimensionality Reduction (DR) techniques such as t-SNE and UMAP are popular for transforming complex datasets into simpler visual representations. However, while effective in uncovering general dataset patterns, these methods may introduce artifacts and suffer from interpretability issues. This paper presents DimVis, a visualization tool that employs <b>supervised</b> Explainable Boosting Machine (EBM) models (trained on user-selected data of interest) as an interpretation assistant for DR projections. Our tool facilitates high-dimensional data analysis by providing an interpretation of feature relevance in visual clusters through interactive exploration of UMAP projections. Specifically, DimVis uses a contrastive EBM model that is trained in real time to differentiate between the data inside and outside a cluster of interest. Taking advantage of the inherent explainable nature of the EBM, we then use this model to interpret the cluster itself via single and pairwise feature comparisons in a ranking based on the EBM model&rsquo;s feature importance. The applicability and effectiveness of DimVis are demonstrated through two use cases involving real-world datasets, and we also discuss the limitations and potential directions for future research.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--7981-optimizing-the-design-of-an-artificial-pancreas-to-improve-diabetes-management-ashok-khanna-et-al-2024>(1/1 | 79/81) Optimizing the Design of an Artificial Pancreas to Improve Diabetes Management (Ashok Khanna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashok Khanna, Olivier Francon, Risto Miikkulainen. (2024)<br><strong>Optimizing the Design of an Artificial Pancreas to Improve Diabetes Management</strong><br><button class=copy-to-clipboard title="Optimizing the Design of an Artificial Pancreas to Improve Diabetes Management" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, cs-LG, cs-NE, q-bio-QM, q-bio.QM<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07949v1.pdf filename=2402.07949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diabetes, a chronic condition that impairs how the body turns food into energy, i.e. blood glucose, affects 38 million people in the US alone. The standard treatment is to supplement carbohydrate intake with an artificial pancreas, i.e. a continuous insulin pump (basal shots), as well as occasional insulin injections (bolus shots). The goal of the treatment is to keep blood glucose at the center of an acceptable range, as measured through a continuous glucose meter. A secondary goal is to minimize injections, which are unpleasant and difficult for some patients to implement. In this study, neuroevolution was used to discover an optimal strategy for the treatment. Based on a dataset of 30 days of treatment and measurements of a single patient, a random forest was first trained to predict future glucose levels. A neural network was then evolved to prescribe carbohydrates, basal pumping levels, and bolus injections. Evolution discovered a Pareto front that reduced deviation from the target and number of injections compared to the original data, thus improving patients&rsquo; quality of life. To make the system easier to adopt, a language interface was developed with a <b>large</b> <b>language</b> <b>model.</b> Thus, these technologies not only improve patient care but also adoption in a broader population.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--8081-protir-iterative-refinement-between-retrievers-and-predictors-for-protein-function-annotation-zuobai-zhang-et-al-2024>(1/1 | 80/81) ProtIR: Iterative Refinement between Retrievers and Predictors for Protein Function Annotation (Zuobai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zuobai Zhang, Jiarui Lu, Vijil Chenthamarakshan, Aurélie Lozano, Payel Das, Jian Tang. (2024)<br><strong>ProtIR: Iterative Refinement between Retrievers and Predictors for Protein Function Annotation</strong><br><button class=copy-to-clipboard title="ProtIR: Iterative Refinement between Retrievers and Predictors for Protein Function Annotation" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07955v1.pdf filename=2402.07955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Protein function annotation is an important yet challenging task in biology. Recent deep learning advancements show significant potential for accurate function prediction by learning from protein sequences and structures. Nevertheless, these predictor-based methods often overlook the modeling of protein similarity, an idea commonly employed in traditional approaches using sequence or structure retrieval tools. To fill this gap, we first study the effect of inter-protein similarity modeling by <b>benchmarking</b> retriever-based methods against predictors on protein function annotation tasks. Our results show that retrievers can match or outperform predictors without large-scale pre-training. Building on these insights, we introduce a novel variational pseudo-likelihood framework, ProtIR, designed to improve function predictors by incorporating inter-protein similarity modeling. This framework iteratively refines knowledge between a function predictor and retriever, thereby combining the strengths of both predictors and retrievers. ProtIR showcases around 10% improvement over vanilla predictor-based methods. Besides, it achieves performance on par with protein language model-based methods, yet without the need for massive pre-training, highlighting the efficacy of our framework. Code will be released upon acceptance.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--8181-estimating-the-effect-of-crosstalk-error-on-circuit-fidelity-using-noisy-intermediate-scale-quantum-devices-sovanmonynuth-heng-et-al-2024>(1/1 | 81/81) Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices (Sovanmonynuth Heng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sovanmonynuth Heng, Myeongseong Go, Youngsun Han. (2024)<br><strong>Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices</strong><br><button class=copy-to-clipboard title="Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, quant-ph, quant-ph<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06952v1.pdf filename=2402.06952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current advancements in technology have focused the attention of the quantum computing community toward exploring the potential of near-term devices whose computing power surpasses that of classical computers in practical applications. An unresolved central question revolves around whether the inherent noise in these devices can be overcome or whether any potential quantum advantage would be limited. There is no doubt that crosstalk is one of the main sources of noise in noisy intermediate-scale quantum (NISQ) systems, and it poses a fundamental challenge to hardware designs. Crosstalk between parallel instructions can corrupt quantum states and cause incorrect program execution. In this study, we present a comprehensive analysis of the crosstalk error effect on NISQ computers. Our approach is extremely straightforward and practical for characterizing the crosstalk error of various multi-qubit devices. In particular, we combine the randomized <b>benchmarking</b> (RB) and simultaneous randomized <b>benchmarking</b> (SRB) protocol to characterize the crosstalk error from the correlation controlled-NOT (CNOT) gate. We demonstrate this protocol experimentally on 5- & 7-qubit devices. Our results demonstrate the crosstalk error model of two different IBM quantum devices over the experimental week and compare the error variation against the machine, number of qubits, quantum volume, processor, and topology of the IBM quantum devices. We then confirm the improvement in the circuit fidelity on different <b>benchmarks</b> by up to 3.06x via inserting an instruction barrier, as compared with an IBM quantum noisy device which offers near-optimal crosstalk mitigation in practice. Most importantly, we provide insight to ensure that the quantum operation can perform its quantum magic undisturbed.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.11</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.13</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-14>cs.CL (14)</a><ul><li><a href=#114--181-gemini-goes-to-med-school-exploring-the-capabilities-of-multimodal-large-language-models-on-medical-challenge-problems--hallucinations-ankit-pal-et-al-2024>(1/14 | 1/81) Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations (Ankit Pal et al., 2024)</a></li><li><a href=#214--281-should-i-try-multiple-optimizers-when-fine-tuning-pre-trained-transformers-for-nlp-tasks-should-i-tune-their-hyperparameters-nefeli-gkouti-et-al-2024>(2/14 | 2/81) Should I try multiple optimizers when fine-tuning pre-trained Transformers for NLP tasks? Should I tune their hyperparameters? (Nefeli Gkouti et al., 2024)</a></li><li><a href=#314--381-gentranslate-large-language-models-are-generative-multilingual-speech-and-machine-translators-yuchen-hu-et-al-2024>(3/14 | 3/81) GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators (Yuchen Hu et al., 2024)</a></li><li><a href=#414--481-nlp-for-knowledge-discovery-and-information-extraction-from-energetics-corpora-francis-g-vangessel-et-al-2024>(4/14 | 4/81) NLP for Knowledge Discovery and Information Extraction from Energetics Corpora (Francis G. VanGessel et al., 2024)</a></li><li><a href=#514--581-semi-supervised-learning-for-bilingual-lexicon-induction-paul-garnier-et-al-2024>(5/14 | 5/81) Semi-Supervised Learning for Bilingual Lexicon Induction (Paul Garnier et al., 2024)</a></li><li><a href=#614--681-investigating-consistency-in-query-based-meeting-summarization-a-comparative-study-of-different-embedding-methods-chen-jia-chen-et-al-2024>(6/14 | 6/81) Investigating Consistency in Query-Based Meeting Summarization: A Comparative Study of Different Embedding Methods (Chen Jia-Chen et al., 2024)</a></li><li><a href=#714--781-instruct-once-chat-consistently-in-multiple-rounds-an-efficient-tuning-framework-for-dialogue-jian-wang-et-al-2024>(7/14 | 7/81) Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue (Jian Wang et al., 2024)</a></li><li><a href=#814--881-a-thorough-examination-of-decoding-methods-in-the-era-of-llms-chufan-shi-et-al-2024>(8/14 | 8/81) A Thorough Examination of Decoding Methods in the Era of LLMs (Chufan Shi et al., 2024)</a></li><li><a href=#914--981-can-llms-recognize-toxicity-structured-toxicity-investigation-framework-and-semantic-based-metric-hyukhun-koh-et-al-2024>(9/14 | 9/81) Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric (Hyukhun Koh et al., 2024)</a></li><li><a href=#1014--1081-speechclip-self-supervised-multi-task-representation-learning-for-speech-via-clip-and-speech-image-data-hsuan-fu-wang-et-al-2024>(10/14 | 10/81) SpeechCLIP+: Self-supervised multi-task representation learning for speech via CLIP and speech-image data (Hsuan-Fu Wang et al., 2024)</a></li><li><a href=#1114--1181-lifi-lightweight-controlled-text-generation-with-fine-grained-control-codes-chufan-shi-et-al-2024>(11/14 | 11/81) LiFi: Lightweight Controlled Text Generation with Fine-Grained Control Codes (Chufan Shi et al., 2024)</a></li><li><a href=#1214--1281-tldr-progress-multi-faceted-literature-exploration-in-text-summarization-shahbaz-syed-et-al-2024>(12/14 | 12/81) TL;DR Progress: Multi-faceted Literature Exploration in Text Summarization (Shahbaz Syed et al., 2024)</a></li><li><a href=#1314--1381-history-development-and-principles-of-large-language-models-an-introductory-survey-zhibo-chu-et-al-2024>(13/14 | 13/81) History, Development, and Principles of Large Language Models-An Introductory Survey (Zhibo Chu et al., 2024)</a></li><li><a href=#1414--1481-event-keyed-summarization-william-gantt-et-al-2024>(14/14 | 14/81) Event-Keyed Summarization (William Gantt et al., 2024)</a></li></ul></li><li><a href=#cslg-25>cs.LG (25)</a><ul><li><a href=#125--1581-openfedllm-training-large-language-models-on-decentralized-private-data-via-federated-learning-rui-ye-et-al-2024>(1/25 | 15/81) OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning (Rui Ye et al., 2024)</a></li><li><a href=#225--1681-principled-penalty-based-methods-for-bilevel-reinforcement-learning-and-rlhf-han-shen-et-al-2024>(2/25 | 16/81) Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF (Han Shen et al., 2024)</a></li><li><a href=#325--1781-in-context-data-distillation-with-tabpfn-junwei-ma-et-al-2024>(3/25 | 17/81) In-Context Data Distillation with TabPFN (Junwei Ma et al., 2024)</a></li><li><a href=#425--1881-generating-chain-of-thoughts-with-a-direct-pairwise-comparison-approach-to-searching-for-the-most-promising-intermediate-thought-zhen-yu-zhang-et-al-2024>(4/25 | 18/81) Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought (Zhen-Yu Zhang et al., 2024)</a></li><li><a href=#525--1981-lirank-industrial-large-scale-ranking-models-at-linkedin-fedor-borisyuk-et-al-2024>(5/25 | 19/81) LiRank: Industrial Large Scale Ranking Models at LinkedIn (Fedor Borisyuk et al., 2024)</a></li><li><a href=#625--2081-a-tale-of-tails-model-collapse-as-a-change-of-scaling-laws-elvis-dohmatob-et-al-2024>(6/25 | 20/81) A Tale of Tails: Model Collapse as a Change of Scaling Laws (Elvis Dohmatob et al., 2024)</a></li><li><a href=#725--2181-clustering-techniques-selection-for-a-hybrid-regression-model-a-case-study-based-on-a-solar-thermal-system-maría-teresa-garcía-ordás-et-al-2024>(7/25 | 21/81) Clustering Techniques Selection for a Hybrid Regression Model: A Case Study Based on a Solar Thermal System (María Teresa García-Ordás et al., 2024)</a></li><li><a href=#825--2281-discriminative-adversarial-unlearning-rohan-sharma-et-al-2024>(8/25 | 22/81) Discriminative Adversarial Unlearning (Rohan Sharma et al., 2024)</a></li><li><a href=#925--2381-deepcover-advancing-rnn-test-coverage-and-online-error-prediction-using-state-machine-extraction-pouria-golshanrad-et-al-2024>(9/25 | 23/81) DeepCover: Advancing RNN Test Coverage and Online Error Prediction using State Machine Extraction (Pouria Golshanrad et al., 2024)</a></li><li><a href=#1025--2481-learning-attributed-graphlets-predictive-graph-mining-by-graphlets-with-trainable-attribute-tajima-shinji-et-al-2024>(10/25 | 24/81) Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute (Tajima Shinji et al., 2024)</a></li><li><a href=#1125--2581-topological-neural-networks-mitigating-the-bottlenecks-of-graph-neural-networks-via-higher-order-interactions-lorenzo-giusti-2024>(11/25 | 25/81) Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions (Lorenzo Giusti, 2024)</a></li><li><a href=#1225--2681-for-better-or-for-worse-learning-minimum-variance-features-with-label-augmentation-muthu-chidambaram-et-al-2024>(12/25 | 26/81) For Better or For Worse? Learning Minimum Variance Features With Label Augmentation (Muthu Chidambaram et al., 2024)</a></li><li><a href=#1325--2781-llm-learning-automata-from-examples-using-natural-language-oracles-marcell-vazquez-chanlatte-et-al-2024>(13/25 | 27/81) $L^*LM$: Learning Automata from Examples using Natural Language Oracles (Marcell Vazquez-Chanlatte et al., 2024)</a></li><li><a href=#1425--2881-distilling-symbolic-priors-for-concept-learning-into-neural-networks-ioana-marinescu-et-al-2024>(14/25 | 28/81) Distilling Symbolic Priors for Concept Learning into Neural Networks (Ioana Marinescu et al., 2024)</a></li><li><a href=#1525--2981-fiddler-cpu-gpu-orchestration-for-fast-inference-of-mixture-of-experts-models-keisuke-kamahori-et-al-2024>(15/25 | 29/81) Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models (Keisuke Kamahori et al., 2024)</a></li><li><a href=#1625--3081-non-linear-fusion-in-federated-learning-a-hypernetwork-approach-to-federated-domain-generalization-marc-bartholet-et-al-2024>(16/25 | 30/81) Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization (Marc Bartholet et al., 2024)</a></li><li><a href=#1725--3181-tree-ensembles-for-contextual-bandits-hannes-nilsson-et-al-2024>(17/25 | 31/81) Tree Ensembles for Contextual Bandits (Hannes Nilsson et al., 2024)</a></li><li><a href=#1825--3281-solving-deep-reinforcement-learning-benchmarks-with-linear-policy-networks-annie-wong-et-al-2024>(18/25 | 32/81) Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks (Annie Wong et al., 2024)</a></li><li><a href=#1925--3381-fast-ucb-type-algorithms-for-stochastic-bandits-with-heavy-and-super-heavy-symmetric-noise-yuriy-dorn-et-al-2024>(19/25 | 33/81) Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise (Yuriy Dorn et al., 2024)</a></li><li><a href=#2025--3481-understanding-the-training-speedup-from-sampling-with-approximate-losses-rudrajit-das-et-al-2024>(20/25 | 34/81) Understanding the Training Speedup from Sampling with Approximate Losses (Rudrajit Das et al., 2024)</a></li><li><a href=#2125--3581-informativeness-of-reward-functions-in-reinforcement-learning-rati-devidze-et-al-2024>(21/25 | 35/81) Informativeness of Reward Functions in Reinforcement Learning (Rati Devidze et al., 2024)</a></li><li><a href=#2225--3681-fedimpro-measuring-and-improving-client-update-in-federated-learning-zhenheng-tang-et-al-2024>(22/25 | 36/81) FedImpro: Measuring and Improving Client Update in Federated Learning (Zhenheng Tang et al., 2024)</a></li><li><a href=#2325--3781-clients-collaborate-flexible-differentially-private-federated-learning-with-guaranteed-improvement-of-utility-privacy-trade-off-yuecheng-li-et-al-2024>(23/25 | 37/81) Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off (Yuecheng Li et al., 2024)</a></li><li><a href=#2425--3881-understanding-test-time-augmentation-masanari-kimura-2024>(24/25 | 38/81) Understanding Test-Time Augmentation (Masanari Kimura, 2024)</a></li><li><a href=#2525--3981-assessing-uncertainty-estimation-methods-for-3d-image-segmentation-under-distribution-shifts-masoumeh-javanbakhat-et-al-2024>(25/25 | 39/81) Assessing Uncertainty Estimation Methods for 3D Image Segmentation under Distribution Shifts (Masoumeh Javanbakhat et al., 2024)</a></li></ul></li><li><a href=#csai-5>cs.AI (5)</a><ul><li><a href=#15--4081-urbankgent-a-unified-large-language-model-agent-framework-for-urban-knowledge-graph-construction-yansong-ning-et-al-2024>(1/5 | 40/81) UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction (Yansong Ning et al., 2024)</a></li><li><a href=#25--4181-realm-rag-driven-enhancement-of-multimodal-electronic-health-records-analysis-via-large-language-models-yinghao-zhu-et-al-2024>(2/5 | 41/81) REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models (Yinghao Zhu et al., 2024)</a></li><li><a href=#35--4281-chemllm-a-chemical-large-language-model-di-zhang-et-al-2024>(3/5 | 42/81) ChemLLM: A Chemical Large Language Model (Di Zhang et al., 2024)</a></li><li><a href=#45--4381-a-factor-graph-model-of-trust-for-a-collaborative-multi-agent-system-behzad-akbari-et-al-2024>(4/5 | 43/81) A Factor Graph Model of Trust for a Collaborative Multi-Agent System (Behzad Akbari et al., 2024)</a></li><li><a href=#55--4481-making-a-prototype-of-seoul-historical-sites-chatbot-using-langchain-jae-young-suh-et-al-2024>(5/5 | 44/81) Making a prototype of Seoul historical sites chatbot using Langchain (Jae Young Suh et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--4581-cochceps-augment-a-novel-self-supervised-contrastive-learning-using-cochlear-cepstrum-based-masking-for-speech-emotion-recognition-ioannis-ziogas-et-al-2024>(1/2 | 45/81) CochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using Cochlear Cepstrum-based Masking for Speech Emotion Recognition (Ioannis Ziogas et al., 2024)</a></li><li><a href=#22--4681-analysis-of-self-supervised-speech-models-on-childrens-speech-and-infant-vocalizations-jialu-li-et-al-2024>(2/2 | 46/81) Analysis of Self-Supervised Speech Models on Children&rsquo;s Speech and Infant Vocalizations (Jialu Li et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#13--4781-near-perfect-coverage-manifold-estimation-in-cellular-networks-via-conditional-gan-washim-uddin-mondal-et-al-2024>(1/3 | 47/81) Near-perfect Coverage Manifold Estimation in Cellular Networks via conditional GAN (Washim Uddin Mondal et al., 2024)</a></li><li><a href=#23--4881-orient-a-priority-aware-energy-efficient-approach-for-latency-sensitive-applications-in-6g-masoud-shokrnezhad-et-al-2024>(2/3 | 48/81) ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G (Masoud Shokrnezhad et al., 2024)</a></li><li><a href=#33--4981-toward-scalable-generative-ai-via-mixture-of-experts-in-mobile-edge-networks-jiacheng-wang-et-al-2024>(3/3 | 49/81) Toward Scalable Generative AI via Mixture of Experts in Mobile Edge Networks (Jiacheng Wang et al., 2024)</a></li></ul></li><li><a href=#cscr-4>cs.CR (4)</a><ul><li><a href=#14--5081-system-level-analysis-of-adversarial-attacks-and-defenses-on-intelligence-in-o-ran-based-cellular-networks-azuka-chiejina-et-al-2024>(1/4 | 50/81) System-level Analysis of Adversarial Attacks and Defenses on Intelligence in O-RAN based Cellular Networks (Azuka Chiejina et al., 2024)</a></li><li><a href=#24--5181-sentinels-of-the-stream-unleashing-large-language-models-for-dynamic-packet-classification-in-software-defined-networks----position-paper-shariq-murtuza-2024>(2/4 | 51/81) Sentinels of the Stream: Unleashing Large Language Models for Dynamic Packet Classification in Software Defined Networks &ndash; Position Paper (Shariq Murtuza, 2024)</a></li><li><a href=#34--5281-whispers-in-the-machine-confidentiality-in-llm-integrated-systems-jonathan-evertz-et-al-2024>(3/4 | 52/81) Whispers in the Machine: Confidentiality in LLM-integrated Systems (Jonathan Evertz et al., 2024)</a></li><li><a href=#44--5381-benchmarking-frameworks-and-comparative-studies-of-controller-area-network-can-intrusion-detection-systems-a-review-shaila-sharmin-et-al-2024>(4/4 | 53/81) Benchmarking Frameworks and Comparative Studies of Controller Area Network (CAN) Intrusion Detection Systems: A Review (Shaila Sharmin et al., 2024)</a></li></ul></li><li><a href=#cscv-7>cs.CV (7)</a><ul><li><a href=#17--5481-domain-adaptable-fine-tune-distillation-framework-for-advancing-farm-surveillance-raza-imam-et-al-2024>(1/7 | 54/81) Domain Adaptable Fine-Tune Distillation Framework For Advancing Farm Surveillance (Raza Imam et al., 2024)</a></li><li><a href=#27--5581-synthesizing-cta-image-data-for-type-b-aortic-dissection-using-stable-diffusion-models-ayman-abaid-et-al-2024>(2/7 | 55/81) Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable Diffusion Models (Ayman Abaid et al., 2024)</a></li><li><a href=#37--5681-semantic-object-level-modeling-for-robust-visual-camera-relocalization-yifan-zhu-et-al-2024>(3/7 | 56/81) Semantic Object-level Modeling for Robust Visual Camera Relocalization (Yifan Zhu et al., 2024)</a></li><li><a href=#47--5781-latent-enhancing-autoencoder-for-occluded-image-classification-ketan-kotwal-et-al-2024>(4/7 | 57/81) Latent Enhancing AutoEncoder for Occluded Image Classification (Ketan Kotwal et al., 2024)</a></li><li><a href=#57--5881-treatment-wise-glioblastoma-survival-inference-with-multi-parametric-preoperative-mri-xiaofeng-liu-et-al-2024>(5/7 | 58/81) Treatment-wise Glioblastoma Survival Inference with Multi-parametric Preoperative MRI (Xiaofeng Liu et al., 2024)</a></li><li><a href=#67--5981-a-change-detection-reality-check-isaac-corley-et-al-2024>(6/7 | 59/81) A Change Detection Reality Check (Isaac Corley et al., 2024)</a></li><li><a href=#77--6081-ossar-towards-open-set-surgical-activity-recognition-in-robot-assisted-surgery-long-bai-et-al-2024>(7/7 | 60/81) OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted Surgery (Long Bai et al., 2024)</a></li></ul></li><li><a href=#statml-2>stat.ML (2)</a><ul><li><a href=#12--6181-generalization-error-of-graph-neural-networks-in-the-mean-field-regime-gholamali-aminian-et-al-2024>(1/2 | 61/81) Generalization Error of Graph Neural Networks in the Mean-field Regime (Gholamali Aminian et al., 2024)</a></li><li><a href=#22--6281-low-rank-approximation-of-structural-redundancy-for-self-supervised-learning-kang-du-et-al-2024>(2/2 | 62/81) Low-Rank Approximation of Structural Redundancy for Self-Supervised Learning (Kang Du et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--6381-cacophony-an-improved-contrastive-audio-text-model-ge-zhu-et-al-2024>(1/2 | 63/81) Cacophony: An Improved Contrastive Audio-Text Model (Ge Zhu et al., 2024)</a></li><li><a href=#22--6481-speech-motion-anomaly-detection-via-cross-modal-translation-of-4d-motion-fields-from-tagged-mri-xiaofeng-liu-et-al-2024>(2/2 | 64/81) Speech motion anomaly detection via cross-modal translation of 4D motion fields from tagged MRI (Xiaofeng Liu et al., 2024)</a></li></ul></li><li><a href=#csro-2>cs.RO (2)</a><ul><li><a href=#12--6581-bayesian-optimization-with-adaptive-kernels-for-robot-control-ruben-martinez-cantin-2024>(1/2 | 65/81) Bayesian Optimization with Adaptive Kernels for Robot Control (Ruben Martinez-Cantin, 2024)</a></li><li><a href=#22--6681-cahsor-competence-aware-high-speed-off-road-ground-navigation-in-se3-anuj-pokhrel-et-al-2024>(2/2 | 66/81) CAHSOR: Competence-Aware High-Speed Off-Road Ground Navigation in SE(3) (Anuj Pokhrel et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--6781-nonlinear-electro-elastic-finite-element-analysis-with-neural-network-constitutive-models-dominik-k-klein-et-al-2024>(1/1 | 67/81) Nonlinear electro-elastic finite element analysis with neural network constitutive models (Dominik K. Klein et al., 2024)</a></li></ul></li><li><a href=#eesssy-2>eess.SY (2)</a><ul><li><a href=#12--6881-high-performance-distributed-control-for-large-scale-linear-systems-a-partitioned-distributed-observer-approach-haotian-xu-et-al-2024>(1/2 | 68/81) High-Performance Distributed Control for Large-Scale Linear Systems: A Partitioned Distributed Observer Approach (Haotian Xu et al., 2024)</a></li><li><a href=#22--6981-implementation-of-kalman-filter-approach-for-active-noise-control-by-using-matlab-dynamic-noise-cancellation-guo-yu-2024>(2/2 | 69/81) Implementation of Kalman Filter Approach for Active Noise Control by Using MATLAB: Dynamic Noise Cancellation (Guo Yu, 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--7081-multi-user-sr-ldpc-codes-via-coded-demixing-with-applications-to-cell-free-systems-jamison-r-ebert-et-al-2024>(1/2 | 70/81) Multi-User SR-LDPC Codes via Coded Demixing with Applications to Cell-Free Systems (Jamison R. Ebert et al., 2024)</a></li><li><a href=#22--7181-treet-transfer-entropy-estimation-via-transformer-omer-luxembourg-et-al-2024>(2/2 | 71/81) TREET: TRansfer Entropy Estimation via Transformer (Omer Luxembourg et al., 2024)</a></li></ul></li><li><a href=#csir-1>cs.IR (1)</a><ul><li><a href=#11--7281-non-autoregressive-generative-models-for-reranking-recommendation-yuxin-ren-et-al-2024>(1/1 | 72/81) Non-autoregressive Generative Models for Reranking Recommendation (Yuxin Ren et al., 2024)</a></li></ul></li><li><a href=#eessiv-2>eess.IV (2)</a><ul><li><a href=#12--7381-an-optimization-framework-for-processing-and-transfer-learning-for-the-brain-tumor-segmentation-tianyi-ren-et-al-2024>(1/2 | 73/81) An Optimization Framework for Processing and Transfer Learning for the Brain Tumor Segmentation (Tianyi Ren et al., 2024)</a></li><li><a href=#22--7481-disentangled-latent-energy-based-style-translation-an-image-level-structural-mri-harmonization-framework-mengqi-wu-et-al-2024>(2/2 | 74/81) Disentangled Latent Energy-Based Style Translation: An Image-Level Structural MRI Harmonization Framework (Mengqi Wu et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--7581-on-leaky-integrate-and-fire-as-spike-train-quantization-operator-on-dirac-superimposed-continuous-time-signals-bernhard-a-moser-et-al-2024>(1/1 | 75/81) On Leaky-Integrate-and Fire as Spike-Train-Quantization Operator on Dirac-Superimposed Continuous-Time Signals (Bernhard A. Moser et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--7681-a-rational-analysis-of-the-speech-to-song-illusion-raja-marjieh-et-al-2024>(1/1 | 76/81) A Rational Analysis of the Speech-to-Song Illusion (Raja Marjieh et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#11--7781-evaluation-metrics-for-automated-typographic-poster-generation-sérgio-m-rebelo-et-al-2024>(1/1 | 77/81) Evaluation Metrics for Automated Typographic Poster Generation (Sérgio M. Rebelo et al., 2024)</a></li></ul></li><li><a href=#cshc-1>cs.HC (1)</a><ul><li><a href=#11--7881-dimvis-interpreting-visual-clusters-in-dimensionality-reduction-with-explainable-boosting-machine-parisa-salmanian-et-al-2024>(1/1 | 78/81) DimVis: Interpreting Visual Clusters in Dimensionality Reduction With Explainable Boosting Machine (Parisa Salmanian et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--7981-optimizing-the-design-of-an-artificial-pancreas-to-improve-diabetes-management-ashok-khanna-et-al-2024>(1/1 | 79/81) Optimizing the Design of an Artificial Pancreas to Improve Diabetes Management (Ashok Khanna et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--8081-protir-iterative-refinement-between-retrievers-and-predictors-for-protein-function-annotation-zuobai-zhang-et-al-2024>(1/1 | 80/81) ProtIR: Iterative Refinement between Retrievers and Predictors for Protein Function Annotation (Zuobai Zhang et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--8181-estimating-the-effect-of-crosstalk-error-on-circuit-fidelity-using-noisy-intermediate-scale-quantum-devices-sovanmonynuth-heng-et-al-2024>(1/1 | 81/81) Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices (Sovanmonynuth Heng et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>