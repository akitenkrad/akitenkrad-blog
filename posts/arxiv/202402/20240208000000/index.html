<!doctype html><html><head><title>arXiv @ 2024.02.08</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.08"><meta property="og:description" content="Primary Categories cond-mat.mes-hall (1) cs.AI (20) cs.AR (2) cs.CE (1) cs.CL (39) cs.CR (5) cs.CV (35) cs.CY (5) cs.DC (3) cs.DS (1) cs.GT (3) cs.HC (4) cs.IR (6) cs.IT (6) cs.LG (88) cs.MA (1) cs.NE (1) cs.NI (2) cs.PF (1) cs.RO (17) cs.SE (7) cs.SI (2) cs.SY (1) eess.AS (2) eess.IV (4) eess.SY (5) math.NA (2) math.OC (1) q-bio.NC (1) q-bio.QM (2) q-fin.RM (1) quant-ph (1) stat.ME (1) stat.ML (6) Keywords category keyword cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240208000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-08T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-08T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.08"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240208000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Thursday, Feb 8, 2024</p></div><div class=title><h1>arXiv @ 2024.02.08</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#cond-matmes-hall-1>cond-mat.mes-hall (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csai-20>cs.AI (20)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#cscl-39>cs.CL (39)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#cscr-5>cs.CR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#cscv-35>cs.CV (35)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#cscy-5>cs.CY (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csdc-3>cs.DC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csgt-3>cs.GT (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csir-6>cs.IR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csit-6>cs.IT (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#cslg-88>cs.LG (88)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#cspf-1>cs.PF (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csro-17>cs.RO (17)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#csse-7>cs.SE (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#cssy-1>cs.SY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#eessiv-4>eess.IV (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#eesssy-5>eess.SY (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#q-bioqm-2>q-bio.QM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#q-finrm-1>q-fin.RM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/#statml-6>stat.ML (6)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>category</th><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><th></th><td>Active Learning</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Adversarial Attack</td><td></td><td>1</td><td>2</td><td>1</td><td></td></tr><tr><th></th><td>Adversarial Learning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Autoencoder</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>BART</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>BERT</td><td></td><td>4</td><td></td><td>1</td><td></td></tr><tr><th></th><td>BLEU</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Bag-of-Words</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Bandit Algorithm</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><th></th><td>ChatGPT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><th></th><td>Chatbot</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><th></th><td>Code Generation</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><th></th><td>Contrastive Learning</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><th></th><td>Convolution</td><td></td><td></td><td>4</td><td>3</td><td>2</td></tr><tr><th></th><td>Convolutional Neural Network</td><td></td><td></td><td>5</td><td>2</td><td>3</td></tr><tr><th></th><td>Counter-factual</td><td>1</td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Counterfactual Reasoning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Data Augmentation</td><td></td><td></td><td>2</td><td>2</td><td></td></tr><tr><th></th><td>Document Classification</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Domain Adaptation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>Fairness</td><td>1</td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Few-shot</td><td></td><td>3</td><td>5</td><td>3</td><td>1</td></tr><tr><th></th><td>Few-shot Learning</td><td></td><td>1</td><td>2</td><td>1</td><td></td></tr><tr><th></th><td>Fine-tuning</td><td>1</td><td>11</td><td>8</td><td>18</td><td>2</td></tr><tr><th></th><td>Foundation Model</td><td>1</td><td></td><td>2</td><td>2</td><td>1</td></tr><tr><th></th><td>GLUE</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>GPT</td><td>3</td><td>12</td><td>1</td><td>3</td><td></td></tr><tr><th></th><td>GPT-2</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>GPT-3</td><td></td><td>7</td><td></td><td></td><td></td></tr><tr><th></th><td>GPT-3.5</td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><th></th><td>GPT-4</td><td>2</td><td>8</td><td></td><td></td><td></td></tr><tr><th></th><td>Generative AI</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Generative Adversarial Network</td><td></td><td></td><td>3</td><td>2</td><td></td></tr><tr><th></th><td>Graph Attention Networks</td><td></td><td></td><td>1</td><td>1</td><td>2</td></tr><tr><th></th><td>Graph Convolutional Network</td><td></td><td>1</td><td></td><td>2</td><td>1</td></tr><tr><th></th><td>Graph Embedding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Graph Neural Network</td><td></td><td></td><td></td><td>6</td><td></td></tr><tr><th></th><td>Grounding</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><th></th><td>Hallucination Detection</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Image2text</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>In-context Learning</td><td>1</td><td>2</td><td></td><td>6</td><td></td></tr><tr><th></th><td>Information Retrieval</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><th></th><td>Instruction Following</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Instruction Tuning</td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><th></th><td>Knowledge Distillation</td><td></td><td>3</td><td>2</td><td>5</td><td></td></tr><tr><th></th><td>LLaMA</td><td></td><td>1</td><td></td><td>2</td><td></td></tr><tr><th></th><td>Label Smoothing</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Language Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Large Language Model</td><td>17</td><td>48</td><td>6</td><td>29</td><td>6</td></tr><tr><th></th><td>Low-Resource</td><td></td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><th></th><td>Mathematical Reasoning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Message-Passing</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Meta Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Model Compression</td><td></td><td></td><td>1</td><td>2</td><td></td></tr><tr><th></th><td>Model Quantization</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>Multi-modal</td><td>9</td><td>6</td><td>10</td><td>4</td><td>3</td></tr><tr><th></th><td>Multiple Instance Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>Mutual Information</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>N-gram</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Natural Language Explanation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Natural Language Generation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><th></th><td>Natural Language Inference</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Neural Machine Translation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><th></th><td>Node Classification</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Noise-tolerant</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Object Detection</td><td></td><td></td><td>3</td><td></td><td>2</td></tr><tr><th></th><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td>4</td><td></td></tr><tr><th></th><td>Out-of-distribution</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><th></th><td>Outlier Detection</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>PaLM</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Perplexity</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Pre-trained Language Model</td><td></td><td>5</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Probabilistic Model</td><td></td><td></td><td></td><td>2</td><td>1</td></tr><tr><th></th><td>Prompt</td><td>1</td><td>9</td><td>4</td><td>5</td><td>1</td></tr><tr><th></th><td>Prompt Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><th></th><td>Quantization</td><td></td><td></td><td>2</td><td>3</td><td></td></tr><tr><th></th><td>Question Answering</td><td>1</td><td>8</td><td>1</td><td></td><td></td></tr><tr><th></th><td>Reasoning</td><td>4</td><td>7</td><td>4</td><td>3</td><td>1</td></tr><tr><th></th><td>Recommendation</td><td></td><td>2</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Reinforcement Learning</td><td>3</td><td>3</td><td>1</td><td>11</td><td>3</td></tr><tr><th></th><td>RoBERTa</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><th></th><td>Rouge</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Sample Size</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Scaling Law</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Self-Attention</td><td></td><td></td><td>1</td><td>4</td><td></td></tr><tr><th></th><td>Self-supervised Learning</td><td></td><td></td><td>3</td><td>5</td><td></td></tr><tr><th></th><td>Self-supervised Pre-training</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Simulation</td><td></td><td>2</td><td></td><td>4</td><td>4</td></tr><tr><th></th><td>Simulator</td><td></td><td>2</td><td></td><td>4</td><td>4</td></tr><tr><th></th><td>Stemming</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Summarization</td><td></td><td>1</td><td></td><td>2</td><td>1</td></tr><tr><th></th><td>Supervised Learning</td><td></td><td>1</td><td>3</td><td>13</td><td>2</td></tr><tr><th></th><td>T5</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>TF-IDF</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Text Generation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><th></th><td>Text Summarization</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Text2image</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>Topic Model</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><th></th><td>Topic Modeling</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><th></th><td>Transfer Learning</td><td></td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><th></th><td>Transformer</td><td>1</td><td></td><td>5</td><td>16</td><td></td></tr><tr><th></th><td>Unsupervised Learning</td><td></td><td>2</td><td>6</td><td>8</td><td></td></tr><tr><th></th><td>Variational Autoencoder</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Vision-and-Language</td><td>1</td><td>1</td><td>4</td><td>1</td><td>1</td></tr><tr><th></th><td>Visual Question Answering</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><th></th><td>Weakly-supervised Learning</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><th></th><td>Word2vec</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Zero-shot</td><td>1</td><td>4</td><td>8</td><td>2</td><td></td></tr><tr><th></th><td>human-in-the-loop</td><td></td><td></td><td></td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-39>cs.CL (39)</h2><h3 id=1277-minds-versus-machines-rethinking-entailment-verification-with-language-models-soumya-sanyal-et-al-2024>(1/277) Minds versus Machines: Rethinking Entailment Verification with Language Models (Soumya Sanyal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soumya Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya Wang, Xiang Ren. (2024)<br><strong>Minds versus Machines: Rethinking Entailment Verification with Language Models</strong><br><button class=copy-to-clipboard title="Minds versus Machines: Rethinking Entailment Verification with Language Models" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, Natural Language Inference, Question Answering, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03686v1.pdf filename=2402.03686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and <b>LLM</b> performance across various <b>reasoning</b> categories. Our benchmark includes datasets from three categories <b>(NLI,</b> contextual <b>QA,</b> and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex <b>reasoning</b> instances. Notably, our findings reveal <b>LLMs&rsquo;</b> superiority in multi-hop <b>reasoning</b> across extended contexts, while humans excel in tasks necessitating simple deductive <b>reasoning.</b> Leveraging these insights, we introduce a <b>fine-tuned</b> Flan-T5 model that outperforms <b>GPT-3.5</b> and rivals with <b>GPT-4,</b> offering a robust open-source solution for entailment verification. As a practical application, we showcase the efficacy of our <b>finetuned</b> model in enhancing self-consistency in model-generated explanations, resulting in a 6% performance boost on average across three multiple-choice <b>question-answering</b> <b>datasets.</b></p></p class="citation"></blockquote><h3 id=2277-training-language-models-to-generate-text-with-citations-via-fine-grained-rewards-chengyu-huang-et-al-2024>(2/277) Training Language Models to Generate Text with Citations via Fine-grained Rewards (Chengyu Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengyu Huang, Zeqiu Wu, Yushi Hu, Wenya Wang. (2024)<br><strong>Training Language Models to Generate Text with Citations via Fine-grained Rewards</strong><br><button class=copy-to-clipboard title="Training Language Models to Generate Text with Citations via Fine-grained Rewards" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: GPT, GPT-3, GPT-3.5, LLaMA, Question Answering, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04315v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04315v1.pdf filename=2402.04315v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While recent <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly <b>prompted</b> <b>LLMs</b> to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller <b>LLMs.</b> In this work, we propose an effective training framework using fine-grained rewards to teach <b>LLMs</b> to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common <b>LLM</b> training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on <b>Question</b> <b>Answering</b> <b>(QA)</b> datasets taken from the ALCE benchmark and validate the model&rsquo;s generalizability using EXPERTQA. On <b>LLaMA-2-7B,</b> the incorporation of fine-grained rewards achieves the best performance among the baselines, even surpassing that of <b>GPT-3.5-turbo.</b></p></p class="citation"></blockquote><h3 id=3277-scaling-laws-for-downstream-task-performance-of-large-language-models-berivan-isik-et-al-2024>(3/277) Scaling Laws for Downstream Task Performance of Large Language Models (Berivan Isik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, Sanmi Koyejo. (2024)<br><strong>Scaling Laws for Downstream Task Performance of Large Language Models</strong><br><button class=copy-to-clipboard title="Scaling Laws for Downstream Task Performance of Large Language Models" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL, stat-ML<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, Transfer Learning, Unsupervised Learning, Neural Machine Translation, BLEU, Large Language Model, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04177v1.pdf filename=2402.04177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Scaling</b> <b>laws</b> provide important insights that can guide the design of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Existing work has primarily focused on studying <b>scaling</b> <b>laws</b> for pretraining (upstream) loss. However, in <b>transfer</b> <b>learning</b> settings, in which <b>LLMs</b> are pretrained on an <b>unsupervised</b> dataset and then <b>finetuned</b> on a downstream task, we often also care about the downstream performance. In this work, we study the <b>scaling</b> <b>behavior</b> in a <b>transfer</b> <b>learning</b> setting, where <b>LLMs</b> are <b>finetuned</b> for <b>machine</b> <b>translation</b> tasks. Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and <b>BLEU</b> score. Our experiments indicate that the size of the <b>finetuning</b> dataset and the distribution alignment between the pretraining and downstream data significantly influence the <b>scaling</b> <b>behavior.</b> With sufficient alignment, both downstream cross-entropy and <b>BLEU</b> score improve monotonically with more pretraining data. In such cases, we show that it is possible to predict the downstream <b>BLEU</b> score with good accuracy using a log-law. However, there are also cases where moderate misalignment causes the <b>BLEU</b> score to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves. By analyzing these observations, we provide new practical insights for choosing appropriate pretraining data.</p></p class="citation"></blockquote><h3 id=4277-harnessing-the-plug-and-play-controller-by-prompting-hao-wang-et-al-2024>(4/277) Harnessing the Plug-and-Play Controller by Prompting (Hao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wang, Lei Sha. (2024)<br><strong>Harnessing the Plug-and-Play Controller by Prompting</strong><br><button class=copy-to-clipboard title="Harnessing the Plug-and-Play Controller by Prompting" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Reinforcement Learning, Language Generation, Natural Language Generation, Natural Language Generation, Text Generation, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04160v1.pdf filename=2402.04160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Controllable <b>text</b> <b>generation</b> is a growing field within <b>natural</b> <b>language</b> <b>generation</b> <b>(NLG)</b> that focuses on producing <b>text</b> <b>that</b> meets specific constraints in real-world applications. Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated <b>text</b> <b>in</b> a flexible manner. However, these methods often compromised the integrity of the <b>language</b> <b>model&rsquo;s</b> decoding process, resulting in less smooth <b>text</b> <b>generation.</b> Alternatively, other techniques utilized multiple attribute <b>prompts</b> to align the generated <b>text</b> <b>with</b> desired attributes, but this approach required <b>prompt</b> design for each attribute and was dependent on the size of the <b>language</b> <b>model.</b> This paper introduces a novel method for flexible attribute control in <b>text</b> <b>generation</b> using <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs).</b> The proposed approach aims to enhance the fluency of generated <b>text</b> <b>by</b> guiding the generation process with PPCs. The key idea is to dynamically adjust the distribution of generated <b>text</b> <b>by</b> modifying <b>prompts,</b> effectively constraining the output space of the <b>language</b> <b>model</b> and influencing the desired attribute. To enable smooth cooperation between the <b>PLM</b> and the PPC, our work innovatively proposes a new model <b>fine-tuning</b> method: <b>Reinforcement</b> <b>Learning</b> with Dynamic Adjust Feedback (RLDAF).This <b>fine-tuning</b> process adapts a small subset of the <b>language</b> <b>model&rsquo;s</b> parameters based on the generating actions taken during the PPC control process. The resulting harmonious collaboration between the <b>PLM</b> and PPC leads to improved smoothness in <b>text</b> <b>generation</b> during inference. Extensive experiments were conducted on the SST2 dataset, and the proposed method outperformed previous approaches in various evaluation metrics, including <b>text</b> <b>fluency</b> and attribute consistency.</p></p class="citation"></blockquote><h3 id=5277-large-language-models-as-moocs-graders-shahriar-golchin-et-al-2024>(5/277) Large Language Models As MOOCs Graders (Shahriar Golchin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shahriar Golchin, Nikhil Garuda, Christopher Impey, Matthew Wenger. (2024)<br><strong>Large Language Models As MOOCs Graders</strong><br><button class=copy-to-clipboard title="Large Language Models As MOOCs Graders" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03776v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03776v2.pdf filename=2402.03776v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student&rsquo;s writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art <b>LLMs:</b> <b>GPT-4</b> and <b>GPT-3.5,</b> across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct <b>LLMs,</b> we use three different <b>prompts</b> based on a variant of the <b>zero-shot</b> chain-of-thought <b>(Zero-shot-CoT)</b> <b>prompting</b> technique: <b>Zero-shot-CoT</b> combined with instructor-provided correct answers; <b>Zero-shot-CoT</b> in conjunction with both instructor-formulated answers and rubrics; and <b>Zero-shot-CoT</b> with instructor-offered correct answers and <b>LLM-generated</b> rubrics. Our results show that <b>Zero-shot-CoT,</b> when integrated with instructor-provided answers and rubrics, produces grades that are more aligned with those assigned by instructors compared to peer grading. However, the History and Philosophy of Astronomy course proves to be more challenging in terms of grading as opposed to other courses. Finally, our study reveals a promising direction for automating grading systems for MOOCs, especially in subjects with well-defined rubrics.</p></p class="citation"></blockquote><h3 id=6277-identifying-reasons-for-contraceptive-switching-from-real-world-data-using-large-language-models-brenda-y-miao-et-al-2024>(6/277) Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models (Brenda Y. Miao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brenda Y. Miao, Christopher YK Williams, Ebenezer Chinedu-Eneh, Travis Zack, Emily Alsentzer, Atul J. Butte, Irene Y. Chen. (2024)<br><strong>Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models</strong><br><button class=copy-to-clipboard title="Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Topic Model, Unsupervised Learning, Zero-shot, BERT, GPT, GPT-4, Large Language Model, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03597v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03597v1.pdf filename=2402.03597v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prescription contraceptives play a critical role in supporting women&rsquo;s reproductive health. With nearly 50 million women in the United States using contraceptives, understanding the factors that drive contraceptives selection and switching is of significant interest. However, many factors related to medication switching are often only captured in unstructured clinical notes and can be difficult to extract. Here, we evaluate the <b>zero-shot</b> abilities of a recently developed <b>large</b> <b>language</b> <b>model,</b> <b>GPT-4</b> (via HIPAA-compliant Microsoft Azure API), to identify reasons for switching between classes of contraceptives from the UCSF Information Commons clinical notes dataset. We demonstrate that <b>GPT-4</b> can accurately extract reasons for contraceptive switching, outperforming baseline <b>BERT-based</b> models with microF1 scores of 0.849 and 0.881 for contraceptive start and stop extraction, respectively. Human evaluation of <b>GPT-4-extracted</b> reasons for switching showed 91.4% accuracy, with minimal hallucinations. Using extracted reasons, we identified patient preference, adverse events, and insurance as key reasons for switching using <b>unsupervised</b> <b>topic</b> <b>modeling</b> approaches. Notably, we also showed using our approach that &ldquo;weight gain/mood change&rdquo; and &ldquo;insurance coverage&rdquo; are disproportionately found as reasons for contraceptive switching in specific demographic populations. Our code and supplemental data are available at <a href=https://github.com/BMiao10/contraceptive-switching>https://github.com/BMiao10/contraceptive-switching</a>.</p></p class="citation"></blockquote><h3 id=7277-detecting-mode-collapse-in-language-models-via-narration-sil-hamilton-2024>(7/277) Detecting Mode Collapse in Language Models via Narration (Sil Hamilton, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sil Hamilton. (2024)<br><strong>Detecting Mode Collapse in Language Models via Narration</strong><br><button class=copy-to-clipboard title="Detecting Mode Collapse in Language Models via Narration" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Reinforcement Learning, Simulation, Simulator, GPT, GPT-3, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04477v1.pdf filename=2402.04477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author&ndash;what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early <b>large</b> <b>language</b> <b>models</b> trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively consistent personae on language models via <b>instruction</b> <b>tuning</b> and <b>reinforcement</b> <b>learning</b> from human feedback (RLHF), but whether aligned models retain the ability to model an arbitrary virtual author has received little scrutiny. By studying 4,374 stories sampled from three OpenAI language models, we show successive versions of <b>GPT-3</b> suffer from increasing degrees of &ldquo;mode collapse&rdquo; whereby overfitting the model during alignment constrains it from generalizing over authorship: models suffering from mode collapse become unable to assume a multiplicity of perspectives. Our method and results are significant for researchers seeking to employ language models in sociological <b>simulations.</b></p></p class="citation"></blockquote><h3 id=8277-rethinking-skill-extraction-in-the-job-market-domain-using-large-language-models-khanh-cao-nguyen-et-al-2024>(8/277) Rethinking Skill Extraction in the Job Market Domain using Large Language Models (Khanh Cao Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khanh Cao Nguyen, Mike Zhang, Syrielle Montariol, Antoine Bosselut. (2024)<br><strong>Rethinking Skill Extraction in the Job Market Domain using Large Language Models</strong><br><button class=copy-to-clipboard title="Rethinking Skill Extraction in the Job Market Domain using Large Language Models" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Few-shot Learning, Supervised Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03832v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03832v1.pdf filename=2402.03832v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skill Extraction involves identifying skills and qualifications mentioned in documents such as job postings and resumes. The task is commonly tackled by training <b>supervised</b> models using a sequence labeling approach with BIO tags. However, the reliance on manually annotated data limits the generalizability of such approaches. Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions. In this paper, we explore the use of <b>in-context</b> <b>learning</b> to overcome these challenges, on a benchmark of 6 uniformized skill extraction datasets. Our approach leverages the <b>few-shot</b> <b>learning</b> capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to identify and extract skills from sentences. We show that <b>LLMs,</b> despite not being on par with traditional <b>supervised</b> models in terms of performance, can better handle syntactically complex skill mentions in skill extraction tasks.</p></p class="citation"></blockquote><h3 id=9277-large-language-models-as-an-indirect-reasoner-contrapositive-and-contradiction-for-automated-reasoning-yanfang-zhang-et-al-2024>(9/277) Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning (Yanfang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanfang Zhang, Yiliu Sun, Yibing Zhan, Dapeng Tao, Dacheng Tao, Chen Gong. (2024)<br><strong>Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning</strong><br><button class=copy-to-clipboard title="Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: GPT, GPT-3, GPT-3.5, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03667v1.pdf filename=2402.03667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, increasing attention has been focused drawn on to improve the ability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to perform complex <b>reasoning.</b> However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct <b>Reasoning</b> (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the <b>reasoning</b> power of <b>LLMs,</b> this paper proposes a novel Indirect <b>Reasoning</b> (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual <b>reasoning</b> and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of <b>LLMs.</b> Secondly, we design a set of <b>prompt</b> templates to trigger <b>LLMs</b> to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and can be straightforwardly integrated with existing DR methods to further boost the <b>reasoning</b> abilities of <b>LLMs.</b> The experimental results on popular <b>LLMs,</b> such as <b>GPT-3.5-turbo</b> and Gemini-pro, show that our IR method enhances the overall accuracy of factual <b>reasoning</b> by 27.33% and mathematical proof by 31.43%, when compared with traditional DR methods. Moreover, the methods combining IR and DR significantly outperform the methods solely using IR or DR, further demonstrating the effectiveness of our strategy.</p></p class="citation"></blockquote><h3 id=10277-professional-agents----evolving-large-language-models-into-autonomous-experts-with-human-level-competencies-zhixuan-chu-et-al-2024>(10/277) Professional Agents &ndash; Evolving Large Language Models into Autonomous Experts with Human-Level Competencies (Zhixuan Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixuan Chu, Yan Wang, Feng Zhu, Lu Yu, Longfei Li, Jinjie Gu. (2024)<br><strong>Professional Agents &ndash; Evolving Large Language Models into Autonomous Experts with Human-Level Competencies</strong><br><button class=copy-to-clipboard title="Professional Agents -- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: ChatGPT, GPT, GPT-4, PaLM, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03628v1.pdf filename=2402.03628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>ChatGPT,</b> <b>PaLM,</b> and <b>GPT-4</b> has catalyzed remarkable advances in natural language processing, demonstrating human-like language fluency and <b>reasoning</b> capacities. This position paper introduces the concept of Professional Agents (PAgents), an application framework harnessing <b>LLM</b> capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies. We posit that PAgents can reshape professional services through continuously developed expertise. Our proposed PAgents framework entails a tri-layered architecture for genesis, evolution, and synergy: a base tool layer, a middle agent layer, and a top synergy layer. This paper aims to spur discourse on promising real-world applications of <b>LLMs.</b> We argue the increasing sophistication and integration of PAgents could lead to AI systems exhibiting professional mastery over complex domains, serving critical needs, and potentially achieving artificial general intelligence.</p></p class="citation"></blockquote><h3 id=11277-less-selecting-influential-data-for-targeted-instruction-tuning-mengzhou-xia-et-al-2024>(11/277) LESS: Selecting Influential Data for Targeted Instruction Tuning (Mengzhou Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, Danqi Chen. (2024)<br><strong>LESS: Selecting Influential Data for Targeted Instruction Tuning</strong><br><button class=copy-to-clipboard title="LESS: Selecting Influential Data for Targeted Instruction Tuning" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, Chatbot, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04333v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04333v1.pdf filename=2402.04333v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> has unlocked powerful capabilities in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> effectively using combined datasets to develop generalpurpose <b>chatbots.</b> However, real-world applications often require a specialized suite of skills (e.g., <b>reasoning).</b> The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted <b>instruction</b> <b>tuning.</b> We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for <b>instruction</b> <b>data</b> selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length <b>instruction</b> <b>data.</b> LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to <b>few-shot</b> examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary <b>reasoning</b> skills for the intended downstream application.</p></p class="citation"></blockquote><h3 id=12277-leak-cheat-repeat-data-contamination-and-evaluation-malpractices-in-closed-source-llms-simone-balloccu-et-al-2024>(12/277) Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs (Simone Balloccu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, Ondřej Dušek. (2024)<br><strong>Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs</strong><br><button class=copy-to-clipboard title="Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03927v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03927v1.pdf filename=2402.03927v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural Language Processing (NLP) research is increasingly focusing on the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI&rsquo;s <b>GPT-3.5</b> and <b>GPT-4,</b> the most prominently used <b>LLMs</b> today, in the context of data contamination. By analysing 255 papers and considering OpenAI&rsquo;s data usage policy, we extensively document the amount of data leaked to these models during the first year after the model&rsquo;s release. We report that these models have been globally exposed to $\sim$4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on <a href=https://leak-llm.github.io/>https://leak-llm.github.io/</a>, where other researchers can contribute to our efforts.</p></p class="citation"></blockquote><h3 id=13277-anls----a-universal-document-processing-metric-for-generative-large-language-models-david-peer-et-al-2024>(13/277) ANLS* &ndash; A Universal Document Processing Metric for Generative Large Language Models (David Peer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Peer, Philemon Schöpf, Volckmar Nebendahl, Alexander Rietzler, Sebastian Stabinger. (2024)<br><em><em>ANLS</em> &ndash; A Universal Document Processing Metric for Generative Large Language Models</em>*<br><button class=copy-to-clipboard title="ANLS* -- A Universal Document Processing Metric for Generative Large Language Models" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Zero-shot, Document Classification, Information Retrieval, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03848v1.pdf filename=2402.03848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditionally, discriminative models have been the predominant choice for tasks like <b>document</b> <b>classification</b> and <b>information</b> <b>extraction.</b> These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative <b>large</b> <b>language</b> <b>models</b> (GLLMs) have <b>prompted</b> a shift in the field due to their enhanced <b>zero-shot</b> capabilities, which eliminate the need for a downstream dataset and computationally expensive <b>fine-tuning.</b> However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including <b>information</b> <b>extraction</b> and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and is still compatible with previously reported ANLS scores. An evaluation of 7 different datasets and 3 different GLLMs using the ANLS* metric is also provided, demonstrating the importance of the proposed metric. We also benchmark a novel approach to generate <b>prompts</b> for <b>documents,</b> <b>called</b> SFT, against other <b>prompting</b> techniques such as LATIN. In 15 out of 21 cases, SFT outperforms other techniques and improves the state-of-the-art, sometimes by as much as $15$ percentage points. Sources are available at <a href=https://github.com/deepopinion/anls_star_metric>https://github.com/deepopinion/anls_star_metric</a></p></p class="citation"></blockquote><h3 id=14277-personalized-language-modeling-from-personalized-human-feedback-xinyu-li-et-al-2024>(14/277) Personalized Language Modeling from Personalized Human Feedback (Xinyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Li, Zachary C. Lipton, Liu Leqi. (2024)<br><strong>Personalized Language Modeling from Personalized Human Feedback</strong><br><button class=copy-to-clipboard title="Personalized Language Modeling from Personalized Human Feedback" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Reinforcement Learning, GPT, Text Summarization, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05133v1.pdf filename=2402.05133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> from Human Feedback (RLHF) is the current dominating framework to <b>fine-tune</b> <b>large</b> <b>language</b> <b>models</b> to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimization. To demonstrate the efficacy of our method, we test it on real-world <b>text</b> <b>summarization</b> data with annotated preferences and annotator information. We <b>fine-tune</b> <b>GPT-J</b> 6B to obtain personalized language (and reward) models, which outperform non-personalized models in terms of aligning with individual preferences.</p></p class="citation"></blockquote><h3 id=15277-democratizing-large-language-models-via-personalized-parameter-efficient-fine-tuning-zhaoxuan-tan-et-al-2024>(15/277) Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning (Zhaoxuan Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, Meng Jiang. (2024)<br><strong>Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning</strong><br><button class=copy-to-clipboard title="Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Recommendation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04401v1.pdf filename=2402.04401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalization in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> is increasingly important, aiming to align <b>LLM&rsquo;s</b> interactions, content, and <b>recommendations</b> with individual user preferences. Recent advances in <b>LLM</b> personalization have spotlighted effective <b>prompt</b> design, by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these approaches were limited due to a lack of model ownership, resulting in constrained customization and privacy issues. Moreover, they often failed to accurately capture user behavior patterns, especially in cases where user data were complex and dynamic. To address these shortcomings, we introduce One PEFT Per User (OPPU), which employs personalized parameter-efficient <b>fine-tuning</b> (PEFT) modules, to store user-specific behavior patterns and preferences. By plugging in users&rsquo; personal PEFT parameters, they can own and use their <b>LLMs</b> personally. OPPU integrates parametric user knowledge in the personal PEFT parameters with the non-parametric knowledge acquired through retrieval and profile. This integration adapts individual <b>LLMs</b> to user behavior shifts. Experimental results demonstrate that OPPU significantly outperforms existing <b>prompt-based</b> methods across seven diverse tasks in the LaMP benchmark. Further in-depth studies reveal OPPU&rsquo;s enhanced capabilities in handling user behavior shifts, modeling users at different active levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods.</p></p class="citation"></blockquote><h3 id=16277-legallens-leveraging-llms-for-legal-violation-identification-in-unstructured-text-dor-bernsohn-et-al-2024>(16/277) LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text (Dor Bernsohn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dor Bernsohn, Gil Semo, Yaron Vazana, Gila Hayat, Ben Hagag, Joel Niklaus, Rohit Saha, Kyryl Truskovskyi. (2024)<br><strong>LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text</strong><br><button class=copy-to-clipboard title="LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Few-shot, Fine-tuning, BERT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04335v1.pdf filename=2402.04335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals. We constructed two datasets using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> which were subsequently validated by domain expert annotators. Both tasks were designed specifically for the context of class-action cases. The experimental design incorporated <b>fine-tuning</b> models from the <b>BERT</b> family and open-source <b>LLMs,</b> and conducting <b>few-shot</b> experiments using closed-source <b>LLMs.</b> Our results, with an F1-score of 62.69% (violation identification) and 81.02% (associating victims), show that our datasets and setups can be used for both tasks. Finally, we publicly release the datasets and the code used for the experiments in order to advance further research in the area of legal natural language processing (NLP).</p></p class="citation"></blockquote><h3 id=17277-behind-the-screen-investigating-chatgpts-dark-personality-traits-and-conspiracy-beliefs-erik-weber-et-al-2024>(17/277) Behind the Screen: Investigating ChatGPT&rsquo;s Dark Personality Traits and Conspiracy Beliefs (Erik Weber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erik Weber, Jérôme Rutinowski, Markus Pauly. (2024)<br><strong>Behind the Screen: Investigating ChatGPT&rsquo;s Dark Personality Traits and Conspiracy Beliefs</strong><br><button class=copy-to-clipboard title="Behind the Screen: Investigating ChatGPT's Dark Personality Traits and Conspiracy Beliefs" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: ChatGPT, GPT, GPT-3, GPT-3.5, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04110v1.pdf filename=2402.04110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>ChatGPT</b> is notorious for its intransparent behavior. This paper tries to shed light on this, providing an in-depth analysis of the dark personality traits and conspiracy beliefs of <b>GPT-3.5</b> and <b>GPT-4.</b> Different psychological tests and questionnaires were employed, including the Dark Factor Test, the Mach-IV Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale. The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between <b>GPT-3.5</b> and <b>GPT-4.</b> For traits that have shown to be interdependent in human studies, correlations were considered. Additionally, system roles corresponding to groups that have shown distinct answering behavior in the corresponding questionnaires were applied to examine the models&rsquo; ability to reflect characteristics associated with these roles in their responses. Dark personality traits and conspiracy beliefs were not particularly pronounced in either model with little differences between <b>GPT-3.5</b> and <b>GPT-4.</b> However, <b>GPT-4</b> showed a pronounced tendency to believe in information withholding. This is particularly intriguing given that <b>GPT-4</b> is trained on a significantly larger dataset than <b>GPT-3.5.</b> Apparently, in this case an increased data exposure correlates with a greater belief in the control of information. An assignment of extreme political affiliations increased the belief in conspiracy theories. Test sequencing affected the models&rsquo; responses and the observed correlations, indicating a form of contextual memory.</p></p class="citation"></blockquote><h3 id=18277-iterative-prompt-refinement-for-radiation-oncology-symptom-extraction-using-teacher-student-large-language-models-reza-khanmohammadi-et-al-2024>(18/277) Iterative Prompt Refinement for Radiation Oncology Symptom Extraction Using Teacher-Student Large Language Models (Reza Khanmohammadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reza Khanmohammadi, Ahmed I Ghanem, Kyle Verdecchia, Ryan Hall, Mohamed Elshaikh, Benjamin Movsas, Hassan Bagher-Ebadian, Indrin Chetty, Mohammad M. Ghassemi, Kundan Thind. (2024)<br><strong>Iterative Prompt Refinement for Radiation Oncology Symptom Extraction Using Teacher-Student Large Language Models</strong><br><button class=copy-to-clipboard title="Iterative Prompt Refinement for Radiation Oncology Symptom Extraction Using Teacher-Student Large Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04075v1.pdf filename=2402.04075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a novel teacher-student architecture utilizing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to improve prostate cancer radiotherapy symptom extraction from clinical notes. Mixtral, the student model, initially extracts symptoms, followed by <b>GPT-4,</b> the teacher model, which refines <b>prompts</b> based on Mixtral&rsquo;s performance. This iterative process involved 294 single symptom clinical notes across 12 symptoms, with up to 16 rounds of refinement per epoch. Results showed significant improvements in extracting symptoms from both single and multi-symptom notes. For 59 single symptom notes, accuracy increased from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and F1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24 to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score from 0.20 to 0.44. These results demonstrate the effectiveness of advanced <b>prompt</b> engineering in <b>LLMs</b> for radiation oncology use.</p></p class="citation"></blockquote><h3 id=19277-distillm-towards-streamlined-distillation-for-large-language-models-jongwoo-ko-et-al-2024>(19/277) DistiLLM: Towards Streamlined Distillation for Large Language Models (Jongwoo Ko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jongwoo Ko, Sungnyun Kim, Tianyi Chen, Se-Young Yun. (2024)<br><strong>DistiLLM: Towards Streamlined Distillation for Large Language Models</strong><br><button class=copy-to-clipboard title="DistiLLM: Towards Streamlined Distillation for Large Language Models" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Instruction Following, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03898v1.pdf filename=2402.03898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>distillation</b> <b>(KD)</b> is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current <b>KD</b> methods for auto-regressive sequence models (e.g., <b>large</b> <b>language</b> <b>models)</b> suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient <b>KD</b> framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including <b>instruction-following</b> <b>tasks,</b> demonstrate the effectiveness of DistiLLM in building high-performing student models while achieving up to 4.3$\times$ speedup compared to recent <b>KD</b> methods.</p></p class="citation"></blockquote><h3 id=20277-soft-prompt-tuning-for-cross-lingual-transfer-when-less-is-more-fred-philippy-et-al-2024>(20/277) Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More (Fred Philippy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fred Philippy, Siwen Guo, Shohreh Haddadan, Cedric Lothritz, Jacques Klein, Tegawendé F. Bissyandé. (2024)<br><strong>Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More</strong><br><button class=copy-to-clipboard title="Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03782v1.pdf filename=2402.03782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Soft <b>Prompt</b> Tuning (SPT) is a parameter-efficient method for adapting <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> to specific tasks by inserting learnable embeddings, or soft <b>prompts,</b> at the input layer of the <b>PLM,</b> without modifying its parameters. This paper investigates the potential of SPT for cross-lingual transfer. Unlike previous studies on SPT for cross-lingual transfer that often <b>fine-tune</b> both the soft <b>prompt</b> and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft <b>prompt.</b> This does not only reduce the computational cost and storage overhead of full-model <b>fine-tuning,</b> but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages. Moreover, we explore how different factors related to the <b>prompt,</b> such as the length or its reparameterization, affect cross-lingual transfer performance.</p></p class="citation"></blockquote><h3 id=21277-sentiment-enhanced-graph-based-sarcasm-explanation-in-dialogue-kun-ouyang-et-al-2024>(21/277) Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue (Kun Ouyang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Ouyang, Liqiang Jing, Xuemeng Song, Meng Liu, Yupeng Hu, Liqiang Nie. (2024)<br><strong>Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue</strong><br><button class=copy-to-clipboard title="Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-MM, cs.CL<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, BART, Natural Language Explanation, Sentiment Analysis, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03658v1.pdf filename=2402.03658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which aims to generate a <b>natural</b> <b>language</b> <b>explanation</b> for the given sarcastic dialogue that involves multiple modalities (i.e., utterance, video, and audio). Although existing studies have achieved great success based on the generative <b>pretrained</b> <b>language</b> <b>model</b> <b>BART,</b> they overlook exploiting the <b>sentiments</b> <b>residing</b> in the utterance, video and audio, which are vital clues for sarcasm explanation. In fact, it is non-trivial to incorporate <b>sentiments</b> <b>for</b> boosting SED performance, due to three main challenges: 1) diverse effects of utterance tokens on <b>sentiments;</b> <b>2)</b> gap between video-audio <b>sentiment</b> <b>signals</b> and the embedding space of <b>BART;</b> and 3) various relations among utterances, utterance <b>sentiments,</b> <b>and</b> video-audio <b>sentiments.</b> <b>To</b> tackle these challenges, we propose a novel <b>sEntiment-enhanceD</b> <b>Graph-based</b> <b>multimodal</b> sarcasm Explanation framework, named EDGE. In particular, we first propose a lexicon-guided utterance <b>sentiment</b> <b>inference</b> module, where a heuristic utterance <b>sentiment</b> <b>refinement</b> strategy is devised. We then develop a module named Joint Cross Attention-based <b>Sentiment</b> <b>Inference</b> (JCA-SI) by extending the <b>multimodal</b> <b>sentiment</b> <b>analysis</b> model JCA to derive the joint <b>sentiment</b> <b>label</b> for each video-audio clip. Thereafter, we devise a context-sentiment graph to comprehensively model the semantic relations among the utterances, utterance <b>sentiments,</b> <b>and</b> video-audio <b>sentiments,</b> <b>to</b> facilitate sarcasm explanation generation. Extensive experiments on the publicly released dataset WITS verify the superiority of our model over cutting-edge methods.</p></p class="citation"></blockquote><h3 id=22277-partially-recentralization-softmax-loss-for-vision-language-models-robustness-hao-wang-et-al-2024>(22/277) Partially Recentralization Softmax Loss for Vision-Language Models Robustness (Hao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wang, Xin Zhang, Jinzhe Jiang, Yaqian Zhao, Chen Li. (2024)<br><strong>Partially Recentralization Softmax Loss for Vision-Language Models Robustness</strong><br><button class=copy-to-clipboard title="Partially Recentralization Softmax Loss for Vision-Language Models Robustness" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Large Language Model, Vision-and-Language, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03627v1.pdf filename=2402.03627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Large</b> <b>Language</b> <b>Models</b> make a breakthrough in natural language processing tasks (NLP), <b>multimodal</b> technique becomes extremely popular. However, it has been shown that <b>multimodal</b> NLP are vulnerable to <b>adversarial</b> <b>attacks,</b> where the outputs of a model can be dramatically changed by a perturbation to the input. While several defense techniques have been proposed both in computer vision and NLP models, the <b>multimodal</b> robustness of models have not been fully explored. In this paper, we study the <b>adversarial</b> <b>robustness</b> provided by modifying loss function of pre-trained <b>multimodal</b> models, by restricting top K softmax outputs. Based on the evaluation and scoring, our experiments show that after a <b>fine-tuning,</b> <b>adversarial</b> <b>robustness</b> of pre-trained models can be significantly improved, against popular attacks. Further research should be studying, such as output diversity, generalization and the robustness-performance trade-off of this kind of loss functions. Our code will be available after this paper is accepted</p></p class="citation"></blockquote><h3 id=23277-evaluating-embeddings-for-one-shot-classification-of-doctor-ai-consultations-olumide-ebenezer-ojo-et-al-2024>(23/277) Evaluating Embeddings for One-Shot Classification of Doctor-AI Consultations (Olumide Ebenezer Ojo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olumide Ebenezer Ojo, Olaronke Oluwayemisi Adebanji, Alexander Gelbukh, Hiram Calvo, Anna Feldman. (2024)<br><strong>Evaluating Embeddings for One-Shot Classification of Doctor-AI Consultations</strong><br><button class=copy-to-clipboard title="Evaluating Embeddings for One-Shot Classification of Doctor-AI Consultations" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Bag-of-Words, GPT-2, N-gram, Word2vec<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04442v1.pdf filename=2402.04442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective communication between healthcare providers and patients is crucial to providing high-quality patient care. In this work, we investigate how Doctor-written and AI-generated texts in healthcare consultations can be classified using state-of-the-art embeddings and one-shot classification systems. By analyzing embeddings such as <b>bag-of-words,</b> character <b>n-grams,</b> <b>Word2Vec,</b> GloVe, fastText, and <b>GPT2</b> embeddings, we examine how well our one-shot classification systems capture semantic information within medical consultations. Results show that the embeddings are capable of capturing semantic features from text in a reliable and adaptable manner. Overall, <b>Word2Vec,</b> GloVe and Character <b>n-grams</b> embeddings performed well, indicating their suitability for modeling targeted to this task. <b>GPT2</b> embedding also shows notable performance, indicating its suitability for models tailored to this task as well. Our machine learning architectures significantly improved the quality of health conversations when training data are scarce, improving communication between patients and healthcare providers.</p></p class="citation"></blockquote><h3 id=24277-the-use-of-a-large-language-model-for-cyberbullying-detection-bayode-ogunleye-et-al-2024>(24/277) The Use of a Large Language Model for Cyberbullying Detection (Bayode Ogunleye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bayode Ogunleye, Babitha Dharmaraj. (2024)<br><strong>The Use of a Large Language Model for Cyberbullying Detection</strong><br><button class=copy-to-clipboard title="The Use of a Large Language Model for Cyberbullying Detection" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: H-3-3, cs-AI, cs-CL, cs-LG, cs.CL, stat-AP<br>Keyword Score: 40<br>Keywords: BERT, RoBERTa, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04088v1.pdf filename=2402.04088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The dominance of social media has added to the channels of bullying for perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. Several machine learning (ML) algorithms have been proposed for this purpose. However, their performances are not consistent due to high class imbalance and generalisation issues. In recent years, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>BERT</b> and <b>RoBERTa</b> have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks. Unfortunately, the <b>LLMs</b> have not been applied extensively for CB detection. In our paper, we explored the use of these models for cyberbullying (CB) detection. We have prepared a new dataset (D2) from existing studies (Formspring and Twitter). Our experimental results for dataset D1 and D2 showed that <b>RoBERTa</b> outperformed other models.</p></p class="citation"></blockquote><h3 id=25277-systematic-biases-in-llm-simulations-of-debates-amir-taubenfeld-et-al-2024>(25/277) Systematic Biases in LLM Simulations of Debates (Amir Taubenfeld et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Taubenfeld, Yaniv Dover, Roi Reichart, Ariel Goldstein. (2024)<br><strong>Systematic Biases in LLM Simulations of Debates</strong><br><button class=copy-to-clipboard title="Systematic Biases in LLM Simulations of Debates" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04049v1.pdf filename=2402.04049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in natural language processing, especially the emergence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> have opened exciting possibilities for constructing computational <b>simulations</b> designed to replicate human behavior accurately. However, <b>LLMs</b> are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of <b>LLMs</b> in simulating human interactions, particularly focusing on <b>LLMs&rsquo;</b> ability to simulate political debates. Our findings indicate a tendency for <b>LLM</b> agents to conform to the model&rsquo;s inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the <b>LLM</b> and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic <b>simulations.</b></p></p class="citation"></blockquote><h3 id=26277-beyond-lines-and-circles-unveiling-the-geometric-reasoning-gap-in-large-language-models-spyridon-mouselinos-et-al-2024>(26/277) Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models (Spyridon Mouselinos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Spyridon Mouselinos, Henryk Michalewski, Mateusz Malinowski. (2024)<br><strong>Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models</strong><br><button class=copy-to-clipboard title="Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03877v1.pdf filename=2402.03877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrate ever-increasing abilities in <b>mathematical</b> <b>and</b> algorithmic tasks, yet their geometric <b>reasoning</b> skills are underexplored. We investigate <b>LLMs&rsquo;</b> abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human <b>mathematical</b> <b>reasoning.</b> Our work reveals notable challenges that the state-of-the-art <b>LLMs</b> face in this domain despite many successes in similar areas. <b>LLMs</b> exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an <b>LLMs-based</b> multi-agents system that enhances their existing <b>reasoning</b> potential by conducting an internal dialogue. This work underscores <b>LLMs&rsquo;</b> current limitations in geometric <b>reasoning</b> and improves geometric <b>reasoning</b> capabilities through self-correction, collaboration, and diverse role specializations.</p></p class="citation"></blockquote><h3 id=27277-inside-llms-internal-states-retain-the-power-of-hallucination-detection-chao-chen-et-al-2024>(27/277) INSIDE: LLMs&rsquo; Internal States Retain the Power of Hallucination Detection (Chao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, Jieping Ye. (2024)<br><strong>INSIDE: LLMs&rsquo; Internal States Retain the Power of Hallucination Detection</strong><br><button class=copy-to-clipboard title="INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Hallucination Detection, Question Answering, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03744v1.pdf filename=2402.03744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge <b>hallucination</b> <b>have</b> raised widespread concerns for the security and reliability of deployed <b>LLMs.</b> Previous efforts in detecting <b>hallucinations</b> <b>have</b> been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within <b>LLMs&rsquo;</b> \textbf{IN}ternal \textbf{S}tates for halluc\textbf{I}nation \textbf{DE}tection (\textbf{INSIDE}). In particular, a simple yet effective \textbf{EigenScore} metric is proposed to better evaluate responses&rsquo; self-consistency, which exploits the eigenvalues of responses&rsquo; covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent <b>hallucination</b> <b>detection,</b> a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident <b>hallucinations.</b> <b>Extensive</b> experiments and ablation studies are performed on several popular <b>LLMs</b> and <b>question-answering</b> <b>(QA)</b> benchmarks, showing the effectiveness of our proposal.</p></p class="citation"></blockquote><h3 id=28277-empowering-language-models-with-active-inquiry-for-deeper-understanding-jing-cheng-pang-et-al-2024>(28/277) Empowering Language Models with Active Inquiry for Deeper Understanding (Jing-Cheng Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing-Cheng Pang, Heng-Bo Fan, Pengyuan Wang, Jia-Hao Xiao, Nan Tang, Si-Hang Yang, Chengxing Jia, Sheng-Jun Huang, Yang Yu. (2024)<br><strong>Empowering Language Models with Active Inquiry for Deeper Understanding</strong><br><button class=copy-to-clipboard title="Empowering Language Models with Active Inquiry for Deeper Understanding" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Active Learning, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03719v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03719v1.pdf filename=2402.03719v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has revolutionized the way that we interact with artificial intelligence systems through natural language. However, <b>LLMs</b> often misinterpret user queries because of their uncertain intention, leading to less helpful responses. In natural human interactions, clarification is sought through targeted <b>questioning</b> <b>to</b> uncover obscure information. Thus, in this paper, we introduce LaMAI (Language Model with <b>Active</b> <b>Inquiry),</b> designed to endow <b>LLMs</b> with this same level of interactive engagement. LaMAI leverages <b>active</b> <b>learning</b> techniques to raise the most informative <b>questions,</b> <b>fostering</b> a dynamic bidirectional dialogue. This approach not only narrows the contextual gap but also refines the output of the <b>LLMs,</b> aligning it more closely with user expectations. Our empirical studies, across a variety of complex datasets where <b>LLMs</b> have limited conversational context, demonstrate the effectiveness of LaMAI. The method improves answer accuracy from 31.9% to 50.9%, outperforming other leading <b>question-answering</b> <b>frameworks.</b> Moreover, in scenarios involving human participants, LaMAI consistently generates responses that are superior or comparable to baseline methods in more than 82% of the cases. The applicability of LaMAI is further evidenced by its successful integration with various <b>LLMs,</b> highlighting its potential for the future of interactive language models.</p></p class="citation"></blockquote><h3 id=29277-leveraging-large-language-models-for-hybrid-workplace-decision-support-yujin-kim-et-al-2024>(29/277) Leveraging Large Language Models for Hybrid Workplace Decision Support (Yujin Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujin Kim, Chin-Chia Hsu. (2024)<br><strong>Leveraging Large Language Models for Hybrid Workplace Decision Support</strong><br><button class=copy-to-clipboard title="Leveraging Large Language Models for Hybrid Workplace Decision Support" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-IR, cs.CL<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03616v1.pdf filename=2402.03616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> hold the potential to perform a variety of text processing tasks and provide textual explanations for proposed actions or decisions. In the era of hybrid work, <b>LLMs</b> can provide intelligent decision support for workers who are designing their hybrid work plans. In particular, they can offer suggestions and explanations to workers balancing numerous decision factors, thereby enhancing their work experience. In this paper, we present a decision support model for workspaces in hybrid work environments, leveraging the <b>reasoning</b> skill of <b>LLMs.</b> We first examine <b>LLM&rsquo;s</b> capability of making suitable workspace suggestions. We find that its <b>reasoning</b> extends beyond the guidelines in the <b>prompt</b> and the <b>LLM</b> can manage the trade-off among the available resources in the workspaces. We conduct an extensive user study to understand workers&rsquo; decision process for workspace choices and evaluate the effectiveness of the system. We observe that a worker&rsquo;s decision could be influenced by the <b>LLM&rsquo;s</b> suggestions and explanations. The participants in our study find the system to be convenient, regardless of whether reasons are provided or not. Our results show that employees can benefit from the <b>LLM-empowered</b> system for their workspace selection in hybrid workplace.</p></p class="citation"></blockquote><h3 id=30277-structured-entity-extraction-using-large-language-models-haolun-wu-et-al-2024>(30/277) Structured Entity Extraction Using Large Language Models (Haolun Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haolun Wu, Ye Yuan, Liana Mikaelyan, Alexander Meulemans, Xue Liu, James Hensman, Bhaskar Mitra. (2024)<br><strong>Structured Entity Extraction Using Large Language Models</strong><br><button class=copy-to-clipboard title="Structured Entity Extraction Using Large Language Models" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04437v1.pdf filename=2402.04437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in machine learning have significantly impacted the field of <b>information</b> <b>extraction,</b> with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> playing a pivotal role in extracting structured <b>information</b> <b>from</b> unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of <b>LLMs</b> for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.</p></p class="citation"></blockquote><h3 id=31277-chatbot-meets-pipeline-augment-large-language-model-with-definite-finite-automaton-yiyou-sun-et-al-2024>(31/277) Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton (Yiyou Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyou Sun, Junjie Hu, Wei Cheng, Haifeng Chen. (2024)<br><strong>Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton</strong><br><button class=copy-to-clipboard title="Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04411v1.pdf filename=2402.04411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the Definite Finite Automaton augmented <b>large</b> <b>language</b> <b>model</b> (DFA-LLM), a novel framework designed to enhance the capabilities of conversational agents using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Traditional <b>LLMs</b> face challenges in generating regulated and compliant responses in special scenarios with predetermined response guidelines, like emotional support and customer service. Our framework addresses these challenges by embedding a Definite Finite Automaton (DFA), learned from training dialogues, within the <b>LLM.</b> This structured approach enables the <b>LLM</b> to adhere to a deterministic response pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable structure through human-readable DFA, context-aware retrieval for responses in conversations, and plug-and-play compatibility with existing <b>LLMs.</b> Extensive benchmarks validate DFA-LLM&rsquo;s effectiveness, indicating its potential as a valuable contribution to the conversational agent.</p></p class="citation"></blockquote><h3 id=32277-anytool-self-reflective-hierarchical-agents-for-large-scale-api-calls-yu-du-et-al-2024>(32/277) AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls (Yu Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Du, Fangyun Wei, Hongyang Zhang. (2024)<br><strong>AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls</strong><br><button class=copy-to-clipboard title="AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04253v1.pdf filename=2402.04253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce AnyTool, a <b>large</b> <b>language</b> <b>model</b> agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of <b>GPT-4,</b> eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a <b>GPT-4</b> variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available at <a href=https://github.com/dyabel/AnyTool>https://github.com/dyabel/AnyTool</a>.</p></p class="citation"></blockquote><h3 id=33277-albnews-a-corpus-of-headlines-for-topic-modeling-in-albanian-erion-çano-et-al-2024>(33/277) AlbNews: A Corpus of Headlines for Topic Modeling in Albanian (Erion Çano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erion Çano, Dario Lamaj. (2024)<br><strong>AlbNews: A Corpus of Headlines for Topic Modeling in Albanian</strong><br><button class=copy-to-clipboard title="AlbNews: A Corpus of Headlines for Topic Modeling in Albanian" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Low-Resource, Topic Model, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04028v1.pdf filename=2402.04028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The scarcity of available text corpora for <b>low-resource</b> languages like Albanian is a serious hurdle for research in natural language processing tasks. This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian. The data can be freely used for conducting <b>topic</b> <b>modeling</b> research. We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples. These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments.</p></p class="citation"></blockquote><h3 id=34277-lv-eval-a-balanced-long-context-benchmark-with-5-length-levels-up-to-256k-tao-yuan-et-al-2024>(34/277) LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K (Tao Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, Yu Wang. (2024)<br><strong>LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K</strong><br><button class=copy-to-clipboard title="LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05136v1.pdf filename=2402.05136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop <b>QA</b> and multi-hop <b>QA,</b> comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across different context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 10 <b>LLMs</b> on LV-Eval and conduct ablation studies on the techniques used in LV-Eval construction. The results reveal that: (i) Commercial <b>LLMs</b> generally outperform open-source <b>LLMs</b> when evaluated within length levels shorter than their claimed context length. However, their overall performance is surpassed by open-source <b>LLMs</b> with longer context lengths. (ii) Extremely long-context <b>LLMs,</b> such as Yi-6B-200k, exhibit a relatively gentle degradation of performance, but their absolute performances may not necessarily be higher than those of <b>LLMs</b> with shorter context lengths. (iii) <b>LLMs&rsquo;</b> performances can significantly degrade in the presence of confusing information, especially in the pressure test of &ldquo;needle in a haystack&rdquo;. (iv) Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation, and these concerns are alleviated in LV-Eval. All datasets and evaluation codes are released at: <a href=https://github.com/infinigence/LVEval>https://github.com/infinigence/LVEval</a>.</p></p class="citation"></blockquote><h3 id=35277-exposing-propaganda-an-analysis-of-stylistic-cues-comparing-human-annotations-and-machine-classification-géraud-faye-et-al-2024>(35/277) Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification (Géraud Faye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Géraud Faye, Benjamin Icard, Morgane Casanova, Julien Chanson, François Maine, François Bancilhon, Guillaume Gadek, Guillaume Gravier, Paul Égré. (2024)<br><strong>Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification</strong><br><button class=copy-to-clipboard title="Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, RoBERTa, TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03780v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03780v2.pdf filename=2402.03780v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, <b>multimodal</b> dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a <b>TF-IDF</b> to serve as a baseline, and four different classifiers: two <b>RoBERTa-based</b> models, CATS using syntax, and one XGBoost combining syntactic and semantic features.</p></p class="citation"></blockquote><h3 id=36277-linear-time-minimum-bayes-risk-decoding-with-reference-aggregation-jannis-vamvas-et-al-2024>(36/277) Linear-time Minimum Bayes Risk Decoding with Reference Aggregation (Jannis Vamvas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jannis Vamvas, Rico Sennrich. (2024)<br><strong>Linear-time Minimum Bayes Risk Decoding with Reference Aggregation</strong><br><button class=copy-to-clipboard title="Linear-time Minimum Bayes Risk Decoding with Reference Aggregation" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Neural Machine Translation, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04251v1.pdf filename=2402.04251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Minimum Bayes Risk (MBR) decoding is a <b>text</b> <b>generation</b> technique that has been shown to improve the quality of <b>machine</b> <b>translations,</b> but is expensive, even if a sampling-based approximation is used. Besides requiring a large number of sampled sequences, it requires the pairwise calculation of a utility metric, which has quadratic complexity. In this paper, we propose to approximate pairwise metric scores with scores calculated against aggregated reference representations. This changes the complexity of utility estimation from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains of MBR decoding. We release our source code at <a href=https://github.com/ZurichNLP/mbr>https://github.com/ZurichNLP/mbr</a></p></p class="citation"></blockquote><h3 id=37277-sparse-graph-representations-for-procedural-instructional-documents-shruti-singh-et-al-2024>(37/277) Sparse Graph Representations for Procedural Instructional Documents (Shruti Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shruti Singh, Rishabh Gupta. (2024)<br><strong>Sparse Graph Representations for Procedural Instructional Documents</strong><br><button class=copy-to-clipboard title="Sparse Graph Representations for Procedural Instructional Documents" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Graph Convolutional Network, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03957v1.pdf filename=2402.03957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computation of document similarity is a critical task in various NLP domains that has applications in deduplication, matching, and <b>recommendation.</b> Traditional approaches for document similarity computation include learning representations of documents and employing a similarity or a distance function over the embeddings. However, pairwise similarities and differences are not efficiently captured by individual representations. Graph representations such as Joint Concept Interaction Graph (JCIG) represent a pair of documents as a joint undirected weighted graph. JCIGs facilitate an interpretable representation of document pairs as a graph. However, JCIGs are undirected, and don&rsquo;t consider the sequential flow of sentences in documents. We propose two approaches to model document similarity by representing document pairs as a directed and sparse JCIG that incorporates sequential information. We propose two algorithms inspired by Supergenome Sorting and Hamiltonian Path that replace the undirected edges with directed edges. Our approach also sparsifies the graph to $O(n)$ edges from JCIG&rsquo;s worst case of $O(n^2)$. We show that our sparse directed graph model architecture consisting of a Siamese encoder and <b>GCN</b> achieves comparable results to the baseline on datasets not containing sequential information and beats the baseline by ten points on an instructional documents dataset containing sequential information.</p></p class="citation"></blockquote><h3 id=38277-stanceosaurus-20-classifying-stance-towards-russian-and-spanish-misinformation-anton-lavrouk-et-al-2024>(38/277) Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation (Anton Lavrouk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anton Lavrouk, Ian Ligon, Tarek Naous, Jonathan Zheng, Alan Ritter, Wei Xu. (2024)<br><strong>Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation</strong><br><button class=copy-to-clipboard title="Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-LG, cs-SI, cs.CL<br>Keyword Score: 20<br>Keywords: Zero-shot, BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03642v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03642v1.pdf filename=2402.03642v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide high-quality, annotated, 5-way stance data extracted from Twitter, suitable for analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus 2.0 iteration, we extend this framework to encompass Russian and Spanish. The former is of current significance due to prevalent misinformation amid escalating tensions with the West and the violent incursion into Ukraine. The latter, meanwhile, represents an enormous community that has been largely overlooked on major social media platforms. By incorporating an additional 3,874 Spanish and Russian tweets over 41 misinformation claims, our objective is to support research focused on these issues. To demonstrate the value of this data, we employed <b>zero-shot</b> cross-lingual transfer on multilingual <b>BERT,</b> yielding results on par with the initial Stanceosaurus study with a macro F1 score of 43 for both languages. This underlines the viability of stance classification as an effective tool for identifying multicultural misinformation.</p></p class="citation"></blockquote><h3 id=39277-pro-han-a-heterogeneous-graph-attention-network-for-profile-based-spoken-language-understanding-dechuan-teng-et-al-2024>(39/277) Pro-HAN: A Heterogeneous Graph Attention Network for Profile-Based Spoken Language Understanding (Dechuan Teng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dechuan Teng, Chunlin Lu, Xiao Xu, Wanxiang Che, Libo Qin. (2024)<br><strong>Pro-HAN: A Heterogeneous Graph Attention Network for Profile-Based Spoken Language Understanding</strong><br><button class=copy-to-clipboard title="Pro-HAN: A Heterogeneous Graph Attention Network for Profile-Based Spoken Language Understanding" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03900v1.pdf filename=2402.03900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, Profile-based Spoken Language Understanding (SLU) has gained increasing attention, which aims to incorporate various types of supplementary profile information (i.e., Knowledge Graph, User Profile, Context Awareness) to eliminate the prevalent ambiguities in user utterances. However, existing approaches can only separately model different profile information, without considering their interrelationships or excluding irrelevant and conflicting information within them. To address the above issues, we introduce a Heterogeneous Graph Attention Network to perform <b>reasoning</b> across multiple Profile information, called Pro-HAN. Specifically, we design three types of edges, denoted as intra-Pro, inter-Pro, and utterance-Pro, to capture interrelationships among multiple Pros. We establish a new state-of-the-art on the ProSLU dataset, with an improvement of approximately 8% across all three metrics. Further analysis experiments also confirm the effectiveness of our method in modeling multi-source profile information.</p></p class="citation"></blockquote><h2 id=cslg-88>cs.LG (88)</h2><h3 id=40277-the-hedgehog--the-porcupine-expressive-linear-attentions-with-softmax-mimicry-michael-zhang-et-al-2024>(40/277) The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry (Michael Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Zhang, Kush Bhatia, Hermann Kumbong, Christopher Ré. (2024)<br><strong>The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry</strong><br><button class=copy-to-clipboard title="The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 100<br>Keywords: Fine-tuning, BERT, GPT, GPT-2, LLaMA, Transformer, GLUE, Large Language Model, Perplexity, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04347v1.pdf filename=2402.04347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Linear attentions have shown potential for improving <b>Transformer</b> efficiency, reducing attention&rsquo;s quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear <b>Transformers</b> from scratch, (2) <b>&ldquo;finetuned-conversion&rdquo;</b> of task-specific <b>Transformers</b> into linear versions that recover task performance, and (3) &ldquo;pretrained-conversion&rdquo; of <b>Transformers</b> such as <b>large</b> <b>language</b> <b>models</b> into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or &ldquo;spiky&rdquo;) weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple trainable MLPs to produce attention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99% of standard <b>Transformer</b> quality in train-from-scratch and <b>finetuned-conversion</b> settings, outperforming prior linear attentions up to 6 <b>perplexity</b> points on WikiText-103 with causal <b>GPTs,</b> and up to 8.7 <b>GLUE</b> score points on <b>finetuned</b> bidirectional <b>BERTs.</b> Hedgehog also enables pretrained-conversion. Converting a pretrained <b>GPT-2</b> into a linear attention variant achieves state-of-the-art 16.7 <b>perplexity</b> on WikiText-103 for 125M subquadratic decoder models. We finally turn a pretrained <b>Llama-2</b> 7B into a viable linear attention <b>Llama.</b> With low-rank adaptation, Hedgehog-Llama2 7B achieves 28.1 higher <b>ROUGE-1</b> points over the base standard attention model, where prior linear attentions lead to 16.5 point drops.</p></p class="citation"></blockquote><h3 id=41277-cehr-gpt-generating-electronic-health-records-with-chronological-patient-timelines-chao-pang-et-al-2024>(41/277) CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines (Chao Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Pang, Xinzhuo Jiang, Nishanth Parameshwar Pavinkurve, Krishna S. Kalluri, Elise L. Minto, Jason Patterson, Linying Zhang, George Hripcsak, Noémie Elhadad, Karthik Natarajan. (2024)<br><strong>CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines</strong><br><button class=copy-to-clipboard title="CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Counter-factual, Generative Adversarial Network, Generative Adversarial Network, GPT, Transformer, Counterfactual Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04400v1.pdf filename=2402.04400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs),</b> generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging <b>Generative</b> <b>Pre-trained</b> <b>Transformers</b> <b>(GPT)</b> for EHR data. This enables applications like disease progression analysis, population estimation, <b>counterfactual</b> <b>reasoning,</b> and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a <b>GPT</b> model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted to the Observational Medical Outcomes Partnership (OMOP) data format.</p></p class="citation"></blockquote><h3 id=42277-large-language-models-to-enhance-bayesian-optimization-tennison-liu-et-al-2024>(42/277) Large Language Models to Enhance Bayesian Optimization (Tennison Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tennison Liu, Nicolás Astorga, Nabeel Seedat, Mihaela van der Schaar. (2024)<br><strong>Large Language Models to Enhance Bayesian Optimization</strong><br><button class=copy-to-clipboard title="Large Language Models to Enhance Bayesian Optimization" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Zero-shot, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03921v1.pdf filename=2402.03921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \texttt{LLAMBO}, a novel approach that integrates the capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> within BO. At a high level, we frame the BO problem in natural language terms, enabling <b>LLMs</b> to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, <b>few-shot</b> <b>learning</b> proficiency, and domain knowledge of <b>LLMs</b> can enhance various components of model-based BO. Our findings illustrate that \texttt{LLAMBO} is effective at <b>zero-shot</b> warmstarting, and improves surrogate modeling and candidate sampling, especially in the early stages of search when observations are sparse. Our approach is performed in context and does not require <b>LLM</b> <b>finetuning.</b> Additionally, it is modular by design, allowing individual components to be integrated into existing BO frameworks, or function cohesively as an end-to-end method. We empirically validate \texttt{LLAMBO}&rsquo;s efficacy on the problem of hyperparameter tuning, highlighting strong empirical performance across a range of diverse benchmarks, proprietary, and synthetic tasks.</p></p class="citation"></blockquote><h3 id=43277-fine-tuned-language-models-generate-stable-inorganic-materials-as-text-nate-gruver-et-al-2024>(43/277) Fine-Tuned Language Models Generate Stable Inorganic Materials as Text (Nate Gruver et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nate Gruver, Anuroop Sriram, Andrea Madotto, Andrew Gordon Wilson, C. Lawrence Zitnick, Zachary Ulissi. (2024)<br><strong>Fine-Tuned Language Models Generate Stable Inorganic Materials as Text</strong><br><button class=copy-to-clipboard title="Fine-Tuned Language Models Generate Stable Inorganic Materials as Text" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-mtrl-sci, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04379v1.pdf filename=2402.04379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose <b>fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> for generation of stable materials. While unorthodox, <b>fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> on text-encoded atomistic data is simple to implement yet reliable, with around 90% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model <b>(fine-tuned</b> <b>LLaMA-2</b> 70B) can generate materials predicted to be metastable at about twice the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text <b>prompting&rsquo;s</b> inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation. Finally, we show that language models&rsquo; ability to capture key symmetries of crystal structures improves with model scale, suggesting that the biases of pretrained <b>LLMs</b> are surprisingly well-suited for atomistic data.</p></p class="citation"></blockquote><h3 id=44277-can-mamba-learn-how-to-learn-a-comparative-study-on-in-context-learning-tasks-jongho-park-et-al-2024>(44/277) Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks (Jongho Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos. (2024)<br><strong>Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks</strong><br><button class=copy-to-clipboard title="Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Graph Attention Networks, Convolution, Transformer, In-context Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04248v1.pdf filename=2402.04248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to <b>Transformer</b> networks in language modeling, by incorporating <b>gating,</b> <b>convolutions,</b> and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their <b>in-context</b> <b>learning</b> <b>(ICL)</b> capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to <b>Transformers.</b> In this study, we evaluate the <b>ICL</b> performance of SSMs, focusing on Mamba, against <b>Transformer</b> models across various tasks. Our results show that SSMs perform comparably to <b>Transformers</b> in standard regression <b>ICL</b> tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, \variant, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing <b>ICL</b> in language models.</p></p class="citation"></blockquote><h3 id=45277-understanding-the-effect-of-noise-in-llm-training-data-with-algorithmic-chains-of-thought-alex-havrilla-et-al-2024>(45/277) Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought (Alex Havrilla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Havrilla, Maia Iyer. (2024)<br><strong>Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought</strong><br><button class=copy-to-clipboard title="Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Few-shot, Fine-tuning, Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04004v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04004v2.pdf filename=2402.04004v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>During both pretraining and <b>fine-tuning,</b> <b>Large</b> <b>Language</b> <b>Models</b> (\textbf{LLMs}) are trained on trillions of tokens of text of widely varying quality. Both phases of training typically involve heuristically filtering out ``low-quality&rsquo;&rsquo; or \textit{noisy} training samples, yet little is known quantitatively about how the type or intensity of noise affects downstream performance. In this work, we study how noise in chain of thought (\textbf{CoT}) impacts task performance in the highly-controlled setting of algorithmically solvable tasks. First, we develop the Traced Integer (\textbf{TInt}) framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers. We then define two types of noise: \textit{static} noise, a local form of noise which is applied after the CoT trace is computed, and \textit{dynamic} noise, a global form of noise which propagates errors in the trace as it is computed. We then evaluate the test performance of pretrained models both <b>prompted</b> and <b>fine-tuned</b> on noised datasets with varying levels of dataset contamination and intensity. We find <b>fine-tuned</b> models are extremely robust to high levels of static noise but struggle significantly more with lower levels of dynamic noise. In contrast, <b>few-shot</b> <b>prompted</b> models appear more sensitive to even static noise. We conclude with a discussion of how our findings impact noise filtering best-practices, in particular emphasizing the importance of removing samples containing destructive dynamic noise with global errors.</p></p class="citation"></blockquote><h3 id=46277-pregip-watermarking-the-pretraining-of-graph-neural-networks-for-deep-intellectual-property-protection-enyan-dai-et-al-2024>(46/277) PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection (Enyan Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enyan Dai, Minhua Lin, Suhang Wang. (2024)<br><strong>PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection</strong><br><button class=copy-to-clipboard title="PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Graph Neural Network, Graph Neural Network, Fine-tuning, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04435v1.pdf filename=2402.04435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretraining on <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> has shown great power in facilitating various downstream tasks. As pretraining generally requires huge amount of data and computational resources, the pretrained <b>GNNs</b> are high-value Intellectual Properties (IP) of the legitimate owner. However, adversaries may illegally copy and deploy the pretrained <b>GNN</b> models for their downstream tasks. Though initial efforts have been made to watermark <b>GNN</b> classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to <b>self-supervised</b> <b>pretraining</b> of <b>GNN</b> models. Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of <b>GNN</b> encoder for IP protection while maintain the high-quality of the embedding space. PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained <b>GNN</b> encoder. A <b>finetuning-resistant</b> watermark injection is further deployed. Theoretical analysis and extensive experiments show the effectiveness of {\method} in IP protection and maintaining high-performance for downstream tasks.</p></p class="citation"></blockquote><h3 id=47277-in-context-learning-agents-are-asymmetric-belief-updaters-johannes-a-schubert-et-al-2024>(47/277) In-context learning agents are asymmetric belief updaters (Johannes A. Schubert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes A. Schubert, Akshay K. Jagadish, Marcel Binz, Eric Schulz. (2024)<br><strong>In-context learning agents are asymmetric belief updaters</strong><br><button class=copy-to-clipboard title="In-context learning agents are asymmetric belief updaters" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Counter-factual, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03969v1.pdf filename=2402.03969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the <b>in-context</b> <b>learning</b> dynamics of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> using three instrumental learning tasks adapted from cognitive psychology. We find that <b>LLMs</b> update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about <b>counterfactual</b> feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized <b>in-context</b> <b>learning</b> agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how <b>in-context</b> <b>learning</b> works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition.</p></p class="citation"></blockquote><h3 id=48277-return-aligned-decision-transformer-tsunehiko-tanaka-et-al-2024>(48/277) Return-Aligned Decision Transformer (Tsunehiko Tanaka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tsunehiko Tanaka, Kenshi Abe, Kaito Ariu, Tetsuro Morimura, Edgar Simo-Serra. (2024)<br><strong>Return-Aligned Decision Transformer</strong><br><button class=copy-to-clipboard title="Return-Aligned Decision Transformer" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning, Supervised Learning, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03923v1.pdf filename=2402.03923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional approaches in <b>offline</b> <b>reinforcement</b> <b>learning</b> aim to learn the optimal policy that maximizes the cumulative reward, also known as return. However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent&rsquo;s performance. Decision <b>Transformer</b> (DT) optimizes a policy that generates actions conditioned on the target return through <b>supervised</b> <b>learning</b> and is equipped with a mechanism to control the agent using the target return. Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT. In this paper, we propose Return-Aligned Decision <b>Transformer</b> (RADT), designed to effectively align the actual return with the target return. Our model decouples returns from the conventional input sequence, which typically consists of returns, states, and actions, to enhance the relationships between returns and states, as well as returns and actions. Extensive experiments show that RADT reduces the discrepancies between the actual return and the target return of DT-based methods.</p></p class="citation"></blockquote><h3 id=49277-asymptotic-generalization-error-of-a-single-layer-graph-convolutional-network-o-duranthon-et-al-2024>(49/277) Asymptotic generalization error of a single-layer graph convolutional network (O. Duranthon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>O. Duranthon, L. Zdeborová. (2024)<br><strong>Asymptotic generalization error of a single-layer graph convolutional network</strong><br><button class=copy-to-clipboard title="Asymptotic generalization error of a single-layer graph convolutional network" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-dis-nn, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Convolution, Convolutional Neural Network, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03818v1.pdf filename=2402.03818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>graph</b> <b>convolutional</b> <b>networks</b> show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of <b>supervised</b> fully connected neural networks. In this article, we predict the performances of a single-layer <b>graph</b> <b>convolutional</b> <b>network</b> <b>(GCN)</b> trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit. Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM. We also study the high signal-to-noise ratio limit, detail the convergence rates of the <b>GCN</b> and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases.</p></p class="citation"></blockquote><h3 id=50277-masked-graph-autoencoder-with-non-discrete-bandwidths-ziwen-zhao-et-al-2024>(50/277) Masked Graph Autoencoder with Non-discrete Bandwidths (Ziwen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziwen Zhao, Yuhua Li, Yixiong Zou, Jiliang Tang, Ruixuan Li. (2024)<br><strong>Masked Graph Autoencoder with Non-discrete Bandwidths</strong><br><button class=copy-to-clipboard title="Masked Graph Autoencoder with Non-discrete Bandwidths" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 50<br>Keywords: Node Classification, Graph Neural Network, Autoencoder, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03814v1.pdf filename=2402.03814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Masked <b>graph</b> <b>autoencoders</b> <b>have</b> emerged as a powerful <b>graph</b> <b>self-supervised</b> <b>learning</b> method that has yet to be fully explored. In this paper, we unveil that the existing discrete edge masking and binary link reconstruction strategies are insufficient to learn topologically informative representations, from the perspective of message propagation on <b>graph</b> <b>neural</b> <b>networks.</b> These limitations include blocking message flows, vulnerability to over-smoothness, and suboptimal neighborhood discriminability. Inspired by these understandings, we explore non-discrete edge masks, which are sampled from a continuous and dispersive probability distribution instead of the discrete Bernoulli distribution. These masks restrict the amount of output messages for each edge, referred to as &ldquo;bandwidths&rdquo;. We propose a novel, informative, and effective topological masked <b>graph</b> <b>autoencoder</b> <b>using</b> bandwidth masking and a layer-wise bandwidth prediction objective. We demonstrate its powerful <b>graph</b> <b>topological</b> <b>learning</b> ability both theoretically and empirically. Our proposed framework outperforms representative baselines in both <b>self-supervised</b> <b>link</b> prediction (improving the discrete edge reconstructors by at most 20%) and <b>node</b> <b>classification</b> on numerous datasets, solely with a structure-learning pretext. Our implementation is available at <a href=https://github.com/Newiz430/Bandana>https://github.com/Newiz430/Bandana</a>.</p></p class="citation"></blockquote><h3 id=51277-weakly-supervised-anomaly-detection-via-knowledge-data-alignment-haihong-zhao-et-al-2024>(51/277) Weakly Supervised Anomaly Detection via Knowledge-Data Alignment (Haihong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haihong Zhao, Chenyi Zi, Yang Liu, Chen Zhang, Yan Zhou, Jia Li. (2024)<br><strong>Weakly Supervised Anomaly Detection via Knowledge-Data Alignment</strong><br><button class=copy-to-clipboard title="Weakly Supervised Anomaly Detection via Knowledge-Data Alignment" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Supervised Learning, Unsupervised Learning, Unsupervised Learning, Weakly-supervised Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03785v1.pdf filename=2402.03785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis. Most methods, which rely on <b>unsupervised</b> <b>learning,</b> are hard to reach satisfactory detection accuracy due to the lack of labels. Weakly <b>Supervised</b> Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance. Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies. In this paper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) to integrate rule knowledge, typically <b>summarized</b> by human experts, to supplement the limited labeled data. Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data. To facilitate this alignment, we employ the Optimal Transport (OT) technique. We then incorporate the OT distance as an additional loss term to the original objective function of WSAD methodologies. Comprehensive experimental results on five real-world datasets demonstrate that our proposed KDAlign framework markedly surpasses its state-of-the-art counterparts, achieving superior performance across various anomaly types.</p></p class="citation"></blockquote><h3 id=52277-similarity-based-neighbor-selection-for-graph-llms-rui-li-et-al-2024>(52/277) Similarity-based Neighbor Selection for Graph LLMs (Rui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Li, Jiwei Li, Jiawei Han, Guoyin Wang. (2024)<br><strong>Similarity-based Neighbor Selection for Graph LLMs</strong><br><button class=copy-to-clipboard title="Similarity-based Neighbor Selection for Graph LLMs" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs-SI, cs.LG<br>Keyword Score: 50<br>Keywords: Node Classification, Graph Neural Network, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03720v1.pdf filename=2402.03720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-attributed graphs (TAGs) present unique challenges for direct processing by Language Learning Models <b>(LLMs),</b> yet their extensive commonsense knowledge and robust <b>reasoning</b> capabilities offer great promise for <b>node</b> <b>classification</b> in TAGs. Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced <b>LLMs.</b> To address these challenges, we introduce Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor selection techniques, SNS effectively improves the quality of selected neighbors, thereby improving graph representation and alleviating issues like over-squashing and heterophily. Besides, as an inductive and training-free approach, SNS demonstrates superior generalization and scalability over traditional <b>GNN</b> methods. Our comprehensive experiments, adhering to standard dataset partitioning practices, demonstrate that SNS, through simple <b>prompt</b> interactions with <b>LLMs,</b> consistently outperforms vanilla <b>GNNs</b> and achieves state-of-the-art results on datasets like PubMed in <b>node</b> <b>classification,</b> showcasing <b>LLMs&rsquo;</b> potential in graph structure understanding. Our research further underscores the significance of graph structure integration in <b>LLM</b> applications and identifies key factors for their success in <b>node</b> <b>classification.</b> Code is available at <a href=https://github.com/ruili33/SNS>https://github.com/ruili33/SNS</a>.</p></p class="citation"></blockquote><h3 id=53277-musicrl-aligning-music-generation-to-human-preferences-geoffrey-cideron-et-al-2024>(53/277) MusicRL: Aligning Music Generation to Human Preferences (Geoffrey Cideron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Geoffrey Cideron, Sertan Girgin, Mauro Verzetti, Damien Vincent, Matej Kastelic, Zalán Borsos, Brian McWilliams, Victor Ungureanu, Olivier Bachem, Olivier Pietquin, Matthieu Geist, Léonard Hussenot, Neil Zeghidour, Andrea Agostinelli. (2024)<br><strong>MusicRL: Aligning Music Generation to Human Preferences</strong><br><button class=copy-to-clipboard title="MusicRL: Aligning Music Generation to Human Preferences" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SD, cs.LG, eess-AS<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Reinforcement Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04229v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04229v1.pdf filename=2402.04229v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose MusicRL, the first music generation system <b>finetuned</b> from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as &ldquo;upbeat work-out music&rdquo; can map to a retro guitar solo or a techno pop beat). Not only this makes <b>supervised</b> training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment <b>finetuning.</b> MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens <b>finetuned</b> with <b>reinforcement</b> <b>learning</b> to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to <b>finetune</b> MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using <b>Reinforcement</b> <b>Learning</b> from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the <b>finetuning</b> of music generation models.</p></p class="citation"></blockquote><h3 id=54277-connecting-the-dots-collaborative-fine-tuning-for-black-box-vision-language-models-zhengbo-wang-et-al-2024>(54/277) Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models (Zhengbo Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengbo Wang, Jian Liang, Ran He, Zilei Wang, Tieniu Tan. (2024)<br><strong>Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models</strong><br><button class=copy-to-clipboard title="Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Few-shot, Fine-tuning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04050v1.pdf filename=2402.04050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the emergence of pretrained <b>vision-language</b> models (VLMs), considerable efforts have been devoted to <b>fine-tuning</b> them for downstream tasks. Despite the progress made in designing efficient <b>fine-tuning</b> methods, such methods require access to the model&rsquo;s parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for <b>fine-tuning</b> black-box VLMs to downstream tasks, where one only has access to the input <b>prompts</b> and the output predictions of the model. CraFT comprises two modules, a <b>prompt</b> generation module for learning text <b>prompts</b> and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algorithm. Extensive experiments on <b>few-shot</b> classification over 15 datasets demonstrate the superiority of CraFT. The results show that CraFT achieves a decent gain of about 12% with 16-shot datasets and only 8,000 queries. Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62% compared to the white-box method.</p></p class="citation"></blockquote><h3 id=55277-efficient-availability-attacks-against-supervised-and-contrastive-learning-simultaneously-yihan-wang-et-al-2024>(55/277) Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously (Yihan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihan Wang, Yifan Zhu, Xiao-Shan Gao. (2024)<br><strong>Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously</strong><br><button class=copy-to-clipboard title="Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Data Augmentation, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04010v1.pdf filename=2402.04010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Availability attacks can prevent the unauthorized use of private <b>data</b> <b>and</b> commercial datasets by generating imperceptible noise and making unlearnable examples before release. Ideally, the obtained unlearnability prevents algorithms from training usable models. When <b>supervised</b> <b>learning</b> (SL) algorithms have failed, a malicious <b>data</b> <b>collector</b> possibly resorts to <b>contrastive</b> <b>learning</b> (CL) algorithms to bypass the protection. Through evaluation, we have found that most of the existing methods are unable to achieve both <b>supervised</b> <b>and</b> <b>contrastive</b> <b>unlearnability,</b> which poses risks to <b>data</b> <b>protection.</b> Different from recent methods based on <b>contrastive</b> <b>error</b> minimization, we employ <b>contrastive-like</b> <b>data</b> <b>augmentations</b> in <b>supervised</b> <b>error</b> minimization or maximization frameworks to obtain attacks effective for both SL and CL. Our proposed AUE and AAP attacks achieve state-of-the-art worst-case unlearnability across SL and CL algorithms with less computation consumption, showcasing prospects in real-world applications.</p></p class="citation"></blockquote><h3 id=56277-billm-pushing-the-limit-of-post-training-quantization-for-llms-wei-huang-et-al-2024>(56/277) BiLLM: Pushing the Limit of Post-Training Quantization for LLMs (Wei Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, Xiaojuan Qi. (2024)<br><strong>BiLLM: Pushing the Limit of Post-Training Quantization for LLMs</strong><br><button class=copy-to-clipboard title="BiLLM: Pushing the Limit of Post-Training Quantization for LLMs" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Quantization, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04291v1.pdf filename=2402.04291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretrained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing <b>quantization</b> techniques fall short of maintaining <b>LLM</b> performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training <b>quantization</b> scheme tailored for pretrained <b>LLMs.</b> Based on the weight distribution of <b>LLMs,</b> BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy inference (e.g. 8.41 <b>perplexity</b> on LLaMA2-70B) with only 1.08-bit weights across various <b>LLMs</b> families and evaluation metrics, outperforms SOTA <b>quantization</b> methods of <b>LLM</b> by significant margins. Moreover, BiLLM enables the binarization process of the <b>LLM</b> with 7 billion weights within 0.5 hours on a single GPU, demonstrating satisfactory time efficiency.</p></p class="citation"></blockquote><h3 id=57277-seabo-a-simple-search-based-method-for-offline-imitation-learning-jiafei-lyu-et-al-2024>(57/277) SEABO: A Simple Search-Based Method for Offline Imitation Learning (Jiafei Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiafei Lyu, Xiaoteng Ma, Le Wan, Runze Liu, Xiu Li, Zongqing Lu. (2024)<br><strong>SEABO: A Simple Search-Based Method for Offline Imitation Learning</strong><br><button class=copy-to-clipboard title="SEABO: A Simple Search-Based Method for Offline Imitation Learning" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03807v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03807v1.pdf filename=2402.03807v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>reinforcement</b> <b>learning</b> (RL) has attracted much attention due to its ability in learning from static <b>offline</b> <b>datasets</b> <b>and</b> eliminating the need of interacting with the environment. Nevertheless, the success of <b>offline</b> <b>RL</b> <b>relies</b> heavily on the <b>offline</b> <b>transitions</b> <b>annotated</b> with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the <b>offline</b> <b>imitation</b> <b>learning</b> (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based <b>offline</b> <b>IL</b> <b>method,</b> tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an <b>unsupervised</b> <b>learning</b> manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to <b>offline</b> <b>RL</b> <b>algorithms</b> with ground-truth rewards, given only a single expert trajectory, and can outperform prior reward learning and <b>offline</b> <b>IL</b> <b>methods</b> across many tasks. Moreover, we demonstrate that SEABO also works well if the expert demonstrations contain only observations. Our code is publicly available at <a href=https://github.com/dmksjfl/SEABO>https://github.com/dmksjfl/SEABO</a>.</p></p class="citation"></blockquote><h3 id=58277-learning-to-generate-explainable-stock-predictions-using-self-reflective-large-language-models-kelvin-j-l-koa-et-al-2024>(58/277) Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models (Kelvin J. L. Koa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kelvin J. L. Koa, Yunshan Ma, Ritchie Ng, Tat-Seng Chua. (2024)<br><strong>Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models</strong><br><button class=copy-to-clipboard title="Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, q-fin-ST<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03659v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03659v2.pdf filename=2402.03659v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for <b>LLMs,</b> as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires <b>LLMs</b> to explain verbally why certain factors are more important than the others. On the other hand, to <b>fine-tune</b> <b>LLMs</b> for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our <b>Summarize-Explain-Predict</b> (SEP) framework, which utilizes a self-reflective agent and Proximal Policy Optimization (PPO) to let a <b>LLM</b> teach itself how to generate explainable stock predictions in a fully autonomous manner. The reflective agent learns how to explain past stock movements through self-reasoning, while the PPO trainer trains the model to generate the most likely explanations from input texts. The training samples for the PPO trainer are also the responses generated during the reflective process, which eliminates the need for human annotators. Using our SEP framework, we <b>fine-tune</b> a <b>LLM</b> that can outperform both traditional deep-learning and <b>LLM</b> methods in prediction accuracy and Matthews correlation coefficient for the stock classification task. To justify the generalization capability of our framework, we further test it on the portfolio construction task, and demonstrate its effectiveness through various portfolio metrics.</p></p class="citation"></blockquote><h3 id=59277-lens-a-foundation-model-for-network-traffic-in-cybersecurity-qineng-wang-et-al-2024>(59/277) Lens: A Foundation Model for Network Traffic in Cybersecurity (Qineng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qineng Wang, Chen Qian, Xiaochang Li, Ziyu Yao, Huajie Shao. (2024)<br><strong>Lens: A Foundation Model for Network Traffic in Cybersecurity</strong><br><button class=copy-to-clipboard title="Lens: A Foundation Model for Network Traffic in Cybersecurity" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Foundation Model, T5, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03646v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03646v2.pdf filename=2402.03646v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Network traffic refers to the amount of data being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic is challenging due to the diverse nature of data packets, which often feature heterogeneous headers and encrypted payloads lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the <b>Transformer</b> encoder or decoder to learn the representations from massive traffic data. However, these methods typically excel in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a <b>foundation</b> <b>model</b> for network traffic that leverages the <b>T5</b> architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the global information while preserving the generative ability, our model can better learn the representations from raw data. To further enhance pre-training effectiveness, we design a novel loss that combines three distinct tasks: Masked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous Traffic Prediction (HTP). Evaluation results across various benchmark datasets demonstrate that the proposed Lens outperforms the baselines in most downstream tasks related to both traffic understanding and generation. Notably, it also requires much less labeled data for <b>fine-tuning</b> compared to current methods.</p></p class="citation"></blockquote><h3 id=60277-iot-network-traffic-analysis-with-deep-learning-mei-liu-et-al-2024>(60/277) IoT Network Traffic Analysis with Deep Learning (Mei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mei Liu, Leon Yang. (2024)<br><strong>IoT Network Traffic Analysis with Deep Learning</strong><br><button class=copy-to-clipboard title="IoT Network Traffic Analysis with Deep Learning" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs-NI, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04469v1.pdf filename=2402.04469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As IoT networks become more complex and generate massive amounts of dynamic data, it is difficult to monitor and detect anomalies using traditional statistical methods and machine learning methods. Deep learning algorithms can process and learn from large amounts of data and can also be trained using <b>unsupervised</b> <b>learning</b> techniques, meaning they don&rsquo;t require labelled data to detect anomalies. This makes it possible to detect new and unknown anomalies that may not have been detected before. Also, deep learning algorithms can be automated and highly scalable; thereby, they can run continuously in the backend and make it achievable to monitor large IoT networks instantly. In this work, we conduct a literature review on the most recent works using deep learning techniques and implement a model using ensemble techniques on the <b>KDD</b> Cup 99 dataset. The experimental results showcase the impressive performance of our deep anomaly detection model, achieving an accuracy of over 98%.</p></p class="citation"></blockquote><h3 id=61277-quip-even-better-llm-quantization-with-hadamard-incoherence-and-lattice-codebooks-albert-tseng-et-al-2024>(61/277) QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks (Albert Tseng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, Christopher De Sa. (2024)<br><strong>QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks</strong><br><button class=copy-to-clipboard title="QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Quantization, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04396v1.pdf filename=2402.04396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Post-training <b>quantization</b> (PTQ) reduces the memory footprint of <b>LLMs</b> by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector <b>quantization</b> techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses <b>fine-tuning</b> to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.</p></p class="citation"></blockquote><h3 id=62277-tag-llm-repurposing-general-purpose-llms-for-specialized-domains-junhong-shen-et-al-2024>(62/277) Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains (Junhong Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, Nicolo Fusi. (2024)<br><strong>Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains</strong><br><button class=copy-to-clipboard title="Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05140v1.pdf filename=2402.05140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general <b>LLMs</b> into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the <b>LLM&rsquo;s</b> embedding layer, to condition the <b>LLM.</b> We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from task functions, our method enables <b>zero-shot</b> generalization to unseen problems through diverse combinations of the input tags. It also boosts <b>LLM&rsquo;s</b> performance in various specialized domains, such as predicting protein or chemical properties and modeling drug-target interactions, outperforming expert models tailored to these tasks.</p></p class="citation"></blockquote><h3 id=63277-harmbench-a-standardized-evaluation-framework-for-automated-red-teaming-and-robust-refusal-mantas-mazeika-et-al-2024>(63/277) HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal (Mantas Mazeika et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, Dan Hendrycks. (2024)<br><strong>HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal</strong><br><button class=copy-to-clipboard title="HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Adversarial Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04249v1.pdf filename=2402.04249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a <b>large-scale</b> <b>comparison</b> <b>of</b> 18 red teaming methods and 33 target <b>LLMs</b> and defenses, yielding novel insights. We also introduce a highly efficient <b>adversarial</b> <b>training</b> method that greatly enhances <b>LLM</b> robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at <a href=https://github.com/centerforaisafety/HarmBench>https://github.com/centerforaisafety/HarmBench</a>.</p></p class="citation"></blockquote><h3 id=64277-improved-generalization-of-weight-space-networks-via-augmentations-aviv-shamsian-et-al-2024>(64/277) Improved Generalization of Weight Space Networks via Augmentations (Aviv Shamsian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aviv Shamsian, Aviv Navon, David W. Zhang, Yan Zhang, Ethan Fetaya, Gal Chechik, Haggai Maron. (2024)<br><strong>Improved Generalization of Weight Space Networks via Augmentations</strong><br><button class=copy-to-clipboard title="Improved Generalization of Weight Space Networks via Augmentations" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Data Augmentation, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04081v1.pdf filename=2402.04081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for <b>data</b> <b>augmentation</b> in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more <b>data.</b> <b>In</b> <b>self-supervised</b> <b>contrastive</b> <b>learning,</b> they yield substantial 5-10% gains in downstream classification.</p></p class="citation"></blockquote><h3 id=65277-entropy-regularized-diffusion-policy-with-q-ensembles-for-offline-reinforcement-learning-ruoqi-zhang-et-al-2024>(65/277) Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning (Ruoqi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruoqi Zhang, Ziwei Luo, Jens Sjölund, Thomas B. Schön, Per Mattsson. (2024)<br><strong>Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 30<br>Keywords: Offline Reinforcement Learning, Out-of-distribution, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04080v1.pdf filename=2402.04080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents advanced techniques of training diffusion policies for <b>offline</b> <b>reinforcement</b> <b>learning</b> (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of <b>offline</b> <b>datasets.</b> <b>To</b> mitigate the impact of inaccurate value functions from <b>out-of-distribution</b> data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in <b>offline</b> <b>RL,</b> <b>our</b> method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at \href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}.</p></p class="citation"></blockquote><h3 id=66277-on-dimensionality-of-feature-vectors-in-mpnns-césar-bravo-et-al-2024>(66/277) On dimensionality of feature vectors in MPNNs (César Bravo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>César Bravo, Alexander Kozachinskiy, Cristóbal Rojas. (2024)<br><strong>On dimensionality of feature vectors in MPNNs</strong><br><button class=copy-to-clipboard title="On dimensionality of feature vectors in MPNNs" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Message-Passing, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03966v1.pdf filename=2402.03966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We revisit the classical result of Morris et al.~(AAAI'19) that <b>message-passing</b> graphs neural networks (MPNNs) are equal in their distinguishing power to the Weisfeiler&ndash;Leman (WL) isomorphism test. Morris et al.~show their <b>simulation</b> result with ReLU activation function and $O(n)$-dimensional feature vectors, where $n$ is the number of nodes of the graph. Recently, by introducing randomness into the architecture, Aamand et al.~(NeurIPS'22) were able to improve this bound to $O(\log n)$-dimensional feature vectors, although at the expense of guaranteeing perfect <b>simulation</b> only with high probability. In all these constructions, to guarantee equivalence to the WL test, the dimension of feature vectors in the MPNN has to increase with the size of the graphs. However, architectures used in practice have feature vectors of constant dimension. Thus, there is a gap between the guarantees provided by these results and the actual characteristics of architectures used in practice. In this paper we close this gap by showing that, for \emph{any} non-polynomial analytic (like the sigmoid) activation function, to guarantee that MPNNs are equivalent to the WL test, feature vectors of dimension $d=1$ is all we need, independently of the size of the graphs. Our main technical insight is that for simulating multi-sets in the WL-test, it is enough to use linear independence of feature vectors over rationals instead of reals. Countability of the set of rationals together with nice properties of analytic functions allow us to carry out the <b>simulation</b> invariant over the iterations of the WL test without increasing the dimension of the feature vectors.</p></p class="citation"></blockquote><h3 id=67277-relu2-wins-discovering-efficient-activation-functions-for-sparse-llms-zhengyan-zhang-et-al-2024>(67/277) ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs (Zhengyan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, Maosong Sun. (2024)<br><strong>ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs</strong><br><button class=copy-to-clipboard title="ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Low-Resource, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03804v1.pdf filename=2402.03804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparse computation offers a compelling solution for the inference of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in <b>low-resource</b> scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based <b>LLMs,</b> leveraging zeros in activation values, we broaden the scope of sparse <b>LLMs</b> beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU <b>LLMs</b> also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of <b>LLMs</b> from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on <b>LLMs</b> utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing ReLU$^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse <b>LLMs.</b> We will release the code to facilitate future research.</p></p class="citation"></blockquote><h3 id=68277-reinforcement-learning-from-bagged-reward-a-transformer-based-approach-for-instance-level-reward-redistribution-yuting-tang-et-al-2024>(68/277) Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution (Yuting Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuting Tang, Xin-Qiang Cai, Yao-Xiang Ding, Qiyu Wu, Guoqing Liu, Masashi Sugiyama. (2024)<br><strong>Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution</strong><br><button class=copy-to-clipboard title="Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03771v1.pdf filename=2402.03771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>reinforcement</b> <b>Learning</b> (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy. However, in many real-world applications, the instant reward signals are not obtainable by the agent. Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory. In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions. To formally study this situation, we introduce a novel RL setting termed <b>Reinforcement</b> <b>Learning</b> from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained. We provide the theoretical study to establish the connection between RLBR and standard RL in Markov Decision Processes (MDPs). To effectively explore the reward distributions within the bagged rewards, we propose a <b>Transformer-based</b> reward model, the Reward Bag <b>Transformer</b> (RBT), which uses the <b>self-attention</b> mechanism for interpreting the contextual nuances and temporal dependencies within each bag. Extensive experimental analyses demonstrate the superiority of our method, particularly in its ability to mimic the original MDP&rsquo;s reward distribution, highlighting its proficiency in contextual understanding and adaptability to environmental dynamics.</p></p class="citation"></blockquote><h3 id=69277-fed-cvlc-compressing-federated-learning-communications-with-variable-length-codes-xiaoxin-su-et-al-2024>(69/277) Fed-CVLC: Compressing Federated Learning Communications with Variable-Length Codes (Xiaoxin Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoxin Su, Yipeng Zhou, Laizhong Cui, John C. S. Lui, Jiangchuan Liu. (2024)<br><strong>Fed-CVLC: Compressing Federated Learning Communications with Variable-Length Codes</strong><br><button class=copy-to-clipboard title="Fed-CVLC: Compressing Federated Learning Communications with Variable-Length Codes" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Model Compression, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03770v1.pdf filename=2402.03770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Federated Learning (FL) paradigm, a parameter server (PS) concurrently communicates with distributed participating clients for <b>model</b> <b>collection,</b> update aggregation, and <b>model</b> <b>distribution</b> over multiple rounds, without touching private data owned by individual clients. FL is appealing in preserving data privacy; yet the communication between the PS and scattered clients can be a severe bottleneck. <b>Model</b> <b>compression</b> algorithms, such as <b>quantization</b> and sparsification, have been suggested but they generally assume a fixed code length, which does not reflect the heterogeneity and variability of <b>model</b> <b>updates.</b> In this paper, through both analysis and experiments, we show strong evidences that variable-length is beneficial for compression in FL. We accordingly present Fed-CVLC (Federated Learning Compression with Variable-Length Codes), which <b>fine-tunes</b> the code length in response of the dynamics of <b>model</b> <b>updates.</b> We develop optimal tuning strategy that minimizes the loss function (equivalent to maximizing the <b>model</b> <b>utility)</b> subject to the budget for communication. We further demonstrate that Fed-CVLC is indeed a general compression design that bridges <b>quantization</b> and sparsification, with greater flexibility. Extensive experiments have been conducted with public datasets to demonstrate that Fed-CVLC remarkably outperforms state-of-the-art baselines, improving <b>model</b> <b>utility</b> by 1.50%-5.44%, or shrinking communication traffic by 16.67%-41.61%.</p></p class="citation"></blockquote><h3 id=70277-enhanced-sampling-of-robust-molecular-datasets-with-uncertainty-based-collective-variables-aik-rui-tan-et-al-2024>(70/277) Enhanced sampling of robust molecular datasets with uncertainty-based collective variables (Aik Rui Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aik Rui Tan, Johannes C. B. Dietschreit, Rafael Gomez-Bombarelli. (2024)<br><strong>Enhanced sampling of robust molecular datasets with uncertainty-based collective variables</strong><br><button class=copy-to-clipboard title="Enhanced sampling of robust molecular datasets with uncertainty-based collective variables" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-comp-ph<br>Keyword Score: 30<br>Keywords: Active Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03753v1.pdf filename=2402.03753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating a data set that is representative of the accessible configuration space of a molecular system is crucial for the robustness of machine learned interatomic potentials (MLIP). However, the complexity of molecular systems, characterized by intricate potential energy surfaces (PESs) with numerous local minima and energy barriers, presents a significant challenge. Traditional methods of data generation, such as random sampling or exhaustive exploration, are either intractable or may not capture rare, but highly informative configurations. In this study, we propose a method that leverages uncertainty as the collective variable (CV) to guide the acquisition of chemically-relevant data points, focusing on regions of the configuration space where ML model predictions are most uncertain. This approach employs a Gaussian Mixture Model-based uncertainty metric from a single model as the CV for biased molecular dynamics <b>simulations.</b> The effectiveness of our approach in overcoming energy barriers and exploring unseen energy minima, thereby enhancing the data set in an <b>active</b> <b>learning</b> framework, is demonstrated on the alanine dipeptide benchmark system.</p></p class="citation"></blockquote><h3 id=71277-learning-granger-causality-from-instance-wise-self-attentive-hawkes-processes-dongxia-wu-et-al-2024>(71/277) Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes (Dongxia Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongxia Wu, Tsuyoshi Idé, Aurélie Lozano, Georgios Kollias, Jiří Navrátil, Naoki Abe, Yi-An Ma, Rose Yu. (2024)<br><strong>Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes</strong><br><button class=copy-to-clipboard title="Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Unsupervised Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03726v1.pdf filename=2402.03726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the problem of learning Granger causality from asynchronous, interdependent, multi-type event sequences. In particular, we are interested in discovering instance-level causal structures in an <b>unsupervised</b> manner. Instance-level causality identifies causal relationships among individual events, providing more fine-grained information for decision-making. Existing work in the literature either requires strong assumptions, such as linearity in the intensity function, or heuristically defined model parameters that do not necessarily meet the requirements of Granger causality. We propose Instance-wise Self-Attentive Hawkes Processes (ISAHP), a novel deep learning framework that can directly infer the Granger causality at the event instance level. ISAHP is the first neural point process model that meets the requirements of Granger causality. It leverages the <b>self-attention</b> mechanism of the <b>transformer</b> to align with the principles of Granger causality. We empirically demonstrate that ISAHP is capable of discovering complex instance-level causal structures that cannot be handled by classical models. We also show that ISAHP achieves state-of-the-art performance in proxy tasks involving type-level causal discovery and instance-level event type prediction.</p></p class="citation"></blockquote><h3 id=72277-rap-retrieval-augmented-planning-with-contextual-memory-for-multimodal-llm-agents-tomoyuki-kagaya-et-al-2024>(72/277) RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents (Tomoyuki Kagaya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, Yang You. (2024)<br><strong>RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents</strong><br><button class=copy-to-clipboard title="RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03610v1.pdf filename=2402.03610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Owing to recent advancements, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents&rsquo; planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and <b>multimodal</b> environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP&rsquo;s effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances <b>multimodal</b> <b>LLM</b> agents&rsquo; performance for embodied tasks. These results highlight RAP&rsquo;s potential in advancing the functionality and applicability of <b>LLM</b> agents in complex, real-world applications.</p></p class="citation"></blockquote><h3 id=73277-scientific-language-modeling-a-quantitative-review-of-large-language-models-in-molecular-science-pengfei-liu-et-al-2024>(73/277) Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science (Pengfei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengfei Liu, Jun Tao, Zhixiang Ren. (2024)<br><strong>Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science</strong><br><button class=copy-to-clipboard title="Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04119v1.pdf filename=2402.04119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field. In particular, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM). However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a <b>multi-modal</b> benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model&rsquo;s compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering. Our pioneering analysis offers an exploration of the learning mechanism and paves the way for advancing SLM in molecular science.</p></p class="citation"></blockquote><h3 id=74277-the-vampprior-mixture-model-andrew-stirn-et-al-2024>(74/277) The VampPrior Mixture Model (Andrew Stirn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Stirn, David A. Knowles. (2024)<br><strong>The VampPrior Mixture Model</strong><br><button class=copy-to-clipboard title="The VampPrior Mixture Model" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04412v1.pdf filename=2402.04412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak & Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between <b>variational</b> <b>inference</b> and Empirical Bayes to cleanly distinguish <b>variational</b> <b>and</b> prior parameters. Using the VMM in a <b>Variational</b> <b>Autoencoder</b> attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.</p></p class="citation"></blockquote><h3 id=75277-denoising-diffusion-probabilistic-models-in-six-simple-steps-richard-e-turner-et-al-2024>(75/277) Denoising Diffusion Probabilistic Models in Six Simple Steps (Richard E. Turner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Richard E. Turner, Cristiana-Diana Diaconu, Stratis Markou, Aliaksandra Shysheya, Andrew Y. K. Foong, Bruno Mlodozeniec. (2024)<br><strong>Denoising Diffusion Probabilistic Models in Six Simple Steps</strong><br><button class=copy-to-clipboard title="Denoising Diffusion Probabilistic Models in Six Simple Steps" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04384v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04384v2.pdf filename=2402.04384v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Denoising Diffusion <b>Probabilistic</b> <b>Models</b> (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we <b>distill</b> down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic <b>probabilistic</b> <b>modelling,</b> Gaussian distributions, maximum likelihood estimation, and deep learning.</p></p class="citation"></blockquote><h3 id=76277-cast-clustering-self-attention-using-surrogate-tokens-for-efficient-transformers-adjorn-van-engelenhoven-et-al-2024>(76/277) CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers (Adjorn van Engelenhoven et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adjorn van Engelenhoven, Nicola Strisciuglio, Estefanía Talavera. (2024)<br><strong>CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers</strong><br><button class=copy-to-clipboard title="CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04239v1.pdf filename=2402.04239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>Transformer</b> architecture has shown to be a powerful tool for a wide range of tasks. It is based on the <b>self-attention</b> mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of <b>Transformers.</b> In this work, we propose a novel Clustering <b>self-Attention</b> mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient <b>transformers.</b> CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries. The <b>self-attention</b> from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence. CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\alpha N)$ where N is the sequence length, and {\alpha} is constant according to the number of clusters and samples per cluster. We show that CAST performs better than or comparable to the baseline <b>Transformers</b> on long-range sequence modeling tasks, while also achieving higher results on time and memory efficiency than other efficient <b>transformers.</b></p></p class="citation"></blockquote><h3 id=77277-gradient-coding-in-decentralized-learning-for-evading-stragglers-chengxi-li-et-al-2024>(77/277) Gradient Coding in Decentralized Learning for Evading Stragglers (Chengxi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengxi Li, Mikael Skoglund. (2024)<br><strong>Gradient Coding in Decentralized Learning for Evading Stragglers</strong><br><button class=copy-to-clipboard title="Gradient Coding in Decentralized Learning for Evading Stragglers" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04193v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04193v2.pdf filename=2402.04193v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide <b>simulation</b> results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.</p></p class="citation"></blockquote><h3 id=78277-reinforcement-learning-with-ensemble-model-predictive-safety-certification-sven-gronauer-et-al-2024>(78/277) Reinforcement Learning with Ensemble Model Predictive Safety Certification (Sven Gronauer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sven Gronauer, Tom Haider, Felippe Schmoeller da Roza, Klaus Diepold. (2024)<br><strong>Reinforcement Learning with Ensemble Model Predictive Safety Certification</strong><br><button class=copy-to-clipboard title="Reinforcement Learning with Ensemble Model Predictive Safety Certification" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04182v1.pdf filename=2402.04182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> algorithms need exploration to learn. However, <b>unsupervised</b> exploration prevents the deployment of such algorithms on safety-critical tasks and limits real-world deployment. In this paper, we propose a new algorithm called Ensemble Model Predictive Safety Certification that combines model-based deep <b>reinforcement</b> <b>learning</b> with tube-based model predictive control to correct the actions taken by a learning agent, keeping safety constraint violations at a minimum through planning. Our approach aims to reduce the amount of prior knowledge about the actual system by requiring only offline data generated by a safe controller. Our results show that we can achieve significantly fewer constraint violations than comparable <b>reinforcement</b> <b>learning</b> methods.</p></p class="citation"></blockquote><h3 id=79277-provably-learning-a-multi-head-attention-layer-sitan-chen-et-al-2024>(79/277) Provably learning a multi-head attention layer (Sitan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sitan Chen, Yuanzhi Li. (2024)<br><strong>Provably learning a multi-head attention layer</strong><br><button class=copy-to-clipboard title="Provably learning a multi-head attention layer" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04084v1.pdf filename=2402.04084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The multi-head attention layer is one of the key components of the <b>transformer</b> architecture that sets it apart from traditional feed-forward models. Given a sequence length $k$, attention matrices $\mathbf{\Theta}_1,\ldots,\mathbf{\Theta}_m\in\mathbb{R}^{d\times d}$, and projection matrices $\mathbf{W}_1,\ldots,\mathbf{W}<em>m\in\mathbb{R}^{d\times d}$, the corresponding multi-head attention layer $F: \mathbb{R}^{k\times d}\to \mathbb{R}^{k\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\mathbf{X}\in\mathbb{R}^{k\times d}$ via $F(\mathbf{X}) \triangleq \sum^m</em>{i=1} \mathrm{softmax}(\mathbf{X}\mathbf{\Theta}_i\mathbf{X}^\top)\mathbf{X}\mathbf{W}_i$. In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem: - Provided ${\mathbf{W}_i, \mathbf{\Theta}_i}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns $F$ to small error given random labeled examples drawn uniformly from ${\pm 1}^{k\times d}$. - We prove computational lower bounds showing that in the worst case, exponential dependence on $m$ is unavoidable. We focus on Boolean $\mathbf{X}$ to mimic the discrete nature of tokens in <b>large</b> <b>language</b> <b>models,</b> though our techniques naturally extend to standard continuous settings, e.g. Gaussian. Our algorithm, which is centered around using examples to sculpt a convex body containing the unknown parameters, is a significant departure from existing provable algorithms for learning feedforward networks, which predominantly exploit algebraic and rotation invariance properties of the Gaussian distribution. In contrast, our analysis is more flexible as it primarily relies on various upper and lower tail bounds for the input distribution and &ldquo;slices&rdquo; thereof.</p></p class="citation"></blockquote><h3 id=80277-lighthgnn-distilling-hypergraph-neural-networks-into-mlps-for-100times-faster-inference-yifan-feng-et-al-2024>(80/277) LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\times$ Faster Inference (Yifan Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Feng, Yihe Luo, Shihui Ying, Yue Gao. (2024)<br><strong>LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\times$ Faster Inference</strong><br><button class=copy-to-clipboard title="LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\times$ Faster Inference" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04296v1.pdf filename=2402.04296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment. In practice, we find that one key barrier to the efficient deployment of HGNNs is the high-order structural dependencies during inference. In this paper, we propose to bridge the gap between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to eliminate the hypergraph dependency of HGNNs and thus reduce computational complexity as well as improve inference speed. Specifically, we introduce LightHGNN and LightHGNN$^+$ for fast inference with low complexity. LightHGNN directly <b>distills</b> the knowledge from teacher HGNNs to student MLPs via soft labels, and LightHGNN$^+$ further explicitly injects reliable high-order correlations into the student MLPs to achieve topology-aware <b>distillation</b> and resistance to over-smoothing. Experiments on eight hypergraph datasets demonstrate that even without hypergraph dependency, the proposed LightHGNNs can still achieve competitive or even better performance than HGNNs and outperform vanilla MLPs by $16.3$ on average. Extensive experiments on three graph datasets further show the average best performance of our LightHGNNs compared with all other methods. Experiments on synthetic hypergraphs with 5.5w vertices indicate LightHGNNs can run $100\times$ faster than HGNNs, showcasing their ability for latency-sensitive deployments.</p></p class="citation"></blockquote><h3 id=81277-gradient-sketches-for-training-data-attribution-and-studying-the-loss-landscape-andrea-schioppa-2024>(81/277) Gradient Sketches for Training Data Attribution and Studying the Loss Landscape (Andrea Schioppa, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Schioppa. (2024)<br><strong>Gradient Sketches for Training Data Attribution and Studying the Loss Landscape</strong><br><button class=copy-to-clipboard title="Gradient Sketches for Training Data Attribution and Studying the Loss Landscape" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Fine-tuning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03994v1.pdf filename=2402.03994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Random projections or sketches of gradients and Hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry. Two important scenarios are training data attribution (tracing a model&rsquo;s behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the Hessian (to analyze the training dynamics), where one needs to store multiple Hessian vector products. While sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks. Motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms. We demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the Hessian spectrum and the computation of the intrinsic dimension when <b>fine-tuning</b> <b>pre-trained</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=82277-a-bias-variance-decomposition-for-ensembles-over-multiple-synthetic-datasets-ossi-räisä-et-al-2024>(82/277) A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets (Ossi Räisä et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ossi Räisä, Antti Honkela. (2024)<br><strong>A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets</strong><br><button class=copy-to-clipboard title="A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03985v1.pdf filename=2402.03985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have highlighted the benefits of generating multiple synthetic datasets for <b>supervised</b> <b>learning,</b> from increased accuracy to more effective model selection and uncertainty estimation. These benefits have clear empirical support, but the theoretical understanding of them is currently very light. We seek to increase the theoretical understanding by deriving bias-variance decompositions for several settings of using multiple synthetic datasets. Our theory predicts multiple synthetic datasets to be especially beneficial for high-variance downstream predictors, and yields a simple rule of thumb to select the appropriate number of synthetic datasets in the case of mean-squared error and Brier score. We investigate how our theory works in practice by evaluating the performance of an ensemble over many synthetic datasets for several real datasets and downstream predictors. The results follow our theory, showing that our insights are also practically relevant.</p></p class="citation"></blockquote><h3 id=83277-discovery-of-the-hidden-world-with-large-language-models-chenxi-liu-et-al-2024>(83/277) Discovery of the Hidden World with Large Language Models (Chenxi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenxi Liu, Yongqiang Chen, Tongliang Liu, Mingming Gong, James Cheng, Bo Han, Kun Zhang. (2024)<br><strong>Discovery of the Hidden World with Large Language Models</strong><br><button class=copy-to-clipboard title="Discovery of the Hidden World with Large Language Models" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ME<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03941v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03941v1.pdf filename=2402.03941v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates <b>LLMs</b> as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, <b>LLMs</b> can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a causal learning module (e.g., the FCI algorithm) that provides both rigorous explanations of the data, as well as useful feedback to further improve the extraction of causal factors by <b>LLMs.</b> We verify the effectiveness of COAT in uncovering the underlying causal system with two case studies of review rating analysis and neuropathic diagnosis.</p></p class="citation"></blockquote><h3 id=84277-employee-turnover-analysis-using-machine-learning-algorithms-mahyar-karimi-et-al-2024>(84/277) Employee Turnover Analysis Using Machine Learning Algorithms (Mahyar Karimi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahyar Karimi, Kamyar Seyedkazem Viliyani. (2024)<br><strong>Employee Turnover Analysis Using Machine Learning Algorithms</strong><br><button class=copy-to-clipboard title="Employee Turnover Analysis Using Machine Learning Algorithms" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03905v1.pdf filename=2402.03905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Employee&rsquo;s knowledge is an organization asset. Turnover may impose apparent and hidden costs and irreparable damages. To overcome and mitigate this risk, employee&rsquo;s condition should be monitored. Due to high complexity of analyzing well-being features, employee&rsquo;s turnover predicting can be delegated to machine learning techniques. In this paper, we discuss employee&rsquo;s attrition rate. Three different <b>supervised</b> <b>learning</b> algorithms comprising AdaBoost, SVM and RandomForest are used to benchmark employee attrition accuracy. Attained models can help out at establishing predictive analytics.</p></p class="citation"></blockquote><h3 id=85277-moment-a-family-of-open-time-series-foundation-models-mononito-goswami-et-al-2024>(85/277) MOMENT: A Family of Open Time-series Foundation Models (Mononito Goswami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, Artur Dubrawski. (2024)<br><strong>MOMENT: A Family of Open Time-series Foundation Models</strong><br><button class=copy-to-clipboard title="MOMENT: A Family of Open Time-series Foundation Models" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03885v1.pdf filename=2402.03885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce MOMENT, a family of open-source <b>foundation</b> <b>models</b> for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series <b>foundation</b> <b>models</b> on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific <b>fine-tuning.</b> Finally, we present several interesting empirical observations about large pre-trained time-series models. Our code is available anonymously at anonymous.4open.science/r/BETT-773F/.</p></p class="citation"></blockquote><h3 id=86277-cascast-skillful-high-resolution-precipitation-nowcasting-via-cascaded-modelling-junchao-gong-et-al-2024>(86/277) CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling (Junchao Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junchao Gong, Lei Bai, Peng Ye, Wanghan Xu, Na Liu, Jianhua Dai, Xiaokang Yang, Wanli Ouyang. (2024)<br><strong>CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling</strong><br><button class=copy-to-clipboard title="CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04290v1.pdf filename=2402.04290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Precipitation nowcasting based on radar data plays a crucial role in extreme weather prediction and has broad implications for disaster management. Despite progresses have been made based on deep learning, two key challenges of precipitation nowcasting are not well-solved: (i) the modeling of complex precipitation system evolutions with different scales, and (ii) accurate forecasts for extreme precipitation. In this work, we propose CasCast, a cascaded framework composed of a deterministic and a <b>probabilistic</b> <b>part</b> to decouple the predictions for mesoscale precipitation distributions and small-scale patterns. Then, we explore training the cascaded framework at the high resolution and conducting the <b>probabilistic</b> <b>modeling</b> in a low dimensional latent space with a frame-wise-guided diffusion <b>transformer</b> for enhancing the optimization of extreme events while reducing computational costs. Extensive experiments on three benchmark radar precipitation datasets show that CasCast achieves competitive performance. Especially, CasCast significantly surpasses the baseline (up to +91.8%) for regional extreme-precipitation nowcasting.</p></p class="citation"></blockquote><h3 id=87277-digital-twin-mobility-profiling-a-spatio-temporal-graph-learning-approach-xin-chen-et-al-2024>(87/277) Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach (Xin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Chen, Mingliang Hou, Tao Tang, Achhardeep Kaur, Feng Xia. (2024)<br><strong>Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach</strong><br><button class=copy-to-clipboard title="Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T09, 68T30, 68U35, I-2-6; I-2-4; H-1-2, cs-AI, cs-HC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03750v1.pdf filename=2402.03750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the arrival of the big data era, mobility profiling has become a viable method of utilizing enormous amounts of mobility data to create an intelligent transportation system. Mobility profiling can extract potential patterns in urban traffic from mobility data and is critical for a variety of traffic-related applications. However, due to the high level of complexity and the huge amount of data, mobility profiling faces huge challenges. Digital Twin (DT) technology paves the way for cost-effective and performance-optimised management by digitally creating a virtual representation of the network to simulate its behaviour. In order to capture the complex spatio-temporal features in traffic scenario, we construct alignment diagrams to assist in completing the spatio-temporal correlation representation and design dilated alignment <b>convolution</b> <b>network</b> (DACN) to learn the fine-grained correlations, i.e., spatio-temporal interactions. We propose a digital twin mobility profiling (DTMP) framework to learn node profiles on a mobility network DT model. Extensive experiments have been conducted upon three real-world datasets. Experimental results demonstrate the effectiveness of DTMP.</p></p class="citation"></blockquote><h3 id=88277-modeling-spatio-temporal-dynamical-systems-with-neural-discrete-learning-and-levels-of-experts-kun-wang-et-al-2024>(88/277) Modeling Spatio-temporal Dynamical Systems with Neural Discrete Learning and Levels-of-Experts (Kun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Wang, Hao Wu, Guibin Zhang, Junfeng Fang, Yuxuan Liang, Yuankai Wu, Roger Zimmermann, Yang Wang. (2024)<br><strong>Modeling Spatio-temporal Dynamical Systems with Neural Discrete Learning and Levels-of-Experts</strong><br><button class=copy-to-clipboard title="Modeling Spatio-temporal Dynamical Systems with Neural Discrete Learning and Levels-of-Experts" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05970v1.pdf filename=2402.05970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the issue of modeling and estimating changes in the state of the spatio-temporal dynamical systems based on a sequence of observations like video frames. Traditional numerical <b>simulation</b> systems depend largely on the initial settings and correctness of the constructed partial differential equations (PDEs). Despite recent efforts yielding significant success in discovering data-driven PDEs with neural networks, the limitations posed by singular scenarios and the absence of local insights prevent them from performing effectively in a broader real-world context. To this end, this paper propose the universal expert module &ndash; that is, optical flow estimation component, to capture the evolution laws of general physical processes in a data-driven fashion. To enhance local insight, we painstakingly design a finer-grained physical pipeline, since local characteristics may be influenced by various internal contextual information, which may contradict the macroscopic properties of the whole system. Further, we harness currently popular neural discrete learning to unveil the underlying important features in its latent space, this process better injects interpretability, which can help us obtain a powerful prior over these discrete random variables. We conduct extensive experiments and ablations to demonstrate that the proposed framework achieves large performance margins, compared with the existing SOTA baselines.</p></p class="citation"></blockquote><h3 id=89277-sub-play-adversarial-policies-against-partially-observed-multi-agent-reinforcement-learning-systems-oubo-ma-et-al-2024>(89/277) SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems (Oubo Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oubo Ma, Yuwen Pu, Linkang Du, Yang Dai, Ruo Wang, Xiaolei Liu, Yingcai Wu, Shouling Ji. (2024)<br><strong>SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems</strong><br><button class=copy-to-clipboard title="SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Recommendation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03741v1.pdf filename=2402.03741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in multi-agent <b>reinforcement</b> <b>learning</b> (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent researches reveal that an attacker can rapidly exploit the victim&rsquo;s vulnerabilities and generate adversarial policies, leading to the victim&rsquo;s failure in specific tasks. For example, reducing the winning rate of a superhuman-level Go AI to around 20%. They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation. In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporates the concept of constructing multiple subgames to mitigate the impact of partial observability and suggests the sharing of transitions among subpolicies to improve the exploitative ability of attackers. Extensive evaluations demonstrate the effectiveness of SUB-PLAY under three typical partial observability limitations. Visualization results indicate that adversarial policies induce significantly different activations of the victims&rsquo; policy networks. Furthermore, we evaluate three potential defenses aimed at exploring ways to mitigate security threats posed by adversarial policies, providing constructive <b>recommendations</b> for deploying MARL in competitive environments.</p></p class="citation"></blockquote><h3 id=90277-differentially-private-high-dimensional-bandits-apurv-shukla-2024>(90/277) Differentially Private High Dimensional Bandits (Apurv Shukla, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Apurv Shukla. (2024)<br><strong>Differentially Private High Dimensional Bandits</strong><br><button class=copy-to-clipboard title="Differentially Private High Dimensional Bandits" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs-SY, cs.LG, eess-SY, math-OC, stat-ML<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03737v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03737v1.pdf filename=2402.03737v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a high-dimensional stochastic contextual linear <b>bandit</b> <b>problem</b> when the parameter vector is $s_{0}$-sparse and the decision maker is subject to privacy constraints under both central and local models of differential privacy. We present PrivateLASSO, a differentially private LASSO <b>bandit</b> <b>algorithm.</b> PrivateLASSO is based on two sub-routines: (i) a sparse hard-thresholding-based privacy mechanism and (ii) an episodic thresholding rule for identifying the support of the parameter $\theta$. We prove minimax private lower bounds and establish privacy and utility guarantees for PrivateLASSO for the central model under standard assumptions.</p></p class="citation"></blockquote><h3 id=91277-clarify-improving-model-robustness-with-natural-language-corrections-yoonho-lee-et-al-2024>(91/277) Clarify: Improving Model Robustness With Natural Language Corrections (Yoonho Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoonho Lee, Michelle S. Lam, Helena Vasconcelos, Michael S. Bernstein, Chelsea Finn. (2024)<br><strong>Clarify: Improving Model Robustness With Natural Language Corrections</strong><br><button class=copy-to-clipboard title="Clarify: Improving Model Robustness With Natural Language Corrections" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03715v1.pdf filename=2402.03715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>supervised</b> <b>learning,</b> models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model&rsquo;s misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model&rsquo;s consistent failure patterns. Then, in an entirely automated way, we use such descriptions to improve the training process by reweighting the training data or gathering additional targeted data. Our user studies show that non-expert users can successfully describe model misconceptions via Clarify, improving worst-group accuracy by an average of 17.1% in two datasets. Additionally, we use Clarify to find and rectify 31 novel hard subpopulations in the ImageNet dataset, improving minority-split accuracy from 21.1% to 28.7%.</p></p class="citation"></blockquote><h3 id=92277-pard-permutation-invariant-autoregressive-diffusion-for-graph-generation-lingxiao-zhao-et-al-2024>(92/277) Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation (Lingxiao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingxiao Zhao, Xueying Ding, Leman Akoglu. (2024)<br><strong>Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation</strong><br><button class=copy-to-clipboard title="Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: GPT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03687v1.pdf filename=2402.03687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block&rsquo;s probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph <b>transformer,</b> which integrates <b>transformer</b> with PPGN. Like <b>GPT,</b> we extend the higher-order graph <b>transformer</b> to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules.</p></p class="citation"></blockquote><h3 id=93277-symbol-correctness-in-deep-neural-networks-containing-symbolic-layers-aaron-bembenek-et-al-2024>(93/277) Symbol Correctness in Deep Neural Networks Containing Symbolic Layers (Aaron Bembenek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aaron Bembenek, Toby Murray. (2024)<br><strong>Symbol Correctness in Deep Neural Networks Containing Symbolic Layers</strong><br><button class=copy-to-clipboard title="Symbol Correctness in Deep Neural Networks Containing Symbolic Layers" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transfer Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03663v1.pdf filename=2402.03663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To handle AI tasks that combine perception and logical <b>reasoning,</b> recent work introduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain &ndash; in addition to traditional neural layers &ndash; symbolic layers: symbolic expressions (e.g., SAT formulas, logic programs) that are evaluated by symbolic solvers during inference. We identify and formalize an intuitive, high-level principle that can guide the design and analysis of NS-DNNs: symbol correctness, the correctness of the intermediate symbols predicted by the neural layers with respect to a (generally unknown) ground-truth symbolic representation of the input data. We demonstrate that symbol correctness is a necessary property for NS-DNN explainability and <b>transfer</b> <b>learning</b> (despite being in general impossible to train for). Moreover, we show that the framework of symbol correctness provides a precise way to reason and communicate about model behavior at neural-symbolic boundaries, and gives insight into the fundamental tradeoffs faced by NS-DNN training algorithms. In doing so, we both identify significant points of ambiguity in prior work, and provide a framework to support further NS-DNN developments.</p></p class="citation"></blockquote><h3 id=94277-transductive-reward-inference-on-graph-bohao-qu-et-al-2024>(94/277) Transductive Reward Inference on Graph (Bohao Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bohao Qu, Xiaofeng Cao, Qing Guo, Yi Chang, Ivor W. Tsang, Chengqi Zhang. (2024)<br><strong>Transductive Reward Inference on Graph</strong><br><button class=copy-to-clipboard title="Transductive Reward Inference on Graph" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03661v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03661v1.pdf filename=2402.03661v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in <b>offline</b> <b>reinforcement</b> <b>learning.</b> Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics. Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards for unlabelled data. Furthermore, we establish the existence of a fixed point during several iterations of the transductive inference process and demonstrate its at least convergence to a local optimum. Empirical evaluations on locomotion and robotic manipulation tasks validate the effectiveness of our approach. The application of our inferred rewards improves the performance in <b>offline</b> <b>reinforcement</b> <b>learning</b> tasks.</p></p class="citation"></blockquote><h3 id=95277-disparate-impact-on-group-accuracy-of-linearization-for-private-inference-saswat-das-et-al-2024>(95/277) Disparate Impact on Group Accuracy of Linearization for Private Inference (Saswat Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saswat Das, Marco Romanelli, Ferdinando Fioretto. (2024)<br><strong>Disparate Impact on Group Accuracy of Linearization for Private Inference</strong><br><button class=copy-to-clipboard title="Disparate Impact on Group Accuracy of Linearization for Private Inference" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CY, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03629v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03629v1.pdf filename=2402.03629v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased <b>fairness</b> costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the <b>fine-tuning</b> step for linearized models can serve as an effective mitigation strategy.</p></p class="citation"></blockquote><h3 id=96277-pres-toward-scalable-memory-based-dynamic-graph-neural-networks-junwei-su-et-al-2024>(96/277) PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks (Junwei Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junwei Su, Difan Zou, Chuan Wu. (2024)<br><strong>PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks</strong><br><button class=copy-to-clipboard title="PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Graph Neural Network, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04284v1.pdf filename=2402.04284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Memory-based Dynamic <b>Graph</b> <b>Neural</b> <b>Networks</b> (MDGNNs) are a family of dynamic <b>graph</b> <b>neural</b> <b>networks</b> that leverage a memory module to extract, <b>distill,</b> and memorize long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, training MDGNNs faces the challenge of handling entangled temporal and structural dependencies, requiring sequential and chronological processing of data sequences to capture accurate temporal patterns. During the batch training, the temporal data points within the same batch will be processed in parallel, while their temporal dependencies are neglected. This issue is referred to as temporal discontinuity and restricts the effective temporal batch size, limiting data parallelism and reducing MDGNNs&rsquo; flexibility in industrial applications. This paper studies the efficient training of MDGNNs at scale, focusing on the temporal discontinuity in training MDGNNs with large temporal batch sizes. We first conduct a theoretical study on the impact of temporal batch size on the convergence of MDGNN training. Based on the analysis, we propose PRES, an iterative prediction-correction scheme combined with a memory coherence learning objective to mitigate the effect of temporal discontinuity, enabling MDGNNs to be trained with significantly larger temporal batches without sacrificing generalization performance. Experimental results demonstrate that our approach enables up to a 4x larger temporal batch (3.4x speed-up) during MDGNN training.</p></p class="citation"></blockquote><h3 id=97277-exploring-higher-order-neural-network-node-interactions-with-total-correlation-thomas-kerby-et-al-2024>(97/277) Exploring higher-order neural network node interactions with total correlation (Thomas Kerby et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Kerby, Teresa White, Kevin Moon. (2024)<br><strong>Exploring higher-order neural network node interactions with total correlation</strong><br><button class=copy-to-clipboard title="Exploring higher-order neural network node interactions with total correlation" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04440v1.pdf filename=2402.04440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In domains such as ecological systems, collaborations, and the human brain the variables interact in complex ways. Yet accurately characterizing higher-order variable interactions (HOIs) is a difficult problem that is further exacerbated when the HOIs change across the data. To solve this problem we propose a new method called Local Correlation Explanation (CorEx) to capture HOIs at a local scale by first clustering data points based on their proximity on the data manifold. We then use a multivariate version of the <b>mutual</b> <b>information</b> called the total correlation, to construct a latent factor representation of the data within each cluster to learn the local HOIs. We use Local CorEx to explore HOIs in synthetic and real world data to extract hidden insights about the data structure. Lastly, we demonstrate Local CorEx&rsquo;s suitability to explore and interpret the inner workings of trained neural networks.</p></p class="citation"></blockquote><h3 id=98277-decentralized-blockchain-based-robust-multi-agent-multi-armed-bandit-mengfan-xu-et-al-2024>(98/277) Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit (Mengfan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengfan Xu, Diego Klabjan. (2024)<br><strong>Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit</strong><br><button class=copy-to-clipboard title="Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-MA, cs.LG<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04417v1.pdf filename=2402.04417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a robust multi-agent multi-armed <b>bandit</b> problem where multiple clients or participants are distributed on a fully decentralized blockchain, with the possibility of some being malicious. The rewards of arms are homogeneous among the clients, following time-invariant stochastic distributions that are revealed to the participants only when the system is secure enough. The system&rsquo;s objective is to efficiently ensure the cumulative rewards gained by the honest participants. To this end and to the best of our knowledge, we are the first to incorporate advanced techniques from blockchains, as well as novel mechanisms, into the system to design optimal strategies for honest participants. This allows various malicious behaviors and the maintenance of participant privacy. More specifically, we randomly select a pool of validators who have access to all participants, design a brand-new consensus mechanism based on digital signatures for these validators, invent a UCB-based strategy that requires less information from participants through secure multi-party computation, and design the chain-participant interaction and an incentive mechanism to encourage participants&rsquo; participation. Notably, we are the first to prove the theoretical guarantee of the proposed algorithms by regret analyses in the context of optimality in blockchains. Unlike existing work that integrates blockchains with learning problems such as federated learning which mainly focuses on numerical optimality, we demonstrate that the regret of honest participants is upper bounded by $log{T}$. This is consistent with the multi-agent multi-armed <b>bandit</b> problem without malicious participants and the robust multi-agent multi-armed <b>bandit</b> problem with purely Byzantine attacks.</p></p class="citation"></blockquote><h3 id=99277-learning-from-time-series-under-temporal-label-noise-sujay-nagaraj-et-al-2024>(99/277) Learning from Time Series under Temporal Label Noise (Sujay Nagaraj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sujay Nagaraj, Walter Gerych, Sana Tonekaboni, Anna Goldenberg, Berk Ustun, Thomas Hartvigsen. (2024)<br><strong>Learning from Time Series under Temporal Label Noise</strong><br><button class=copy-to-clipboard title="Learning from Time Series under Temporal Label Noise" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Noise-tolerant<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04398v1.pdf filename=2402.04398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many sequential classification tasks are affected by label noise that varies over time. Such noise can cause label quality to improve, worsen, or periodically change over time. We first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series. In this setting, multiple labels are recorded in sequence while being corrupted by a time-dependent noise function. We first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform. We then propose methods that can train <b>noise-tolerant</b> classifiers by estimating the temporal label noise function directly from data. We show that our methods lead to state-of-the-art performance in the presence of diverse temporal label noise functions using real and synthetic data.</p></p class="citation"></blockquote><h3 id=100277-fairwire-fair-graph-generation-o-deniz-kose-et-al-2024>(100/277) FairWire: Fair Graph Generation (O. Deniz Kose et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>O. Deniz Kose, Yanning Shen. (2024)<br><strong>FairWire: Fair Graph Generation</strong><br><button class=copy-to-clipboard title="FairWire: Fair Graph Generation" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04383v1.pdf filename=2402.04383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning over graphs has recently attracted growing attention due to its ability to analyze and learn complex relations within critical interconnected systems. However, the disparate impact that is amplified by the use of biased graph structures in these algorithms has raised significant concerns for the deployment of them in real-world decision systems. In addition, while synthetic graph generation has become pivotal for privacy and scalability considerations, the impact of generative learning algorithms on the structural bias has not yet been investigated. Motivated by this, this work focuses on the analysis and mitigation of structural bias for both real and synthetic graphs. Specifically, we first theoretically analyze the sources of structural bias that result in disparity for the predictions of dyadic relations. To alleviate the identified bias factors, we design a novel <b>fairness</b> regularizer that offers a versatile use. Faced with the bias amplification in graph generation models that is brought to light in this work, we further propose a fair graph generation framework, FairWire, by leveraging our fair regularizer design in a generative model. Experimental results on real-world networks validate that the proposed tools herein deliver effective structural bias mitigation for both real and synthetic graphs.</p></p class="citation"></blockquote><h3 id=101277-nercc-nested-regression-coded-computing-for-resilient-distributed-prediction-serving-systems-parsa-moradi-et-al-2024>(101/277) NeRCC: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems (Parsa Moradi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parsa Moradi, Mohammad Ali Maddah-Ali. (2024)<br><strong>NeRCC: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems</strong><br><button class=copy-to-clipboard title="NeRCC: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04377v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04377v2.pdf filename=2402.04377v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Resilience against stragglers is a critical element of prediction serving systems, tasked with executing inferences on input data for a pre-trained machine-learning model. In this paper, we propose NeRCC, as a general straggler-resistant framework for approximate coded computing. NeRCC includes three layers: (1) encoding regression and sampling, which generates coded data points, as a combination of original data points, (2) computing, in which a cluster of workers run inference on the coded data points, (3) decoding regression and sampling, which approximately recovers the predictions of the original data points from the available predictions on the coded data points. We argue that the overall objective of the framework reveals an underlying interconnection between two regression models in the encoding and decoding layers. We propose a solution to the nested regressions problem by summarizing their dependence on two regularization terms that are jointly optimized. Our extensive experiments on different datasets and various machine learning models, including LeNet5, RepVGG, and Vision <b>Transformer</b> (ViT), demonstrate that NeRCC accurately approximates the original predictions in a wide range of stragglers, outperforming the state-of-the-art by up to 23%.</p></p class="citation"></blockquote><h3 id=102277-scaling-laws-for-learning-with-real-and-surrogate-data-ayush-jain-et-al-2024>(102/277) Scaling laws for learning with real and surrogate data (Ayush Jain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayush Jain, Andrea Montanari, Eren Sasoglu. (2024)<br><strong>Scaling laws for learning with real and surrogate data</strong><br><button class=copy-to-clipboard title="Scaling laws for learning with real and surrogate data" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04376v1.pdf filename=2402.04376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models. Blurring distinctions, we refer to such data as `surrogate data&rsquo;. We define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ In order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ The test error of models trained on mixtures of real and surrogate data is well described by a <b>scaling</b> <b>law.</b> This can be used to predict the optimal weighting and the gain from surrogate data.</p></p class="citation"></blockquote><h3 id=103277-neural-networks-learn-statistics-of-increasing-complexity-nora-belrose-et-al-2024>(103/277) Neural Networks Learn Statistics of Increasing Complexity (Nora Belrose et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nora Belrose, Quintin Pope, Lucia Quirke, Alex Mallen, Xiaoli Fern. (2024)<br><strong>Neural Networks Learn Statistics of Increasing Complexity</strong><br><button class=copy-to-clipboard title="Neural Networks Learn Statistics of Increasing Complexity" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04362v1.pdf filename=2402.04362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The distributional simplicity bias (DSB) posits that neural networks learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we present compelling new evidence for the DSB by showing that networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later. We also extend the DSB to discrete domains by proving an equivalence between token $n$-gram frequencies and the moments of embedding vectors, and by finding empirical evidence for the bias in <b>LLMs.</b> Finally we use optimal transport methods to surgically edit the low-order statistics of one class to match those of another, and show that early-training networks treat the edited samples as if they were drawn from the target class. Code is available at <a href=https://github.com/EleutherAI/features-across-time>https://github.com/EleutherAI/features-across-time</a>.</p></p class="citation"></blockquote><h3 id=104277-enhance-dnn-adversarial-robustness-and-efficiency-via-injecting-noise-to-non-essential-neurons-zhenyu-liu-et-al-2024>(104/277) Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to Non-Essential Neurons (Zhenyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Liu, Garrett Gagnon, Swagath Venkataramani, Liu Liu. (2024)<br><strong>Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to Non-Essential Neurons</strong><br><button class=copy-to-clipboard title="Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to Non-Essential Neurons" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04325v1.pdf filename=2402.04325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Neural Networks (DNNs) have revolutionized a wide range of industries, from healthcare and finance to automotive, by offering unparalleled capabilities in data analysis and decision-making. Despite their transforming impact, DNNs face two critical challenges: the vulnerability to <b>adversarial</b> <b>attacks</b> and the increasing computational costs associated with more complex and larger models. In this paper, we introduce an effective method designed to simultaneously enhance <b>adversarial</b> <b>robustness</b> and execution efficiency. Unlike prior studies that enhance robustness via uniformly injecting noise, we introduce a non-uniform noise injection algorithm, strategically applied at each DNN layer to disrupt <b>adversarial</b> <b>perturbations</b> introduced in attacks. By employing approximation techniques, our approach identifies and protects essential neurons while strategically introducing noise into non-essential neurons. Our experimental results demonstrate that our method successfully enhances both robustness and efficiency across several attack scenarios, model architectures, and datasets.</p></p class="citation"></blockquote><h3 id=105277-informed-reinforcement-learning-for-situation-aware-traffic-rule-exceptions-daniel-bogdoll-et-al-2024>(105/277) Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions (Daniel Bogdoll et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Bogdoll, Jing Qin, Moritz Nekolla, Ahmed Abouelazm, Tim Joseph, J. Marius Zöllner. (2024)<br><strong>Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions</strong><br><button class=copy-to-clipboard title="Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-RO, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04168v1.pdf filename=2402.04168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> is a highly active research field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure. In this work, we introduce Informed <b>Reinforcement</b> <b>Learning,</b> where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents.</p></p class="citation"></blockquote><h3 id=106277-attention-with-markov-a-framework-for-principled-analysis-of-transformers-via-markov-chains-ashok-vardhan-makkuva-et-al-2024>(106/277) Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains (Ashok Vardhan Makkuva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, Michael Gastpar. (2024)<br><strong>Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains</strong><br><button class=copy-to-clipboard title="Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-IT, cs-LG, cs.LG, math-IT, stat-ML<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04161v1.pdf filename=2402.04161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, attention-based <b>transformers</b> have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of <b>transformers</b> through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the <b>transformer</b> architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer <b>transformers</b> and show the existence of global minima and bad local minima contingent upon the specific data characteristics and the <b>transformer</b> architecture. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. We further investigate these findings in the broader context of higher order Markov chains and deeper architectures, and outline open problems in this arena. Code is available at \url{https://github.com/Bond1995/Markov}.</p></p class="citation"></blockquote><h3 id=107277-ovor-oneprompt-with-virtual-outlier-regularization-for-rehearsal-free-class-incremental-learning-wei-cheng-huang-et-al-2024>(107/277) OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning (Wei-Cheng Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei-Cheng Huang, Chun-Fu Chen, Hsiang Hsu. (2024)<br><strong>OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04129v1.pdf filename=2402.04129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works have shown that by using large pre-trained models along with learnable <b>prompts,</b> rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones. Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together. In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. Recent <b>prompt-based</b> methods often require a pool of task-specific <b>prompts,</b> in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate <b>prompt</b> from the pool. This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper. We illustrate that a simplified <b>prompt-based</b> method can achieve results comparable to previous state-of-the-art (SOTA) methods equipped with a <b>prompt</b> pool, using much less learnable parameters and lower inference cost. Our regularization method has demonstrated its compatibility with different <b>prompt-based</b> methods, boosting those previous SOTA rehearsal-free CIL methods&rsquo; accuracy on the ImageNet-R and CIFAR-100 benchmarks. Our source code is available at <a href=https://github.com/jpmorganchase/ovor>https://github.com/jpmorganchase/ovor</a>.</p></p class="citation"></blockquote><h3 id=108277-hierarchical-delay-attribution-classification-using-unstructured-text-in-train-management-systems-anton-borg-et-al-2024>(108/277) Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems (Anton Borg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anton Borg, Per Lingvall, Martin Svensson. (2024)<br><strong>Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems</strong><br><button class=copy-to-clipboard title="Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04108v1.pdf filename=2402.04108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>EU directives stipulate a systematic follow-up of train delays. In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code. However, this delay attribution code is assigned manually, which is a complex task. In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated. The text is transformed using <b>TF-IDF,</b> and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration. Further, the problem is modeled as both a hierarchical and flat approach. The results indicate that a hierarchical approach performs better than a flat approach. Both approaches perform better than the random uniform classifier but perform worse than the manual classification.</p></p class="citation"></blockquote><h3 id=109277-retrieve-to-explain-evidence-driven-predictions-with-language-models-ravi-patel-et-al-2024>(109/277) Retrieve to Explain: Evidence-driven Predictions with Language Models (Ravi Patel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ravi Patel, Angus Brayne, Rogier Hintzen, Daniel Jaroslawicz, Georgiana Neculae, Dane Corneil. (2024)<br><strong>Retrieve to Explain: Evidence-driven Predictions with Language Models</strong><br><button class=copy-to-clipboard title="Retrieve to Explain: Evidence-driven Predictions with Language Models" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04068v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04068v1.pdf filename=2402.04068v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For <b>human-in-the-loop</b> processes, opaque predictions can drive lack of trust, limiting a model&rsquo;s impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.</p></p class="citation"></blockquote><h3 id=110277-link-prediction-with-relational-hypergraphs-xingyue-huang-et-al-2024>(110/277) Link Prediction with Relational Hypergraphs (Xingyue Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyue Huang, Miguel Romero Orth, Pablo Barceló, Michael M. Bronstein, İsmail İlkan Ceylan. (2024)<br><strong>Link Prediction with Relational Hypergraphs</strong><br><button class=copy-to-clipboard title="Link Prediction with Relational Hypergraphs" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04062v1.pdf filename=2402.04062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Link prediction with knowledge <b>graphs</b> <b>has</b> <b>been</b> thoroughly studied in <b>graph</b> <b>machine</b> <b>learning,</b> leading to a rich landscape of <b>graph</b> <b>neural</b> <b>network</b> architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge <b>graphs,</b> <b>where</b> <b>every</b> relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model architectures substantially outperform every baseline for inductive link prediction, and lead to state-of-the-art results for transductive link prediction. Our study therefore unlocks applications of <b>graph</b> <b>neural</b> <b>networks</b> to fully relational structures.</p></p class="citation"></blockquote><h3 id=111277-more-flexible-pac-bayesian-meta-learning-by-learning-learning-algorithms-hossein-zakerinia-et-al-2024>(111/277) More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms (Hossein Zakerinia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hossein Zakerinia, Amin Behjati, Christoph H. Lampert. (2024)<br><strong>More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms</strong><br><button class=copy-to-clipboard title="More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04054v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04054v1.pdf filename=2402.04054v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new framework for studying <b>meta-learning</b> <b>methods</b> using PAC-Bayesian theory. Its main advantage over previous work is that it allows for more flexibility in how the transfer of knowledge between tasks is realized. For previous approaches, this could only happen indirectly, by means of learning prior distributions over models. In contrast, the new generalization bounds that we prove express the process of <b>meta-learning</b> <b>much</b> more directly as learning the learning algorithm that should be used for future tasks. The flexibility of our framework makes it suitable to analyze a wide range of <b>meta-learning</b> <b>mechanisms</b> and even design new mechanisms. Other than our theoretical contributions we also show empirically that our framework improves the prediction quality in practical <b>meta-learning</b> <b>mechanisms.</b></p></p class="citation"></blockquote><h3 id=112277-analysis-of-linear-mode-connectivity-via-permutation-based-weight-matching-akira-ito-et-al-2024>(112/277) Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching (Akira Ito et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akira Ito, Masanori Yamada, Atsutoshi Kumagai. (2024)<br><strong>Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching</strong><br><button class=copy-to-clipboard title="Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04051v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04051v1.pdf filename=2402.04051v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding <b>stochastic</b> <b>gradient</b> <b>descent&rsquo;s</b> effectiveness and its application in areas like model merging. We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself. We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM mainly align the directions of singular vectors associated with large singular values across models. This alignment brings the singular vectors with large singular values, which determine the model functionality, closer between pre-merged and post-merged models, so that the post-merged model retains functionality similar to the pre-merged models, making it easy to satisfy LMC. Finally, we analyze the difference between WM and straight-through estimator (STE), a dataset-dependent permutation search method, and show that WM outperforms STE, especially when merging three or more models.</p></p class="citation"></blockquote><h3 id=113277-cross-entropy-versus-label-smoothing-a-neural-collapse-perspective-li-guo-et-al-2024>(113/277) Cross Entropy versus Label Smoothing: A Neural Collapse Perspective (Li Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Guo, Keith Ross, Zifan Zhao, George Andriopoulos, Shuyang Ling, Yufeng Xu, Zixuan Dong. (2024)<br><strong>Cross Entropy versus Label Smoothing: A Neural Collapse Perspective</strong><br><button class=copy-to-clipboard title="Cross Entropy versus Label Smoothing: A Neural Collapse Perspective" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Label Smoothing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03979v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03979v2.pdf filename=2402.03979v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Label</b> <b>smoothing</b> loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies <b>label</b> <b>smoothing</b> from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with <b>label</b> <b>smoothing</b> converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under <b>label</b> <b>smoothing</b> loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under <b>label</b> <b>smoothing</b> loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under <b>label</b> <b>smoothing</b> have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empirical evidence and theoretical results, not only provides nuanced insights into the differences between <b>label</b> <b>smoothing</b> and cross-entropy losses, but also serves as an example of how the powerful neural collapse framework can be used to improve our understanding of DNNs.</p></p class="citation"></blockquote><h3 id=114277-tabular-data-is-attention-all-you-need-guri-zabërgja-et-al-2024>(114/277) Tabular Data: Is Attention All You Need? (Guri Zabërgja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guri Zabërgja, Arlind Kadra, Josif Grabocka. (2024)<br><strong>Tabular Data: Is Attention All You Need?</strong><br><button class=copy-to-clipboard title="Tabular Data: Is Attention All You Need?" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03970v1.pdf filename=2402.03970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data. Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data. In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also <b>transformer-based</b> architectures against traditional multi-layer perceptrons (MLP) with residual connections. In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees. Furthermore, we assess that <b>transformer-based</b> architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets. As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications.</p></p class="citation"></blockquote><h3 id=115277-compound-returns-reduce-variance-in-reinforcement-learning-brett-daley-et-al-2024>(115/277) Compound Returns Reduce Variance in Reinforcement Learning (Brett Daley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brett Daley, Martha White, Marlos C. Machado. (2024)<br><strong>Compound Returns Reduce Variance in Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Compound Returns Reduce Variance in Reinforcement Learning" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03903v1.pdf filename=2402.03903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multistep returns, such as $n$-step returns and $\lambda$-returns, are commonly used to improve the sample efficiency of <b>reinforcement</b> <b>learning</b> (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns &ndash; weighted averages of $n$-step returns &ndash; to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing that two-bootstrap returns can improve the sample efficiency of $n$-step deep RL agents, with little additional computational cost.</p></p class="citation"></blockquote><h3 id=116277-a-phase-transition-between-positional-and-semantic-learning-in-a-solvable-model-of-dot-product-attention-hugo-cui-et-al-2024>(116/277) A phase transition between positional and semantic learning in a solvable model of dot-product attention (Hugo Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugo Cui, Freya Behrens, Florent Krzakala, Lenka Zdeborová. (2024)<br><strong>A phase transition between positional and semantic learning in a solvable model of dot-product attention</strong><br><button class=copy-to-clipboard title="A phase transition between positional and semantic learning in a solvable model of dot-product attention" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03902v1.pdf filename=2402.03902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning). For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism. On the theoretical side, we study the learning of a non-linear <b>self-attention</b> layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product attention layer to linear positional baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to sufficient data.</p></p class="citation"></blockquote><h3 id=117277-efficient-generation-of-hidden-outliers-for-improved-outlier-detection-jose-cribeiro-ramallo-et-al-2024>(117/277) Efficient Generation of Hidden Outliers for Improved Outlier Detection (Jose Cribeiro-Ramallo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jose Cribeiro-Ramallo, Vadim Arzamasov, Klemens Böhm. (2024)<br><strong>Efficient Generation of Hidden Outliers for Improved Outlier Detection</strong><br><button class=copy-to-clipboard title="Efficient Generation of Hidden Outliers for Improved Outlier Detection" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Outlier Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03846v1.pdf filename=2402.03846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Outlier</b> <b>generation</b> is a popular technique used for solving important <b>outlier</b> <b>detection</b> tasks. Generating <b>outliers</b> <b>with</b> realistic behavior is challenging. Popular existing methods tend to disregard the &lsquo;multiple views&rsquo; property of <b>outliers</b> <b>in</b> high-dimensional spaces. The only existing method accounting for this property falls short in efficiency and effectiveness. We propose BISECT, a new <b>outlier</b> <b>generation</b> method that creates realistic <b>outliers</b> <b>mimicking</b> said property. To do so, BISECT employs a novel proposition introduced in this article stating how to efficiently generate said realistic <b>outliers.</b> <b>Our</b> method has better guarantees and complexity than the current methodology for recreating &lsquo;multiple views&rsquo;. We use the synthetic <b>outliers</b> <b>generated</b> by BISECT to effectively enhance <b>outlier</b> <b>detection</b> in diverse datasets, for multiple use cases. For instance, oversampling with BISECT reduced the error by up to 3 times when compared with the baselines.</p></p class="citation"></blockquote><h3 id=118277-estimating-barycenters-of-distributions-with-neural-optimal-transport-alexander-kolesov-et-al-2024>(118/277) Estimating Barycenters of Distributions with Neural Optimal Transport (Alexander Kolesov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Kolesov, Petr Mokrov, Igor Udovichenko, Milena Gazdieva, Gudmund Pammer, Evgeny Burnaev, Alexander Korotin. (2024)<br><strong>Estimating Barycenters of Distributions with Neural Optimal Transport</strong><br><button class=copy-to-clipboard title="Estimating Barycenters of Distributions with Neural Optimal Transport" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03828v1.pdf filename=2402.03828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a collection of probability measures, a practitioner sometimes needs to find an &ldquo;average&rdquo; distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level <b>adversarial</b> <b>learning</b> objective and works for general cost functions. These are key advantages of our method, since the typical <b>adversarial</b> <b>algorithms</b> leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups.</p></p class="citation"></blockquote><h3 id=119277-expediting-in-network-federated-learning-by-voting-based-consensus-model-compression-xiaoxin-su-et-al-2024>(119/277) Expediting In-Network Federated Learning by Voting-Based Consensus Model Compression (Xiaoxin Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoxin Su, Yipeng Zhou, Laizhong Cui, Song Guo. (2024)<br><strong>Expediting In-Network Federated Learning by Voting-Based Consensus Model Compression</strong><br><button class=copy-to-clipboard title="Expediting In-Network Federated Learning by Voting-Based Consensus Model Compression" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Model Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03815v1.pdf filename=2402.03815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, federated learning (FL) has gained momentum because of its capability in preserving data privacy. To conduct <b>model</b> <b>training</b> by FL, multiple clients exchange <b>model</b> <b>updates</b> with a parameter server via Internet. To accelerate the communication speed, it has been explored to deploy a programmable switch (PS) in lieu of the parameter server to coordinate clients. The challenge to deploy the PS in FL lies in its scarce memory space, prohibiting running memory consuming aggregation algorithms on the PS. To overcome this challenge, we propose Federated Learning in-network Aggregation with Compression (FediAC) algorithm, consisting of two phases: client voting and <b>model</b> <b>aggregating.</b> In the former phase, clients report their significant <b>model</b> <b>update</b> indices to the PS to estimate global significant <b>model</b> <b>updates.</b> In the latter phase, clients upload global significant <b>model</b> <b>updates</b> to the PS for aggregation. FediAC consumes much less memory space and communication traffic than existing works because the first phase can guarantee consensus compression across clients. The PS easily aligns <b>model</b> <b>update</b> indices to swiftly complete aggregation in the second phase. Finally, we conduct extensive experiments by using public datasets to demonstrate that FediAC remarkably surpasses the state-of-the-art baselines in terms of <b>model</b> <b>accuracy</b> and communication traffic.</p></p class="citation"></blockquote><h3 id=120277-no-regret-reinforcement-learning-in-smooth-mdps-davide-maran-et-al-2024>(120/277) No-Regret Reinforcement Learning in Smooth MDPs (Davide Maran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davide Maran, Alberto Maria Metelli, Matteo Papini, Marcello Restell. (2024)<br><strong>No-Regret Reinforcement Learning in Smooth MDPs</strong><br><button class=copy-to-clipboard title="No-Regret Reinforcement Learning in Smooth MDPs" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03792v1.pdf filename=2402.03792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Obtaining no-regret guarantees for <b>reinforcement</b> <b>learning</b> (RL) in the case of problems with continuous state and/or action spaces is still one of the major open challenges in the field. Recently, a variety of solutions have been proposed, but besides very specific settings, the general problem remains unsolved. In this paper, we introduce a novel structural assumption on the Markov decision processes (MDPs), namely $\nu-$smoothness, that generalizes most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs). To face this challenging scenario, we propose two algorithms for regret minimization in $\nu-$smooth MDPs. Both algorithms build upon the idea of constructing an MDP representation through an orthogonal feature map based on Legendre polynomials. The first algorithm, \textsc{Legendre-Eleanor}, archives the no-regret property under weaker assumptions but is computationally inefficient, whereas the second one, \textsc{Legendre-LSVI}, runs in polynomial time, although for a smaller class of problems. After analyzing their regret properties, we compare our results with state-of-the-art ones from RL theory, showing that our algorithms achieve the best guarantees.</p></p class="citation"></blockquote><h3 id=121277-learning-a-decision-tree-algorithm-with-transformers-yufan-zhuang-et-al-2024>(121/277) Learning a Decision Tree Algorithm with Transformers (Yufan Zhuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufan Zhuang, Liyuan Liu, Chandan Singh, Jingbo Shang, Jianfeng Gao. (2024)<br><strong>Learning a Decision Tree Algorithm with Transformers</strong><br><button class=copy-to-clipboard title="Learning a Decision Tree Algorithm with Transformers" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03774v1.pdf filename=2402.03774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a <b>transformer-based</b> model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.</p></p class="citation"></blockquote><h3 id=122277-cross-task-linearity-emerges-in-the-pretraining-finetuning-paradigm-zhanpeng-zhou-et-al-2024>(122/277) Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm (Zhanpeng Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanpeng Zhou, Zijun Chen, Yilan Chen, Bo Zhang, Junchi Yan. (2024)<br><strong>Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm</strong><br><button class=copy-to-clipboard title="Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03660v1.pdf filename=2402.03660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pretraining-finetuning paradigm has become the prevailing trend in modern deep learning. In this work, we discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and <b>finetuned</b> on different tasks, termed as Cross-Task Linearity (CTL). Specifically, if we linearly interpolate the weights of two <b>finetuned</b> models, the features in the weight-interpolated model are approximately equal to the linear interpolation of features in two <b>finetuned</b> models at each layer. Such cross-task linearity has not been noted in peer literature. We provide comprehensive empirical evidence supporting that CTL consistently occurs for <b>finetuned</b> models that start from the same pretrained checkpoint. We conjecture that in the pretraining-finetuning paradigm, neural networks essentially function as linear maps, mapping from the parameter space to the feature space. Based on this viewpoint, our study unveils novel insights into explaining model merging/editing, particularly by translating operations from the parameter space to the feature space. Furthermore, we delve deeper into the underlying factors for the emergence of CTL, emphasizing the impact of pretraining.</p></p class="citation"></blockquote><h3 id=123277-cambranch-contrastive-learning-with-augmented-milps-for-branching-jiacheng-lin-et-al-2024>(123/277) CAMBranch: Contrastive Learning with Augmented MILPs for Branching (Jiacheng Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacheng Lin, Meng Xu, Zhihua Xiong, Huangang Wang. (2024)<br><strong>CAMBranch: Contrastive Learning with Augmented MILPs for Branching</strong><br><button class=copy-to-clipboard title="CAMBranch: Contrastive Learning with Augmented MILPs for Branching" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03647v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03647v1.pdf filename=2402.03647v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B&amp;B) branching policies for solving Mixed Integer Linear Programming (MILP). These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance. However, collecting expert samples for imitation learning, particularly for Strong Branching, is a time-consuming endeavor. To address this challenge, we propose \textbf{C}ontrastive Learning with \textbf{A}ugmented \textbf{M}ILPs for \textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs (AMILPs) by applying variable shifting to limited expert data from their original MILPs. This approach enables the acquisition of a considerable number of labeled expert samples. CAMBranch leverages both MILPs and AMILPs for imitation learning and employs <b>contrastive</b> <b>learning</b> to enhance the model&rsquo;s ability to capture MILP features, thereby improving the quality of branching decisions. Experimental results demonstrate that CAMBranch, trained with only 10% of the complete dataset, exhibits superior performance. Ablation studies further validate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=124277-neural-network-approximators-for-marginal-map-in-probabilistic-circuits-shivvrat-arya-et-al-2024>(124/277) Neural Network Approximators for Marginal MAP in Probabilistic Circuits (Shivvrat Arya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivvrat Arya, Tahrima Rahman, Vibhav Gogate. (2024)<br><strong>Neural Network Approximators for Marginal MAP in Probabilistic Circuits</strong><br><button class=copy-to-clipboard title="Neural Network Approximators for Marginal MAP in Probabilistic Circuits" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03621v1.pdf filename=2402.03621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Probabilistic circuits (PCs) such as sum-product networks efficiently represent large multi-variate probability distributions. They are preferred in practice over other probabilistic representations such as Bayesian and Markov networks because PCs can solve marginal inference (MAR) tasks in time that scales linearly in the size of the network. Unfortunately, the maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in these models. Inspired by the recent work on using neural networks for generating near-optimal solutions to optimization problems such as integer linear programming, we propose an approach that uses neural networks to approximate (M)MAP inference in PCs. The key idea in our approach is to approximate the cost of an assignment to the query variables using a continuous multilinear function, and then use the latter as a loss function. The two main benefits of our new method are that it is <b>self-supervised</b> and after the neural network is learned, it requires only linear time to output a solution. We evaluate our new approach on several benchmark datasets and show that it outperforms three competing linear time approximations, max-product inference, max-marginal inference and sequential estimation, which are used in practice to solve MMAP tasks in PCs.</p></p class="citation"></blockquote><h3 id=125277-breaking-symmetry-when-training-transformers-chunsheng-zuo-et-al-2024>(125/277) Breaking Symmetry When Training Transformers (Chunsheng Zuo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunsheng Zuo, Michael Guerzhoy. (2024)<br><strong>Breaking Symmetry When Training Transformers</strong><br><button class=copy-to-clipboard title="Breaking Symmetry When Training Transformers" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05969v1.pdf filename=2402.05969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As we show in this paper, the prediction for output token $n+1$ of <b>Transformer</b> architectures without one of the mechanisms of positional encodings and causal attention is invariant to permutations of input tokens $1, 2, &mldr;, n-1$. Usually, both mechanisms are employed and the symmetry with respect to the input tokens is broken. Recently, it has been shown that one can train <b>Transformers</b> without positional encodings. This must be enabled by the causal attention mechanism. In this paper, we elaborate on the argument that the causal connection mechanism must be responsible for the fact that <b>Transformers</b> are able to model input sequences where the order is important. Vertical &ldquo;slices&rdquo; of <b>Transformers</b> are all encouraged to represent the same location $k$ in the input sequence. We hypothesize that residual connections contribute to this phenomenon, and demonstrate evidence for this.</p></p class="citation"></blockquote><h3 id=126277-learning-metrics-that-maximise-power-for-accelerated-ab-tests-olivier-jeunen-et-al-2024>(126/277) Learning Metrics that Maximise Power for Accelerated A/B-Tests (Olivier Jeunen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olivier Jeunen, Aleksei Ustimenko. (2024)<br><strong>Learning Metrics that Maximise Power for Accelerated A/B-Tests</strong><br><button class=copy-to-clipboard title="Learning Metrics that Maximise Power for Accelerated A/B-Tests" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs.LG, stat-AP, stat-ML<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03915v1.pdf filename=2402.03915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent. We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs. Empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the North Star. Alternatively, we can obtain constant statistical power at a <b>sample</b> <b>size</b> that is down to 12% of what the North Star requires, significantly reducing the cost of experimentation.</p></p class="citation"></blockquote><h3 id=127277-adaflow-imitation-learning-with-variance-adaptive-flow-based-policies-xixi-hu-et-al-2024>(127/277) AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies (Xixi Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xixi Hu, Bo Liu, Xingchao Liu, Qiang Liu. (2024)<br><strong>AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies</strong><br><button class=copy-to-clipboard title="AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04292v1.pdf filename=2402.04292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion-based imitation learning improves Behavioral Cloning (BC) on <b>multi-modal</b> decision-making, but comes at the cost of significantly slower inference due to the recursion in the diffusion process. It urges us to design efficient policy generators while keeping the ability to generate diverse actions. To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling. AdaFlow represents the policy with state-conditioned ordinary differential equations (ODEs), which are known as probability flows. We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs. With this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making AdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity. Interestingly, it automatically reduces to a one-step generator when the action distribution is uni-modal. Our comprehensive empirical evaluation shows that AdaFlow achieves high performance across all dimensions, including success rate, behavioral diversity, and inference speed. The code is available at <a href=https://github.com/hxixixh/AdaFlow>https://github.com/hxixixh/AdaFlow</a></p></p class="citation"></blockquote><h2 id=cscv-35>cs.CV (35)</h2><h3 id=128277-exploring-low-resource-medical-image-classification-with-weakly-supervised-prompt-learning-fudan-zheng-et-al-2024>(128/277) Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning (Fudan Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fudan Zheng, Jindong Cao, Weijiang Yu, Zhiguang Chen, Nong Xiao, Yutong Lu. (2024)<br><strong>Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning</strong><br><button class=copy-to-clipboard title="Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 100<br>Keywords: Few-shot, Few-shot Learning, Low-Resource, Supervised Learning, Unsupervised Learning, Weakly-supervised Learning, Zero-shot, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03783v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03783v1.pdf filename=2402.03783v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most advances in medical image recognition supporting clinical auxiliary diagnosis meet challenges due to the <b>low-resource</b> situation in the medical field, where annotations are highly expensive and professional. This <b>low-resource</b> problem can be alleviated by leveraging the transferable representations of large-scale pre-trained <b>vision-language</b> models via relevant medical text <b>prompts.</b> <b>However,</b> existing pre-trained <b>vision-language</b> models require domain experts to carefully design the medical <b>prompts,</b> <b>which</b> greatly increases the burden on clinicians. To address this problem, we propose a weakly <b>supervised</b> <b>prompt</b> <b>learning</b> method MedPrompt to automatically generate medical <b>prompts,</b> <b>which</b> includes an <b>unsupervised</b> pre-trained <b>vision-language</b> model and a weakly <b>supervised</b> <b>prompt</b> <b>learning</b> model. The <b>unsupervised</b> pre-trained <b>vision-language</b> model utilizes the natural correlation between medical images and corresponding medical texts for pre-training, without any manual annotations. The weakly <b>supervised</b> <b>prompt</b> <b>learning</b> model only utilizes the classes of images in the dataset to guide the learning of the specific class vector in the <b>prompt,</b> <b>while</b> the learning of other context vectors in the <b>prompt</b> <b>requires</b> no manual annotations for guidance. To the best of our knowledge, this is the first model to automatically generate medical <b>prompts.</b> <b>With</b> these <b>prompts,</b> <b>the</b> pre-trained <b>vision-language</b> model can be freed from the strong expert dependency of manual annotation and manual <b>prompt</b> <b>design.</b> Experimental results show that the model using our automatically generated <b>prompts</b> <b>outperforms</b> its full-shot learning hand-crafted <b>prompts</b> <b>counterparts</b> with only a minimal number of labeled samples for <b>few-shot</b> <b>learning,</b> and reaches superior or comparable accuracy on <b>zero-shot</b> image classification. The proposed <b>prompt</b> <b>generator</b> is lightweight and therefore can be embedded into any network architecture.</p></p class="citation"></blockquote><h3 id=129277-a-hard-to-beat-baseline-for-training-free-clip-based-adaptation-zhengbo-wang-et-al-2024>(129/277) A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation (Zhengbo Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, Tieniu Tan. (2024)<br><strong>A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation</strong><br><button class=copy-to-clipboard title="A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 80<br>Keywords: Few-shot, Fine-tuning, Out-of-distribution, Unsupervised Learning, Unsupervised Learning, Zero-shot, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04087v1.pdf filename=2402.04087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable <b>zero-shot</b> capacity. Recent research has focused on developing efficient <b>fine-tuning</b> methods, such as <b>prompt</b> <b>learning</b> and adapter, to enhance CLIP&rsquo;s performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes&rsquo; formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original <b>zero-shot</b> classifier within CLIP. Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on <b>few-shot</b> classification, imbalanced learning, and <b>out-of-distribution</b> generalization. In addition, we extend our method to base-to-new generalization and <b>unsupervised</b> <b>learning,</b> once again demonstrating its superiority over competing approaches. Our code is publicly available at \url{https://github.com/mrflogs/ICLR24}.</p></p class="citation"></blockquote><h3 id=130277-vision-superalignment-weak-to-strong-generalization-for-vision-foundation-models-jianyuan-guo-et-al-2024>(130/277) Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models (Jianyuan Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianyuan Guo, Hanting Chen, Chengcheng Wang, Kai Han, Chang Xu, Yunhe Wang. (2024)<br><strong>Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models</strong><br><button class=copy-to-clipboard title="Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 80<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Foundation Model, Knowledge Distillation, Knowledge Distillation, Transfer Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03749v1.pdf filename=2402.03749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>large</b> <b>language</b> <b>models</b> have sparked interest in their extraordinary and near-superhuman capabilities, leading researchers to explore methods for evaluating and optimizing these abilities, which is called superalignment. In this context, our paper delves into the realm of vision <b>foundation</b> <b>models,</b> focusing on the concept of weak-to-strong generalization, which involves using a weaker model to supervise a stronger one, aiming to enhance the latter&rsquo;s capabilities beyond the former&rsquo;s limits. We introduce a novel and adaptively adjustable loss function for weak-to-strong supervision. Our comprehensive experiments span various scenarios, including <b>few-shot</b> <b>learning,</b> <b>transfer</b> <b>learning,</b> noisy label learning, and common <b>knowledge</b> <b>distillation</b> settings. The results are striking: our approach not only exceeds the performance benchmarks set by strong-to-strong generalization but also surpasses the outcomes of <b>fine-tuning</b> strong models with whole datasets. This compelling evidence underscores the significant potential of weak-to-strong generalization, showcasing its capability to substantially elevate the performance of vision <b>foundation</b> <b>models.</b> The code is available at <a href=https://github.com/ggjy/vision_weak_to_strong>https://github.com/ggjy/vision_weak_to_strong</a>.</p></p class="citation"></blockquote><h3 id=131277-quest-low-bit-diffusion-model-quantization-via-efficient-selective-finetuning-haoxuan-wang-et-al-2024>(131/277) QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning (Haoxuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, Yan Yan. (2024)<br><strong>QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning</strong><br><button class=copy-to-clipboard title="QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Model Compression, Model Quantization, Quantization, Quantization, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03666v1.pdf filename=2402.03666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion <b>models</b> <b>have</b> achieved remarkable success in image generation tasks, yet their practical deployment is restrained by the high memory and time consumption. While <b>quantization</b> paves a way for diffusion <b>model</b> <b>compression</b> and acceleration, existing methods totally fail when the <b>models</b> <b>are</b> <b>quantized</b> to low-bits. In this paper, we unravel three properties in <b>quantized</b> diffusion <b>models</b> <b>that</b> compromise the efficacy of current methods: imbalanced activation distributions, imprecise temporal information, and vulnerability to perturbations of specific modules. To alleviate the intensified low-bit <b>quantization</b> difficulty <b>stemming</b> from the distribution imbalance, we propose <b>finetuning</b> the <b>quantized</b> <b>model</b> <b>to</b> better adapt to the activation distribution. Building on this idea, we identify two critical types of <b>quantized</b> layers: those holding vital temporal information and those sensitive to reduced bit-width, and <b>finetune</b> them to mitigate performance degradation with efficiency. We empirically verify that our approach modifies the activation distribution and provides meaningful temporal information, facilitating easier and more accurate <b>quantization.</b> Our method is evaluated over three high-resolution image generation tasks and achieves state-of-the-art performance under various bit-width settings, as well as being the first method to generate readable images on full 4-bit (i.e. W4A4) Stable Diffusion.</p></p class="citation"></blockquote><h3 id=132277-shield--an-evaluation-benchmark-for-face-spoofing-and-forgery-detection-with-multimodal-large-language-models-yichen-shi-et-al-2024>(132/277) SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models (Yichen Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichen Shi, Yuhao Gao, Yingxin Lai, Hongyang Wang, Jun Feng, Lei He, Jun Wan, Changsheng Chen, Zitong Yu, Xiaochun Cao. (2024)<br><strong>SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Few-shot, Generative Adversarial Network, Multi-modal, Multi-modal, Zero-shot, Grounding, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04178v1.pdf filename=2402.04178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) have demonstrated remarkable problem-solving capabilities in various vision fields (e.g., generic object recognition and <b>grounding)</b> based on strong visual semantic representation and language <b>reasoning</b> ability. However, whether MLLMs are sensitive to subtle visual spoof/forged clues and how they perform in the domain of face attack detection (e.g., face spoofing and forgery detection) is still unexplored. In this paper, we introduce a new benchmark, namely SHIELD, to evaluate the ability of MLLMs on face spoofing and forgery detection. Specifically, we design true/false and multiple-choice questions to evaluate <b>multimodal</b> face data in these two face security tasks. For the face anti-spoofing task, we evaluate three different modalities (i.e., RGB, infrared, depth) under four types of presentation attacks (i.e., print attack, replay attack, rigid mask, paper mask). For the face forgery detection task, we evaluate <b>GAN-based</b> and diffusion-based data with both visual and acoustic modalities. Each question is subjected to both <b>zero-shot</b> and <b>few-shot</b> tests under standard and chain of thought (COT) settings. The results indicate that MLLMs hold substantial potential in the face security domain, offering advantages over traditional specific models in terms of interpretability, <b>multimodal</b> flexible <b>reasoning,</b> and joint face spoof and forgery detection. Additionally, we develop a novel Multi-Attribute Chain of Thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images, which provides rich task-related knowledge for subtle spoof/forged clue mining. Extensive experiments in separate face anti-spoofing, separate face forgery detection, and joint detection tasks demonstrate the effectiveness of the proposed MA-COT. The project is available at https$:$//github.com/laiyingxin2/SHIELD</p></p class="citation"></blockquote><h3 id=133277-tuning-large-multimodal-models-for-videos-using-reinforcement-learning-from-ai-feedback-daechul-ahn-et-al-2024>(133/277) Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback (Daechul Ahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, Jonghyun Choi. (2024)<br><strong>Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback</strong><br><button class=copy-to-clipboard title="Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Reinforcement Learning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03746v1.pdf filename=2402.03746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>large</b> <b>language</b> <b>models</b> have influenced the development of video <b>large</b> <b>multimodal</b> <b>models</b> (VLMMs). The previous approaches for VLMMs involved <b>Supervised</b> <b>Fine-Tuning</b> (SFT) with instruction-tuned datasets, integrating <b>LLM</b> with visual encoders, and adding additional learnable modules. Video and text <b>multimodal</b> alignment remains challenging, primarily due to the deficient volume and quality of <b>multimodal</b> instruction-tune data compared to text-only data. We present a novel alignment strategy that employs <b>multimodal</b> AI system to oversee itself called <b>Reinforcement</b> <b>Learning</b> from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. In specific, we propose context-aware reward modeling by providing detailed video descriptions as context during the generation of preference feedback in order to enrich the understanding of video content. Demonstrating enhanced performance across diverse video benchmarks, our <b>multimodal</b> RLAIF approach, VLM-RLAIF, outperforms existing approaches, including the SFT model. We commit to open-sourcing our code, models, and datasets to foster further research in this area.</p></p class="citation"></blockquote><h3 id=134277-convincing-rationales-for-visual-question-answering-reasoning-kun-li-et-al-2024>(134/277) Convincing Rationales for Visual Question Answering Reasoning (Kun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Li, George Vosselman, Michael Ying Yang. (2024)<br><strong>Convincing Rationales for Visual Question Answering Reasoning</strong><br><button class=copy-to-clipboard title="Convincing Rationales for Visual Question Answering Reasoning" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Zero-shot, Question Answering, Reasoning, Visual Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03896v1.pdf filename=2402.03896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> is a challenging task of predicting the answer to a <b>question</b> <b>about</b> the content of an image. It requires deep understanding of both the textual <b>question</b> <b>and</b> <b>visual</b> <b>image.</b> <b>Prior</b> works directly evaluate the answering models by simply calculating the accuracy of the predicted answers. However, the inner <b>reasoning</b> behind the prediction is disregarded in such a &ldquo;black box&rdquo; system, and we do not even know if one can trust the predictions. In some cases, the models still get the correct answers even when they focus on irrelevant <b>visual</b> <b>regions</b> <b>or</b> textual tokens, which makes the models unreliable and illogical. To generate both <b>visual</b> <b>and</b> <b>textual</b> rationales next to the predicted answer to the given image/question pair, we propose Convincing Rationales for <b>VQA,</b> CRVQA. Considering the extra annotations brought by the new outputs, {CRVQA} is trained and evaluated by samples converted from some existing <b>VQA</b> datasets and their <b>visual</b> <b>labels.</b> <b>The</b> extensive experiments demonstrate that the <b>visual</b> <b>and</b> <b>textual</b> rationales support the prediction of the answers, and further improve the accuracy. Furthermore, {CRVQA} achieves competitive performance on generic <b>VQA</b> datatsets in the <b>zero-shot</b> evaluation setting. The dataset and source code will be released under <a href=https://github.com/lik1996/CRVQA2024>https://github.com/lik1996/CRVQA2024</a>.</p></p class="citation"></blockquote><h3 id=135277-a-data-centric-approach-for-unsupervised-domain-generalization-via-retrieval-from-web-scale-multimodal-data-christopher-liao-et-al-2024>(135/277) A Data Centric Approach for Unsupervised Domain Generalization via Retrieval from Web Scale Multimodal Data (Christopher Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher Liao, Theodoros Tsiligkaridis, Brian Kulis. (2024)<br><strong>A Data Centric Approach for Unsupervised Domain Generalization via Retrieval from Web Scale Multimodal Data</strong><br><button class=copy-to-clipboard title="A Data Centric Approach for Unsupervised Domain Generalization via Retrieval from Web Scale Multimodal Data" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 46<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Unsupervised Learning, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04416v1.pdf filename=2402.04416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain generalization (DG) is an important problem that learns a model that can generalize to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive. For this setting, we tackle the <b>multimodal</b> version of the <b>unsupervised</b> domain generalization (UDG) problem, which uses a large task-agnostic unlabeled source dataset, such as LAION-2B during <b>finetuning.</b> Our framework does not explicitly assume any relationship between the source dataset and target task. Instead, it relies only on the premise that the source dataset can be efficiently searched in a joint <b>vision-language</b> space. For this <b>multimodal</b> UDG setting, we propose a novel method to build a small ($&lt;$100K) subset of the source data in three simple steps: (1) diversified retrieval using label names as queries, (2) rank pseudo-labeling, and (3) clustering to find representative samples. To demonstrate the value of studying the <b>multimodal</b> UDG problem, we compare our results against state-of-the-art source-free DG and <b>zero-shot</b> (ZS) methods on their respective benchmarks and show up to 10% improvement in accuracy on 20 diverse target datasets. Additionally, our multi-stage dataset construction method achieves 3% improvement on average over nearest neighbors retrieval. Code is available: <a href=https://github.com/Chris210634/mudg>https://github.com/Chris210634/mudg</a></p></p class="citation"></blockquote><h3 id=136277-the-instinctive-bias-spurious-images-lead-to-hallucination-in-mllms-tianyang-han-et-al-2024>(136/277) The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs (Tianyang Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, Tong Zhang. (2024)<br><strong>The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs</strong><br><button class=copy-to-clipboard title="The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Multi-modal, GPT, Text2image, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03757v1.pdf filename=2402.03757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently experienced remarkable progress, where the advent of <b>multi-modal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) has endowed <b>LLMs</b> with visual capabilities, leading to impressive performances in various <b>multi-modal</b> tasks. However, those powerful MLLMs such as <b>GPT-4V</b> still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 <b>text-image</b> pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs&rsquo; robustness in the presence of misleading images. The resource is available in <a href=https://github.com/MasaiahHan/CorrelationQA>https://github.com/MasaiahHan/CorrelationQA</a>.</p></p class="citation"></blockquote><h3 id=137277-pre-training-of-lightweight-vision-transformers-on-small-datasets-with-minimally-scaled-images-jen-hong-tan-2024>(137/277) Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images (Jen Hong Tan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jen Hong Tan. (2024)<br><strong>Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images</strong><br><button class=copy-to-clipboard title="Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03752v1.pdf filename=2402.03752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Can a lightweight Vision <b>Transformer</b> (ViT) match or exceed the performance of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> like ResNet on small datasets with small image resolutions? This report demonstrates that a pure ViT can indeed achieve superior performance through pre-training, using a masked auto-encoder technique with minimal image scaling. Our experiments on the CIFAR-10 and CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters and a multiply-accumulate (MAC) count below 0.27G, qualifying them as &rsquo;lightweight&rsquo; models. Unlike previous approaches, our method attains state-of-the-art performance among similar lightweight <b>transformer-based</b> architectures without significantly scaling up images from CIFAR-10 and CIFAR-100. This achievement underscores the efficiency of our model, not only in handling small datasets but also in effectively processing images close to their original scale.</p></p class="citation"></blockquote><h3 id=138277-eva-clip-18b-scaling-clip-to-18-billion-parameters-quan-sun-et-al-2024>(138/277) EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters (Quan Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Xinlong Wang. (2024)<br><strong>EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters</strong><br><button class=copy-to-clipboard title="EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Foundation Model, Multi-modal, Multi-modal, Zero-shot, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04252v1.pdf filename=2402.04252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scaling up contrastive language-image pretraining (CLIP) is critical for empowering both vision and <b>multimodal</b> models. We present EVA-CLIP-18B, the largest and most powerful open-source CLIP model to date, with 18-billion parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an exceptional 80.7% <b>zero-shot</b> top-1 accuracy averaged across 27 widely recognized image classification benchmarks, outperforming its forerunner EVA-CLIP (5-billion parameters) and other open-source CLIP models by a large margin. Remarkably, we observe a consistent performance improvement with the model size scaling of EVA-CLIP, despite maintaining a constant training dataset of 2-billion <b>image-text</b> pairs from LAION-2B and COYO-700M. This dataset is openly available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B) employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the potential of EVA-style weak-to-strong visual model scaling. With our model weights made publicly available, we hope to facilitate future research in vision and <b>multimodal</b> <b>foundation</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=139277-cogcom-train-large-vision-language-models-diving-into-details-through-chain-of-manipulations-ji-qi-et-al-2024>(139/277) CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations (Ji Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, Jie Tang. (2024)<br><strong>CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations</strong><br><button class=copy-to-clipboard title="CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Grounding, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04236v1.pdf filename=2402.04236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-Language</b> Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual <b>reasoning,</b> and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., <b>grounding)</b> acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual <b>reasoning,</b> and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this <b>reasoning</b> mechanism. Experiments show that our model achieves the state-of-the-art performance across 8 benchmarks from 3 categories, and a limited number of training steps with the data swiftly gains a competitive performance. The code and data are publicly available at <a href=https://github.com/THUDM/CogCoM>https://github.com/THUDM/CogCoM</a>.</p></p class="citation"></blockquote><h3 id=140277-low-rank-attention-side-tuning-for-parameter-efficient-fine-tuning-ningyuan-tang-et-al-2024>(140/277) Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning (Ningyuan Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ningyuan Tang, Minghao Fu, Ke Zhu, Jianxin Wu. (2024)<br><strong>Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning</strong><br><button class=copy-to-clipboard title="Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04009v1.pdf filename=2402.04009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>finetuning</b> a large pretrained model to downstream tasks, parameter-efficient <b>fine-tuning</b> (PEFT) methods can effectively <b>finetune</b> pretrained models with few trainable parameters, but suffer from high GPU memory consumption and slow training speed. Because learnable parameters from these methods are entangled with the pretrained model, gradients related to the frozen pretrained model&rsquo;s parameters have to be computed and stored during <b>finetuning.</b> We propose Low-rank Attention Side-Tuning (LAST), which disentangles the trainable module from the pretrained model by freezing not only parameters but also outputs of the pretrained network. LAST trains a side-network composed of only low-rank <b>self-attention</b> modules. By viewing the pretrained model as a frozen feature extractor, the side-network takes intermediate output from the pretrained model and focus on learning task-specific knowledge. We also show that LAST can be highly parallel across multiple optimization objectives, making it very efficient in downstream task adaptation, for example, in finding optimal hyperparameters. LAST outperforms previous state-of-the-art methods on VTAB-1K and other visual adaptation tasks with roughly only 30% of GPU memory footprint and 60% of training time compared to existing PEFT methods, but achieves significantly higher accuracy.</p></p class="citation"></blockquote><h3 id=141277-yolopoint-joint-keypoint-and-object-detection-anton-backhaus-et-al-2024>(141/277) YOLOPoint Joint Keypoint and Object Detection (Anton Backhaus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anton Backhaus, Thorsten Luettel, Hans-Joachim Wuensche. (2024)<br><strong>YOLOPoint Joint Keypoint and Object Detection</strong><br><button class=copy-to-clipboard title="YOLOPoint Joint Keypoint and Object Detection" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03989v1.pdf filename=2402.03989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intelligent vehicles of the future must be capable of understanding and navigating safely through their surroundings. Camera-based vehicle systems can use keypoints as well as <b>objects</b> <b>as</b> low- and high-level landmarks for GNSS-independent SLAM and visual odometry. To this end we propose YOLOPoint, a <b>convolutional</b> <b>neural</b> <b>network</b> model that simultaneously detects keypoints and <b>objects</b> <b>in</b> an image by combining YOLOv5 and SuperPoint to create a single forward-pass network that is both real-time capable and accurate. By using a shared backbone and a light-weight network structure, YOLOPoint is able to perform competitively on both the HPatches and KITTI benchmarks.</p></p class="citation"></blockquote><h3 id=142277-energy-based-domain-adaptive-segmentation-with-depth-guidance-jinjing-zhu-et-al-2024>(142/277) Energy-based Domain-Adaptive Segmentation with Depth Guidance (Jinjing Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinjing Zhu, Zhedong Hu, Tae-Kyun Kim, Lin Wang. (2024)<br><strong>Energy-based Domain-Adaptive Segmentation with Depth Guidance</strong><br><button class=copy-to-clipboard title="Energy-based Domain-Adaptive Segmentation with Depth Guidance" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Self-supervised Learning, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03795v1.pdf filename=2402.03795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent endeavors have been made to leverage <b>self-supervised</b> depth estimation as guidance in <b>unsupervised</b> <b>domain</b> <b>adaptation</b> (UDA) for semantic segmentation. Prior arts, however, overlook the discrepancy between semantic and depth features, as well as the reliability of feature fusion, thus leading to suboptimal segmentation performance. To address this issue, we propose a novel UDA framework called SMART (croSs <b>doMain</b> <b>semAntic</b> segmentation based on eneRgy esTimation) that utilizes Energy-Based Models (EBMs) to obtain task-adaptive features and achieve reliable feature fusion for semantic segmentation with <b>self-supervised</b> depth estimates. Our framework incorporates two novel components: energy-based feature fusion (EB2F) and energy-based reliable fusion Assessment (RFA) modules. The EB2F module produces task-adaptive semantic and depth features by explicitly measuring and reducing their discrepancy using Hopfield energy for better feature fusion. The RFA module evaluates the reliability of the feature fusion using an energy score to improve the effectiveness of depth guidance. Extensive experiments on two datasets demonstrate that our method achieves significant performance gains over prior works, validating the effectiveness of our energy-based learning approach.</p></p class="citation"></blockquote><h3 id=143277-attacknet-enhancing-biometric-security-via-tailored-convolutional-neural-network-architectures-for-liveness-detection-oleksandr-kuznetsov-et-al-2024>(143/277) AttackNet: Enhancing Biometric Security via Tailored Convolutional Neural Network Architectures for Liveness Detection (Oleksandr Kuznetsov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oleksandr Kuznetsov, Dmytro Zakharov, Emanuele Frontoni, Andrea Maranesi. (2024)<br><strong>AttackNet: Enhancing Biometric Security via Tailored Convolutional Neural Network Architectures for Liveness Detection</strong><br><button class=copy-to-clipboard title="AttackNet: Enhancing Biometric Security via Tailored Convolutional Neural Network Architectures for Liveness Detection" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03769v1.pdf filename=2402.03769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Biometric security is the cornerstone of modern identity verification and authentication systems, where the integrity and reliability of biometric samples is of paramount importance. This paper introduces AttackNet, a bespoke <b>Convolutional</b> <b>Neural</b> <b>Network</b> architecture, meticulously designed to combat spoofing threats in biometric systems. Rooted in deep learning methodologies, this model offers a layered defense mechanism, seamlessly transitioning from low-level feature extraction to high-level pattern discernment. Three distinctive architectural phases form the crux of the model, each underpinned by judiciously chosen activation functions, normalization techniques, and dropout layers to ensure robustness and resilience against <b>adversarial</b> <b>attacks.</b> Benchmarking our model across diverse datasets affirms its prowess, showcasing superior performance metrics in comparison to contemporary models. Furthermore, a detailed comparative analysis accentuates the model&rsquo;s efficacy, drawing parallels with prevailing state-of-the-art methodologies. Through iterative refinement and an informed architectural strategy, AttackNet underscores the potential of deep learning in safeguarding the future of biometric security.</p></p class="citation"></blockquote><h3 id=144277-cat-sam-conditional-tuning-network-for-few-shot-adaptation-of-segmentation-anything-model-aoran-xiao-et-al-2024>(144/277) CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of Segmentation Anything Model (Aoran Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aoran Xiao, Weihao Xuan, Heli Qi, Yun Xing, Ruijie Ren, Xiaoqin Zhang, Shijian Lu. (2024)<br><strong>CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of Segmentation Anything Model</strong><br><button class=copy-to-clipboard title="CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of Segmentation Anything Model" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Few-shot, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03631v1.pdf filename=2402.03631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent Segment Anything Model (SAM) has demonstrated remarkable <b>zero-shot</b> capability and flexible geometric <b>prompting</b> in general image segmentation. However, SAM often struggles when handling various unconventional images, such as aerial, medical, and non-RGB images. This paper presents CAT-SAM, a ConditionAl Tuning network that adapts SAM toward various unconventional target tasks with just <b>few-shot</b> target samples. CAT-SAM freezes the entire SAM and adapts its mask decoder and image encoder simultaneously with a small number of learnable parameters. The core design is a <b>prompt</b> bridge structure that enables decoder-conditioned joint tuning of the heavyweight image encoder and the lightweight mask decoder. The bridging maps the <b>prompt</b> token of the mask decoder to the image encoder, fostering synergic adaptation of the encoder and the decoder with mutual benefits. We develop two representative tuning strategies for the image encoder which leads to two CAT-SAM variants: one injecting learnable <b>prompt</b> tokens in the input space and the other inserting lightweight adapter networks. Extensive experiments over 11 unconventional tasks show that both CAT-SAM variants achieve superior target segmentation performance consistently even under the very challenging one-shot adaptation setup. Project page: \url{https://xiaoaoran.github.io/projects/CAT-SAM}</p></p class="citation"></blockquote><h3 id=145277-detection-transformer-for-teeth-detection-segmentation-and-numbering-in-oral-rare-diseases-focus-on-data-augmentation-and-inpainting-techniques-hocine-kadi-et-al-2024>(145/277) Detection Transformer for Teeth Detection, Segmentation, and Numbering in Oral Rare Diseases: Focus on Data Augmentation and Inpainting Techniques (Hocine Kadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hocine Kadi, Théo Sourget, Marzena Kawczynski, Sara Bendjama, Bruno Grollemund, Agnès Bloch-Zupan. (2024)<br><strong>Detection Transformer for Teeth Detection, Segmentation, and Numbering in Oral Rare Diseases: Focus on Data Augmentation and Inpainting Techniques</strong><br><button class=copy-to-clipboard title="Detection Transformer for Teeth Detection, Segmentation, and Numbering in Oral Rare Diseases: Focus on Data Augmentation and Inpainting Techniques" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Data Augmentation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04408v1.pdf filename=2402.04408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we focused on deep learning image processing in the context of oral rare diseases, which pose challenges due to limited <b>data</b> <b>availability.</b> A crucial step involves teeth detection, segmentation and numbering in panoramic radiographs. To this end, we used a dataset consisting of 156 panoramic radiographs from individuals with rare oral diseases and labeled by experts. We trained the Detection <b>Transformer</b> (DETR) neural network for teeth detection, segmentation, and numbering the 52 teeth classes. In addition, we used <b>data</b> <b>augmentation</b> techniques, including geometric transformations. Finally, we generated new panoramic images using inpainting techniques with stable diffusion, by removing teeth from a panoramic radiograph and integrating teeth into it. The results showed a mAP exceeding 0,69 for DETR without <b>data</b> <b>augmentation.</b> The mAP was improved to 0,82 when <b>data</b> <b>augmentation</b> techniques are used. Furthermore, we observed promising performances when using new panoramic radiographs generated with inpainting technique, with mAP of 0,76.</p></p class="citation"></blockquote><h3 id=146277-u-shaped-vision-mamba-for-single-image-dehazing-zhuoran-zheng-et-al-2024>(146/277) U-shaped Vision Mamba for Single Image Dehazing (Zhuoran Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoran Zheng, Chen Wu. (2024)<br><strong>U-shaped Vision Mamba for Single Image Dehazing</strong><br><button class=copy-to-clipboard title="U-shaped Vision Mamba for Single Image Dehazing" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04139v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04139v2.pdf filename=2402.04139v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Currently, <b>Transformer</b> is the most popular architecture for image dehazing, but due to its large computational complexity, its ability to handle long-range dependency is limited on resource-constrained devices. To tackle this challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient single-image dehazing network. Inspired by the State Space Sequence Models (SSMs), a new deep sequence model known for its power to handle long sequences, we design a Bi-SSM block that integrates the local feature extraction ability of the <b>convolutional</b> layer with the ability of the SSM to capture long-range dependencies. Extensive experimental results demonstrate the effectiveness of our method. Our method provides a more highly efficient idea of long-range dependency modeling for image dehazing as well as other image restoration tasks. The URL of the code is \url{https://github.com/zzr-idam/UVM-Net}. Our method takes only \textbf{0.009} seconds to infer a $325 \times 325$ resolution image (100FPS) without I/O handling time.</p></p class="citation"></blockquote><h3 id=147277-vrmm-a-volumetric-relightable-morphable-head-model-haotian-yang-et-al-2024>(147/277) VRMM: A Volumetric Relightable Morphable Head Model (Haotian Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Yang, Mingwu Zheng, Chongyang Ma, Yu-Kun Lai, Pengfei Wan, Haibin Huang. (2024)<br><strong>VRMM: A Volumetric Relightable Morphable Head Model</strong><br><button class=copy-to-clipboard title="VRMM: A Volumetric Relightable Morphable Head Model" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04101v1.pdf filename=2402.04101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations. This framework, designed with <b>self-supervised</b> <b>learning,</b> significantly reduces the constraints for training data, making it more feasible in practice. The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions. We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation. Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM. Such an approach enables accurate 3D face reconstruction from even a single portrait input. Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling.</p></p class="citation"></blockquote><h3 id=148277-boosting-adversarial-transferability-across-model-genus-by-deformation-constrained-warping-qinliang-lin-et-al-2024>(148/277) Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping (Qinliang Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinliang Lin, Cheng Luo, Zenghao Niu, Xilin He, Weicheng Xie, Yuanbo Hou, Linlin Shen, Siyang Song. (2024)<br><strong>Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping</strong><br><button class=copy-to-clipboard title="Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03951v1.pdf filename=2402.03951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adversarial examples generated by a surrogate model typically exhibit limited transferability to unknown target systems. To address this problem, many transferability enhancement approaches (e.g., input transformation and model augmentation) have been proposed. However, they show poor performances in attacking systems having different model genera from the surrogate model. In this paper, we propose a novel and generic attacking strategy, called Deformation-Constrained Warping Attack (DeCoWA), that can be effectively applied to cross model genus attack. Specifically, DeCoWA firstly augments input examples via an elastic deformation, namely Deformation-Constrained Warping (DeCoW), to obtain rich local details of the augmented input. To avoid severe distortion of global semantics led by random deformation, DeCoW further constrains the strength and direction of the warping transformation by a novel adaptive control strategy. Extensive experiments demonstrate that the transferable examples crafted by our DeCoWA on <b>CNN</b> surrogates can significantly hinder the performance of <b>Transformers</b> (and vice versa) on various tasks, including image classification, video action recognition, and audio recognition. Code is made available at <a href=https://github.com/LinQinLiang/DeCoWA>https://github.com/LinQinLiang/DeCoWA</a>.</p></p class="citation"></blockquote><h3 id=149277-reviewing-fid-and-sid-metrics-on-generative-adversarial-networks-ricardo-de-deijn-et-al-2024>(149/277) Reviewing FID and SID Metrics on Generative Adversarial Networks (Ricardo de Deijn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ricardo de Deijn, Aishwarya Batra, Brandon Koch, Naseef Mansoor, Hema Makkena. (2024)<br><strong>Reviewing FID and SID Metrics on Generative Adversarial Networks</strong><br><button class=copy-to-clipboard title="Reviewing FID and SID Metrics on Generative Adversarial Networks" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03654v1.pdf filename=2402.03654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growth of <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> models has increased the ability of image processing and provides numerous industries with the technology to produce realistic image transformations. However, with the field being recently established there are new evaluation metrics that can further this research. Previous research has shown the Fr'echet Inception Distance (FID) to be an effective metric when testing these image-to-image <b>GANs</b> in real-world applications. Signed Inception Distance (SID), a founded metric in 2023, expands on FID by allowing unsigned distances. This paper uses public datasets that consist of fa\c{c}ades, cityscapes, and maps within Pix2Pix and CycleGAN models. After training these models are evaluated on both inception distance metrics which measure the generating performance of the trained models. Our findings indicate that usage of the metric SID incorporates an efficient and effective metric to complement, or even exceed the ability shown using the FID for the image-to-image <b>GANs</b></p></p class="citation"></blockquote><h3 id=150277-intensive-vision-guided-network-for-radiology-report-generation-fudan-zheng-et-al-2024>(150/277) Intensive Vision-guided Network for Radiology Report Generation (Fudan Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fudan Zheng, Mengfei Li, Ying Wang, Weijiang Yu, Ruixuan Wang, Zhiguang Chen, Nong Xiao, Yutong Lu. (2024)<br><strong>Intensive Vision-guided Network for Radiology Report Generation</strong><br><button class=copy-to-clipboard title="Intensive Vision-guided Network for Radiology Report Generation" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Multi-modal, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03754v1.pdf filename=2402.03754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic radiology report generation is booming due to its huge application potential for the healthcare industry. However, existing computer vision and natural language processing approaches to tackle this problem are limited in two aspects. First, when extracting image features, most of them neglect multi-view <b>reasoning</b> in vision and model single-view structure of medical images, such as space-view or channel-view. However, clinicians rely on multi-view imaging information for comprehensive judgment in daily clinical diagnosis. Second, when generating reports, they overlook context <b>reasoning</b> with <b>multi-modal</b> information and focus on pure textual optimization utilizing retrieval-based methods. We aim to address these two issues by proposing a model that better simulates clinicians&rsquo; perspectives and generates more accurate reports. Given the above limitation in feature extraction, we propose a Globally-intensive Attention (GIA) module in the medical image encoder to simulate and integrate multi-view vision perception. GIA aims to learn three types of vision perception: depth view, space view, and pixel view. On the other hand, to address the above problem in report generation, we explore how to involve <b>multi-modal</b> signals to generate precisely matched reports, i.e., how to integrate previously predicted words with region-aware visual content in next word prediction. Specifically, we design a Visual Knowledge-guided Decoder (VKGD), which can adaptively consider how much the model needs to rely on visual information and previously predicted text to assist next word prediction. Hence, our final Intensive Vision-guided Network (IVGN) framework includes a GIA-guided Visual Encoder and the VKGD. Experiments on two commonly-used datasets IU X-Ray and MIMIC-CXR demonstrate the superior ability of our method compared with other state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=151277-breaking-data-silos-cross-domain-learning-for-multi-agent-perception-from-independent-private-sources-jinlong-li-et-al-2024>(151/277) Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources (Jinlong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinlong Li, Baolu Li, Xinyu Liu, Runsheng Xu, Jiaqi Ma, Hongkai Yu. (2024)<br><strong>Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources</strong><br><button class=copy-to-clipboard title="Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04273v1.pdf filename=2402.04273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The diverse agents in multi-agent perception systems may be from different companies. Each company might use the identical classic neural network architecture based encoder for feature extraction. However, the data source to train the various agents is independent and private in each company, leading to the Distribution Gap of different private data for training distinct agents in multi-agent perception system. The data silos by the above Distribution Gap could result in a significant performance decline in multi-agent perception. In this paper, we thoroughly examine the impact of the distribution gap on existing multi-agent perception systems. To break the data silos, we introduce the Feature Distribution-aware Aggregation (FDA) framework for cross-domain learning to mitigate the above Distribution Gap in multi-agent perception. FDA comprises two key components: Learnable Feature Compensation Module and Distribution-aware Statistical Consistency Module, both aimed at enhancing intermediate features to minimize the distribution gap among multi-agent features. Intensive experiments on the public OPV2V and V2XSet datasets underscore FDA&rsquo;s effectiveness in point cloud-based 3D <b>object</b> <b>detection,</b> presenting it as an invaluable augmentation to existing multi-agent perception systems.</p></p class="citation"></blockquote><h3 id=152277-consisti2v-enhancing-visual-consistency-for-image-to-video-generation-weiming-ren-et-al-2024>(152/277) ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation (Weiming Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiming Ren, Harry Yang, Ge Zhang, Cong Wei, Xinrun Du, Stephen Huang, Wenhu Chen. (2024)<br><strong>ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation</strong><br><button class=copy-to-clipboard title="ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04324v1.pdf filename=2402.04324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image-to-video (I2V) generation aims to use the initial frame (alongside a text <b>prompt)</b> to create a video sequence. A grand challenge in I2V generation is to maintain visual consistency throughout the video: existing methods often struggle to preserve the integrity of the subject, background, and style from the first frame, as well as ensure a fluid and logical progression within the video narrative. To mitigate these issues, we propose ConsistI2V, a diffusion-based method to enhance visual consistency for I2V generation. Specifically, we introduce (1) spatiotemporal attention over the first frame to maintain spatial and motion consistency, (2) noise initialization from the low-frequency band of the first frame to enhance layout consistency. These two approaches enable ConsistI2V to generate highly consistent videos. We also extend the proposed approaches to show their potential to improve consistency in auto-regressive long video generation and camera motion control. To verify the effectiveness of our method, we propose I2V-Bench, a comprehensive evaluation benchmark for I2V generation. Our automatic and human evaluation results demonstrate the superiority of ConsistI2V over existing methods.</p></p class="citation"></blockquote><h3 id=153277-analysis-of-deep-image-prior-and-exploiting-self-guidance-for-image-reconstruction-shijun-liang-et-al-2024>(153/277) Analysis of Deep Image Prior and Exploiting Self-Guidance for Image Reconstruction (Shijun Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shijun Liang, Evan Bell, Qing Qu, Rongrong Wang, Saiprasad Ravishankar. (2024)<br><strong>Analysis of Deep Image Prior and Exploiting Self-Guidance for Image Reconstruction</strong><br><button class=copy-to-clipboard title="Analysis of Deep Image Prior and Exploiting Self-Guidance for Image Reconstruction" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04097v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04097v2.pdf filename=2402.04097v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability of deep image prior (DIP) to recover high-quality images from incomplete or corrupted measurements has made it popular in inverse problems in image restoration and medical imaging including magnetic resonance imaging (MRI). However, conventional DIP suffers from severe overfitting and spectral bias effects. In this work, we first provide an analysis of how DIP recovers information from undersampled imaging measurements by analyzing the training dynamics of the underlying networks in the kernel regime for different architectures. This study sheds light on important underlying properties for DIP-based recovery. Current research suggests that incorporating a reference image as network input can enhance DIP&rsquo;s performance in image reconstruction compared to using random inputs. However, obtaining suitable reference images requires supervision, and raises practical difficulties. In an attempt to overcome this obstacle, we further introduce a self-driven reconstruction process that concurrently optimizes both the network weights and the input while eliminating the need for training data. Our method incorporates a novel denoiser regularization term which enables robust and stable joint estimation of both the network input and reconstructed image. We demonstrate that our self-guided method surpasses both the original DIP and modern <b>supervised</b> methods in terms of MR image reconstruction performance and outperforms previous DIP-based schemes for image inpainting.</p></p class="citation"></blockquote><h3 id=154277-polyp-ddpm-diffusion-based-semantic-polyp-synthesis-for-enhanced-segmentation-zolnamar-dorjsembe-et-al-2024>(154/277) Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation (Zolnamar Dorjsembe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zolnamar Dorjsembe, Hsing-Kuo Pao, Furen Xiao. (2024)<br><strong>Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation</strong><br><button class=copy-to-clipboard title="Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04031v1.pdf filename=2402.04031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of <b>data</b> <b>limitations,</b> high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real <b>data).</b> <b>Our</b> method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater <b>data</b> <b>augmentation</b> capabilities to improve segmentation models. The source code and pretrained weights for Polyp-DDPM are made publicly available at <a href=https://github.com/mobaidoctor/polyp-ddpm>https://github.com/mobaidoctor/polyp-ddpm</a>.</p></p class="citation"></blockquote><h3 id=155277-controllable-diverse-sampling-for-diffusion-based-motion-behavior-forecasting-yiming-xu-et-al-2024>(155/277) Controllable Diverse Sampling for Diffusion Based Motion Behavior Forecasting (Yiming Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Xu, Hao Cheng, Monika Sester. (2024)<br><strong>Controllable Diverse Sampling for Diffusion Based Motion Behavior Forecasting</strong><br><button class=copy-to-clipboard title="Controllable Diverse Sampling for Diffusion Based Motion Behavior Forecasting" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03981v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03981v1.pdf filename=2402.03981v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In autonomous driving tasks, trajectory prediction in complex traffic environments requires adherence to real-world context conditions and behavior multimodalities. Existing methods predominantly rely on prior assumptions or generative models trained on curated data to learn road agents&rsquo; stochastic behavior bounded by scene constraints. However, they often face mode averaging issues due to data imbalance and simplistic priors, and could even suffer from mode collapse due to unstable training and single ground truth supervision. These issues lead the existing methods to a loss of predictive diversity and adherence to the scene constraints. To address these challenges, we introduce a novel trajectory generator named Controllable Diffusion Trajectory (CDT), which integrates map information and social interactions into a <b>Transformer-based</b> conditional denoising diffusion model to guide the prediction of future trajectories. To ensure multimodality, we incorporate behavioral tokens to direct the trajectory&rsquo;s modes, such as going straight, turning right or left. Moreover, we incorporate the predicted endpoints as an alternative behavioral token into the CDT model to facilitate the prediction of accurate trajectories. Extensive experiments on the Argoverse 2 benchmark demonstrate that CDT excels in generating diverse and scene-compliant trajectories in complex urban settings.</p></p class="citation"></blockquote><h3 id=156277-eschernet-a-generative-model-for-scalable-view-synthesis-xin-kong-et-al-2024>(156/277) EscherNet: A Generative Model for Scalable View Synthesis (Xin Kong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, Andrew J. Davison. (2024)<br><strong>EscherNet: A Generative Model for Scalable View Synthesis</strong><br><button class=copy-to-clipboard title="EscherNet: A Generative Model for Scalable View Synthesis" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03908v1.pdf filename=2402.03908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis &ndash; it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses <b>zero-shot</b> novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: \url{https://kxhit.github.io/EscherNet}.</p></p class="citation"></blockquote><h3 id=157277-deep-msfop-multiple-spectral-filter-operators-preservation-in-deep-functional-maps-for-unsupervised-shape-matching-feifan-luo-et-al-2024>(157/277) Deep MSFOP: Multiple Spectral filter Operators Preservation in Deep Functional Maps for Unsupervised Shape Matching (Feifan Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feifan Luo, Qingsong Li, Ling Hu, Xinru Liu, Haojun Xu, Haibo Wang, Ting Li, Shengjun Liu. (2024)<br><strong>Deep MSFOP: Multiple Spectral filter Operators Preservation in Deep Functional Maps for Unsupervised Shape Matching</strong><br><button class=copy-to-clipboard title="Deep MSFOP: Multiple Spectral filter Operators Preservation in Deep Functional Maps for Unsupervised Shape Matching" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03904v1.pdf filename=2402.03904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel constraint called Multiple Spectral filter Operators Preservation (MSFOR) to compute functional maps and based on it, develop an efficient deep functional map architecture called Deep MSFOP for shape matching. The core idea is that, instead of using the general descriptor preservation constraint, we require our maps to preserve multiple spectral filter operators. This allows us to incorporate more informative geometrical information, contained in different frequency bands of functions, into the functional map computing. This can be confirmed by that some previous techniques like wavelet preservation and LBO commutativity are actually our special cases. Moreover, we also develop a very efficient way to compute the maps with MSFOP constraint, which can be conveniently embedded into the deep learning, especially having learnable filter operators. Utilizing the above results, we finally design our Deep MSFOP pipeline, equipped with a suitable <b>unsupervised</b> loss jointly penalizing the functional map and the underlying pointwise map. Our deep functional map has notable advantages, including that the functional map is more geometrically informative and guaranteed to be proper, and the computing is numerically stable. Extensive experimental results on different datasets demonstrate that our approach outperforms the existing state-of-the-art methods, especially in challenging settings like non-isometric and inconsistent topology datasets.</p></p class="citation"></blockquote><h3 id=158277-mobilevlm-v2-faster-and-stronger-baseline-for-vision-language-model-xiangxiang-chu-et-al-2024>(158/277) MobileVLM V2: Faster and Stronger Baseline for Vision Language Model (Xiangxiang Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, Chunhua Shen. (2024)<br><strong>MobileVLM V2: Faster and Stronger Baseline for Vision Language Model</strong><br><button class=copy-to-clipboard title="MobileVLM V2: Faster and Stronger Baseline for Vision Language Model" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03766v1.pdf filename=2402.03766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce MobileVLM V2, a family of significantly improved vision language models upon MobileVLM, which proves that a delicate orchestration of novel architectural design, an improved training scheme tailored for mobile VLMs, and rich high-quality dataset curation can substantially benefit VLMs&rsquo; performance. Specifically, MobileVLM V2 1.7B achieves better or on-par performance on standard VLM benchmarks compared with much larger VLMs at the 3B scale. Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our models will be released at <a href=https://github.com/Meituan-AutoML/MobileVLM>https://github.com/Meituan-AutoML/MobileVLM</a> .</p></p class="citation"></blockquote><h3 id=159277-attention-based-shape-and-gait-representations-learning-for-video-based-cloth-changing-person-re-identification-vuong-d-nguyen-et-al-2024>(159/277) Attention-based Shape and Gait Representations Learning for Video-based Cloth-Changing Person Re-Identification (Vuong D. Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vuong D. Nguyen, Samiha Mirza, Pranav Mantini, Shishir K. Shah. (2024)<br><strong>Attention-based Shape and Gait Representations Learning for Video-based Cloth-Changing Person Re-Identification</strong><br><button class=copy-to-clipboard title="Attention-based Shape and Gait Representations Learning for Video-based Cloth-Changing Person Re-Identification" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Graph Attention Networks<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03716v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03716v1.pdf filename=2402.03716v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current state-of-the-art Video-based Person Re-Identification (Re-ID) primarily relies on appearance features extracted by deep learning models. These methods are not applicable for long-term analysis in real-world scenarios where persons have changed clothes, making appearance information unreliable. In this work, we deal with the practical problem of Video-based Cloth-Changing Person Re-ID (VCCRe-ID) by proposing &ldquo;Attention-based Shape and Gait Representations Learning&rdquo; (ASGL) for VCCRe-ID. Our ASGL framework improves Re-ID performance under clothing variations by learning clothing-invariant gait cues using a Spatial-Temporal Graph Attention Network (ST-GAT). Given the 3D-skeleton-based spatial-temporal graph, our proposed ST-GAT comprises multi-head attention modules, which are able to enhance the robustness of gait embeddings under viewpoint changes and occlusions. The ST-GAT amplifies the important motion ranges and reduces the influence of noisy poses. Then, the multi-head learning module effectively reserves beneficial local temporal dynamics of movement. We also boost discriminative power of person representations by learning body shape cues using a <b>GAT.</b> Experiments on two large-scale VCCRe-ID datasets demonstrate that our proposed framework outperforms state-of-the-art methods by 12.2% in rank-1 accuracy and 7.0% in mAP.</p></p class="citation"></blockquote><h3 id=160277-foolsdedit-deceptively-steering-your-edits-towards-targeted-attribute-aware-distribution-qi-zhou-et-al-2024>(160/277) FoolSDEdit: Deceptively Steering Your Edits Towards Targeted Attribute-aware Distribution (Qi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Zhou, Dongxia Wang, Tianlin Li, Zhihong Xu, Yang Liu, Kui Ren, Wenhai Wang, Qing Guo. (2024)<br><strong>FoolSDEdit: Deceptively Steering Your Edits Towards Targeted Attribute-aware Distribution</strong><br><button class=copy-to-clipboard title="FoolSDEdit: Deceptively Steering Your Edits Towards Targeted Attribute-aware Distribution" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03705v1.pdf filename=2402.03705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Guided image synthesis methods, like SDEdit based on the diffusion model, excel at creating realistic images from user inputs such as stroke paintings. However, existing efforts mainly focus on image quality, often overlooking a key point: the diffusion model represents a data distribution, not individual images. This introduces a low but critical chance of generating images that contradict user intentions, raising ethical concerns. For example, a user inputting a stroke painting with female characteristics might, with some probability, get male faces from SDEdit. To expose this potential vulnerability, we aim to build an <b>adversarial</b> <b>attack</b> forcing SDEdit to generate a specific data distribution aligned with a specified attribute (e.g., female), without changing the input&rsquo;s attribute characteristics. We propose the Targeted Attribute Generative Attack (TAGA), using an attribute-aware objective function and optimizing the <b>adversarial</b> <b>noise</b> added to the input stroke painting. Empirical studies reveal that traditional <b>adversarial</b> <b>noise</b> struggles with TAGA, while natural perturbations like exposure and motion blur easily alter generated images&rsquo; attributes. To execute effective attacks, we introduce FoolSDEdit: We design a joint <b>adversarial</b> <b>exposure</b> and blur attack, adding exposure and motion blur to the stroke painting and optimizing them together. We optimize the execution strategy of various perturbations, framing it as a network architecture search problem. We create the SuperPert, a graph representing diverse execution strategies for different perturbations. After training, we obtain the optimized execution strategy for effective TAGA against SDEdit. Comprehensive experiments on two datasets show our method compelling SDEdit to generate a targeted attribute-aware data distribution, significantly outperforming baselines.</p></p class="citation"></blockquote><h3 id=161277-beam-beta-distribution-ray-denoising-for-multi-view-3d-object-detection-feng-liu-et-al-2024>(161/277) BEAM: Beta Distribution Ray Denoising for Multi-view 3D Object Detection (Feng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Liu, Tengteng Huang, Qianjing Zhang, Haotian Yao, Chi Zhang, Fang Wan, Qixiang Ye, Yanzhao Zhou. (2024)<br><strong>BEAM: Beta Distribution Ray Denoising for Multi-view 3D Object Detection</strong><br><button class=copy-to-clipboard title="BEAM: Beta Distribution Ray Denoising for Multi-view 3D Object Detection" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03634v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03634v1.pdf filename=2402.03634v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-view 3D <b>object</b> <b>detectors</b> struggle with duplicate predictions due to the lack of depth information, resulting in false positive detections. In this study, we introduce BEAM, a novel Beta Distribution Ray Denoising approach that can be applied to any DETR-style multi-view 3D detector to explicitly incorporate structure prior knowledge of the scene. By generating rays from cameras to <b>objects</b> <b>and</b> sampling spatial denoising queries from the Beta distribution family along these rays, BEAM enhances the model&rsquo;s ability to distinguish spatial hard negative samples arising from ambiguous depths. BEAM is a plug-and-play technique that adds only marginal computational costs during training, while impressively preserving the inference speed. Extensive experiments and ablation studies on the NuScenes dataset demonstrate significant improvements over strong baselines, outperforming the state-of-the-art method StreamPETR by 1.9% mAP. The code will be available at <a href=https://github.com/LiewFeng/BEAM>https://github.com/LiewFeng/BEAM</a>.</p></p class="citation"></blockquote><h3 id=162277-grasp-graph-structured-pyramidal-whole-slide-image-representation-ali-khajegili-mirabadi-et-al-2024>(162/277) GRASP: GRAph-Structured Pyramidal Whole Slide Image Representation (Ali Khajegili Mirabadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Khajegili Mirabadi, Graham Archibald, Amirali Darbandsari, Alberto Contreras-Sanz, Ramin Ebrahim Nakhli, Maryam Asadi, Allen Zhang, C. Blake Gilks, Peter Black, Gang Wang, Hossein Farahani, Ali Bashashati. (2024)<br><strong>GRASP: GRAph-Structured Pyramidal Whole Slide Image Representation</strong><br><button class=copy-to-clipboard title="GRASP: GRAph-Structured Pyramidal Whole Slide Image Representation" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Multiple Instance Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03592v1.pdf filename=2402.03592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cancer subtyping is one of the most challenging tasks in digital pathology, where <b>Multiple</b> <b>Instance</b> <b>Learning</b> (MIL) by processing gigapixel whole slide images (WSIs) has been in the spotlight of recent research. However, MIL approaches do not take advantage of inter- and intra-magnification information contained in WSIs. In this work, we present GRASP, a novel graph-structured multi-magnification framework for processing WSIs in digital pathology. Our approach is designed to dynamically emulate the pathologist&rsquo;s behavior in handling WSIs and benefits from the hierarchical structure of WSIs. GRASP, which introduces a convergence-based node aggregation instead of traditional pooling mechanisms, outperforms state-of-the-art methods over two distinct cancer datasets by a margin of up to 10% balanced accuracy, while being 7 times smaller than the closest-performing state-of-the-art model in terms of the number of parameters. Our results show that GRASP is dynamic in finding and consulting with different magnifications for subtyping cancers and is reliable and stable across different hyperparameters. The model&rsquo;s behavior has been evaluated by two expert pathologists confirming the interpretability of the model&rsquo;s dynamic. We also provide a theoretical foundation, along with empirical evidence, for our work, explaining how GRASP interacts with different magnifications and nodes in the graph to make predictions. We believe that the strong characteristics yet simple structure of GRASP will encourage the development of interpretable, structure-based designs for WSI representation in digital pathology. Furthermore, we publish two large graph datasets of rare Ovarian and Bladder cancers to contribute to the field.</p></p class="citation"></blockquote><h2 id=statml-6>stat.ML (6)</h2><h3 id=163277-pac-bayesian-adversarially-robust-generalization-bounds-for-graph-neural-network-tan-sun-et-al-2024>(163/277) PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network (Tan Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tan Sun, Junhong Lin. (2024)<br><strong>PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network</strong><br><button class=copy-to-clipboard title="PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 80<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Message-Passing, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04038v1.pdf filename=2402.04038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have gained popularity for various <b>graph-related</b> <b>tasks.</b> <b>However,</b> similar to deep neural networks, <b>GNNs</b> are also vulnerable to <b>adversarial</b> <b>attacks.</b> Empirical studies have shown that adversarially robust generalization has a pivotal role in establishing effective defense algorithms against <b>adversarial</b> <b>attacks.</b> In this paper, we contribute by providing adversarially robust generalization bounds for two kinds of popular <b>GNNs,</b> <b>graph</b> <b>convolutional</b> <b>network</b> <b>(GCN)</b> and message passing <b>graph</b> <b>neural</b> <b>network,</b> using the PAC-Bayesian framework. Our result reveals that spectral norm of the diffusion matrix on the <b>graph</b> <b>and</b> <b>spectral</b> norm of the weights as well as the perturbation factor govern the robust generalization bounds of both models. Our bounds are nontrivial generalizations of the results developed in (Liao et al., 2020) from the standard setting to <b>adversarial</b> <b>setting</b> while avoiding exponential dependence of the maximum node degree. As corollaries, we derive better PAC-Bayesian robust generalization bounds for <b>GCN</b> in the standard setting, which improve the bounds in (Liao et al., 2020) by avoiding exponential dependence on the maximum node degree.</p></p class="citation"></blockquote><h3 id=164277-gaussian-process-regression-with-sliced-wasserstein-weisfeiler-lehman-graph-kernels-raphaël-carpintero-perez-et-al-2024>(164/277) Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels (Raphaël Carpintero Perez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raphaël Carpintero Perez, Sébastien da Veiga, Josselin Garnier, Brian Staber. (2024)<br><strong>Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels</strong><br><button class=copy-to-clipboard title="Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 40<br>Keywords: Graph Classification, Gaussian Process, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03838v1.pdf filename=2402.03838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> <b>learning</b> has recently garnered significant attention in the field of computational physics due to its ability to effectively extract complex patterns for tasks like solving partial differential equations, or predicting material properties. Traditionally, such datasets consist of inputs given as meshes with a large number of nodes representing the problem geometry (seen as <b>graphs),</b> <b>and</b> corresponding outputs obtained with a numerical solver. This means the <b>supervised</b> <b>learning</b> model must be able to handle large and sparse <b>graphs</b> <b>with</b> continuous node attributes. In this work, we focus on <b>Gaussian</b> <b>process</b> regression, for which we introduce the Sliced Wasserstein Weisfeiler-Lehman (SWWL) <b>graph</b> <b>kernel.</b> In contrast to existing <b>graph</b> <b>kernels,</b> the proposed SWWL kernel enjoys positive definiteness and a drastic complexity reduction, which makes it possible to process datasets that were previously impossible to handle. The new kernel is first validated on <b>graph</b> <b>classification</b> for molecular datasets, where the input <b>graphs</b> <b>have</b> a few tens of nodes. The efficiency of the SWWL kernel is then illustrated on <b>graph</b> <b>regression</b> in computational fluid dynamics and solid mechanics, where the input <b>graphs</b> <b>are</b> made up of tens of thousands of nodes.</p></p class="citation"></blockquote><h3 id=165277-statistical-test-for-anomaly-detections-by-variational-auto-encoders-daiki-miwa-et-al-2024>(165/277) Statistical Test for Anomaly Detections by Variational Auto-Encoders (Daiki Miwa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daiki Miwa, Tomohiro Shiraishi, Vo Nguyen Le Duy, Teruyuki Katsuoka, Ichiro Takeuchi. (2024)<br><strong>Statistical Test for Anomaly Detections by Variational Auto-Encoders</strong><br><button class=copy-to-clipboard title="Statistical Test for Anomaly Detections by Variational Auto-Encoders" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03724v1.pdf filename=2402.03724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we consider the reliability assessment of anomaly detection (AD) using <b>Variational</b> <b>Autoencoder</b> (VAE). Over the last decade, VAE-based AD has been actively studied in various perspective, from method development to applied research. However, when the results of ADs are used in high-stakes decision-making, such as in medical diagnosis, it is necessary to ensure the reliability of the detected anomalies. In this study, we propose the VAE-AD Test as a method for quantifying the statistical reliability of VAE-based AD within the framework of statistical testing. Using the VAE-AD Test, the reliability of the anomaly regions detected by a VAE can be quantified in the form of p-values. This means that if an anomaly is declared when the p-value is below a certain threshold, it is possible to control the probability of false detection to a desired level. Since the VAE-AD Test is constructed based on a new statistical inference framework called selective inference, its validity is theoretically guaranteed in finite samples. To demonstrate the validity and effectiveness of the proposed VAE-AD Test, numerical experiments on artificial data and applications to brain image analysis are conducted.</p></p class="citation"></blockquote><h3 id=166277-interpretable-multi-source-data-fusion-through-latent-variable-gaussian-process-sandipp-krishnan-ravi-et-al-2024>(166/277) Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process (Sandipp Krishnan Ravi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sandipp Krishnan Ravi, Yigitcan Comlek, Wei Chen, Arjun Pathak, Vipul Gupta, Rajnikant Umretiya, Andrew Hoffman, Ghanshyam Pilania, Piyush Pandita, Sayan Ghosh, Nathaniel Mckeever, Liping Wang. (2024)<br><strong>Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process</strong><br><button class=copy-to-clipboard title="Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04146v1.pdf filename=2402.04146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data). The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities. A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources. However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization. Towards resolving this issue, a multi-source data fusion framework based on Latent Variable <b>Gaussian</b> <b>Process</b> (LVGP) is proposed. The individual data sources are tagged as a characteristic categorical variable that are mapped into a physically interpretable latent space, allowing the development of source-aware data fusion modeling. Additionally, a dissimilarity metric based on the latent variables of LVGP is introduced to study and understand the differences in the sources of data. The proposed approach is demonstrated on and analyzed through two mathematical (representative parabola problem, 2D Ackley function) and two materials science (design of FeCrAl and SmCoFe alloys) case studies. From the case studies, it is observed that compared to using single-source and source unaware ML models, the proposed multi-source data fusion framework can provide better predictions for sparse-data problems, interpretability regarding the sources, and enhanced modeling capabilities by taking advantage of the correlations and relationships among different sources.</p></p class="citation"></blockquote><h3 id=167277-a-general-theory-for-kernel-packets-from-state-space-model-to-compactly-supported-basis-liang-ding-et-al-2024>(167/277) A General Theory for Kernel Packets: from state space model to compactly supported basis (Liang Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Ding, Rui Tuo. (2024)<br><strong>A General Theory for Kernel Packets: from state space model to compactly supported basis</strong><br><button class=copy-to-clipboard title="A General Theory for Kernel Packets: from state space model to compactly supported basis" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04022v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04022v3.pdf filename=2402.04022v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is well known that the state space (SS) model formulation of a <b>Gaussian</b> <b>process</b> (GP) can lower its training and prediction time both to O(n) for n data points. We prove that an $m$-dimensional SS model formulation of GP is equivalent to a concept we introduce as the general right Kernel Packet (KP): a transformation for the GP covariance function $K$ such that $\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j \leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}<em>t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. We extend this idea to the backward SS model formulation of the GP, leading to the concept of the left KP for next $m$ consecutive points: $\sum</em>{i=0}^{m}b_i{D}<em>t^{(j)}K(t,t</em>{m+i})=0$ for any $t\geq t_{2m}$. By combining both left and right KPs, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported KP functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and $j=0,\cdots,m-1$. KPs further reduce the prediction time of GP to O(log n) or even O(1), can be applied to more general problems involving the derivative of GPs, and have multi-dimensional generalization for scattered data.</p></p class="citation"></blockquote><h3 id=168277-subsampling-is-not-magic-why-large-batch-sizes-work-for-differentially-private-stochastic-optimisation-ossi-räisä-et-al-2024>(168/277) Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation (Ossi Räisä et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ossi Räisä, Joonas Jälkö, Antti Honkela. (2024)<br><strong>Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation</strong><br><button class=copy-to-clipboard title="Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CR, cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03990v1.pdf filename=2402.03990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the effect of the batch size to the total gradient variance in differentially private <b>stochastic</b> <b>gradient</b> <b>descent</b> (DP-SGD), seeking a theoretical explanation for the usefulness of large batch sizes. As DP-SGD is the basis of modern DP deep learning, its properties have been widely studied, and recent works have empirically found large batch sizes to be beneficial. However, theoretical explanations of this benefit are currently heuristic at best. We first observe that the total gradient variance in DP-SGD can be decomposed into subsampling-induced and noise-induced variances. We then prove that in the limit of an infinite number of iterations, the effective noise-induced variance is invariant to the batch size. The remaining subsampling-induced variance decreases with larger batch sizes, so large batches reduce the effective total gradient variance. We confirm numerically that the asymptotic regime is relevant in practical settings when the batch size is not small, and find that outside the asymptotic regime, the total gradient variance decreases even more with large batch sizes. We also find a sufficient condition that implies that large batch sizes similarly reduce effective DP noise variance for one iteration of DP-SGD.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=169277-reborn-reinforcement-learned-boundary-segmentation-with-iterative-training-for-unsupervised-asr-liang-hsuan-tseng-et-al-2024>(169/277) REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR (Liang-Hsuan Tseng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang-Hsuan Tseng, En-Pei Hu, Cheng-Han Chiang, Yuan Tseng, Hung-yi Lee, Lin-shan Lee, Shao-Hua Sun. (2024)<br><strong>REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR</strong><br><button class=copy-to-clipboard title="REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-SD, eess-AS, eess.AS<br>Keyword Score: 70<br>Keywords: Reinforcement Learning, Supervised Learning, Unsupervised Learning, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03988v1.pdf filename=2402.03988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> aims to learn the mapping between the <b>speech</b> <b>signal</b> and its corresponding textual transcription without the supervision of paired <b>speech-text</b> <b>data.</b> A word/phoneme in the <b>speech</b> <b>signal</b> is represented by a segment of <b>speech</b> <b>signal</b> with variable length and unknown boundary, and this segmental structure makes learning the mapping between <b>speech</b> <b>and</b> text challenging, especially without paired data. In this paper, we propose REBORN, <b>Reinforcement-Learned</b> <b>Boundary</b> Segmentation with Iterative Training for <b>Unsupervised</b> <b>ASR.</b> REBORN alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in <b>speech</b> <b>signals</b> and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription. Since <b>supervised</b> data for training the segmentation model is not available, we use <b>reinforcement</b> <b>learning</b> to train the segmentation model to favor segmentations that yield phoneme sequence predictions with a lower <b>perplexity.</b> We conduct extensive experiments and find that under the same setting, REBORN outperforms all prior <b>unsupervised</b> <b>ASR</b> models on LibriSpeech, TIMIT, and five non-English languages in Multilingual LibriSpeech. We comprehensively analyze why the boundaries learned by REBORN improve the <b>unsupervised</b> <b>ASR</b> performance.</p></p class="citation"></blockquote><h3 id=170277-listen-chat-and-edit-text-guided-soundscape-modification-for-enhanced-auditory-experience-xilin-jiang-et-al-2024>(170/277) Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced Auditory Experience (Xilin Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xilin Jiang, Cong Han, Yinghao Aaron Li, Nima Mesgarani. (2024)<br><strong>Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced Auditory Experience</strong><br><button class=copy-to-clipboard title="Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced Auditory Experience" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-SD, eess-AS, eess.AS<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Zero-shot, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03710v1.pdf filename=2402.03710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In daily life, we encounter a variety of sounds, both desirable and undesirable, with limited control over their presence and volume. Our work introduces &ldquo;Listen, Chat, and Edit&rdquo; (LCE), a novel <b>multimodal</b> sound mixture editor that modifies each sound source in a mixture based on user-provided text instructions. LCE distinguishes itself with a user-friendly chat interface and its unique ability to edit multiple sound sources simultaneously within a mixture, without needing to separate them. Users input open-vocabulary text <b>prompts,</b> which are interpreted by a <b>large</b> <b>language</b> <b>model</b> to create a semantic filter for editing the sound mixture. The system then decomposes the mixture into its components, applies the semantic filter, and reassembles it into the desired output. We developed a 160-hour dataset with over 100k mixtures, including speech and various audio sources, along with text <b>prompts</b> for diverse editing tasks like extraction, removal, and volume control. Our experiments demonstrate significant improvements in signal quality across all editing tasks and robust performance in <b>zero-shot</b> scenarios with varying numbers and types of sound sources.</p></p class="citation"></blockquote><h2 id=cscr-5>cs.CR (5)</h2><h3 id=171277-explainable-adversarial-learning-framework-on-physical-layer-secret-keys-combating-malicious-reconfigurable-intelligent-surface-zhuangkun-wei-et-al-2024>(171/277) Explainable Adversarial Learning Framework on Physical Layer Secret Keys Combating Malicious Reconfigurable Intelligent Surface (Zhuangkun Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuangkun Wei, Wenxiu Hu, Weisi Guo. (2024)<br><strong>Explainable Adversarial Learning Framework on Physical Layer Secret Keys Combating Malicious Reconfigurable Intelligent Surface</strong><br><button class=copy-to-clipboard title="Explainable Adversarial Learning Framework on Physical Layer Secret Keys Combating Malicious Reconfigurable Intelligent Surface" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 70<br>Keywords: Adversarial Learning, Explainable AI, Generative Adversarial Network, Generative Adversarial Network, Mutual Information, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06663v1.pdf filename=2402.06663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of reconfigurable intelligent surfaces (RIS) is a double-edged sword to physical layer security (PLS). Whilst a legitimate RIS can yield beneficial impacts including increased channel randomness to enhance physical layer secret key generation (PL-SKG), malicious RIS can poison legitimate channels and crack most of existing PL-SKGs. In this work, we propose an <b>adversarial</b> <b>learning</b> framework between legitimate parties (namely Alice and Bob) to address this Man-in-the-middle malicious RIS (MITM-RIS) eavesdropping. First, the theoretical <b>mutual</b> <b>information</b> gap between legitimate pairs and MITM-RIS is deduced. Then, Alice and Bob leverage <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> to learn to achieve a common feature surface that does not have <b>mutual</b> <b>information</b> overlap with MITM-RIS. Next, we aid signal processing interpretation of black-box neural networks by using a symbolic <b>explainable</b> <b>AI</b> (xAI) representation. These symbolic terms of dominant neurons aid feature engineering-based validation and future design of PLS common feature space. <b>Simulation</b> results show that our proposed <b>GAN-based</b> and symbolic-based PL-SKGs can achieve high key agreement rates between legitimate users, and is even resistant to MITM-RIS Eve with the knowledge of legitimate feature generation (NNs or formulas). This therefore paves the way to secure wireless communications with untrusted reflective devices in future 6G.</p></p class="citation"></blockquote><h3 id=172277-llm-agents-can-autonomously-hack-websites-richard-fang-et-al-2024>(172/277) LLM Agents can Autonomously Hack Websites (Richard Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang. (2024)<br><strong>LLM Agents can Autonomously Hack Websites</strong><br><button class=copy-to-clipboard title="LLM Agents can Autonomously Hack Websites" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06664v1.pdf filename=2402.06664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these <b>LLMs</b> can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how <b>LLM</b> agents would affect cybersecurity. However, not much is known about the offensive capabilities of <b>LLM</b> agents. In this work, we show that <b>LLM</b> agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that <b>GPT-4</b> is capable of such hacks, but existing open-source models are not. Finally, we show that <b>GPT-4</b> is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=173277-use-of-multi-cnns-for-section-analysis-in-static-malware-detection-tony-quertier-et-al-2024>(173/277) Use of Multi-CNNs for Section Analysis in Static Malware Detection (Tony Quertier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tony Quertier, Grégoire Barrué. (2024)<br><strong>Use of Multi-CNNs for Section Analysis in Static Malware Detection</strong><br><button class=copy-to-clipboard title="Use of Multi-CNNs for Section Analysis in Static Malware Detection" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04102v1.pdf filename=2402.04102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing research on malware detection focuses almost exclusively on the detection rate. However, in some cases, it is also important to understand the results of our algorithm, or to obtain more information, such as where to investigate in the file for an analyst. In this aim, we propose a new model to analyze Portable Executable files. Our method consists in splitting the files in different sections, then transform each section into an image, in order to train <b>convolutional</b> <b>neural</b> <b>networks</b> to treat specifically each identified section. Then we use all these scores returned by <b>CNNs</b> to compute a final detection score, using models that enable us to improve our analysis of the importance of each section in the final score.</p></p class="citation"></blockquote><h3 id=174277-lipstick-corruptibility-aware-and-explainable-graph-neural-network-based-oracle-less-attack-on-logic-locking-yeganeh-aghamohammadi-et-al-2024>(174/277) LIPSTICK: Corruptibility-Aware and Explainable Graph Neural Network-based Oracle-Less Attack on Logic Locking (Yeganeh Aghamohammadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeganeh Aghamohammadi, Amin Rezaei. (2024)<br><strong>LIPSTICK: Corruptibility-Aware and Explainable Graph Neural Network-based Oracle-Less Attack on Logic Locking</strong><br><button class=copy-to-clipboard title="LIPSTICK: Corruptibility-Aware and Explainable Graph Neural Network-based Oracle-Less Attack on Logic Locking" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Graph Neural Network, Zero Trust<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04235v1.pdf filename=2402.04235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a <b>zero-trust</b> <b>fabless</b> paradigm, designers are increasingly concerned about hardware-based attacks on the semiconductor supply chain. Logic locking is a design-for-trust method that adds extra key-controlled gates in the circuits to prevent hardware intellectual property theft and overproduction. While attackers have traditionally relied on an oracle to attack logic-locked circuits, machine learning attacks have shown the ability to retrieve the secret key even without access to an oracle. In this paper, we first examine the limitations of state-of-the-art machine learning attacks and argue that the use of key hamming distance as the sole model-guiding structural metric is not always useful. Then, we develop, train, and test a corruptibility-aware <b>graph</b> <b>neural</b> <b>network-based</b> oracle-less attack on logic locking that takes into consideration both the structure and the behavior of the circuits. Our model is explainable in the sense that we analyze what the machine learning model has interpreted in the training process and how it can perform a successful attack. Chip designers may find this information beneficial in securing their designs while avoiding incremental fixes.</p></p class="citation"></blockquote><h3 id=175277-cops-a-compact-on-device-pipeline-for-real-time-smishing-detection-harichandana-b-s-s-et-al-2024>(175/277) COPS: A Compact On-device Pipeline for real-time Smishing detection (Harichandana B S S et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harichandana B S S, Sumit Kumar, Manjunath Bhimappa Ujjinakoppa, Barath Raj Kandur Raja. (2024)<br><strong>COPS: A Compact On-device Pipeline for real-time Smishing detection</strong><br><button class=copy-to-clipboard title="COPS: A Compact On-device Pipeline for real-time Smishing detection" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04173v1.pdf filename=2402.04173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smartphones have become indispensable in our daily lives and can do almost everything, from communication to online shopping. However, with the increased usage, cybercrime aimed at mobile devices is rocketing. Smishing attacks, in particular, have observed a significant upsurge in recent years. This problem is further exacerbated by the perpetrator creating new deceptive websites daily, with an average life cycle of under 15 hours. This renders the standard practice of keeping a database of malicious URLs ineffective. To this end, we propose a novel on-device pipeline: COPS that intelligently identifies features of fraudulent messages and URLs to alert the user in real-time. COPS is a lightweight pipeline with a detection module based on the Disentangled <b>Variational</b> <b>Autoencoder</b> of size 3.46MB for smishing and URL phishing detection, and we benchmark it on open datasets. We achieve an accuracy of 98.15% and 99.5%, respectively, for both tasks, with a false negative and false positive rate of a mere 0.037 and 0.015, outperforming previous works with the added advantage of ensuring real-time alerts on resource-constrained devices.</p></p class="citation"></blockquote><h2 id=csai-20>cs.AI (20)</h2><h3 id=176277-self-discover-large-language-models-self-compose-reasoning-structures-pei-zhou-et-al-2024>(176/277) Self-Discover: Large Language Models Self-Compose Reasoning Structures (Pei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, Denny Zhou, Swaroop Mishra, Huaixiu Steven Zheng. (2024)<br><strong>Self-Discover: Large Language Models Self-Compose Reasoning Structures</strong><br><button class=copy-to-clipboard title="Self-Discover: Large Language Models Self-Compose Reasoning Structures" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 70<br>Keywords: GPT, GPT-4, PaLM, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03620v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03620v1.pdf filename=2402.03620v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce SELF-DISCOVER, a general framework for <b>LLMs</b> to self-discover the task-intrinsic <b>reasoning</b> structures to tackle complex <b>reasoning</b> problems that are challenging for typical <b>prompting</b> methods. Core to the framework is a self-discovery process where <b>LLMs</b> select multiple atomic <b>reasoning</b> modules such as critical thinking and step-by-step thinking, and compose them into an explicit <b>reasoning</b> structure for <b>LLMs</b> to follow during decoding. SELF-DISCOVER substantially improves <b>GPT-4</b> and <b>PaLM</b> 2&rsquo;s performance on challenging <b>reasoning</b> benchmarks such as BigBench-Hard, grounded agent <b>reasoning,</b> and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered <b>reasoning</b> structures are universally applicable across model families: from <b>PaLM</b> 2-L to <b>GPT-4,</b> and from <b>GPT-4</b> to Llama2, and share commonalities with human <b>reasoning</b> patterns.</p></p class="citation"></blockquote><h3 id=177277-advancing-legal-reasoning-the-integration-of-ai-to-navigate-complexities-and-biases-in-global-jurisprudence-with-semi-automated-arbitration-processes-saaps-michael-deshazer-2024>(177/277) Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs) (Michael De&rsquo;Shazer, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael De&rsquo;Shazer. (2024)<br><strong>Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)</strong><br><button class=copy-to-clipboard title="Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CY, cs-HC, cs.AI<br>Keyword Score: 40<br>Keywords: Fairness, Generative AI, GPT, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04140v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04140v2.pdf filename=2402.04140v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically <b>generative</b> <b>AI)</b> in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions. By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law. SHIRLEY is the name of the AI-based application (built on top of OpenAI&rsquo;s <b>GPT</b> technology), focusing on detecting logical inconsistencies and biases across various legal decisions. SHIRLEY analysis is aggregated and is accompanied by a comparison-oriented AI-based application called SAM (also an ALM) to identify relative deviations in SHIRLEY bias detections. Further, a CRITIC is generated within semi-autonomous arbitration process via the ALM, SARA. A novel approach is introduced in the utilization of an AI arbitrator to critically evaluate biases and qualitative-in-nature nuances identified by the aforementioned AI applications (SAM in concert with SHIRLEY), based on the Hague Rules on Business and Human Rights Arbitration. This Semi-Automated Arbitration Process (SAAP) aims to uphold the integrity and <b>fairness</b> of legal judgments by ensuring a nuanced debate-resultant &ldquo;understanding&rdquo; through a hybrid system of AI and human-based collaborative analysis.</p></p class="citation"></blockquote><h3 id=178277-scemqa-a-scientific-college-entrance-level-multimodal-question-answering-benchmark-zhenwen-liang-et-al-2024>(178/277) SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark (Zhenwen Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, Tianyu Yang, Jiajun Jiao, Renjie Pi, Jipeng Zhang, Xiangliang Zhang. (2024)<br><strong>SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark</strong><br><button class=copy-to-clipboard title="SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Question Answering, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05138v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05138v1.pdf filename=2402.05138v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper introduces SceMQA, a novel benchmark for scientific <b>multimodal</b> <b>question</b> <b>answering</b> at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI models&rsquo; abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied <b>questions</b> <b>to</b> facilitate a more thorough and accurate assessment of <b>reasoning</b> capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs), across various experimental settings. The results show that further research and development are needed in developing more capable MLLM, as highlighted by only 50% to 60% accuracy achieved by the strongest models. Our benchmark and analysis will be available at <a href=https://scemqa.github.io/>https://scemqa.github.io/</a></p></p class="citation"></blockquote><h3 id=179277-read-to-play-r2-play-decision-transformer-with-multimodal-game-instruction-yonggang-jin-et-al-2024>(179/277) Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction (Yonggang Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu Xiang, Shawn Yue, Stephen W. Huang, Wenhu Chen, Zhaofeng He, Jie Fu. (2024)<br><strong>Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction</strong><br><button class=copy-to-clipboard title="Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Reinforcement Learning, Transformer, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04154v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04154v2.pdf filename=2402.04154v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within <b>Reinforcement</b> <b>Learning.</b> However, these works encounter challenges in extending their capabilities to new tasks. Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction. However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks. This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay <b>instructions,</b> <b>thereby</b> facilitating a &ldquo;read-to-play&rdquo; capability. Drawing inspiration from the success of <b>multimodal</b> <b>instruction</b> <b>tuning</b> in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of <b>multimodal</b> game <b>instructions</b> <b>to</b> incorporate <b>instruction</b> <b>tuning</b> into a decision <b>transformer.</b> Experimental results demonstrate that incorporating <b>multimodal</b> game <b>instructions</b> <b>significantly</b> enhances the decision <b>transformer&rsquo;s</b> multitasking and generalization capabilities.</p></p class="citation"></blockquote><h3 id=180277-comparing-abstraction-in-humans-and-large-language-models-using-multimodal-serial-reproduction-sreejan-kumar-et-al-2024>(180/277) Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction (Sreejan Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sreejan Kumar, Raja Marjieh, Byron Zhang, Declan Campbell, Michael Y. Hu, Umang Bhatt, Brenden Lake, Thomas L. Griffiths. (2024)<br><strong>Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction</strong><br><button class=copy-to-clipboard title="Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI, q-bio-NC<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03618v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03618v1.pdf filename=2402.03618v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans extract useful abstractions of the world from noisy sensory data. Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions. Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language. To investigate the effect language on the formation of abstractions, we implement a novel <b>multimodal</b> serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa. We ran unimodal and <b>multimodal</b> chains with both humans and <b>GPT-4</b> and find that adding language as a modality has a larger effect on human reproductions than <b>GPT-4&rsquo;s.</b> This suggests human visual and linguistic representations are more dissociable than those of <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=181277-can-generative-agents-predict-emotion-ciaran-regan-et-al-2024>(181/277) Can Generative Agents Predict Emotion? (Ciaran Regan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ciaran Regan, Nanami Iwahashi, Shogo Tanaka, Mizuki Oka. (2024)<br><strong>Can Generative Agents Predict Emotion?</strong><br><button class=copy-to-clipboard title="Can Generative Agents Predict Emotion?" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 30<br>Keywords: In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04232v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04232v2.pdf filename=2402.04232v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of <b>LLMs</b> is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative <b>LLM</b> agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event. Finally, the new experience is then added to the agents memory to be used in the creation of future norms. By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios. The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary. We hope that this paper is another step towards the alignment of generative agents.</p></p class="citation"></blockquote><h3 id=182277-revorder-a-novel-method-for-enhanced-arithmetic-in-language-models-si-shen-et-al-2024>(182/277) RevOrder: A Novel Method for Enhanced Arithmetic in Language Models (Si Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Si Shen, Peijun Shen, Danhao Zhu. (2024)<br><strong>RevOrder: A Novel Method for Enhanced Arithmetic in Language Models</strong><br><button class=copy-to-clipboard title="RevOrder: A Novel Method for Enhanced Arithmetic in Language Models" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03822v1.pdf filename=2402.03822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts <b>LLM</b> performance in division tasks, particularly with <b>large</b> <b>numbers</b> <b>where</b> traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to <b>fine-tune</b> the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.</p></p class="citation"></blockquote><h3 id=183277-position-paper-against-spurious-sparks---dovelating-inflated-ai-claims-patrick-altmeyer-et-al-2024>(183/277) Position Paper: Against Spurious Sparks $-$ Dovelating Inflated AI Claims (Patrick Altmeyer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Altmeyer, Andrew M. Demetriou, Antony Bartlett, Cynthia C. S. Liem. (2024)<br><strong>Position Paper: Against Spurious Sparks $-$ Dovelating Inflated AI Claims</strong><br><button class=copy-to-clipboard title="Position Paper: Against Spurious Sparks $-$ Dovelating Inflated AI Claims" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03962v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03962v2.pdf filename=2402.03962v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans have a tendency to see &lsquo;human&rsquo;-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to <b>LLMs.</b> In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI research outcomes.</p></p class="citation"></blockquote><h3 id=184277-cadren-contextual-anchor-driven-relational-network-for-controllable-cross-graphs-node-importance-estimation-zijie-zhong-et-al-2024>(184/277) CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation (Zijie Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijie Zhong, Yunhui Zhang, Ziyi Chang, Zengchang Qin. (2024)<br><strong>CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation</strong><br><button class=copy-to-clipboard title="CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: 68T07, cs-AI, cs-CL, cs-IR, cs.AI<br>Keyword Score: 20<br>Keywords: Zero-shot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05135v1.pdf filename=2402.05135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Node Importance Estimation (NIE) is crucial for integrating external information into <b>Large</b> <b>Language</b> <b>Models</b> through Retriever-Augmented Generation. Traditional methods, focusing on static, single-graph characteristics, lack adaptability to new graphs and user-specific requirements. CADReN, our proposed method, addresses these limitations by introducing a Contextual Anchor (CA) mechanism. This approach enables the network to assess node importance relative to the CA, considering both structural and semantic features within Knowledge Graphs (KGs). Extensive experiments show that CADReN achieves better performance in cross-graph NIE task, with <b>zero-shot</b> prediction ability. CADReN is also proven to match the performance of previous models on single-graph NIE task. Additionally, we introduce and opensource two new datasets, RIC200 and WK1K, specifically designed for cross-graph NIE research, providing a valuable resource for future developments in this domain.</p></p class="citation"></blockquote><h3 id=185277-quantagent-seeking-holy-grail-in-trading-by-self-improving-large-language-model-saizhuo-wang-et-al-2024>(185/277) QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model (Saizhuo Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saizhuo Wang, Hang Yuan, Lionel M. Ni, Jian Guo. (2024)<br><strong>QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model</strong><br><button class=copy-to-clipboard title="QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI, q-fin-CP<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03755v1.pdf filename=2402.03755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous agents based on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> that devise plans and tackle real-world challenges have gained prominence.However, tailoring these agents for specialized domains like quantitative investment remains a formidable task. The core challenge involves efficiently building and integrating a domain-specific knowledge base for the agent&rsquo;s learning process. This paper introduces a principled framework to address this challenge, comprising a two-layer loop.In the inner loop, the agent refines its responses by drawing from its knowledge base, while in the outer loop, these responses are tested in real-world scenarios to automatically enhance the knowledge base with new insights.We demonstrate that our approach enables the agent to progressively approximate optimal behavior with provable efficiency.Furthermore, we instantiate this framework through an autonomous agent for mining trading signals named QuantAgent. Empirical results showcase QuantAgent&rsquo;s capability in uncovering viable financial signals and enhancing the accuracy of financial forecasts.</p></p class="citation"></blockquote><h3 id=186277-limits-of-large-language-models-in-debating-humans-james-flamino-et-al-2024>(186/277) Limits of Large Language Models in Debating Humans (James Flamino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James Flamino, Mohammed Shahid Modi, Boleslaw K. Szymanski, Brendan Cross, Colton Mikolajczyk. (2024)<br><strong>Limits of Large Language Models in Debating Humans</strong><br><button class=copy-to-clipboard title="Limits of Large Language Models in Debating Humans" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-HC, cs.AI, stat-AP<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06049v1.pdf filename=2402.06049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown remarkable promise in their ability to interact proficiently with humans. Subsequently, their potential use as artificial confederates and surrogates in sociological experiments involving conversation is an exciting prospect. But how viable is this idea? This paper endeavors to test the limits of current-day <b>LLMs</b> with a pre-registered study integrating real people with <b>LLM</b> agents acting as people. The study focuses on debate-based opinion consensus formation in three environments: humans only, agents and humans, and agents only. Our goal is to understand how <b>LLM</b> agents influence humans, and how capable they are in debating like humans. We find that <b>LLMs</b> can blend in and facilitate human productivity but are less convincing in debate, with their behavior ultimately deviating from human&rsquo;s. We elucidate these primary failings and anticipate that <b>LLMs</b> must evolve further before being viable debaters.</p></p class="citation"></blockquote><h3 id=187277-improving-contextual-congruence-across-modalities-for-effective-multimodal-marketing-using-knowledge-infused-learning-trilok-padhi-et-al-2024>(187/277) Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning (Trilok Padhi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Trilok Padhi, Ugur Kursuncu, Yaman Kumar, Valerie L. Shalin, Lane Peterson Fronczek. (2024)<br><strong>Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning</strong><br><button class=copy-to-clipboard title="Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-7; I-2-10; I-2-4; I-2-1, cs-AI, cs-CL, cs-CV, cs-CY, cs-HC, cs.AI<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03607v1.pdf filename=2402.03607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience <b>multimodal</b> information online. However, large Language <b>(LLMs)</b> and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships. Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues. In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of <b>multi-modal</b> marketing campaigns. While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive <b>multi-modal</b> campaigns and the assessment and augmentation of marketing theory.</p></p class="citation"></blockquote><h3 id=188277-the-essential-role-of-causality-in-foundation-world-models-for-embodied-ai-tarun-gupta-et-al-2024>(188/277) The Essential Role of Causality in Foundation World Models for Embodied AI (Tarun Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tarun Gupta, Wenbo Gong, Chao Ma, Nick Pawlowski, Agrin Hilmkil, Meyer Scetbon, Ade Famoti, Ashley Juan Llorens, Jianfeng Gao, Stefan Bauer, Danica Kragic, Bernhard Schölkopf, Cheng Zhang. (2024)<br><strong>The Essential Role of Causality in Foundation World Models for Embodied AI</strong><br><button class=copy-to-clipboard title="The Essential Role of Causality in Foundation World Models for Embodied AI" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs-RO, cs.AI<br>Keyword Score: 13<br>Keywords: Foundation Model, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06665v1.pdf filename=2402.06665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>foundation</b> <b>models,</b> especially in large <b>multi-modal</b> models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents would require the ability to perform new tasks in many different real-world environments. However, current <b>foundation</b> <b>models</b> fail to accurately model physical interactions with the real world thus not sufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building <b>foundation</b> <b>world</b> models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitate meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook for future research.</p></p class="citation"></blockquote><h3 id=189277-counterfactual-generation-with-answer-set-programming-sopam-dasgupta-et-al-2024>(189/277) Counterfactual Generation with Answer Set Programming (Sopam Dasgupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sopam Dasgupta, Farhad Shakerin, Joaquín Arias, Elmer Salazar, Gopal Gupta. (2024)<br><strong>Counterfactual Generation with Answer Set Programming</strong><br><button class=copy-to-clipboard title="Counterfactual Generation with Answer Set Programming" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04382v1.pdf filename=2402.04382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models that automate decision-making are increasingly being used in consequential areas such as loan approvals, pretrial bail approval, hiring, and many more. Unfortunately, most of these models are black-boxes, i.e., they are unable to reveal how they reach these prediction decisions. A need for transparency demands justification for such predictions. An affected individual might also desire explanations to understand why a decision was made. Ethical and legal considerations may further require informing the individual of changes in the input attribute that could be made to produce a desirable outcome. This paper focuses on the latter problem of automatically generating <b>counterfactual</b> explanations. We propose a framework <b>Counterfactual</b> Generation with s(CASP) (CFGS) that utilizes answer set programming (ASP) and the s(CASP) goal-directed ASP system to automatically generate <b>counterfactual</b> explanations from rules generated by rule-based machine learning (RBML) algorithms. In our framework, we show how <b>counterfactual</b> explanations are computed and justified by imagining worlds where some or all factual assumptions are altered/changed. More importantly, we show how we can navigate between these worlds, namely, go from our original world/scenario where we obtain an undesired outcome to the imagined world/scenario where we obtain a desired/favourable outcome.</p></p class="citation"></blockquote><h3 id=190277-pedestrian-crossing-decisions-can-be-explained-by-bounded-optimal-decision-making-under-noisy-visual-perception-yueyang-wang-et-al-2024>(190/277) Pedestrian crossing decisions can be explained by bounded optimal decision-making under noisy visual perception (Yueyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yueyang Wang, Aravinda Ramakrishnan Srinivasan, Jussi P. P. Jokinen, Antti Oulasvirta, Gustav Markkula. (2024)<br><strong>Pedestrian crossing decisions can be explained by bounded optimal decision-making under noisy visual perception</strong><br><button class=copy-to-clipboard title="Pedestrian crossing decisions can be explained by bounded optimal decision-making under noisy visual perception" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04370v1.pdf filename=2402.04370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a model of pedestrian crossing decisions, based on the theory of computational rationality. It is assumed that crossing decisions are boundedly optimal, with bounds on optimality arising from human cognitive limitations. While previous models of pedestrian behaviour have been either &lsquo;black-box&rsquo; machine learning models or mechanistic models with explicit assumptions about cognitive factors, we combine both approaches. Specifically, we model mechanistically noisy human visual perception and assumed rewards in crossing, but we use <b>reinforcement</b> <b>learning</b> to learn bounded optimal behaviour policy. The model reproduces a larger number of known empirical phenomena than previous models, in particular: (1) the effect of the time to arrival of an approaching vehicle on whether the pedestrian accepts the gap, the effect of the vehicle&rsquo;s speed on both (2) gap acceptance and (3) pedestrian timing of crossing in front of yielding vehicles, and (4) the effect on this crossing timing of the stopping distance of the yielding vehicle. Notably, our findings suggest that behaviours previously framed as &lsquo;biases&rsquo; in decision-making, such as speed-dependent gap acceptance, might instead be a product of rational adaptation to the constraints of visual perception. Our approach also permits fitting the parameters of cognitive constraints and rewards per individual, to better account for individual differences. To conclude, by leveraging both RL and mechanistic modelling, our model offers novel insights about pedestrian behaviour, and may provide a useful foundation for more accurate and scalable pedestrian models.</p></p class="citation"></blockquote><h3 id=191277-task-success-is-not-enough-investigating-the-use-of-video-language-models-as-behavior-critics-for-catching-undesirable-agent-behaviors-lin-guan-et-al-2024>(191/277) &lsquo;Task Success&rsquo; is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors (Lin Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Guan, Yifan Zhou, Denis Liu, Yantian Zha, Heni Ben Amor, Subbarao Kambhampati. (2024)<br><strong>&lsquo;Task Success&rsquo; is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors</strong><br><button class=copy-to-clipboard title="'Task Success' is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-RO, cs.AI<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04210v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04210v1.pdf filename=2402.04210v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to catch undesirable robot behaviors in videos? To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies. Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes. Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement. The dataset and codebase are released at: <a href=https://guansuns.github.io/pages/vlm-critic>https://guansuns.github.io/pages/vlm-critic</a>.</p></p class="citation"></blockquote><h3 id=192277-human-like-geometric-abstraction-in-large-pre-trained-neural-networks-declan-campbell-et-al-2024>(192/277) Human-Like Geometric Abstraction in Large Pre-trained Neural Networks (Declan Campbell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Declan Campbell, Sreejan Kumar, Tyler Giallanza, Thomas L. Griffiths, Jonathan D. Cohen. (2024)<br><strong>Human-Like Geometric Abstraction in Large Pre-trained Neural Networks</strong><br><button class=copy-to-clipboard title="Human-Like Geometric Abstraction in Large Pre-trained Neural Networks" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI, q-bio-NC<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04203v1.pdf filename=2402.04203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans possess a remarkable capacity to recognize and manipulate abstract structure, which is especially apparent in the domain of geometry. Recent research in cognitive science suggests neural networks do not share this capacity, concluding that human geometric abilities come from discrete symbolic structure in human mental representations. However, progress in artificial intelligence (AI) suggests that neural networks begin to demonstrate more human-like <b>reasoning</b> after scaling up standard architectures in both model size and amount of training data. In this study, we revisit empirical results in cognitive science on geometric visual processing and identify three key biases in geometric visual processing: a sensitivity towards complexity, regularity, and the perception of parts and relations. We test tasks from the literature that probe these biases in humans and find that large pre-trained neural network models used in AI demonstrate more human-like abstract geometric processing.</p></p class="citation"></blockquote><h3 id=193277-embedding-knowledge-graphs-in-degenerate-clifford-algebras-louis-mozart-kamdem-et-al-2024>(193/277) Embedding Knowledge Graphs in Degenerate Clifford Algebras (Louis Mozart Kamdem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Louis Mozart Kamdem, Caglar Demir, Axel-Cyrille Ngonga. (2024)<br><strong>Embedding Knowledge Graphs in Degenerate Clifford Algebras</strong><br><button class=copy-to-clipboard title="Embedding Knowledge Graphs in Degenerate Clifford Algebras" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Graph Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04870v1.pdf filename=2402.04870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Clifford algebras are a natural generalization of the real numbers, the complex numbers, and the quaternions. So far, solely Clifford algebras of the form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been studied in the context of knowledge <b>graph</b> <b>embeddings.</b> We propose to consider nilpotent base vectors with a nilpotency index of two. In these spaces, denoted $Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from the absence of higher-order interactions between real and complex parts of entity embeddings. We design two new models for the discovery of the parameters $p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$, and $r$. The second predicts $(p, q,r)$ based on an embedding of the input knowledge <b>graph</b> <b>computed</b> using neural networks. The results of our evaluation on seven benchmark datasets suggest that nilpotent vectors can help capture embeddings better. Our comparison against the state of the art suggests that our approach generalizes better than other approaches on all datasets w.r.t. the MRR it achieves on validation data. We also show that a greedy search suffices to discover values of $p$, $q$ and $r$ that are close to optimal.</p></p class="citation"></blockquote><h3 id=194277-a-call-for-embodied-ai-giuseppe-paolo-et-al-2024>(194/277) A call for embodied AI (Giuseppe Paolo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giuseppe Paolo, Jonas Gonzalez-Billandon, Balázs Kégl. (2024)<br><strong>A call for embodied AI</strong><br><button class=copy-to-clipboard title="A call for embodied AI" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03824v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03824v1.pdf filename=2402.03824v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly <b>Large</b> <b>Language</b> <b>Models.</b> We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning. By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston&rsquo;s active inference principle, offering a comprehensive approach to EAI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future Embodied AI research. Highlighting the importance of creating Embodied AI agents capable of seamless communication, collaboration, and coexistence with humans and other intelligent entities within real-world environments, we aim to steer the AI community towards addressing the multifaceted challenges and seizing the opportunities that lie ahead in the quest for AGI.</p></p class="citation"></blockquote><h3 id=195277-logical-specifications-guided-dynamic-task-sampling-for-reinforcement-learning-agents-yash-shukla-et-al-2024>(195/277) Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents (Yash Shukla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yash Shukla, Tanushree Burman, Abhishek Kulkarni, Robert Wright, Alvaro Velasquez, Jivko Sinapov. (2024)<br><strong>Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents</strong><br><button class=copy-to-clipboard title="Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-RO, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03678v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03678v2.pdf filename=2402.03678v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshold performance on complex sequential decision-making problems compared to state-of-the-art RM and Automaton-guided RL baselines, such as Q-Learning for Reward Machines and Compositional RL from logical Specifications (DIRL). Moreover, we demonstrate that our method outperforms RM and Automaton-guided RL baselines in terms of sample-efficiency, both in a partially observable robotic task and in a continuous control robotic manipulation task.</p></p class="citation"></blockquote><h2 id=eessiv-4>eess.IV (4)</h2><h3 id=196277-what-limits-performance-of-weakly-supervised-deep-learning-for-chest-ct-classification-fakrul-islam-tushar-et-al-2024>(196/277) What limits performance of weakly supervised deep learning for chest CT classification? (Fakrul Islam Tushar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fakrul Islam Tushar, Vincent M. D&rsquo;Anniballe, Geoffrey D. Rubin, Joseph Y. Lo. (2024)<br><strong>What limits performance of weakly supervised deep learning for chest CT classification?</strong><br><button class=copy-to-clipboard title="What limits performance of weakly supervised deep learning for chest CT classification?" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV<br>Keyword Score: 60<br>Keywords: Supervised Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04419v1.pdf filename=2402.04419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Weakly</b> <b>supervised</b> <b>learning</b> with noisy data has drawn attention in the medical imaging community due to the sparsity of high-quality disease labels. However, little is known about the limitations of such <b>weakly</b> <b>supervised</b> <b>learning</b> and the effect of these constraints on disease classification performance. In this paper, we test the effects of such <b>weak</b> <b>supervision</b> by examining model tolerance for three conditions. First, we examined model tolerance for noisy data by incrementally increasing error in the labels within the training data. Second, we assessed the impact of dataset size by varying the amount of training data. Third, we compared performance differences between binary and multi-label classification. Results demonstrated that the model could endure up to 10% added label error before experiencing a decline in disease classification performance. Disease classification performance steadily rose as the amount of training data was increased for all disease classes, before experiencing a plateau in performance at 75% of training data. Last, the binary model outperformed the multilabel model in every disease category. However, such interpretations may be misleading, as the binary model was heavily influenced by co-occurring diseases and may not have learned the specific features of the disease in the image. In conclusion, this study may help the medical imaging community understand the benefits and risks of <b>weak</b> <b>supervision</b> with noisy labels. Such studies demonstrate the need to build diverse, large-scale datasets and to develop explainable and responsible AI.</p></p class="citation"></blockquote><h3 id=197277-deep-learning-based-correction-and-unmixing-of-hyperspectral-images-for-brain-tumor-surgery-david-black-et-al-2024>(197/277) Deep Learning-Based Correction and Unmixing of Hyperspectral Images for Brain Tumor Surgery (David Black et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Black, Jaidev Gill, Andrew Xie, Benoit Liquet, Antonio Di leva, Walter Stummer, Eric Suero Molina. (2024)<br><strong>Deep Learning-Based Correction and Unmixing of Hyperspectral Images for Brain Tumor Surgery</strong><br><button class=copy-to-clipboard title="Deep Learning-Based Correction and Unmixing of Hyperspectral Images for Brain Tumor Surgery" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV, q-bio-TO<br>Keyword Score: 50<br>Keywords: Autoencoder, Few-shot, Self-supervised Learning, Semi-Supervised Training, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03761v1.pdf filename=2402.03761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperspectral Imaging (HSI) for fluorescence-guided brain tumor resection enables visualization of differences between tissues that are not distinguishable to humans. This augmentation can maximize brain tumor resection, improving patient outcomes. However, much of the processing in HSI uses simplified linear methods that are unable to capture the non-linear, wavelength-dependent phenomena that must be modeled for accurate recovery of fluorophore abundances. We therefore propose two deep learning models for correction and unmixing, which can account for the nonlinear effects and produce more accurate estimates of abundances. Both models use an <b>autoencoder-like</b> architecture to process the captured spectra. One is trained with protoporphyrin IX (PpIX) concentration labels. The other undergoes <b>semi-supervised</b> <b>training,</b> first learning hyperspectral unmixing <b>self-supervised</b> and then learning to correct fluorescence emission spectra for heterogeneous optical and geometric properties using a reference white-light reflectance spectrum in a <b>few-shot</b> manner. The models were evaluated against phantom and pig brain data with known PpIX concentration; the <b>supervised</b> model achieved Pearson correlation coefficients (R values) between the known and computed PpIX concentrations of 0.997 and 0.990, respectively, whereas the classical approach achieved only 0.93 and 0.82. The <b>semi-supervised</b> <b>approach&rsquo;s</b> R values were 0.98 and 0.91, respectively. On human data, the <b>semi-supervised</b> <b>model</b> gives qualitatively more realistic results than the classical method, better removing bright spots of specular reflectance and reducing the variance in PpIX abundance over biopsies that should be relatively homogeneous. These results show promise for using deep learning to improve HSI in fluorescence-guided neurosurgery.</p></p class="citation"></blockquote><h3 id=198277-conunetr-a-conditional-transformer-network-for-3d-micro-ct-embryonic-cartilage-segmentation-nishchal-sapkota-et-al-2024>(198/277) ConUNETR: A Conditional Transformer Network for 3D Micro-CT Embryonic Cartilage Segmentation (Nishchal Sapkota et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nishchal Sapkota, Yejia Zhang, Susan M. Motch Perrine, Yuhan Hsi, Sirui Li, Meng Wu, Greg Holmes, Abdul R. Abdulai, Ethylin W. Jabs, Joan T. Richtsmeier, Danny Z Chen. (2024)<br><strong>ConUNETR: A Conditional Transformer Network for 3D Micro-CT Embryonic Cartilage Segmentation</strong><br><button class=copy-to-clipboard title="ConUNETR: A Conditional Transformer Network for 3D Micro-CT Embryonic Cartilage Segmentation" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03695v1.pdf filename=2402.03695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Studying the morphological development of cartilaginous and osseous structures is critical to the early detection of life-threatening skeletal dysmorphology. Embryonic cartilage undergoes rapid structural changes within hours, introducing biological variations and morphological shifts that limit the generalization of deep learning-based segmentation models that infer across multiple embryonic age groups. Obtaining individual models for each age group is expensive and less effective, while direct transfer (predicting an age unseen during training) suffers a potential performance drop due to morphological shifts. We propose a novel <b>Transformer-based</b> segmentation model with improved biological priors that better <b>distills</b> morphologically diverse information through conditional mechanisms. This enables a single model to accurately predict cartilage across multiple age groups. Experiments on the mice cartilage dataset show the superiority of our new model compared to other competitive segmentation models. Additional studies on a separate mice cartilage dataset with a distinct mutation show that our model generalizes well and effectively captures age-based cartilage morphology patterns.</p></p class="citation"></blockquote><h3 id=199277-3d-volumetric-super-resolution-in-radiology-using-3d-rrdb-gan-juhyung-ha-et-al-2024>(199/277) 3D Volumetric Super-Resolution in Radiology Using 3D RRDB-GAN (Juhyung Ha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juhyung Ha, Nian Wang, Surendra Maharjan, Xuhong Zhang. (2024)<br><strong>3D Volumetric Super-Resolution in Radiology Using 3D RRDB-GAN</strong><br><button class=copy-to-clipboard title="3D Volumetric Super-Resolution in Radiology Using 3D RRDB-GAN" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04171v1.pdf filename=2402.04171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces the 3D Residual-in-Residual Dense Block <b>GAN</b> (3D RRDB-GAN) for 3D super-resolution for radiology imagery. A key aspect of 3D RRDB-GAN is the integration of a 2.5D perceptual loss function, which contributes to improved volumetric image quality and realism. The effectiveness of our model was evaluated through 4x super-resolution experiments across diverse datasets, including Mice Brain MRH, OASIS, HCP1200, and MSD-Task-6. These evaluations, encompassing both quantitative metrics like LPIPS and FID and qualitative assessments through sample visualizations, demonstrate the models effectiveness in detailed image analysis. The 3D RRDB-GAN offers a significant contribution to medical imaging, particularly by enriching the depth, clarity, and volumetric detail of medical images. Its application shows promise in enhancing the interpretation and analysis of complex medical imagery from a comprehensive 3D perspective.</p></p class="citation"></blockquote><h2 id=csir-6>cs.IR (6)</h2><h3 id=200277-can-large-language-models-detect-rumors-on-social-media-qiang-liu-et-al-2024>(200/277) Can Large Language Models Detect Rumors on Social Media? (Qiang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiang Liu, Xiang Tao, Junfei Wu, Shu Wu, Liang Wang. (2024)<br><strong>Can Large Language Models Detect Rumors on Social Media?</strong><br><button class=copy-to-clipboard title="Can Large Language Models Detect Rumors on Social Media?" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Few-shot, Zero-shot, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03916v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03916v2.pdf filename=2402.03916v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we investigate to use <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for rumor detection on social media. However, it is challenging for <b>LLMs</b> to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to <b>LLMs</b> may not concentrate on key clues in the complex propagation information, and have trouble in <b>reasoning</b> when facing massive and redundant information. Accordingly, we propose an <b>LLM-empowered</b> Rumor Detection (LeRuD) approach, in which we design <b>prompts</b> to teach <b>LLMs</b> to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing <b>LLMs&rsquo;</b> burden. We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 3.2% to 7.7%. Meanwhile, by applying <b>LLMs,</b> LeRuD requires no data for training, and thus shows more promising rumor detection ability in <b>few-shot</b> or <b>zero-shot</b> scenarios.</p></p class="citation"></blockquote><h3 id=201277-the-potential-of-automl-for-recommender-systems-tobias-vente-et-al-2024>(201/277) The Potential of AutoML for Recommender Systems (Tobias Vente et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Vente, Joeran Beel. (2024)<br><strong>The Potential of AutoML for Recommender Systems</strong><br><button class=copy-to-clipboard title="The Potential of AutoML for Recommender Systems" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 30<br>Keywords: Model Compression, Recommender System, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04453v1.pdf filename=2402.04453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated <b>Machine</b> <b>Learning</b> (AutoML) has greatly advanced applications of <b>Machine</b> <b>Learning</b> (ML) including <b>model</b> <b>compression,</b> <b>machine</b> <b>translation,</b> and computer vision. <b>Recommender</b> <b>Systems</b> (RecSys) can be seen as an application of ML. Yet, AutoML has found little attention in the RecSys community; nor has RecSys found notable attention in the AutoML community. Only few and relatively simple Automated <b>Recommender</b> <b>Systems</b> (AutoRecSys) libraries exist that adopt AutoML techniques. However, these libraries are based on student projects and do not offer the features and thorough development of AutoML libraries. We set out to determine how AutoML libraries perform in the scenario of an inexperienced user who wants to implement a <b>recommender</b> <b>system.</b> We compared the predictive performance of 60 AutoML, AutoRecSys, ML, and RecSys algorithms from 15 libraries, including a mean predictor baseline, on 14 explicit feedback RecSys datasets. To simulate the perspective of an inexperienced user, the algorithms were evaluated with default hyperparameters. We found that AutoML and AutoRecSys libraries performed best. AutoML libraries performed best for six of the 14 datasets (43%), but it was not always the same AutoML library performing best. The single-best library was the AutoRecSys library Auto-Surprise, which performed best on five datasets (36%). On three datasets (21%), AutoML libraries performed poorly, and RecSys libraries with default parameters performed best. Although, while obtaining 50% of all placements in the top five per dataset, RecSys algorithms fall behind AutoML on average. ML algorithms generally performed the worst.</p></p class="citation"></blockquote><h3 id=202277-reliability-quality-measures-for-recommender-systems-jesús-bobadilla-et-al-2024>(202/277) Reliability quality measures for recommender systems (Jesús Bobadilla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jesús Bobadilla, Abraham Gutierrez, Fernando Ortega, Bo Zhu. (2024)<br><strong>Reliability quality measures for recommender systems</strong><br><button class=copy-to-clipboard title="Reliability quality measures for recommender systems" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04457v1.pdf filename=2402.04457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Users want to know the reliability of the <b>recommendations;</b> they do not accept high predictions if there is no reliability evidence. <b>Recommender</b> <b>systems</b> should provide reliability values associated with the predictions. Research into reliability measures requires the existence of simple, plausible and universal reliability quality measures. Research into <b>recommender</b> <b>system</b> quality measures has focused on accuracy. Moreover, novelty, serendipity and diversity have been studied; nevertheless there is an important lack of research into reliability/confidence quality measures. This paper proposes a reliability quality prediction measure (RPI) and a reliability quality <b>recommendation</b> measure (RRI). Both quality measures are based on the hypothesis that the more suitable a reliability measure is, the better accuracy results it will provide when applied. These reliability quality measures show accuracy improvements when appropriated reliability values are associated with their predictions (i.e. high reliability values associated with correct predictions or low reliability values associated with incorrect predictions). The proposed reliability quality metrics will lead to the design of brand new <b>recommender</b> <b>system</b> reliability measures. These measures could be applied to different matrix factorization techniques and to content-based, context-aware and social <b>recommendation</b> approaches. The <b>recommender</b> <b>system</b> reliability measures designed could be tested, compared and improved using the proposed reliability quality metrics.</p></p class="citation"></blockquote><h3 id=203277-on-practical-diversified-recommendation-with-controllable-category-diversity-framework-tao-zhang-et-al-2024>(203/277) On Practical Diversified Recommendation with Controllable Category Diversity Framework (Tao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Zhang, Luwei Yang, Zhibo Xiao, Wen Jiang, Wei Ning. (2024)<br><strong>On Practical Diversified Recommendation with Controllable Category Diversity Framework</strong><br><button class=copy-to-clipboard title="On Practical Diversified Recommendation with Controllable Category Diversity Framework" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03801v1.pdf filename=2402.03801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems</b> have made significant strides in various industries, primarily driven by extensive efforts to enhance <b>recommendation</b> accuracy. However, this pursuit of accuracy has inadvertently given rise to echo chamber/filter bubble effects. Especially in industry, it could impair user&rsquo;s experiences and prevent user from accessing a wider range of items. One of the solutions is to take diversity into account. However, most of existing works focus on user&rsquo;s explicit preferences, while rarely exploring user&rsquo;s non-interaction preferences. These neglected non-interaction preferences are especially important for broadening user&rsquo;s interests in alleviating echo chamber/filter bubble effects.Therefore, in this paper, we first define diversity as two distinct definitions, i.e., user-explicit diversity (U-diversity) and user-item non-interaction diversity (N-diversity) based on user historical behaviors. Then, we propose a succinct and effective method, named as Controllable Category Diversity Framework (CCDF) to achieve both high U-diversity and N-diversity simultaneously.Specifically, CCDF consists of two stages, User-Category Matching and Constrained Item Matching. The User-Category Matching utilizes the DeepU2C model and a combined loss to capture user&rsquo;s preferences in categories, and then selects the top-$K$ categories with a controllable parameter $K$.These top-$K$ categories will be used as trigger information in Constrained Item Matching. Offline experimental results show that our proposed DeepU2C outperforms state-of-the-art diversity-oriented methods, especially on N-diversity task. The whole framework is validated in a real-world production environment by conducting online A/B testing.</p></p class="citation"></blockquote><h3 id=204277-retrieval-augmented-cross-modal-tag-recommendation-in-software-qa-sites-sijin-lu-et-al-2024>(204/277) Retrieval Augmented Cross-Modal Tag Recommendation in Software Q&amp;A Sites (Sijin Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sijin Lu, Pengyu Xu, Bing Liu, Hongjian Sun, Liping Jing, Jian Yu. (2024)<br><strong>Retrieval Augmented Cross-Modal Tag Recommendation in Software Q&amp;A Sites</strong><br><button class=copy-to-clipboard title="Retrieval Augmented Cross-Modal Tag Recommendation in Software Q&A Sites" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03635v1.pdf filename=2402.03635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Posts in software Q&amp;A sites often consist of three main parts: title, description and code, which are interconnected and jointly describe the question. Existing tag <b>recommendation</b> methods often treat different modalities as a whole or inadequately consider the interaction between different modalities. Additionally, they focus on extracting information directly from the post itself, neglecting the information from external knowledge sources. Therefore, we propose a Retrieval Augmented Cross-Modal (RACM) Tag <b>Recommendation</b> Model in Software Q&amp;A Sites. Specifically, we first use the input post as a query and enhance the representation of different modalities by retrieving information from external knowledge sources. For the retrieval-augmented representations, we employ a cross-modal context-aware attention to leverage the main modality description for targeted feature extraction across the submodalities title and code. In the fusion process, a gate mechanism is employed to achieve fine-grained feature selection, controlling the amount of information extracted from the submodalities. Finally, the fused information is used for tag <b>recommendation.</b> Experimental results on three real-world datasets demonstrate that our model outperforms the state-of-the-art counterparts.</p></p class="citation"></blockquote><h3 id=205277-understanding-and-counteracting-feature-level-bias-in-click-through-rate-prediction-jinqiu-jin-et-al-2024>(205/277) Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction (Jinqiu Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinqiu Jin, Sihao Ding, Wenjie Wang, Fuli Feng. (2024)<br><strong>Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction</strong><br><button class=copy-to-clipboard title="Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03600v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03600v1.pdf filename=2402.03600v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Common click-through rate (CTR) prediction recommender models tend to exhibit feature-level bias, which leads to unfair <b>recommendations</b> among item groups and inaccurate <b>recommendations</b> for users. While existing methods address this issue by adjusting the learning of CTR models, such as through additional optimization objectives, they fail to consider how the bias is caused within these models. To address this research gap, our study performs a top-down analysis on representative CTR models. Through blocking different components of a trained CTR model one by one, we identify the key contribution of the linear component to feature-level bias. We conduct a theoretical analysis of the learning process for the weights in the linear component, revealing how group-wise properties of training data influence them. Our experimental and statistical analyses demonstrate a strong correlation between imbalanced positive sample ratios across item groups and feature-level bias. Based on this understanding, we propose a minimally invasive yet effective strategy to counteract feature-level bias in CTR models by removing the biased linear weights from trained models. Additionally, we present a linear weight adjusting strategy that requires fewer random exposure records than relevant debiasing methods. The superiority of our proposed strategies are validated through extensive experiments on three real-world datasets.</p></p class="citation"></blockquote><h2 id=csro-17>cs.RO (17)</h2><h3 id=206277-human-observation-inspired-trajectory-prediction-for-autonomous-driving-in-mixed-autonomy-traffic-environments-haicheng-liao-et-al-2024>(206/277) Human Observation-Inspired Trajectory Prediction for Autonomous Driving in Mixed-Autonomy Traffic Environments (Haicheng Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haicheng Liao, Shangqian Liu, Yongkang Li, Zhenning Li, Chengyue Wang, Bonan Wang, Yanchen Guan, Chengzhong Xu. (2024)<br><strong>Human Observation-Inspired Trajectory Prediction for Autonomous Driving in Mixed-Autonomy Traffic Environments</strong><br><button class=copy-to-clipboard title="Human Observation-Inspired Trajectory Prediction for Autonomous Driving in Mixed-Autonomy Traffic Environments" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Graph Attention Networks, Graph Attention Networks, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04318v1.pdf filename=2402.04318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the burgeoning field of autonomous vehicles (AVs), trajectory prediction remains a formidable challenge, especially in mixed autonomy environments. Traditional approaches often rely on computational methods such as time-series analysis. Our research diverges significantly by adopting an interdisciplinary approach that integrates principles of human cognition and observational behavior into trajectory prediction models for AVs. We introduce a novel &ldquo;adaptive visual sector&rdquo; mechanism that mimics the dynamic allocation of attention human drivers exhibit based on factors like spatial orientation, proximity, and driving speed. Additionally, we develop a &ldquo;dynamic traffic <b>graph&rdquo;</b> <b>using</b> <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNN)</b> and <b>Graph</b> <b>Attention</b> <b>Networks</b> <b>(GAT)</b> to capture spatio-temporal dependencies among agents. Benchmark tests on the NGSIM, HighD, and MoCAD datasets reveal that our model (GAVA) outperforms state-of-the-art baselines by at least 15.2%, 19.4%, and 12.0%, respectively. Our findings underscore the potential of leveraging human cognition principles to enhance the proficiency and adaptability of trajectory prediction algorithms in AVs. The code for the proposed model is available at our Github.</p></p class="citation"></blockquote><h3 id=207277-reinforcement-learning-for-collision-free-flight-exploiting-deep-collision-encoding-mihir-kulkarni-et-al-2024>(207/277) Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding (Mihir Kulkarni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mihir Kulkarni, Kostas Alexis. (2024)<br><strong>Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding</strong><br><button class=copy-to-clipboard title="Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Simulation, Simulator, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03947v1.pdf filename=2402.03947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work contributes a novel deep navigation policy that enables collision-free flight of aerial robots based on a modular approach exploiting deep collision encoding and <b>reinforcement</b> <b>learning.</b> The proposed solution builds upon a deep collision encoder that is trained on both simulated and real depth images using <b>supervised</b> <b>learning</b> such that it compresses the high-dimensional depth data to a low-dimensional latent space encoding collision information while accounting for the robot size. This compressed encoding is combined with an estimate of the robot&rsquo;s odometry and the desired target location to train a deep <b>reinforcement</b> <b>learning</b> navigation policy that offers low-latency computation and robust sim2real performance. A set of <b>simulation</b> and experimental studies in diverse environments are conducted and demonstrate the efficiency of the emerged behavior and its resilience in real-life deployments.</p></p class="citation"></blockquote><h3 id=208277-belief-scene-graphs-expanding-partial-scenes-with-objects-through-computation-of-expectation-mario-a-v-saucedo-et-al-2024>(208/277) Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation (Mario A. V. Saucedo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mario A. V. Saucedo, Akash Patel, Akshit Saradagi, Christoforos Kanellakis, George Nikolakopoulos. (2024)<br><strong>Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation</strong><br><button class=copy-to-clipboard title="Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Graph Convolutional Network, Convolution, Convolutional Neural Network, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03840v1.pdf filename=2402.03840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this article, we propose the novel concept of Belief Scene Graphs, which are utility-driven extensions of partial 3D scene graphs, that enable efficient high-level task planning with partial information. We propose a graph-based learning methodology for the computation of belief (also referred to as expectation) on any given 3D scene graph, which is then used to strategically add new nodes (referred to as blind nodes) that are relevant for a robotic mission. We propose the method of Computation of Expectation based on Correlation Information (CECI), to reasonably approximate real Belief/Expectation, by learning histograms from available training data. A novel Graph <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(GCN)</b> model is developed, to learn CECI from a repository of 3D scene graphs. As no database of 3D scene graphs exists for the training of the novel CECI model, we present a novel methodology for generating a 3D scene graph dataset based on semantically annotated real-life 3D spaces. The generated dataset is then utilized to train the proposed CECI model and for extensive validation of the proposed method. We establish the novel concept of \textit{Belief Scene Graphs} (BSG), as a core component to integrate expectations into abstract representations. This new concept is an evolution of the classical 3D scene graph concept and aims to enable high-level <b>reasoning</b> for the task planning and optimization of a variety of robotics missions. The efficacy of the overall framework has been evaluated in an object search scenario, and has also been tested on a real-life experiment to emulate human common sense of unseen-objects.</p></p class="citation"></blockquote><h3 id=209277-automatic-robotic-development-through-collaborative-framework-by-large-language-models-zhirong-luan-et-al-2024>(209/277) Automatic Robotic Development through Collaborative Framework by Large Language Models (Zhirong Luan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhirong Luan, Yujun Lai. (2024)<br><strong>Automatic Robotic Development through Collaborative Framework by Large Language Models</strong><br><button class=copy-to-clipboard title="Automatic Robotic Development through Collaborative Framework by Large Language Models" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Fine-tuning, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03699v1.pdf filename=2402.03699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the remarkable <b>code</b> <b>generation</b> abilities of <b>large</b> <b>language</b> <b>models</b> <b>LLMs,</b> they still face challenges in complex task handling. Robot development, a highly intricate field, inherently demands human involvement in task allocation and collaborative teamwork . To enhance robot development, we propose an innovative automated collaboration framework inspired by real-world robot developers. This framework employs multiple <b>LLMs</b> in distinct roles analysts, programmers, and testers. Analysts delve deep into user requirements, enabling programmers to produce precise <b>code,</b> <b>while</b> testers <b>fine-tune</b> the parameters based on user feedback for practical robot application. Each <b>LLM</b> tackles diverse, critical tasks within the development process. Clear collaboration rules emulate real world teamwork among <b>LLMs.</b> Analysts, programmers, and testers form a cohesive team overseeing strategy, <b>code,</b> <b>and</b> parameter adjustments . Through this framework, we achieve complex robot development without requiring specialized knowledge, relying solely on non experts participation.</p></p class="citation"></blockquote><h3 id=210277-rl-vlm-f-reinforcement-learning-from-vision-language-foundation-model-feedback-yufei-wang-et-al-2024>(210/277) RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback (Yufei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David Held, Zackory Erickson. (2024)<br><strong>RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback</strong><br><button class=copy-to-clipboard title="RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Foundation Model, Reinforcement Learning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03681v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03681v2.pdf filename=2402.03681v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reward engineering has long been a challenge in <b>Reinforcement</b> <b>Learning</b> (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent&rsquo;s visual observations, by leveraging feedbacks from vision language <b>foundation</b> <b>models</b> (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent&rsquo;s image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly <b>prompting</b> these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulated, and deformable objects - without the need for human supervision, outperforming prior methods that use large pretrained models for reward generation under the same assumptions.</p></p class="citation"></blockquote><h3 id=211277-hierarchical-large-language-models-in-cloud-edge-end-architecture-for-heterogeneous-robot-cluster-control-zhirong-luan-et-al-2024>(211/277) Hierarchical Large Language Models in Cloud Edge End Architecture for Heterogeneous Robot Cluster Control (Zhirong Luan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhirong Luan, Yujun Lai. (2024)<br><strong>Hierarchical Large Language Models in Cloud Edge End Architecture for Heterogeneous Robot Cluster Control</strong><br><button class=copy-to-clipboard title="Hierarchical Large Language Models in Cloud Edge End Architecture for Heterogeneous Robot Cluster Control" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03703v1.pdf filename=2402.03703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite their powerful semantic understanding and <b>code</b> <b>generation</b> capabilities, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> still face challenges when dealing with complex tasks. Multi agent strategy generation and motion control are highly complex domains that inherently require experts from multiple fields to collaborate. To enhance multi agent strategy generation and motion control, we propose an innovative architecture that employs the concept of a cloud edge end hierarchical structure. By leveraging multiple <b>large</b> <b>language</b> <b>models</b> with distinct areas of expertise, we can efficiently generate strategies and perform task decomposition. Introducing the cosine similarity approach,aligning task decomposition instructions with robot task sequences at the vector level, we can identify subtasks with incomplete task decomposition and iterate on them multiple times to ultimately generate executable machine task sequences.The robot is guided through these task sequences to complete tasks of higher complexity. With this architecture, we implement the process of natural language control of robots to perform complex tasks, and successfully address the challenge of multi agent execution of open tasks in open scenarios and the problem of task decomposition.</p></p class="citation"></blockquote><h3 id=212277-a-survey-of-offline-and-online-learning-based-algorithms-for-multirotor-uavs-serhat-sönmez-et-al-2024>(212/277) A Survey of Offline and Online Learning-Based Algorithms for Multirotor UAVs (Serhat Sönmez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Serhat Sönmez, Matthew J. Rutherford, Kimon P. Valavanis. (2024)<br><strong>A Survey of Offline and Online Learning-Based Algorithms for Multirotor UAVs</strong><br><button class=copy-to-clipboard title="A Survey of Offline and Online Learning-Based Algorithms for Multirotor UAVs" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04418v1.pdf filename=2402.04418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multirotor UAVs are used for a wide spectrum of civilian and public domain applications. Navigation controllers endowed with different attributes and onboard sensor suites enable multirotor autonomous or semi-autonomous, safe flight, operation, and functionality under nominal and detrimental conditions and external disturbances, even when flying in uncertain and dynamically changing environments. During the last decade, given the faster-than-exponential increase of available computational power, different learning-based algorithms have been derived, implemented, and tested to navigate and control, among other systems, multirotor UAVs. Learning algorithms have been, and are used to derive data-driven based models, to identify parameters, to track objects, to develop navigation controllers, and to learn the environment in which multirotors operate. Learning algorithms combined with model-based control techniques have been proven beneficial when applied to multirotors. This survey <b>summarizes</b> published research since 2015, dividing algorithms, techniques, and methodologies into offline and online learning categories, and then, further classifying them into machine learning, deep learning, and <b>reinforcement</b> <b>learning</b> sub-categories. An integral part and focus of this survey are on online learning algorithms as applied to multirotors with the aim to register the type of learning techniques that are either hard or almost hard real-time implementable, as well as to understand what information is learned, why, and how, and how fast. The outcome of the survey offers a clear understanding of the recent state-of-the-art and of the type and kind of learning-based algorithms that may be implemented, tested, and executed in real-time.</p></p class="citation"></blockquote><h3 id=213277-intelligent-collective-escape-of-swarm-robots-based-on-a-novel-fish-inspired-self-adaptive-approach-with-neurodynamic-models-junfei-li-et-al-2024>(213/277) Intelligent Collective Escape of Swarm Robots Based on a Novel Fish-inspired Self-adaptive Approach with Neurodynamic Models (Junfei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junfei Li, Simon X. Yang. (2024)<br><strong>Intelligent Collective Escape of Swarm Robots Based on a Novel Fish-inspired Self-adaptive Approach with Neurodynamic Models</strong><br><button class=copy-to-clipboard title="Intelligent Collective Escape of Swarm Robots Based on a Novel Fish-inspired Self-adaptive Approach with Neurodynamic Models" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04228v1.pdf filename=2402.04228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fish schools present high-efficiency group behaviors through simple individual interactions to collective migration and dynamic escape from the predator. The school behavior of fish is usually a good inspiration to design control architecture for swarm robots. In this paper, a novel fish-inspired self-adaptive approach is proposed for collective escape for the swarm robots. In addition, a bio-inspired neural network (BINN) is introduced to generate collision-free escape robot trajectories through the combination of attractive and repulsive forces. Furthermore, to cope with dynamic environments, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in the changing environment. Similar to fish escape maneuvers, <b>simulation</b> and experimental results show that the swarm robots are capable of collectively leaving away from the threats. Several comparison studies demonstrated that the proposed approach can significantly improve the effectiveness and efficiency of system performance, and the flexibility and robustness in complex environments.</p></p class="citation"></blockquote><h3 id=214277-explaining-autonomy-enhancing-human-robot-interaction-through-explanation-generation-with-large-language-models-david-sobrín-hidalgo-et-al-2024>(214/277) Explaining Autonomy: Enhancing Human-Robot Interaction through Explanation Generation with Large Language Models (David Sobrín-Hidalgo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Sobrín-Hidalgo, Miguel A. González-Santamarta, Ángel M. Guerrero-Higueras, Francisco J. Rodríguez-Lera, Vicente Matellán-Olivera. (2024)<br><strong>Explaining Autonomy: Enhancing Human-Robot Interaction through Explanation Generation with Large Language Models</strong><br><button class=copy-to-clipboard title="Explaining Autonomy: Enhancing Human-Robot Interaction through Explanation Generation with Large Language Models" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04206v1.pdf filename=2402.04206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a system designed to generate explanations for the actions performed by an autonomous robot in Human-Robot Interaction (HRI). Explainability in robotics, encapsulated within the concept of an eXplainable Autonomous Robot (XAR), is a growing research area. The work described in this paper aims to take advantage of the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in performing natural language processing tasks. This study focuses on the possibility of generating explanations using such models in combination with a Retrieval Augmented Generation (RAG) method to interpret data gathered from the logs of autonomous systems. In addition, this work also presents a formalization of the proposed explanation system. It has been evaluated through a navigation test from the European Robotics League (ERL), a Europe-wide social robotics competition. Regarding the obtained results, a validation questionnaire has been conducted to measure the quality of the explanations from the perspective of technical users. The results obtained during the experiment highlight the potential utility of <b>LLMs</b> in achieving explanatory capabilities in robots.</p></p class="citation"></blockquote><h3 id=215277-prediction-horizon-requirements-for-automated-driving-optimizing-safety-comfort-and-efficiency-manuel-muñoz-sánchez-et-al-2024>(215/277) Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency (Manuel Muñoz Sánchez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manuel Muñoz Sánchez, Chris van der Ploeg, Robin Smit, Jos Elfring, Emilia Silvas, René van de Molengraft. (2024)<br><strong>Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency</strong><br><button class=copy-to-clipboard title="Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03893v1.pdf filename=2402.03893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting the movement of other road users is beneficial for improving automated vehicle (AV) performance. However, the relationship between the time horizon associated with these predictions and AV performance remains unclear. Despite the existence of numerous trajectory prediction algorithms, no studies have been conducted on how varying prediction lengths affect AV safety and other vehicle performance metrics, resulting in undefined horizon requirements for prediction methods. Our study addresses this gap by examining the effects of different prediction horizons on AV performance, focusing on safety, comfort, and efficiency. Through multiple experiments using a state-of-the-art, risk-based predictive trajectory planner, we simulated predictions with horizons up to 20 seconds. Based on our <b>simulations,</b> we propose a framework for specifying the minimum required and optimal prediction horizons based on specific AV performance criteria and application needs. Our results indicate that a horizon of 1.6 seconds is required to prevent collisions with crossing pedestrians, horizons of 7-8 seconds yield the best efficiency, and horizons up to 15 seconds improve passenger comfort. We conclude that prediction horizon requirements are application-dependent, and recommend aiming for a prediction horizon of 11.8 seconds as a general guideline for applications involving crossing pedestrians.</p></p class="citation"></blockquote><h3 id=216277-online-informative-sampling-using-semantic-features-in-underwater-environments-shrutika-vishal-thengane-et-al-2024>(216/277) Online Informative Sampling using Semantic Features in Underwater Environments (Shrutika Vishal Thengane et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shrutika Vishal Thengane, Yu Xiang Tan, Marcel Bartholomeus Prasetyo, Malika Meghjani. (2024)<br><strong>Online Informative Sampling using Semantic Features in Underwater Environments</strong><br><button class=copy-to-clipboard title="Online Informative Sampling using Semantic Features in Underwater Environments" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Object Detection, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03636v1.pdf filename=2402.03636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The underwater world remains largely unexplored, with Autonomous Underwater Vehicles (AUVs) playing a crucial role in sub-sea explorations. However, continuous monitoring of underwater environments using AUVs can generate a significant amount of data. In addition, sending live data feed from an underwater environment requires dedicated on-board data storage options for AUVs which can hinder requirements of other higher priority tasks. Informative sampling techniques offer a solution by condensing observations. In this paper, we present a semantically-aware online informative sampling (ON-IS) approach which samples an AUV&rsquo;s visual experience in real-time. Specifically, we obtain visual features from a <b>fine-tuned</b> <b>object</b> <b>detection</b> model to align the sampling outcomes with the desired semantic information. Our contributions are (a) a novel Semantic Online Informative Sampling (SON-IS) algorithm, (b) a user study to validate the proposed approach and (c) a novel evaluation metric to score our proposed algorithm with respect to the suggested samples by human subjects</p></p class="citation"></blockquote><h3 id=217277-integration-of-4d-bim-and-robot-task-planning-creation-and-flow-of-construction-related-information-for-action-level-simulation-of-indoor-wall-frame-installation-hafiz-oyediran-et-al-2024>(217/277) Integration of 4D BIM and Robot Task Planning: Creation and Flow of Construction-Related Information for Action-Level Simulation of Indoor Wall Frame Installation (Hafiz Oyediran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hafiz Oyediran, William Turner, Kyungki Kim, Matthew Barrows. (2024)<br><strong>Integration of 4D BIM and Robot Task Planning: Creation and Flow of Construction-Related Information for Action-Level Simulation of Indoor Wall Frame Installation</strong><br><button class=copy-to-clipboard title="Integration of 4D BIM and Robot Task Planning: Creation and Flow of Construction-Related Information for Action-Level Simulation of Indoor Wall Frame Installation" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03602v1.pdf filename=2402.03602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An obstacle toward construction robotization is the lack of methods to plan robot operations within the entire construction planning process. Despite the strength in modeling construction site conditions, 4D BIM technologies cannot perform construction robot task planning considering the contexts of given work environments. To address this limitation, this study presents a framework that integrates 4D BIM and robot task planning, presents an information flow for the integration, and performs high-level robot task planning and detailed <b>simulation.</b> The framework uniquely incorporates a construction robot knowledge base that derives robot-related modeling requirements to augment a 4D BIM model. Then, the 4D BIM model is converted into a robot <b>simulation</b> world where a robot performs a sequence of actions retrieving construction-related information. A case study focusing on the interior wall frame installation demonstrates the potential of systematic integration in achieving context-aware robot task planning and <b>simulation</b> in construction environments.</p></p class="citation"></blockquote><h3 id=218277-enhancing-embodied-object-detection-through-language-image-pre-training-and-implicit-object-memory-nicolas-harvey-chapman-et-al-2024>(218/277) Enhancing Embodied Object Detection through Language-Image Pre-training and Implicit Object Memory (Nicolas Harvey Chapman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Harvey Chapman, Feras Dayoub, Will Browne, Chris Lehnert. (2024)<br><strong>Enhancing Embodied Object Detection through Language-Image Pre-training and Implicit Object Memory</strong><br><button class=copy-to-clipboard title="Enhancing Embodied Object Detection through Language-Image Pre-training and Implicit Object Memory" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Object Detection, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03721v1.pdf filename=2402.03721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep-learning and large scale language-image training have produced image <b>object</b> <b>detectors</b> that generalise well to diverse environments and semantic classes. However, single-image <b>object</b> <b>detectors</b> trained on internet data are not optimally tailored for the embodied conditions inherent in robotics. Instead, robots must detect <b>objects</b> <b>from</b> complex <b>multi-modal</b> data streams involving depth, localisation and temporal correlation, a task termed embodied <b>object</b> <b>detection.</b> Paradigms such as Video <b>Object</b> <b>Detection</b> (VOD) and Semantic Mapping have been proposed to leverage such embodied data streams, but existing work fails to enhance performance using language-image training. In response, we investigate how an image <b>object</b> <b>detector</b> pre-trained using language-image data can be extended to perform embodied <b>object</b> <b>detection.</b> We propose a novel implicit <b>object</b> <b>memory</b> that uses projective geometry to aggregate the features of detected <b>objects</b> <b>across</b> long temporal horizons. The spatial and temporal information accumulated in memory is then used to enhance the image features of the base detector. When tested on embodied data streams sampled from diverse indoor scenes, our approach improves the base <b>object</b> <b>detector</b> by 3.09 mAP, outperforming alternative external memories designed for VOD and Semantic Mapping. Our method also shows a significant improvement of 16.90 mAP relative to baselines that perform embodied <b>object</b> <b>detection</b> without first training on language-image data, and is robust to sensor noise and domain shift experienced in real-world deployment.</p></p class="citation"></blockquote><h3 id=219277-aed-adaptable-error-detection-for-few-shot-imitation-policy-jia-fong-yeh-et-al-2024>(219/277) AED: Adaptable Error Detection for Few-shot Imitation Policy (Jia-Fong Yeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jia-Fong Yeh, Kuo-Han Hung, Pang-Chi Lo, Chi-Ming Chung, Tsung-Han Wu, Hung-Ting Su, Yi-Ting Chen, Winston H. Hsu. (2024)<br><strong>AED: Adaptable Error Detection for Few-shot Imitation Policy</strong><br><button class=copy-to-clipboard title="AED: Adaptable Error Detection for Few-shot Imitation Policy" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03860v1.pdf filename=2402.03860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study how to report <b>few-shot</b> imitation (FSI) policies&rsquo; behavior errors in novel environments, a novel task named adaptable error detection (AED). The potential to cause serious damage to surrounding areas limits the application of FSI policies in real-world scenarios. Thus, a robust system is necessary to notify operators when FSI policies are inconsistent with the intent of demonstrations. We develop a cross-domain benchmark for the challenging AED task, consisting of 329 base and 158 novel environments. This task introduces three challenges, including (1) detecting behavior errors in novel environments, (2) behavior errors occurring without revealing notable changes, and (3) lacking complete temporal information of the rollout due to the necessity of online detection. To address these challenges, we propose Pattern Observer (PrObe) to parse discernible patterns in the policy feature representations of normal or error states, whose effectiveness is verified in the proposed benchmark. Through our comprehensive evaluation, PrObe consistently surpasses strong baselines and demonstrates a robust capability to identify errors arising from a wide range of FSI policies. Moreover, we conduct comprehensive ablations and experiments (error correction, demonstration quality, etc.) to validate the practicality of our proposed task and methodology.</p></p class="citation"></blockquote><h3 id=220277-environment-centric-learning-approach-for-gait-synthesis-in-terrestrial-soft-robots-caitlin-freeman-et-al-2024>(220/277) Environment-Centric Learning Approach for Gait Synthesis in Terrestrial Soft Robots (Caitlin Freeman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Caitlin Freeman, Arun Niddish Mahendran, Vishesh Vikas. (2024)<br><strong>Environment-Centric Learning Approach for Gait Synthesis in Terrestrial Soft Robots</strong><br><button class=copy-to-clipboard title="Environment-Centric Learning Approach for Gait Synthesis in Terrestrial Soft Robots" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03617v1.pdf filename=2402.03617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Locomotion gaits are fundamental for control of soft terrestrial robots. However, synthesis of these gaits is challenging due to modeling of robot-environment interaction and lack of a mathematical framework. This work presents an environment-centric, data-driven and fault-tolerant <b>probabilistic</b> <b>Model-Free</b> Control (pMFC) framework that allows for soft multi-limb robots to learn from their environment and synthesize diverse sets of locomotion gaits for realizing open-loop control. Here, discretization of factors dominating robot-environment interactions enables an environment-specific graphical representation where the edges encode experimental locomotion data corresponding to the robot motion primitives. In this graph, locomotion gaits are defined as simple cycles that are transformation invariant, i.e., the locomotion is independent of the starting vertex of these periodic cycles. Gait synthesis, the problem of finding optimal locomotion gaits for a given substrate, is formulated as Binary Integer Linear Programming (BILP) problems with a linearized cost function, linear constraints, and iterative simple cycle detection. Experimentally, gaits are synthesized for varying robot-environment interactions. Variables include robot morphology - three-limb and four-limb robots, TerreSoRo-III and TerreSoRo-IV; substrate - rubber mat, whiteboard and carpet; and actuator functionality - simulated loss of robot limb actuation. On an average, gait synthesis improves the translation and rotation speeds by 82% and 97% respectively. The results highlight that data-driven methods are vital to soft robot locomotion control due to the significant influence of unexpected asymmetries in the system and the dependence of optimal gait sequences on the experimental robot-environment interaction.</p></p class="citation"></blockquote><h3 id=221277-spatial-assisted-human-drone-collaborative-navigation-and-interaction-through-immersive-mixed-reality-luca-morando-et-al-2024>(221/277) Spatial Assisted Human-Drone Collaborative Navigation and Interaction through Immersive Mixed Reality (Luca Morando et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Morando, Giuseppe Loianno. (2024)<br><strong>Spatial Assisted Human-Drone Collaborative Navigation and Interaction through Immersive Mixed Reality</strong><br><button class=copy-to-clipboard title="Spatial Assisted Human-Drone Collaborative Navigation and Interaction through Immersive Mixed Reality" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04070v1.pdf filename=2402.04070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aerial robots have the potential to play a crucial role in assisting humans with complex and dangerous tasks. Nevertheless, the future industry demands innovative solutions to streamline the interaction process between humans and drones to enable seamless collaboration and efficient co-working. In this paper, we present a novel tele-immersive framework that promotes cognitive and physical collaboration between humans and robots through Mixed Reality (MR). This framework incorporates a novel bi-directional spatial awareness and a <b>multi-modal</b> virtual-physical interaction approaches. The former seamlessly integrates the physical and virtual worlds, offering bidirectional egocentric and exocentric environmental representations. The latter, leveraging the proposed spatial representation, further enhances the collaboration combining a robot planning algorithm for obstacle avoidance with a variable admittance control. This allows users to issue commands based on virtual forces while maintaining compatibility with the environment map. We validate the proposed approach by performing several collaborative planning and exploration tasks involving a drone and an user equipped with a MR headset.</p></p class="citation"></blockquote><h3 id=222277-mmaud-a-comprehensive-multi-modal-anti-uav-dataset-for-modern-miniature-drone-threats-shenghai-yuan-et-al-2024>(222/277) MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats (Shenghai Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shenghai Yuan, Yizhuo Yang, Thien Hoang Nguyen, Thien-Minh Nguyen, Jianfei Yang, Fen Liu, Jianping Li, Han Wang, Lihua Xie. (2024)<br><strong>MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats</strong><br><button class=copy-to-clipboard title="MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03706v1.pdf filename=2402.03706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive <b>Multi-Modal</b> Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for developing accurate and efficient solutions. Our proposed modalities are cost-effective and highly adaptable, allowing users to experiment and implement new UAV threat detection tools. Our dataset closely simulates real-world scenarios by incorporating ambient heavy machinery sounds. This approach enhances the dataset&rsquo;s applicability, capturing the exact challenges faced during proximate vehicular operations. It is expected that MMAUD can play a pivotal role in advancing UAV threat detection, classification, trajectory estimation capabilities, and beyond. Our dataset, codes, and designs will be available in <a href=https://github.com/ntu-aris/MMAUD>https://github.com/ntu-aris/MMAUD</a>.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=223277-botsscl-social-bot-detection-with-self-supervised-contrastive-learning-mohammad-majid-akhtar-et-al-2024>(223/277) BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning (Mohammad Majid Akhtar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Majid Akhtar, Navid Shadman Bhuiyan, Rahat Masood, Muhammad Ikram, Salil S. Kanhere. (2024)<br><strong>BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning</strong><br><button class=copy-to-clipboard title="BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CY, cs-LG, cs-SI, cs.SI<br>Keyword Score: 50<br>Keywords: Contrastive Learning, Self-supervised Learning, Supervised Learning, Unsupervised Learning, Botnet Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03740v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03740v1.pdf filename=2402.03740v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The detection of automated accounts, also known as &ldquo;social <b>bots&rdquo;,</b> <b>has</b> been an increasingly important concern for online social networks (OSNs). While several methods have been proposed for detecting social <b>bots,</b> <b>significant</b> research gaps remain. First, current models exhibit limitations in detecting sophisticated <b>bots</b> <b>that</b> aim to mimic genuine OSN users. Second, these methods often rely on simplistic profile features, which are susceptible to manipulation. In addition to their vulnerability to adversarial manipulations, these models lack generalizability, resulting in subpar performance when trained on one dataset and tested on another. To address these challenges, we propose a novel framework for social <b>Bot</b> <b>detection</b> with <b>Self-Supervised</b> <b>Contrastive</b> <b>Learning</b> (BotSSCL). Our framework leverages <b>contrastive</b> <b>learning</b> to distinguish between social <b>bots</b> <b>and</b> humans in the embedding space to improve linear separability. The high-level representations derived by BotSSCL enhance its resilience to variations in data distribution and ensure generalizability. We evaluate BotSSCL&rsquo;s robustness against adversarial attempts to manipulate <b>bot</b> <b>accounts</b> to evade detection. Experiments on two datasets featuring sophisticated <b>bots</b> <b>demonstrate</b> that BotSSCL outperforms other <b>supervised,</b> <b>unsupervised,</b> and <b>self-supervised</b> baseline methods. We achieve approx. 6% and approx. 8% higher (F1) performance than SOTA on both datasets. In addition, BotSSCL also achieves 67% F1 when trained on one dataset and tested with another, demonstrating its generalizability. Lastly, BotSSCL increases adversarial complexity and only allows 4% success to the adversary in evading detection.</p></p class="citation"></blockquote><h3 id=224277-understanding-trends-patterns-and-dynamics-in-global-acquisitions-a-network-perspective-ghazal-kalhor-et-al-2024>(224/277) Understanding Trends, Patterns, and Dynamics in Global Acquisitions: A Network Perspective (Ghazal Kalhor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ghazal Kalhor, Behnam Bahrak. (2024)<br><strong>Understanding Trends, Patterns, and Dynamics in Global Acquisitions: A Network Perspective</strong><br><button class=copy-to-clipboard title="Understanding Trends, Patterns, and Dynamics in Global Acquisitions: A Network Perspective" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03910v1.pdf filename=2402.03910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Studying acquisitions offers invaluable insights into startup trends, aiding informed investment decisions for businesses. However, the scarcity of studies in this domain <b>prompts</b> our focus on shedding light in this area. Employing Crunchbase data, our study delves into the global network of company acquisitions using diverse network analysis techniques. Our findings unveil an acquisition network characterized by a primarily sparse structure comprising localized dense connections. We reveal a prevalent tendency among organizations to acquire companies within their own country and industry. Furthermore, our temporal analysis indicates a growth in network communities over time, accompanied by a trend toward a sparser network. Through centrality metrics computation in the cross-city acquisition network, we identify New York, London, and San Francisco as pivotal and central hubs in the global economic landscape. Finally, we show that the United States, United Kingdom, and Germany are predominant countries in international acquisitions.</p></p class="citation"></blockquote><h2 id=q-bioqm-2>q-bio.QM (2)</h2><h3 id=225277-moltc-towards-molecular-relational-modeling-in-language-models-junfeng-fang-et-al-2024>(225/277) MolTC: Towards Molecular Relational Modeling In Language Models (Junfeng Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junfeng Fang, Shuai Zhang, Chang Wu, Zhiyuan Liu, Sihang Li, Kun Wang, Wenjie Du, Xiang Wang. (2024)<br><strong>MolTC: Towards Molecular Relational Modeling In Language Models</strong><br><button class=copy-to-clipboard title="MolTC: Towards Molecular Relational Modeling In Language Models" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 43<br>Keywords: Graph Neural Network, Multi-modal, Parameter Sharing, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03781v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03781v2.pdf filename=2402.03781v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the issue of information underutilization, as it hinders the sharing of interaction mechanism learned across diverse datasets. To address these challenges, this work proposes a novel <b>LLM-based</b> <b>multi-modal</b> framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which effectively integrate graphical information of two molecules in pair. For achieving a unified MRL, MolTC innovatively develops a dynamic <b>parameter-sharing</b> <b>strategy</b> for cross-dataset information sharing. Moreover, to train MolTC efficiently, we introduce a Multi-hierarchical CoT concept to refine its training paradigm, and conduct a comprehensive Molecular Interactive Instructions dataset for the development of biochemical <b>LLMs</b> involving MRL. Our experiments, conducted across various datasets involving over 4,000,000 molecular pairs, exhibit the superiority of our method over current <b>GNN</b> and <b>LLM-based</b> baselines. Code is available at <a href=https://github.com/MangoKiller/MolTC>https://github.com/MangoKiller/MolTC</a>.</p></p class="citation"></blockquote><h3 id=226277-progress-and-opportunities-of-foundation-models-in-bioinformatics-qing-li-et-al-2024>(226/277) Progress and Opportunities of Foundation Models in Bioinformatics (Qing Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qing Li, Zhihang Hu, Yixuan Wang, Lei Li, Yimin Fan, Irwin King, Le Song, Yu Li. (2024)<br><strong>Progress and Opportunities of Foundation Models in Bioinformatics</strong><br><button class=copy-to-clipboard title="Progress and Opportunities of Foundation Models in Bioinformatics" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: I-2-1, cs-AI, cs-CL, 92-02, cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 16<br>Keywords: Foundation Model, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04286v1.pdf filename=2402.04286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bioinformatics has witnessed a paradigm shift with the increasing integration of artificial intelligence (AI), particularly through the adoption of <b>foundation</b> <b>models</b> (FMs). These AI techniques have rapidly advanced, addressing historical challenges in bioinformatics such as the scarcity of annotated data and the presence of data noise. FMs are particularly adept at handling large-scale, unlabeled data, a common scenario in biological contexts due to the time-consuming and costly nature of experimentally determining labeled data. This characteristic has allowed FMs to excel and achieve notable results in various downstream validation tasks, demonstrating their ability to represent diverse biological entities effectively. Undoubtedly, FMs have ushered in a new era in computational biology, especially in the realm of deep learning. The primary goal of this survey is to conduct a systematic investigation and summary of FMs in bioinformatics, tracing their evolution, current research status, and the methodologies employed. Central to our focus is the application of FMs to specific biological problems, aiming to guide the research community in choosing appropriate FMs for their research needs. We delve into the specifics of the problem at hand including sequence analysis, structure prediction, function annotation, and <b>multimodal</b> integration, comparing the structures and advancements against traditional methods. Furthermore, the review analyses challenges and limitations faced by FMs in biology, such as data noise, model explainability, and potential biases. Finally, we outline potential development paths and strategies for FMs in future biological research, setting the stage for continued innovation and application in this rapidly evolving field. This comprehensive review serves not only as an academic resource but also as a roadmap for future explorations and applications of FMs in biology.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=227277-personality-trait-recognition-using-ecg-spectrograms-and-deep-learning-muhammad-mohsin-altaf-et-al-2024>(227/277) Personality Trait Recognition using ECG Spectrograms and Deep Learning (Muhammad Mohsin Altaf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Mohsin Altaf, Saadat Ullah Khan, Muhammad Majd, Syed Muhammad Anwar. (2024)<br><strong>Personality Trait Recognition using ECG Spectrograms and Deep Learning</strong><br><button class=copy-to-clipboard title="Personality Trait Recognition using ECG Spectrograms and Deep Learning" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-LG, cs.HC, eess-SP<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04326v1.pdf filename=2402.04326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an innovative approach to recognizing personality traits using deep learning (DL) methods applied to electrocardiogram (ECG) signals. Within the framework of detecting the big five personality traits model encompassing extra-version, neuroticism, agreeableness, conscientiousness, and openness, the research explores the potential of ECG-derived spectrograms as informative features. Optimal window sizes for spectrogram generation are determined, and a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN),</b> specifically Resnet-18, and visual <b>transformer</b> (ViT) are employed for feature extraction and personality trait classification. The study utilizes the publicly available ASCERTAIN dataset, which comprises various physiological signals, including ECG recordings, collected from 58 participants during the presentation of video stimuli categorized by valence and arousal levels. The outcomes of this study demonstrate noteworthy performance in personality trait classification, consistently achieving F1-scores exceeding 0.9 across different window sizes and personality traits. These results emphasize the viability of ECG signal spectrograms as a valuable modality for personality trait recognition, with Resnet-18 exhibiting effectiveness in discerning distinct personality traits.</p></p class="citation"></blockquote><h3 id=228277-embedding-large-language-models-into-extended-reality-opportunities-and-challenges-for-inclusion-engagement-and-privacy-efe-bozkir-et-al-2024>(228/277) Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy (Efe Bozkir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Efe Bozkir, Süleyman Özdel, Ka Hei Carrie Lau, Mengdi Wang, Hong Gao, Enkelejda Kasneci. (2024)<br><strong>Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy</strong><br><button class=copy-to-clipboard title="Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03907v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03907v1.pdf filename=2402.03907v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in computer graphics, hardware, artificial intelligence (AI), and human-computer interaction likely lead to extended reality (XR) devices and setups being more pervasive. While these devices and setups provide users with interactive, engaging, and immersive experiences with different sensing modalities, such as eye and hand trackers, many non-player characters are utilized in a pre-scripted way or by conventional AI techniques. In this paper, we argue for using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in XR by embedding them in virtual avatars or as narratives to facilitate more inclusive experiences through <b>prompt</b> engineering according to user profiles and <b>fine-tuning</b> the <b>LLMs</b> for particular purposes. We argue that such inclusion will facilitate diversity for XR use. In addition, we believe that with the versatile conversational capabilities of <b>LLMs,</b> users will engage more with XR environments, which might help XR be more used in everyday life. Lastly, we speculate that combining the information provided to <b>LLM-powered</b> environments by the users and the biometric data obtained through the sensors might lead to novel privacy invasions. While studying such possible privacy invasions, user privacy concerns and preferences should also be investigated. In summary, despite some challenges, embedding <b>LLMs</b> into XR is a promising and novel research area with several opportunities.</p></p class="citation"></blockquote><h3 id=229277-genlens-a-systematic-evaluation-of-visual-genai-model-outputs-tica-lin-et-al-2024>(229/277) GenLens: A Systematic Evaluation of Visual GenAI Model Outputs (Tica Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tica Lin, Hanspeter Pfister, Jui-Hsien Wang. (2024)<br><strong>GenLens: A Systematic Evaluation of Visual GenAI Model Outputs</strong><br><button class=copy-to-clipboard title="GenLens: A Systematic Evaluation of Visual GenAI Model Outputs" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Fairness, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03700v1.pdf filename=2402.03700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid development of <b>generative</b> <b>AI</b> (GenAI) models in computer vision necessitates effective evaluation methods to ensure their quality and <b>fairness.</b> Existing tools primarily focus on dataset quality assurance and model explainability, leaving a significant gap in GenAI output evaluation during model development. Current practices often depend on developers&rsquo; subjective visual assessments, which may lack scalability and generalizability. This paper bridges this gap by conducting a formative study with GenAI model developers in an industrial setting. Our findings led to the development of GenLens, a visual analytic interface designed for the systematic evaluation of GenAI model outputs during the early stages of model development. GenLens offers a quantifiable approach for overviewing and annotating failure cases, customizing issue tags and classifications, and aggregating annotations from multiple users to enhance collaboration. A user study with model developers reveals that GenLens effectively enhances their workflow, evidenced by high satisfaction rates and a strong intent to integrate it into their practices. This research underscores the importance of robust early-stage evaluation tools in GenAI development, contributing to the advancement of fair and high-quality GenAI models.</p></p class="citation"></blockquote><h3 id=230277-human-emotions-analysis-and-recognition-using-eeg-signals-in-response-to-360circ-videos-haseeb-ur-rahman-abbasi-et-al-2024>(230/277) Human Emotions Analysis and Recognition Using EEG Signals in Response to 360$^\circ$ Videos (Haseeb ur Rahman Abbasi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haseeb ur Rahman Abbasi, Zeeshan Rashid, Muhammad Majid, Syed Muhammad Anwar. (2024)<br><strong>Human Emotions Analysis and Recognition Using EEG Signals in Response to 360$^\circ$ Videos</strong><br><button class=copy-to-clipboard title="Human Emotions Analysis and Recognition Using EEG Signals in Response to 360$^\circ$ Videos" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04142v1.pdf filename=2402.04142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Emotion</b> <b>recognition</b> (ER) technology is an integral part for developing innovative applications such as drowsiness detection and health monitoring that plays a pivotal role in contemporary society. This study delves into ER using electroencephalography (EEG), within immersive virtual reality (VR) environments. There are four main stages in our proposed methodology including data acquisition, pre-processing, feature extraction, and <b>emotion</b> <b>classification.</b> Acknowledging the limitations of existing 2D datasets, we introduce a groundbreaking 3D VR dataset to elevate the precision of <b>emotion</b> <b>elicitation.</b> Leveraging the Interaxon Muse headband for EEG recording and Oculus Quest 2 for VR stimuli, we meticulously recorded data from 40 participants, prioritizing subjects without reported mental illnesses. Pre-processing entails rigorous cleaning, uniform truncation, and the application of a Savitzky-Golay filter to the EEG data. Feature extraction encompasses a comprehensive analysis of metrics such as power spectral density, correlation, rational and divisional asymmetry, and power spectrum. To ensure the robustness of our model, we employed a 10-fold cross-validation, revealing an average validation accuracy of 85.54%, with a noteworthy maximum accuracy of 90.20% in the best fold. Subsequently, the trained model demonstrated a commendable test accuracy of 82.03%, promising favorable outcomes.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=231277-arman-a-reconfigurable-monolithic-3d-accelerator-architecture-for-convolutional-neural-networks-ali-sedaghatgoo-et-al-2024>(231/277) ARMAN: A Reconfigurable Monolithic 3D Accelerator Architecture for Convolutional Neural Networks (Ali Sedaghatgoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Sedaghatgoo, Amir M. Hajisadeghi, Mahmoud Momtazpour, Nader Bagherzadeh. (2024)<br><strong>ARMAN: A Reconfigurable Monolithic 3D Accelerator Architecture for Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="ARMAN: A Reconfigurable Monolithic 3D Accelerator Architecture for Convolutional Neural Networks" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-ET, cs.AR<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04431v1.pdf filename=2402.04431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> has emerged as a powerful and versatile tool for artificial intelligence (AI) applications. Conventional computing architectures face challenges in meeting the demanding processing requirements of compute-intensive <b>CNN</b> applications, as they suffer from limited throughput and low utilization. To this end, specialized accelerators have been developed to speed up <b>CNN</b> computations. However, as we demonstrate in this paper via extensive design space exploration, different neural network models have different characteristics, which calls for different accelerator architectures and configurations to match their computing demand. We show that a one-size-fits-all fixed architecture does not guarantee optimal power/energy/performance trade-off. To overcome this challenge, this paper proposes ARMAN, a novel reconfigurable systolic-array-based accelerator architecture based on Monolithic 3D (M3D) technology for <b>CNN</b> inference. The proposed accelerator offers the flexibility to reconfigure among different scale-up or scale-out arrangements depending on the neural network structure, providing the optimal trade-off across power, energy, and performance for various neural network models. We demonstrate the effectiveness of our approach through evaluations of multiple benchmarks. The results demonstrate that the proposed accelerator exhibits up to 2x, 2.24x, 1.48x, and 2x improvements in terms of execution cycles, power, energy, and EDP respectively, over the non-configurable architecture.</p></p class="citation"></blockquote><h3 id=232277-heam--hashed-embedding-acceleration-using-processing-in-memory-youngsuk-kim-et-al-2024>(232/277) HEAM : Hashed Embedding Acceleration using Processing-In-Memory (Youngsuk Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youngsuk Kim, Hyuk-Jae Lee, Chae Eun Rhee. (2024)<br><strong>HEAM : Hashed Embedding Acceleration using Processing-In-Memory</strong><br><button class=copy-to-clipboard title="HEAM : Hashed Embedding Acceleration using Processing-In-Memory" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs.AR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04032v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04032v1.pdf filename=2402.04032v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s data centers, personalized <b>recommendation</b> systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations. Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth. However, these solutions fall short when dealing with the expanding size of personalized <b>recommendation</b> systems. <b>Recommendation</b> models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers. Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources. This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate <b>recommendation</b> systems in which compositional embedding is utilized-a technique aimed at reducing the size of embedding tables. The architecture is organized into a three-tier memory hierarchy consisting of conventional DIMM, 3D-stacked DRAM with a base die-level Processing-In-Memory (PIM), and a bank group-level PIM incorporating a Look-Up-Table. This setup is specifically designed to accommodate the unique aspects of compositional embedding, such as temporal locality and embedding table capacity. This design effectively reduces bank access, improves access efficiency, and enhances overall throughput, resulting in a 6.3 times speedup and 58.9% energy savings compared to the baseline.</p></p class="citation"></blockquote><h2 id=cscy-5>cs.CY (5)</h2><h3 id=233277-the-world-of-generative-ai-deepfakes-and-large-language-models-alakananda-mitra-et-al-2024>(233/277) The World of Generative AI: Deepfakes and Large Language Models (Alakananda Mitra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alakananda Mitra, Saraju P. Mohanty, Elias Kougianos. (2024)<br><strong>The World of Generative AI: Deepfakes and Large Language Models</strong><br><button class=copy-to-clipboard title="The World of Generative AI: Deepfakes and Large Language Models" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CL, cs-CY, cs.CY<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04373v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04373v1.pdf filename=2402.04373v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We live in the era of <b>Generative</b> <b>Artificial</b> Intelligence (GenAI). Deepfakes and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are two examples of GenAI. Deepfakes, in particular, pose an alarming threat to society as they are capable of spreading misinformation and changing the truth. <b>LLMs</b> are powerful language models that generate general-purpose language. However due to its <b>generative</b> <b>aspect,</b> it can also be a risk for people if used with ill intentions. The ethical use of these technologies is a big concern. This short article tries to find out the interrelationship between them.</p></p class="citation"></blockquote><h3 id=234277-measuring-implicit-bias-in-explicitly-unbiased-large-language-models-xuechunzi-bai-et-al-2024>(234/277) Measuring Implicit Bias in Explicitly Unbiased Large Language Models (Xuechunzi Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, Thomas L. Griffiths. (2024)<br><strong>Measuring Implicit Bias in Explicitly Unbiased Large Language Models</strong><br><button class=copy-to-clipboard title="Measuring Implicit Bias in Explicitly Unbiased Large Language Models" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CL, cs-CY, cs.CY<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04105v1.pdf filename=2402.04105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can pass explicit bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as <b>LLMs</b> become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both of these challenges by introducing two measures of bias inspired by psychology: <b>LLM</b> Implicit Association Test (IAT) Bias, which is a <b>prompt-based</b> method for revealing implicit bias; and <b>LLM</b> Decision Bias for detecting subtle discrimination in decision-making tasks. Using these measures, we found pervasive human-like stereotype biases in 6 <b>LLMs</b> across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others). Our <b>prompt-based</b> measure of implicit bias correlates with embedding-based methods but better predicts downstream behaviors measured by <b>LLM</b> Decision Bias. This measure is based on asking the <b>LLM</b> to decide between individuals, motivated by psychological results indicating that relative not absolute evaluations are more related to implicit biases. Using <b>prompt-based</b> measures informed by psychology allows us to effectively expose nuanced biases and subtle discrimination in proprietary <b>LLMs</b> that do not show explicit bias on standard benchmarks.</p></p class="citation"></blockquote><h3 id=235277-ai-language-models-as-role-playing-tools-not-human-participants-zhicheng-lin-2024>(235/277) AI language models as role-playing tools, not human participants (Zhicheng Lin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhicheng Lin. (2024)<br><strong>AI language models as role-playing tools, not human participants</strong><br><button class=copy-to-clipboard title="AI language models as role-playing tools, not human participants" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04470v1.pdf filename=2402.04470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in AI invite misuse of language models as replacements for human participants. We argue that treating their responses as glimpses into an average human mind fundamentally mischaracterizes these statistical algorithms and that language models should be embraced as flexible <b>simulation</b> tools, able to mimic diverse behaviors without possessing human traits themselves.</p></p class="citation"></blockquote><h3 id=236277-prioritizing-safeguarding-over-autonomy-risks-of-llm-agents-for-science-xiangru-tang-et-al-2024>(236/277) Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science (Xiangru Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein. (2024)<br><strong>Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science</strong><br><button class=copy-to-clipboard title="Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.CY<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04247v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04247v2.pdf filename=2402.04247v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intelligent agents powered by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in <b>LLM-based</b> agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific <b>LLM</b> agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.</p></p class="citation"></blockquote><h3 id=237277-measuring-machine-learning-harms-from-stereotypes-requires-understanding-who-is-being-harmed-by-which-errors-in-what-ways-angelina-wang-et-al-2024>(237/277) Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways (Angelina Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angelina Wang, Xuechunzi Bai, Solon Barocas, Su Lin Blodgett. (2024)<br><strong>Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways</strong><br><button class=copy-to-clipboard title="Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04420v1.pdf filename=2402.04420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As machine learning applications proliferate, we need an understanding of their potential for harm. However, current <b>fairness</b> metrics are rarely grounded in human psychological experiences of harm. Drawing on the social psychology of stereotypes, we use a case study of gender stereotypes in image search to examine how people react to machine learning errors. First, we use survey studies to show that not all machine learning errors reflect stereotypes nor are equally harmful. Then, in experimental studies we randomly expose participants to stereotype-reinforcing, -violating, and -neutral machine learning errors. We find stereotype-reinforcing errors induce more experientially (i.e., subjectively) harmful experiences, while having minimal changes to cognitive beliefs, attitudes, or behaviors. This experiential harm impacts women more than men. However, certain stereotype-violating errors are more experientially harmful for men, potentially due to perceived threats to masculinity. We conclude that harm cannot be the sole guide in <b>fairness</b> mitigation, and propose a nuanced perspective depending on who is experiencing what harm and why.</p></p class="citation"></blockquote><h2 id=csit-6>cs.IT (6)</h2><h3 id=238277-sensing-mutual-information-with-random-signals-in-gaussian-channels-bridging-sensing-and-communication-metrics-lei-xie-et-al-2024>(238/277) Sensing Mutual Information with Random Signals in Gaussian Channels: Bridging Sensing and Communication Metrics (Lei Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Xie, Fan Liu, Jiajin Luo, Shenghui Song. (2024)<br><strong>Sensing Mutual Information with Random Signals in Gaussian Channels: Bridging Sensing and Communication Metrics</strong><br><button class=copy-to-clipboard title="Sensing Mutual Information with Random Signals in Gaussian Channels: Bridging Sensing and Communication Metrics" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Mutual Information, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03919v1.pdf filename=2402.03919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sensing performance is typically evaluated by classical radar metrics, such as Cramer-Rao bound and signal-to-clutter-plus-noise ratio. The recent development of the integrated sensing and communication (ISAC) framework motivated the efforts to unify the performance metric for sensing and communication, where <b>mutual</b> <b>information</b> (MI) was proposed as a sensing performance metric with deterministic signals. However, the need of communication in ISAC systems necessitates the transmission of random signals for sensing applications, whereas an explicit evaluation for the sensing <b>mutual</b> <b>information</b> (SMI) with random signals is not yet available in the literature. This paper aims to fill the research gap and investigate the unification of sensing and communication performance metrics. For that purpose, we first derive the explicit expression for the SMI with random signals utilizing random matrix theory. On top of that, we further build up the connections between SMI and traditional sensing metrics, such as ergodic minimum mean square error (EMMSE), ergodic linear minimum mean square error (ELMMSE), and ergodic Bayesian Cram'{e}r-Rao bound (EBCRB). Such connections open up the opportunity to unify sensing and communication performance metrics, which facilitates the analysis and design for ISAC systems. Finally, SMI is utilized to optimize the precoder for both sensing-only and ISAC applications. <b>Simulation</b> results validate the accuracy of the theoretical results and the effectiveness of the proposed precoding designs.</p></p class="citation"></blockquote><h3 id=239277-on-learning-spatial-provenance-in-privacy-constrained-wireless-networks-manish-bansal-et-al-2024>(239/277) On Learning Spatial Provenance in Privacy-Constrained Wireless Networks (Manish Bansal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manish Bansal, Pramsu Srivastava, J. Harshan. (2024)<br><strong>On Learning Spatial Provenance in Privacy-Constrained Wireless Networks</strong><br><button class=copy-to-clipboard title="On Learning Spatial Provenance in Privacy-Constrained Wireless Networks" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-NI, cs.IT, math-IT<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, BLOOM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03702v1.pdf filename=2402.03702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Vehicle-to-Everything networks that involve multi-hop communication, the Road Side Units (RSUs) typically aim to collect location information from the participating vehicles to provide security and network diagnostics features. While the vehicles commonly use the Global Positioning System (GPS) for navigation, they may refrain from sharing their precise GPS coordinates with the RSUs due to privacy concerns. Therefore, to jointly address the high localization requirements by the RSUs as well as the vehicles&rsquo; privacy, we present a novel spatial-provenance framework wherein each vehicle uses <b>Bloom</b> filters to embed their partial location information when forwarding the packets. In this framework, the RSUs and the vehicles agree upon fragmenting the coverage area into several smaller regions so that the vehicles can embed the identity of their regions through <b>Bloom</b> filters. Given the probabilistic nature of <b>Bloom</b> filters, we derive an analytical expression on the error-rates in provenance recovery and then pose an optimization problem to choose the underlying parameters. With the help of extensive <b>simulation</b> results, we show that our method offers near-optimal <b>Bloom</b> filter parameters in learning spatial provenance. Some interesting trade-offs between the communication-overhead, spatial privacy of the vehicles and the error rates in provenance recovery are also discussed.</p></p class="citation"></blockquote><h3 id=240277-batch-universal-prediction-marco-bondaschi-et-al-2024>(240/277) Batch Universal Prediction (Marco Bondaschi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Bondaschi, Michael Gastpar. (2024)<br><strong>Batch Universal Prediction</strong><br><button class=copy-to-clipboard title="Batch Universal Prediction" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs.IT, math-IT, stat-ML<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03901v1.pdf filename=2402.03901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently gained much popularity due to their surprising ability at generating human-like English sentences. <b>LLMs</b> are essentially predictors, estimating the probability of a sequence of words given the past. Therefore, it is natural to evaluate their performance from a universal prediction perspective. In order to do that fairly, we introduce the notion of batch regret as a modification of the classical average regret, and we study its asymptotical value for add-constant predictors, in the case of memoryless sources and first-order Markov sources.</p></p class="citation"></blockquote><h3 id=241277-vector-approximate-message-passing-with-arbitrary-iid-noise-priors-mohamed-akrout-et-al-2024>(241/277) Vector Approximate Message Passing With Arbitrary I.I.D. Noise Priors (Mohamed Akrout et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Akrout, Tiancheng Gao, Faouzi Bellili, Amine Mezghani. (2024)<br><strong>Vector Approximate Message Passing With Arbitrary I.I.D. Noise Priors</strong><br><button class=copy-to-clipboard title="Vector Approximate Message Passing With Arbitrary I.I.D. Noise Priors" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04111v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04111v1.pdf filename=2402.04111v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Approximate message passing (AMP) algorithms are devised under the Gaussianity assumption of the measurement noise vector. In this work, we relax this assumption within the vector AMP (VAMP) framework to arbitrary independent and identically distributed (i.i.d.) noise priors. We do so by rederiving the linear minimum mean square error (LMMSE) to accommodate both the noise and signal estimations within the message passing steps of VAMP. Numerical results demonstrate how our proposed algorithm handles non-Gaussian noise models as compared to VAMP. This extension to general noise priors enables the use of AMP algorithms in a wider range of engineering applications where non-Gaussian noise models are more appropriate.</p></p class="citation"></blockquote><h3 id=242277-tail-erasure-correcting-codes-boaz-moav-et-al-2024>(242/277) Tail-Erasure-Correcting Codes (Boaz Moav et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boaz Moav, Ryan Gabrys, Eitan Yaakobi. (2024)<br><strong>Tail-Erasure-Correcting Codes</strong><br><button class=copy-to-clipboard title="Tail-Erasure-Correcting Codes" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03987v1.pdf filename=2402.03987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing demand for data storage has <b>prompted</b> the exploration of new techniques, with molecular data storage being a promising alternative. In this work, we develop coding schemes for a new storage paradigm that can be represented as a collection of two-dimensional arrays. Motivated by error patterns observed in recent prototype architectures, our study focuses on correcting erasures in the last few symbols of each row, and also correcting arbitrary deletions across rows. We present code constructions and explicit encoders and decoders that are shown to be nearly optimal in many scenarios. We show that the new coding schemes are capable of effectively mitigating these errors, making these emerging storage platforms potentially promising solutions.</p></p class="citation"></blockquote><h3 id=243277-fundamental-limits-of-two-hop-mimo-channels-an-asymptotic-approach-zeyan-zhuang-et-al-2024>(243/277) Fundamental Limits of Two-Hop MIMO Channels: An Asymptotic Approach (Zeyan Zhuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyan Zhuang, Xin Zhang, Dongfang Xu, Shenghui Song. (2024)<br><strong>Fundamental Limits of Two-Hop MIMO Channels: An Asymptotic Approach</strong><br><button class=copy-to-clipboard title="Fundamental Limits of Two-Hop MIMO Channels: An Asymptotic Approach" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03772v1.pdf filename=2402.03772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-antenna relays and intelligent reflecting surfaces (IRSs) have been utilized to construct favorable channels to improve the performance of wireless systems. A common feature between relay systems and IRS-aided systems is the two-hop multiple-input multiple-output (MIMO) channel. As a result, the <b>mutual</b> <b>information</b> (MI) of two-hop MIMO channels has been widely investigated with very engaging results. However, a rigorous investigation on the fundamental limits of two-hop MIMO channels, i.e., the first and second-order analysis, is not yet available in the literature, due to the difficulties caused by the two-hop (product) channel and the noise introduced by the relay (active IRS). In this paper, we employ large-scale random matrix theory (RMT), specifically Gaussian tools, to derive the closed-form deterministic approximation for the mean and variance of the MI. Additionally, we determine the convergence rate for the mean, variance and the characteristic function of the MI, and prove the asymptotic Gaussianity. Furthermore, we also investigate the analytical properties of the fundamental equations that describe the closed-form approximation and prove the existence and uniqueness of the solution. An iterative algorithm is then proposed to obtain the solution for the fundamental equations. Numerical results validate the accuracy of the theoretical analysis.</p></p class="citation"></blockquote><h2 id=csse-7>cs.SE (7)</h2><h3 id=244277-assured-llm-based-software-engineering-nadia-alshahwan-et-al-2024>(244/277) Assured LLM-Based Software Engineering (Nadia Alshahwan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nadia Alshahwan, Mark Harman, Inna Harper, Alexandru Marginean, Shubho Sengupta, Eddy Wang. (2024)<br><strong>Assured LLM-Based Software Engineering</strong><br><button class=copy-to-clipboard title="Assured LLM-Based Software Engineering" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04380v1.pdf filename=2402.04380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we address the following question: How can we use <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to improve code independently of a human, while ensuring that the improved code - does not regress the properties of the original code? - improves the original in a verifiable and measurable way? To address this question, we advocate Assured <b>LLM-Based</b> Software Engineering; a generate-and-test approach, inspired by Genetic Improvement. Assured LLMSE applies a series of semantic filters that discard code that fails to meet these twin guarantees. This overcomes the potential problem of <b>LLM&rsquo;s</b> propensity to hallucinate. It allows us to generate code using <b>LLMs,</b> independently of any human. The human plays the role only of final code reviewer, as they would do with code generated by other human engineers. This paper is an outline of the content of the keynote by Mark Harman at the International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering, Monday 15th April 2024, Lisbon, Portugal.</p></p class="citation"></blockquote><h3 id=245277-multi-line-ai-assisted-code-authoring-omer-dunay-et-al-2024>(245/277) Multi-line AI-assisted Code Authoring (Omer Dunay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omer Dunay, Daniel Cheng, Adam Tait, Parth Thakkar, Peter C Rigby, Andy Chiu, Imad Ahmad, Arun Ganesan, Chandra Maddila, Vijayaraghavan Murali, Ali Tayyebi, Nachiappan Nagappan. (2024)<br><strong>Multi-line AI-assisted Code Authoring</strong><br><button class=copy-to-clipboard title="Multi-line AI-assisted Code Authoring" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04141v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04141v1.pdf filename=2402.04141v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>CodeCompose is an AI-assisted code authoring tool powered by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> that provides inline suggestions to 10&rsquo;s of thousands of developers at Meta. In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions. This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers. First, we discuss how multi-line suggestions can have a &lsquo;jarring&rsquo; effect, as the <b>LLM&rsquo;s</b> suggestions constantly move around the developer&rsquo;s existing code, which would otherwise result in decreased productivity and satisfaction. Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users. These model-hosting optimizations sped up multi-line suggestion latency by 2.5x. Finally, we conduct experiments on 10&rsquo;s of thousands of engineers to understand how multi-line suggestions impact the user experience and contrast this with single-line suggestions. Our experiments reveal that (i) multi-line suggestions account for 42% of total characters accepted (despite only accounting for 16% for displayed suggestions) (ii) multi-line suggestions almost doubled the percentage of keystrokes saved for users from 9% to 17%. Multi-line CodeCompose has been rolled out to all engineers at Meta, and less than 1% of engineers have opted out of multi-line suggestions.</p></p class="citation"></blockquote><h3 id=246277-automated-description-generation-for-software-patches-thanh-trong-vu-et-al-2024>(246/277) Automated Description Generation for Software Patches (Thanh Trong Vu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thanh Trong Vu, Tuan-Dung Bui, Thanh-Dat Do, Thu-Trang Nguyen, Hieu Dinh Vo, Son Nguyen. (2024)<br><strong>Automated Description Generation for Software Patches</strong><br><button class=copy-to-clipboard title="Automated Description Generation for Software Patches" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03805v1.pdf filename=2402.03805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software patches are pivotal in refining and evolving codebases, addressing bugs, vulnerabilities, and optimizations. Patch descriptions provide detailed accounts of changes, aiding comprehension and collaboration among developers. However, manual description creation poses challenges in terms of time consumption and variations in quality and detail. In this paper, we propose PATCHEXPLAINER, an approach that addresses these challenges by framing patch description generation as a <b>machine</b> <b>translation</b> task. In PATCHEXPLAINER, we leverage explicit representations of critical elements, historical context, and syntactic conventions. Moreover, the translation model in PATCHEXPLAINER is designed with an awareness of description similarity. Particularly, the model is explicitly trained to recognize and incorporate similarities present in patch descriptions clustered into groups, improving its ability to generate accurate and consistent descriptions across similar patches. The dual objectives maximize similarity and accurately predict affiliating groups. Our experimental results on a large dataset of real-world software patches show that PATCHEXPLAINER consistently outperforms existing methods, with improvements up to 189% in <b>BLEU,</b> 5.7X in Exact Match rate, and 154% in Semantic Similarity, affirming its effectiveness in generating software patch descriptions.</p></p class="citation"></blockquote><h3 id=247277-improving-automated-code-reviews-learning-from-experience-hong-yi-lin-et-al-2024>(247/277) Improving Automated Code Reviews: Learning from Experience (Hong Yi Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hong Yi Lin, Patanamon Thongtanunam, Christoph Treude, Wachiraphan Charoenwet. (2024)<br><strong>Improving Automated Code Reviews: Learning from Experience</strong><br><button class=copy-to-clipboard title="Improving Automated Code Reviews: Learning from Experience" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03777v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03777v1.pdf filename=2402.03777v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern code review is a critical quality assurance process that is widely adopted in both industry and open source software environments. This process can help newcomers learn from the feedback of experienced reviewers; however, it often brings a <b>large</b> <b>workload</b> <b>and</b> stress to reviewers. To alleviate this burden, the field of automated code reviews aims to automate the process, teaching <b>large</b> <b>language</b> <b>models</b> to provide reviews on submitted code, just as a human would. A recent approach pre-trained and <b>fine-tuned</b> the code intelligent language model on a <b>large-scale</b> <b>code</b> <b>review</b> corpus. However, such techniques did not fully utilise quality reviews amongst the training data. Indeed, reviewers with a higher level of experience or familiarity with the code will likely provide deeper insights than the others. In this study, we set out to investigate whether higher-quality reviews can be generated from automated code review models that are trained based on an experience-aware oversampling technique. Through our quantitative and qualitative evaluation, we find that experience-aware oversampling can increase the correctness, level of information, and meaningfulness of reviews generated by the current state-of-the-art model without introducing new data. The results suggest that a vast amount of high-quality reviews are underutilised with current training strategies. This work sheds light on resource-efficient ways to boost automated code review models.</p></p class="citation"></blockquote><h3 id=248277-investigating-the-utility-of-chatgpt-in-the-issue-tracking-system-an-exploratory-study-joy-krishan-das-et-al-2024>(248/277) Investigating the Utility of ChatGPT in the Issue Tracking System: An Exploratory Study (Joy Krishan Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joy Krishan Das, Saikat Mondal, Chanchal K. Roy. (2024)<br><strong>Investigating the Utility of ChatGPT in the Issue Tracking System: An Exploratory Study</strong><br><button class=copy-to-clipboard title="Investigating the Utility of ChatGPT in the Issue Tracking System: An Exploratory Study" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: ChatGPT, Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03735v1.pdf filename=2402.03735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Issue tracking systems serve as the primary tool for incorporating external users and customizing a software project to meet the users&rsquo; requirements. However, the limited number of contributors and the challenge of identifying the best approach for each issue often impede effective resolution. Recently, an increasing number of developers are turning to AI tools like <b>ChatGPT</b> to enhance problem-solving efficiency. While previous studies have demonstrated the potential of <b>ChatGPT</b> in areas such as automatic program repair, debugging, and <b>code</b> <b>generation,</b> there is a lack of study on how developers explicitly utilize <b>ChatGPT</b> to resolve issues in their tracking system. Hence, this study aims to examine the interaction between <b>ChatGPT</b> and developers to analyze their prevalent activities and provide a resolution. In addition, we assess the <b>code</b> <b>reliability</b> by confirming if the <b>code</b> <b>produced</b> by <b>ChatGPT</b> was integrated into the project&rsquo;s codebase using the clone detection tool NiCad. Our investigation reveals that developers mainly use <b>ChatGPT</b> for brainstorming solutions but often opt to write their <b>code</b> <b>instead</b> of using <b>ChatGPT-generated</b> <b>code,</b> <b>possibly</b> due to concerns over the generation of &ldquo;hallucinated <b>code&rdquo;,</b> <b>as</b> highlighted in the literature.</p></p class="citation"></blockquote><h3 id=249277-enhancing-llm-based-coding-tools-through-native-integration-of-ide-derived-static-context-yichen-li-et-al-2024>(249/277) Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context (Yichen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichen Li, Yun Peng, Yintong Huo, Michael R. Lyu. (2024)<br><strong>Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context</strong><br><button class=copy-to-clipboard title="Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03630v1.pdf filename=2402.03630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have achieved remarkable success in code completion, as evidenced by their essential roles in developing code assistant services such as Copilot. Being trained on in-file contexts, current <b>LLMs</b> are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for <b>large</b> <b>software</b> <b>projects</b> that require cross-file information. Existing research on <b>LLM-based</b> repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of <b>LLMs.</b> In this paper, we argue that Integrated Development Environments (IDEs) can provide direct, accurate and real-time cross-file information for repository-level code completion. We propose IDECoder, a practical framework that leverages IDE native static contexts for cross-context construction and diagnosis results for self-refinement. IDECoder utilizes the rich cross-context information available in IDEs to enhance the capabilities of <b>LLMs</b> of repository-level code completion. We conducted preliminary experiments to validate the performance of IDECoder and observed that this synergy represents a promising trend for future exploration.</p></p class="citation"></blockquote><h3 id=250277-studying-vulnerable-code-entities-in-r-zixiao-zhao-et-al-2024>(250/277) Studying Vulnerable Code Entities in R (Zixiao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixiao Zhao, Millon Madhur Das, Fatemeh H. Fard. (2024)<br><strong>Studying Vulnerable Code Entities in R</strong><br><button class=copy-to-clipboard title="Studying Vulnerable Code Entities in R" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04421v1.pdf filename=2402.04421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained Code Language Models (Code-PLMs) have shown many advancements and achieved state-of-the-art results for many software engineering tasks in the past few years. These models are mainly targeted for popular programming languages such as Java and Python, leaving out many other ones like R. Though R has a wide community of developers and users, there is little known about the applicability of Code-PLMs for R. In this preliminary study, we aim to investigate the vulnerability of Code-PLMs for code entities in R. For this purpose, we use an R dataset of code and comment pairs and then apply CodeAttack, a black-box attack model that uses the structure of code to generate adversarial code samples. We investigate how the model can attack different entities in R. This is the first step towards understanding the importance of R token types, compared to popular programming languages (e.g., Java). We limit our study to code <b>summarization.</b> Our results show that the most vulnerable code entity is the identifier, followed by some syntax tokens specific to R. The results can shed light on the importance of token types and help in developing models for code <b>summarization</b> and method name prediction for the R language.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=251277-resource-aware-hierarchical-federated-learning-in-wireless-video-caching-networks-md-ferdous-pervej-et-al-2024>(251/277) Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks (Md Ferdous Pervej et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Ferdous Pervej, Andreas F. Molisch. (2024)<br><strong>Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks</strong><br><button class=copy-to-clipboard title="Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-LG, cs-NI, cs-SY, cs.NI, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04216v1.pdf filename=2402.04216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks. Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs). As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users&rsquo; future demands. Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user&rsquo;s future content requests. A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content. Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive the convergence bound of the proposed algorithm. Based on this bound, we minimize a weighted utility function for jointly configuring the controllable parameters to train the RawHFL energy efficiently under practical resource constraints. Our extensive <b>simulation</b> results validate the proposed algorithm&rsquo;s superiority, in terms of test accuracy and energy cost, over existing baselines.</p></p class="citation"></blockquote><h3 id=252277-demarking-a-defense-for-network-flow-watermarking-in-real-time-yali-yuan-et-al-2024>(252/277) DeMarking: A Defense for Network Flow Watermarking in Real-Time (Yali Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yali Yuan, Jian Ge, Guang Cheng. (2024)<br><strong>DeMarking: A Defense for Network Flow Watermarking in Real-Time</strong><br><button class=copy-to-clipboard title="DeMarking: A Defense for Network Flow Watermarking in Real-Time" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03760v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03760v2.pdf filename=2402.03760v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The network flow watermarking technique associates the two communicating parties by actively modifying certain characteristics of the stream generated by the sender so that it covertly carries some special marking information. Some curious users communicating with the hidden server as a Tor client may attempt de-anonymization attacks to uncover the real identity of the hidden server by using this technique. This compromises the privacy of the anonymized communication system. Therefore, we propose a defense scheme against flow watermarking. The scheme is based on deep neural networks and utilizes <b>generative</b> <b>adversarial</b> <b>networks</b> to convert the original Inter-Packet Delays (IPD) into new IPDs generated by the model. We also adopt the concept of <b>adversarial</b> <b>attacks</b> to ensure that the detector will produce an incorrect classification when detecting these new IPDs. This approach ensures that these IPDs are considered &ldquo;clean&rdquo;, effectively covering the potential watermarks. This scheme is effective against time-based flow watermarking techniques.</p></p class="citation"></blockquote><h2 id=eesssy-5>eess.SY (5)</h2><h3 id=253277-a-digital-twin-design-methodology-for-control-simulation-and-monitoring-of-fluidic-circuits-veyis-gunes-2024>(253/277) A Digital Twin Design Methodology for Control, Simulation, and Monitoring of Fluidic Circuits (Veyis Gunes, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Veyis Gunes. (2024)<br><strong>A Digital Twin Design Methodology for Control, Simulation, and Monitoring of Fluidic Circuits</strong><br><button class=copy-to-clipboard title="A Digital Twin Design Methodology for Control, Simulation, and Monitoring of Fluidic Circuits" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-ET, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04058v1.pdf filename=2402.04058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a synthesis method for the design of digital twins applicable to various systems (pneumatic, hydraulic, electrical/electronic circuits). The methodology allows representing the operation of these systems through an active digital twin, thereby enabling a more suitable and easier computer-aided design, <b>simulation,</b> control, and monitoring. Furthermore, our methodology enables the detection of a system&rsquo;s actions on its own inputs (for example, in pneumatics: backflow of gases trapped in part of a fluidic system onto its own inputs). During the <b>simulation</b> or monitoring phase, the approach also facilitates real-time diagnosis of the controlled system. The outputs, on the controlled physical system or its digital twin, do not depend only on the current inputs but also on the history of the inputs and the history of internal states and variables. In other words, the underlying sequential logic has a memory while an only combinational logic approach does not. These capabilities can contribute to the digital transformation of the factory of the future.</p></p class="citation"></blockquote><h3 id=254277-pmsm-transient-response-optimization-by-end-to-end-optimal-control-yuta-kawachi-et-al-2024>(254/277) PMSM transient response optimization by end-to-end optimal control (Yuta Kawachi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuta Kawachi, Mitsuru Ambai, Yuichi Yoshida, Gaku Takano. (2024)<br><strong>PMSM transient response optimization by end-to-end optimal control</strong><br><button class=copy-to-clipboard title="PMSM transient response optimization by end-to-end optimal control" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03820v1.pdf filename=2402.03820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speed responses of motors, especially Permanent Magnet Synchronous Motors (PMSMs), are increasing in importance for recent applications, such as electric vehicles or quadrotors. These applications require quick acceleration performance. However, commercial controllers are based mainly on Proportional-Integral (PI) controllers, which are suitable for eliminating steady-state errors but unsuitable for transient response optimization. In this paper, we replaced whole conventional controllers with an end-to-end <b>Recurrent</b> <b>Neural</b> <b>Network</b> <b>(RNN)</b> that has a regularized transition matrix. Our end-to-end controller directly minimizes the transient response time on the basis of optimal control theory. Computer-simulated results show that speed response indices improved using the <b>RNN</b> rather than a PI controller, while both were under comparable power losses. The current vector trajectories of the <b>RNN</b> showed that the <b>RNN</b> could automatically determine arbitrary trajectories in the flux-weakening region in accordance with an arbitrarily designed loss function. In contrast, the traditional flux-weakening methods using PI controllers have pre-determined current vector trajectories.</p></p class="citation"></blockquote><h3 id=255277-pso-based-adaptive-nmpc-for-uranium-extraction-scrubbing-operation-in-spent-nuclear-fuel-treatment-process-duc-tri-vo-et-al-2024>(255/277) PSO-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process (Duc-Tri Vo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duc-Tri Vo, Ionela Prodan, Laurent Lefèvre, Vincent Vanel, Sylvain Costenoble, Binh Dinh. (2024)<br><strong>PSO-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process</strong><br><button class=copy-to-clipboard title="PSO-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03656v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03656v1.pdf filename=2402.03656v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the particularities of adaptive optimal control of the uranium extraction-scrubbing operation in the PUREX process. The process dynamics are nonlinear, high dimensional, and have limited online measurements. In addition, analysis and developments are based on a qualified <b>simulation</b> program called PAREX, which was validated with laboratory and industrial data. The control objective is to stabilize the process at a desired solvent saturation level, guaranteeing constraints and handling disturbances. The developed control strategy relies on optimization-based methods for computing control inputs and estimates, i.e., Nonlinear Model Predictive Control (NMPC) and Nonlinear Moving Horizon Estimation (NMHE). The designs of these two associated algorithms are tailored for this process&rsquo;s particular dynamics and are implemented through an enhanced Particle Swarm Optimization (PSO) to guarantee constraint satisfaction. Software-in-the-loop <b>simulations</b> using PAREX show that the designed control scheme effectively satisfies control objectives and guarantees constraints during operation.</p></p class="citation"></blockquote><h3 id=256277-equitable-networked-microgrid-topology-reconfiguration-for-wildfire-risk-mitigation-yuqi-zhou-et-al-2024>(256/277) Equitable Networked Microgrid Topology Reconfiguration for Wildfire Risk Mitigation (Yuqi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Zhou, Ahmed Zamzam, Andrey Bernstein. (2024)<br><strong>Equitable Networked Microgrid Topology Reconfiguration for Wildfire Risk Mitigation</strong><br><button class=copy-to-clipboard title="Equitable Networked Microgrid Topology Reconfiguration for Wildfire Risk Mitigation" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04444v1.pdf filename=2402.04444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Increasing amount of wildfires in recent years consistently challenges the safe and reliable operations of power systems. To prevent power lines and other electrical components from causing wildfires under extreme conditions, electric utilities often deploy public safety power shutoffs (PSPS) to mitigate the wildfire risks therein. Although PSPS are effective countermeasures against wildfires, uncoordinated strategies can cause disruptions in electricity supply and even lead to cascading failures. Meanwhile, it is extremely important to consider mitigating biased decisions on different communities and populations during the implementation of shutoff actions. In this work, we primarily focus on the dynamic reconfiguration problem of networked microgrids with distributed energy resources. In particular, we formulate a rolling horizon optimization problem allowing for flexible network reconfiguration at each time interval to mitigate wildfire risks. To promote equity and <b>fairness</b> during the span of shutoffs, we further enforce a range of constraints associated with load shedding to discourage disproportionate impact on individual load blocks. Numerical studies on the modified IEEE 13-bus system and a larger-sized Smart-DS system demonstrate the performance of the proposed algorithm towards more equitable power shutoff operations.</p></p class="citation"></blockquote><h3 id=257277-design-and-implementation-of-a-real-time-onboard-system-for-a-stratospheric-balloon-mission-using-commercial-off-the-self-components-and-a-model-based-approach-angel-grover-perez-munoz-et-al-2024>(257/277) Design and implementation of a real-time onboard system for a stratospheric balloon mission using commercial off-the-self components and a model-based approach (Angel-Grover Perez-Munoz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angel-Grover Perez-Munoz, Jose-Carlos Gamazo-Real, David Gonzalez-Barcena, Juan Zamorano. (2024)<br><strong>Design and implementation of a real-time onboard system for a stratospheric balloon mission using commercial off-the-self components and a model-based approach</strong><br><button class=copy-to-clipboard title="Design and implementation of a real-time onboard system for a stratospheric balloon mission using commercial off-the-self components and a model-based approach" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: astro-ph-IM, cs-AR, cs-OS, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04079v1.pdf filename=2402.04079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stratospheric balloons have emerged as an affordable and flexible alternative to traditional spacecrafts as they are implemented using commercial off-the-shelf (COTS) equipment without following strict methodologies. HERCCULES is a stratospheric balloon mission that aims to characterize the convective heat and radiative environment in the stratosphere. The purpose of this article is to present the HERCCULES onboard software (OBSW) whose design and complexity is comparable to that of satellite systems, since it must control about sixty COTS equipment using a single Raspberry Pi 4B as onboard computer and ensure the real-time requirements. Compared to similar systems, novel contributions are presented as the OBSW is developed following modelbased and component-based approaches using the TASTE toolchain from the European Space Agency (ESA) for automatic <b>code</b> <b>generation.</b> Besides, the OBSW is verified and validated following the ESA standards and the results obtained demonstrate the suitability and efficiency of the solution and the selected methodologies.</p></p class="citation"></blockquote><h2 id=cssy-1>cs.SY (1)</h2><h3 id=258277-robust-data-enabled-predictive-leading-cruise-control-via-reachability-analysis-shuai-li-et-al-2024>(258/277) Robust Data-EnablEd Predictive Leading Cruise Control via Reachability Analysis (Shuai Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Li, Chaoyi Chen, Haotian Zheng, Jiawei Wang, Qing Xu, Keqiang Li. (2024)<br><strong>Robust Data-EnablEd Predictive Leading Cruise Control via Reachability Analysis</strong><br><button class=copy-to-clipboard title="Robust Data-EnablEd Predictive Leading Cruise Control via Reachability Analysis" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SY<br>Categories: cs-SY, cs.SY, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03897v1.pdf filename=2402.03897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data-driven predictive control promises modelfree wave-dampening strategies for Connected and Autonomous Vehicles (CAVs) in mixed traffic flow. However, the performance suffers from unknown noise and disturbances, which could occur in offline data collection and online predictive control. In this paper, we propose a Robust Data-EnablEd Predictive Leading Cruise Control (RDeeP-LCC) method based on reachability analysis, aiming to achieve safe and optimal control of CAVs under bounded process noise and external disturbances. Precisely, we decouple the mixed platoon system into an error system and a nominal system, and tighten the constraint via the data-driven reachable set technique. Then, the enhanced safety constraint is integrated with the data-driven predictive control formulation to achieve stronger robust control performance for CAVs. <b>Simulations</b> validate the effectiveness of the proposed method in mitigating traffic waves with better robustness.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=259277-finite-volumes-for-the-gross-pitaevskii-equation-quentin-chauleur-2024>(259/277) Finite volumes for the Gross-Pitaevskii equation (Quentin Chauleur, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quentin Chauleur. (2024)<br><strong>Finite volumes for the Gross-Pitaevskii equation</strong><br><button class=copy-to-clipboard title="Finite volumes for the Gross-Pitaevskii equation" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03821v1.pdf filename=2402.03821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the approximation by a Voronoi finite-volume scheme of the Gross-Pitaevskii equation with time-dependent potential in two and three dimensions. We perform an explicit splitting scheme for the time integration alongside a two-point flux approximation scheme in space. We rigorously analyze the error bounds relying on discrete uniform Sobolev inequalities. We also prove the convergence of the pseudo-vorticity of the wave function. We finally perform some numerical <b>simulations</b> to illustrate our theoretical results.</p></p class="citation"></blockquote><h3 id=260277-strong-approximation-of-the-time-fractional-cahn--hilliard-equation-driven-by-a-fractionally-integrated-additive-noise-mariam-al-maskari-et-al-2024>(260/277) Strong approximation of the time-fractional Cahn&ndash;Hilliard equation driven by a fractionally integrated additive noise (Mariam Al-Maskari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mariam Al-Maskari, Samir Karaa. (2024)<br><strong>Strong approximation of the time-fractional Cahn&ndash;Hilliard equation driven by a fractionally integrated additive noise</strong><br><button class=copy-to-clipboard title="Strong approximation of the time-fractional Cahn--Hilliard equation driven by a fractionally integrated additive noise" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03790v1.pdf filename=2402.03790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider the numerical approximation of a time-fractional stochastic Cahn&ndash;Hilliard equation driven by an additive fractionally integrated Gaussian noise. The model involves a Caputo fractional derivative in time of order $\alpha\in(0,1)$ and a fractional time-integral noise of order $\gamma\in[0,1]$. The numerical scheme approximates the model by a piecewise linear finite element method in space and a <b>convolution</b> quadrature in time (for both time-fractional operators), along with the $L^2$-projection for the noise. We carefully investigate the spatially semidiscrete and fully discrete schemes, and obtain strong convergence rates by using clever energy arguments. The temporal H"older continuity property of the solution played a key role in the error analysis. Unlike the stochastic Allen&ndash;Cahn equation, the presence of the unbounded elliptic operator in front of the cubic nonlinearity in the underlying model adds complexity and challenges to the error analysis. To overcome these difficulties, several new techniques and error estimates are developed. The study concludes with numerical examples that validate the theoretical findings.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=261277-association-between-prefrontal-fnirs-signals-during-cognitive-tasks-and-college-scholastic-ability-test-csat-scores-analysis-using-a-quantum-annealing-approach-yeaju-kim-et-al-2024>(261/277) Association between Prefrontal fNIRS signals during Cognitive tasks and College scholastic ability test (CSAT) scores: Analysis using a quantum annealing approach (Yeaju Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeaju Kim, Junggu Choi, Bora Kim, Yongwan Park, Jihyun Cha, Jongkwan Choi, Sanghoon Han. (2024)<br><strong>Association between Prefrontal fNIRS signals during Cognitive tasks and College scholastic ability test (CSAT) scores: Analysis using a quantum annealing approach</strong><br><button class=copy-to-clipboard title="Association between Prefrontal fNIRS signals during Cognitive tasks and College scholastic ability test (CSAT) scores: Analysis using a quantum annealing approach" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-ET, q-bio-NC, q-bio.NC, quant-ph<br>Keyword Score: 20<br>Keywords: Question Answering, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04287v1.pdf filename=2402.04287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Academic achievement is a critical measure of intellectual ability, <b>prompting</b> extensive research into cognitive tasks as potential predictors. Neuroimaging technologies, such as functional near-infrared spectroscopy (fNIRS), offer insights into brain hemodynamics, allowing understanding of the link between cognitive performance and academic achievement. Herein, we explored the association between cognitive tasks and academic achievement by analyzing prefrontal fNIRS signals. A novel quantum annealer <b>(QA)</b> feature selection algorithm was applied to fNIRS data to identify cognitive tasks correlated with CSAT scores. Twelve features (signal mean, median, variance, peak, number of peaks, sum of peaks, slope, minimum, kurtosis, skewness, standard deviation, and root mean square) were extracted from fNIRS signals at two time windows (10- and 60-second) to compare results from various feature variable conditions. The feature selection results from the <b>QA-based</b> and XGBoost regressor algorithms were compared to validate the former&rsquo;s performance. In a three-step validation process using multiple linear regression models, correlation coefficients between the feature variables and the CSAT scores, model fitness (adjusted R2), and model prediction error (RMSE) values were calculated. The quantum annealer demonstrated comparable performance to classical machine learning models, and specific cognitive tasks, including verbal fluency, recognition, and the Corsi block tapping task, were correlated with academic achievement. Group analyses revealed stronger associations between Tower of London and N-back tasks with higher CSAT scores. Quantum annealing algorithms have significant potential in feature selection using fNIRS data, and represents a novel research approach. Future studies should explore predictors of academic achievement and cognitive ability.</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=262277-gambling-based-confidence-sequences-for-bounded-random-vectors-j-jon-ryu-et-al-2024>(262/277) Gambling-Based Confidence Sequences for Bounded Random Vectors (J. Jon Ryu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>J. Jon Ryu, Gregory W. Wornell. (2024)<br><strong>Gambling-Based Confidence Sequences for Bounded Random Vectors</strong><br><button class=copy-to-clipboard title="Gambling-Based Confidence Sequences for Bounded Random Vectors" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-IT, math-IT, math-ST, stat-ME, stat-TH, stat.ME<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03683v1.pdf filename=2402.03683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A confidence sequence (CS) is a sequence of confidence sets that contains a target parameter of an underlying stochastic process at any time step with high probability. This paper proposes a new approach to constructing CSs for means of bounded multivariate stochastic processes using a general gambling framework, extending the recently established coin toss framework for bounded random processes. The proposed gambling framework provides a general recipe for constructing CSs for categorical and probability-vector-valued observations, as well as for general bounded multidimensional observations through a simple reduction. This paper specifically explores the use of the mixture portfolio, akin to Cover&rsquo;s universal portfolio, in the proposed framework and investigates the properties of the resulting CSs. <b>Simulations</b> demonstrate the tightness of these confidence sequences compared to existing methods. When applied to the sampling without-replacement setting for finite categorical data, it is shown that the resulting CS based on a universal gambling strategy is provably tighter than that of the posterior-prior ratio martingale proposed by Waudby-Smith and Ramdas.</p></p class="citation"></blockquote><h2 id=csdc-3>cs.DC (3)</h2><h3 id=263277-argo-an-auto-tuning-runtime-system-for-scalable-gnn-training-on-multi-core-processor-yi-chien-lin-et-al-2024>(263/277) ARGO: An Auto-Tuning Runtime System for Scalable GNN Training on Multi-Core Processor (Yi-Chien Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi-Chien Lin, Yuyang Chen, Sameh Gobriel, Nilesh Jain, Gopi Krishna Jha, Viktor Prasanna. (2024)<br><strong>ARGO: An Auto-Tuning Runtime System for Scalable GNN Training on Multi-Core Processor</strong><br><button class=copy-to-clipboard title="ARGO: An Auto-Tuning Runtime System for Scalable GNN Training on Multi-Core Processor" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03671v1.pdf filename=2402.03671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> become popular, libraries like PyTorch-Geometric (PyG) and Deep <b>Graph</b> <b>Library</b> <b>(DGL)</b> are proposed; these libraries have emerged as the de facto standard for implementing <b>GNNs</b> because they provide <b>graph-oriented</b> <b>APIs</b> <b>and</b> are purposefully designed to manage the inherent sparsity and irregularity in <b>graph</b> <b>structures.</b> <b>However,</b> these libraries show poor scalability on multi-core processors, which under-utilizes the available platform resources and limits the performance. This is because <b>GNN</b> training is a resource-intensive workload with high volume of irregular data accessing, and existing libraries fail to utilize the memory bandwidth efficiently. To address this challenge, we propose ARGO, a novel runtime system for <b>GNN</b> training that offers scalable performance. ARGO exploits multi-processing and core-binding techniques to improve platform resource utilization. We further develop an auto-tuner that searches for the optimal configuration for multi-processing and core-binding. The auto-tuner works automatically, making it completely transparent from the user. Furthermore, the auto-tuner allows ARGO to adapt to various platforms, <b>GNN</b> models, datasets, etc. We evaluate ARGO on two representative <b>GNN</b> models and four widely-used datasets on two platforms. With the proposed autotuner, ARGO is able to select a near-optimal configuration by exploring only 5% of the design space. ARGO speeds up state-of-the-art <b>GNN</b> libraries by up to 5.06x and 4.54x on a four-socket Ice Lake machine with 112 cores and a two-socket Sapphire Rapids machine with 64 cores, respectively. Finally, ARGO can seamlessly integrate into widely-used <b>GNN</b> libraries (e.g., DGL, PyG) with few lines of code and speed up <b>GNN</b> training.</p></p class="citation"></blockquote><h3 id=264277-adaptive-blockwise-task-interleaved-pipeline-parallelism-ding-tang-et-al-2024>(264/277) Adaptive Blockwise Task-interleaved Pipeline Parallelism (Ding Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ding Tang, Lijuan Jiang, Minxi Jin, Jiecheng Zhou, Hengjie Li, Xingcheng Zhang, Zhilin Pei. (2024)<br><strong>Adaptive Blockwise Task-interleaved Pipeline Parallelism</strong><br><button class=copy-to-clipboard title="Adaptive Blockwise Task-interleaved Pipeline Parallelism" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03791v1.pdf filename=2402.03791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient distributed training serves as a powerful catalyst and an essential foundation for the development of large-scale neural networks. In distributed training scenarios, various pipeline parallelism methods are cleverly designed and widely employed. In this paper, we propose ZeroPP, a highly efficient and flexible pipeline parallelism method that trades off pipeline bubbles, memory usage, and communication through adaptive scheduling units. ZeroPP achieves minimal pipeline bubbles by carefully staggering the computation tasks of forward, input gradient, and weight gradient within a scheduling unit. Additionally, ZeroPP optimizes the combination of pipeline parallelism and fully sharded data parallelism using a blockwise schedule. We conduct experiments with popular <b>GPT-style</b> models and observe up to a 30% increase in throughput compared to the state-of-the-art breath-first pipeline parallelism. Besides, our evaluation also demonstrates up to a 68% increase in throughput and a 10% reduction in memory consumption compared to the memory-efficient 1F1B method.</p></p class="citation"></blockquote><h3 id=265277-agent-based-triangle-counting-and-its-applications-in-anonymous-graphs-prabhat-kumar-chand-et-al-2024>(265/277) Agent-Based Triangle Counting and its Applications in Anonymous Graphs (Prabhat Kumar Chand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prabhat Kumar Chand, Apurba Das, Anisur Rahaman Molla. (2024)<br><strong>Agent-Based Triangle Counting and its Applications in Anonymous Graphs</strong><br><button class=copy-to-clipboard title="Agent-Based Triangle Counting and its Applications in Anonymous Graphs" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-MA, cs.DC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03653v1.pdf filename=2402.03653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Triangle counting in a graph is a fundamental problem and has a wide range of applications in various domains. It is crucial in understanding the structural properties of a graph and is often used as a building block for more complex graph analytics. In this paper, we solve the triangle counting problem in an anonymous graph in a distributed setting using mobile agents and subsequently use this as a subroutine to tackle the truss decomposition and triangle centrality problem. The paper employs mobile agents, placed on the nodes of the graph to coordinate among themselves to solve the triangle enumeration problem for the graph. Following the literature, we consider the synchronous systems where each robot executes its tasks concurrently with all others and hence time complexity can be measured as the number of rounds needed to complete the task. The graph is anonymous, i.e., without any node labels or IDs, but the agents are autonomous with distinct IDs and have limited memory. Agents can only communicate with other agents locally i.e., if and only if they are at the same node. The goal is to devise algorithms that minimise both the time required for triangle counting and the memory usage at each agent. We further demonstrate how the triangle count obtained through the mobile agent approach can be leveraged to address the truss decomposition, triangle centrality and local clustering coefficient problems, which involves finding maximal sub-graphs with strong interconnections. Truss decomposition helps in identifying maximal, highly interconnected sub-graphs, or trusses, within a network, thus, revealing the structural cohesion and tight-knit communities in complex graphs, facilitating the analysis of relationships and information flow in various fields, such as social networks, biology, and <b>recommendation</b> systems.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=266277-interpretable-domain-knowledge-enhanced-machine-learning-framework-on-axial-capacity-prediction-of-circular-cfst-columns-dian-wang-et-al-2024>(266/277) Interpretable domain knowledge enhanced machine learning framework on axial capacity prediction of circular CFST columns (Dian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dian Wang, Zhigang Ren, Gen Kondo, Peipeng Li. (2024)<br><strong>Interpretable domain knowledge enhanced machine learning framework on axial capacity prediction of circular CFST columns</strong><br><button class=copy-to-clipboard title="Interpretable domain knowledge enhanced machine learning framework on axial capacity prediction of circular CFST columns" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04405v1.pdf filename=2402.04405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a novel machine learning framework, integrating domain knowledge, to accurately predict the bearing capacity of CFSTs, bridging the gap between traditional engineering and machine learning techniques. Utilizing a comprehensive database of 2621 experimental data points on CFSTs, we developed a Domain Knowledge Enhanced Neural Network (DKNN) model. This model incorporates advanced feature engineering techniques, including Pearson correlation, XGBoost, and Random tree algorithms. The DKNN model demonstrated a marked improvement in prediction accuracy, with a Mean Absolute Percentage Error (MAPE) reduction of over 50% compared to existing models. Its robustness was confirmed through extensive performance assessments, maintaining high accuracy even in noisy environments. Furthermore, sensitivity and SHAP analysis were conducted to assess the contribution of each effective parameter to axial load capacity and propose design <b>recommendations</b> for the diameter of cross-section, material strength range and material combination. This research advances CFST predictive modelling, showcasing the potential of integrating machine learning with domain expertise in structural engineering. The DKNN model sets a new benchmark for accuracy and reliability in the field.</p></p class="citation"></blockquote><h2 id=csgt-3>cs.GT (3)</h2><h3 id=267277-fair-interval-scheduling-of-indivisible-chores-sarfaraz-equbal-et-al-2024>(267/277) Fair Interval Scheduling of Indivisible Chores (Sarfaraz Equbal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarfaraz Equbal, Rohit Gurjar, Yatharth Kumar, Swaprava Nath, Rohit Vaish. (2024)<br><strong>Fair Interval Scheduling of Indivisible Chores</strong><br><button class=copy-to-clipboard title="Fair Interval Scheduling of Indivisible Chores" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04353v1.pdf filename=2402.04353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of fairly assigning a set of discrete tasks (or chores) among a set of agents with additive valuations. Each chore is associated with a start and finish time, and each agent can perform at most one chore at any given time. The goal is to find a fair and efficient schedule of the chores, where <b>fairness</b> pertains to satisfying envy-freeness up to one chore (EF1) and efficiency pertains to maximality (i.e., no unallocated chore can be feasibly assigned to any agent). Our main result is a polynomial-time algorithm for computing an EF1 and maximal schedule for two agents under monotone valuations when the conflict constraints constitute an arbitrary interval graph. The algorithm uses a coloring technique in interval graphs that may be of independent interest. For an arbitrary number of agents, we provide an algorithm for finding a fair schedule under identical dichotomous valuations when the constraints constitute a path graph. We also show that stronger <b>fairness</b> and efficiency properties, including envy-freeness up to any chore (EFX) along with maximality and EF1 along with Pareto optimality, cannot be achieved.</p></p class="citation"></blockquote><h3 id=268277-approximating-the-core-via-iterative-coalition-sampling-ian-gemp-et-al-2024>(268/277) Approximating the Core via Iterative Coalition Sampling (Ian Gemp et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ian Gemp, Marc Lanctot, Luke Marris, Yiran Mao, Edgar Duéñez-Guzmán, Sarah Perrin, Andras Gyorgy, Romuald Elie, Georgios Piliouras, Michael Kaisers, Daniel Hennes, Kalesha Bullard, Kate Larson, Yoram Bachrach. (2024)<br><strong>Approximating the Core via Iterative Coalition Sampling</strong><br><button class=copy-to-clipboard title="Approximating the Core via Iterative Coalition Sampling" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-MA, cs.GT<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03928v1.pdf filename=2402.03928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The core is a central solution concept in cooperative game theory, defined as the set of feasible allocations or payments such that no subset of agents has incentive to break away and form their own subgroup or coalition. However, it has long been known that the core (and approximations, such as the least-core) are hard to compute. This limits our ability to analyze cooperative games in general, and to fully embrace cooperative game theory contributions in domains such as <b>explainable</b> <b>AI</b> (XAI), where the core can complement the Shapley values to identify influential features or instances supporting predictions by black-box models. We propose novel iterative algorithms for computing variants of the core, which avoid the computational bottleneck of many other approaches; namely solving large linear programs. As such, they scale better to very large problems as we demonstrate across different classes of cooperative games, including weighted voting games, induced subgraph games, and marginal contribution networks. We also explore our algorithms in the context of XAI, providing further evidence of the power of the core for such applications.</p></p class="citation"></blockquote><h3 id=269277-rlas-for-2-seat-stv-elections-revisited-michelle-blom-et-al-2024>(269/277) RLAs for 2-Seat STV Elections: Revisited (Michelle Blom et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michelle Blom, Peter J. Stuckey, Vanessa Teague, Damjan Vukcevic. (2024)<br><strong>RLAs for 2-Seat STV Elections: Revisited</strong><br><button class=copy-to-clipboard title="RLAs for 2-Seat STV Elections: Revisited" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03707v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03707v1.pdf filename=2402.03707v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single Transferable Vote (STV) elections are a principled approach to electing multiple candidates in a single election. Each ballot has a starting value of 1, and a candidate is elected if they gather a total vote value more than a defined quota. Votes over the quota have their value reduced by a transfer value so as to remove the quota, and are passed to the next candidate on the ballot. Risk-limiting audits (RLAs) are a statistically sound approach to election auditing which guarantees that failure to detect an error in the result is bounded by a limit. A first approach to RLAs for 2-seat STV elections has been defined. In this paper we show how we can improve this approach by <b>reasoning</b> about lower bounds on transfer values, and how we can extend the approach to partially audit an election, if the method does not support a full audit.</p></p class="citation"></blockquote><h2 id=cspf-1>cs.PF (1)</h2><h3 id=270277-acceleration-and-energy-consumption-optimization-in-cascading-classifiers-for-face-detection-on-low-cost-arm-biglittle-asymmetric-architectures-alberto-corpas-et-al-2024>(270/277) Acceleration and energy consumption optimization in cascading classifiers for face detection on low-cost ARM big.LITTLE asymmetric architectures (Alberto Corpas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Corpas, Luis Costero, Guillermo Botella, Francisco D. Igual, Carlos García, Manuel Rodríguez. (2024)<br><strong>Acceleration and energy consumption optimization in cascading classifiers for face detection on low-cost ARM big.LITTLE asymmetric architectures</strong><br><button class=copy-to-clipboard title="Acceleration and energy consumption optimization in cascading classifiers for face detection on low-cost ARM big.LITTLE asymmetric architectures" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PF<br>Categories: cs-PF, cs.PF<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04090v1.pdf filename=2402.04090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a mechanism to accelerate and optimize the energy consumption of a face detection software based on Haar-like cascading classifiers, taking advantage of the features of low-cost Asymmetric Multicore Processors (AMPs) with limited power budget. A modelling and task scheduling/allocation is proposed in order to efficiently make use of the existing features on big.LITTLE ARM processors, including: (I) source-code adaptation for parallel computing, which enables code acceleration by applying the OmpSs programming model, a task-based programming model that handles data-dependencies between tasks in a transparent fashion; (II) different OmpSs task allocation policies which take into account the processor asymmetry and can dynamically set processing resources in a more efficient way based on their particular features. The proposed mechanism can be efficiently applied to take advantage of the processing elements existing on low-cost and low-energy multi-core embedded devices executing <b>object</b> <b>detection</b> algorithms based on cascading classifiers. Although these classifiers yield the best results for detection algorithms in the field of computer vision, their high computational requirements prevent them from being used on these devices under real-time requirements. Finally, we compare the energy efficiency of a heterogeneous architecture based on asymmetric multicore processors with a suitable task scheduling, with that of a homogeneous symmetric architecture.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=271277-on-convergence-of-adam-for-stochastic-optimization-under-relaxed-assumptions-yusu-hong-et-al-2024>(271/277) On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions (Yusu Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusu Hong, Junhong Lin. (2024)<br><strong>On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions</strong><br><button class=copy-to-clipboard title="On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC, stat-ML<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03982v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03982v1.pdf filename=2402.03982v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Adaptive Momentum Estimation (Adam) algorithm is highly effective in training various deep learning tasks. Despite this, there&rsquo;s limited theoretical understanding for Adam, especially when focusing on its vanilla form in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. In this paper, we study vanilla Adam under these challenging conditions. We introduce a comprehensive noise model which governs affine variance noise, bounded noise and sub-Gaussian noise. We show that Adam can find a stationary point with a $\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of <b>stochastic</b> <b>first-order</b> <b>algorithms</b> up to logarithm factors. More importantly, we reveal that Adam is free of tuning step-sizes with any problem-parameters, yielding a better adaptation property than the <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> under the same conditions. We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to more accurately capture the smooth property of many practical objective functions.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=272277-joint-intrinsic-motivation-for-coordinated-exploration-in-multi-agent-deep-reinforcement-learning-maxime-toquebiau-et-al-2024>(272/277) Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning (Maxime Toquebiau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maxime Toquebiau, Nicolas Bredeche, Faïz Benamar, Jae-Yun Jun. (2024)<br><strong>Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03972v1.pdf filename=2402.03972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-agent deep <b>reinforcement</b> <b>learning</b> (MADRL) problems often encounter the challenge of sparse rewards. This challenge becomes even more pronounced when coordination among agents is necessary. As performance depends not only on one agent&rsquo;s behavior but rather on the joint behavior of multiple agents, finding an adequate solution becomes significantly harder. In this context, a group of agents can benefit from actively exploring different joint strategies in order to determine the most efficient one. In this paper, we propose an approach for rewarding strategies where agents collectively exhibit novel behaviors. We present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation method that follows the centralized learning with decentralized execution paradigm. JIM rewards joint trajectories based on a centralized measure of novelty designed to function in continuous environments. We demonstrate the strengths of this approach both in a synthetic environment designed to reveal shortcomings of state-of-the-art MADRL methods, and in simulated robotic tasks. Results show that joint exploration is crucial for solving tasks where the optimal strategy requires a high level of coordination.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=273277-using-metaheuristics-for-the-location-of-bicycle-stations-christian-cintrano-et-al-2024>(273/277) Using metaheuristics for the location of bicycle stations (Christian Cintrano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Cintrano, Francisco Chicano, Enrique Alba. (2024)<br><strong>Using metaheuristics for the location of bicycle stations</strong><br><button class=copy-to-clipboard title="Using metaheuristics for the location of bicycle stations" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03945v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03945v1.pdf filename=2402.03945v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we solve the problem of finding the best locations to place stations for depositing/collecting shared bicycles. To do this, we model the problem as the p-median problem, that is a major existing localization problem in optimization. The p-median problem seeks to place a set of facilities (bicycle stations) in a way that minimizes the distance between a set of clients (citizens) and their closest facility (bike station). We have used a genetic algorithm, iterated local search, particle swarm optimization, simulated annealing, and variable neighbourhood search, to find the best locations for the bicycle stations and study their comparative advantages. We use irace to parameterize each algorithm automatically, to contribute with a methodology to <b>fine-tune</b> algorithms automatically. We have also studied different real data (distance and weights) from diverse open data sources from a real city, Malaga (Spain), hopefully leading to a final smart city application. We have compared our results with the implemented solution in Malaga. Finally, we have analyzed how we can use our proposal to improve the existing system in the city by adding more stations.</p></p class="citation"></blockquote><h2 id=cond-matmes-hall-1>cond-mat.mes-hall (1)</h2><h3 id=274277-fully-autonomous-tuning-of-a-spin-qubit-jonas-schuff-et-al-2024>(274/277) Fully autonomous tuning of a spin qubit (Jonas Schuff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Schuff, Miguel J. Carballido, Madeleine Kotzagiannidis, Juan Carlos Calvo, Marco Caselli, Jacob Rawling, David L. Craig, Barnaby van Straaten, Brandon Severin, Federico Fedele, Simon Svab, Pierre Chevalier Kwon, Rafael S. Eggli, Taras Patlatiuk, Nathan Korda, Dominik Zumbühl, Natalia Ares. (2024)<br><strong>Fully autonomous tuning of a spin qubit</strong><br><button class=copy-to-clipboard title="Fully autonomous tuning of a spin qubit" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mes-hall<br>Categories: cond-mat-mes-hall, cond-mat.mes-hall, cs-LG, quant-ph<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03931v1.pdf filename=2402.03931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spanning over two decades, the study of qubits in semiconductors for quantum computing has yielded significant breakthroughs. However, the development of large-scale semiconductor quantum circuits is still limited by challenges in efficiently tuning and operating these circuits. Identifying optimal operating conditions for these qubits is complex, involving the exploration of vast parameter spaces. This presents a real &rsquo;needle in the haystack&rsquo; problem, which, until now, has resisted complete automation due to device variability and fabrication imperfections. In this study, we present the first fully autonomous tuning of a semiconductor qubit, from a grounded device to Rabi oscillations, a clear indication of successful qubit operation. We demonstrate this automation, achieved without <b>human</b> <b>intervention,</b> in a Ge/Si core/shell nanowire device. Our approach integrates deep learning, Bayesian optimization, and computer vision techniques. We expect this automation algorithm to apply to a wide range of semiconductor qubit devices, allowing for statistical studies of qubit quality metrics. As a demonstration of the potential of full automation, we characterise how the Rabi frequency and g-factor depend on barrier gate voltages for one of the qubits found by the algorithm. Twenty years after the initial demonstrations of spin qubit operation, this significant advancement is poised to finally catalyze the operation of large, previously unexplored quantum circuits.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=275277-geometric-quantum-machine-learning-of-bqpa-protocols-and-latent-graph-classifiers-chukwudubem-umeano-et-al-2024>(275/277) Geometric quantum machine learning of BQP$^A$ protocols and latent graph classifiers (Chukwudubem Umeano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chukwudubem Umeano, Vincent E. Elfving, Oleksandr Kyriienko. (2024)<br><strong>Geometric quantum machine learning of BQP$^A$ protocols and latent graph classifiers</strong><br><button class=copy-to-clipboard title="Geometric quantum machine learning of BQP$^A$ protocols and latent graph classifiers" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cond-mat-dis-nn, cs-LG, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03871v1.pdf filename=2402.03871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Geometric quantum machine learning (GQML) aims to embed problem symmetries for learning efficient solving protocols. However, the question remains if (G)QML can be routinely used for constructing protocols with an exponential separation from classical analogs. In this Letter we consider Simon&rsquo;s problem for learning properties of Boolean functions, and show that this can be related to an <b>unsupervised</b> circuit classification problem. Using the workflow of geometric QML, we learn from first principles Simon&rsquo;s algorithm, thus discovering an example of BQP$^A\neq$BPP protocol with respect to some dataset (oracle $A$). Our key findings include the development of an equivariant feature map for embedding Boolean functions, based on twirling with respect to identified bitflip and permutational symmetries, and measurement based on invariant observables with a sampling advantage. The proposed workflow points to the importance of data embeddings and classical post-processing, while keeping the variational circuit as a trivial identity operator. Next, developing the intuition for the function learning, we visualize instances as directed computational hypergraphs, and observe that the GQML protocol can access their global topological features for distinguishing bijective and surjective functions. Finally, we discuss the prospects for learning other BQP$^A$-type protocols, and conjecture that this depends on the ability of simplifying embeddings-based oracles $A$ applied as a linear combination of unitaries.</p></p class="citation"></blockquote><h2 id=q-finrm-1>q-fin.RM (1)</h2><h3 id=276277-explainable-automated-machine-learning-for-credit-decisions-enhancing-human-artificial-intelligence-collaboration-in-financial-engineering-marc-schmitt-2024>(276/277) Explainable Automated Machine Learning for Credit Decisions: Enhancing Human Artificial Intelligence Collaboration in Financial Engineering (Marc Schmitt, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marc Schmitt. (2024)<br><strong>Explainable Automated Machine Learning for Credit Decisions: Enhancing Human Artificial Intelligence Collaboration in Financial Engineering</strong><br><button class=copy-to-clipboard title="Explainable Automated Machine Learning for Credit Decisions: Enhancing Human Artificial Intelligence Collaboration in Financial Engineering" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.RM<br>Categories: cs-LG, q-fin-CP, q-fin-RM, q-fin.RM<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03806v1.pdf filename=2402.03806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the integration of <b>Explainable</b> <b>Automated</b> Machine Learning (AutoML) in the realm of financial engineering, specifically focusing on its application in credit decision-making. The rapid evolution of Artificial Intelligence (AI) in finance has necessitated a balance between sophisticated algorithmic decision-making and the need for transparency in these systems. The focus is on how AutoML can streamline the development of robust machine learning models for credit scoring, while <b>Explainable</b> <b>AI</b> (XAI) methods, particularly SHapley Additive exPlanations (SHAP), provide insights into the models&rsquo; decision-making processes. This study demonstrates how the combination of AutoML and XAI not only enhances the efficiency and accuracy of credit decisions but also fosters trust and collaboration between humans and AI systems. The findings underscore the potential of <b>explainable</b> <b>AutoML</b> in improving the transparency and accountability of AI-driven financial decisions, aligning with regulatory requirements and ethical considerations.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=277277-an-effective-branch-and-bound-algorithm-with-new-bounding-methods-for-the-maximum-s-bundle-problem-jinghui-xue-et-al-2024>(277/277) An Effective Branch-and-Bound Algorithm with New Bounding Methods for the Maximum $s$-Bundle Problem (Jinghui Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinghui Xue, Jiongzhi Zheng, Mingming Jin, Kun He. (2024)<br><strong>An Effective Branch-and-Bound Algorithm with New Bounding Methods for the Maximum $s$-Bundle Problem</strong><br><button class=copy-to-clipboard title="An Effective Branch-and-Bound Algorithm with New Bounding Methods for the Maximum $s$-Bundle Problem" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs-LG, cs.DS<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03736v1.pdf filename=2402.03736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Maximum s-Bundle Problem (MBP) addresses the task of identifying a maximum s-bundle in a given graph. A graph G=(V, E) is called an s-bundle if its vertex connectivity is at least |V|-s, where the vertex connectivity equals the minimum number of vertices whose deletion yields a disconnected or trivial graph. MBP is NP-hard and holds relevance in numerous realworld scenarios emphasizing the vertex connectivity. Exact algorithms for MBP mainly follow the branch-and-bound (BnB) framework, whose performance heavily depends on the quality of the upper bound on the cardinality of a maximum s-bundle and the initial lower bound with graph reduction. In this work, we introduce a novel Partition-based Upper Bound (PUB) that leverages the graph partitioning technique to achieve a tighter upper bound compared to existing ones. To increase the lower bound, we propose to do short random walks on a clique to generate larger initial solutions. Then, we propose a new BnB algorithm that uses the initial lower bound and PUB in preprocessing for graph reduction, and uses PUB in the BnB search process for branch <b>pruning.</b> Extensive experiments with diverse s values demonstrate the significant progress of our algorithm over state-of-the-art BnB MBP algorithms. Moreover, our initial lower bound can also be generalized to other relaxation clique problems.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.07</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.09</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-39>cs.CL (39)</a><ul><li><a href=#1277-minds-versus-machines-rethinking-entailment-verification-with-language-models-soumya-sanyal-et-al-2024>(1/277) Minds versus Machines: Rethinking Entailment Verification with Language Models (Soumya Sanyal et al., 2024)</a></li><li><a href=#2277-training-language-models-to-generate-text-with-citations-via-fine-grained-rewards-chengyu-huang-et-al-2024>(2/277) Training Language Models to Generate Text with Citations via Fine-grained Rewards (Chengyu Huang et al., 2024)</a></li><li><a href=#3277-scaling-laws-for-downstream-task-performance-of-large-language-models-berivan-isik-et-al-2024>(3/277) Scaling Laws for Downstream Task Performance of Large Language Models (Berivan Isik et al., 2024)</a></li><li><a href=#4277-harnessing-the-plug-and-play-controller-by-prompting-hao-wang-et-al-2024>(4/277) Harnessing the Plug-and-Play Controller by Prompting (Hao Wang et al., 2024)</a></li><li><a href=#5277-large-language-models-as-moocs-graders-shahriar-golchin-et-al-2024>(5/277) Large Language Models As MOOCs Graders (Shahriar Golchin et al., 2024)</a></li><li><a href=#6277-identifying-reasons-for-contraceptive-switching-from-real-world-data-using-large-language-models-brenda-y-miao-et-al-2024>(6/277) Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models (Brenda Y. Miao et al., 2024)</a></li><li><a href=#7277-detecting-mode-collapse-in-language-models-via-narration-sil-hamilton-2024>(7/277) Detecting Mode Collapse in Language Models via Narration (Sil Hamilton, 2024)</a></li><li><a href=#8277-rethinking-skill-extraction-in-the-job-market-domain-using-large-language-models-khanh-cao-nguyen-et-al-2024>(8/277) Rethinking Skill Extraction in the Job Market Domain using Large Language Models (Khanh Cao Nguyen et al., 2024)</a></li><li><a href=#9277-large-language-models-as-an-indirect-reasoner-contrapositive-and-contradiction-for-automated-reasoning-yanfang-zhang-et-al-2024>(9/277) Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning (Yanfang Zhang et al., 2024)</a></li><li><a href=#10277-professional-agents----evolving-large-language-models-into-autonomous-experts-with-human-level-competencies-zhixuan-chu-et-al-2024>(10/277) Professional Agents &ndash; Evolving Large Language Models into Autonomous Experts with Human-Level Competencies (Zhixuan Chu et al., 2024)</a></li><li><a href=#11277-less-selecting-influential-data-for-targeted-instruction-tuning-mengzhou-xia-et-al-2024>(11/277) LESS: Selecting Influential Data for Targeted Instruction Tuning (Mengzhou Xia et al., 2024)</a></li><li><a href=#12277-leak-cheat-repeat-data-contamination-and-evaluation-malpractices-in-closed-source-llms-simone-balloccu-et-al-2024>(12/277) Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs (Simone Balloccu et al., 2024)</a></li><li><a href=#13277-anls----a-universal-document-processing-metric-for-generative-large-language-models-david-peer-et-al-2024>(13/277) ANLS* &ndash; A Universal Document Processing Metric for Generative Large Language Models (David Peer et al., 2024)</a></li><li><a href=#14277-personalized-language-modeling-from-personalized-human-feedback-xinyu-li-et-al-2024>(14/277) Personalized Language Modeling from Personalized Human Feedback (Xinyu Li et al., 2024)</a></li><li><a href=#15277-democratizing-large-language-models-via-personalized-parameter-efficient-fine-tuning-zhaoxuan-tan-et-al-2024>(15/277) Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning (Zhaoxuan Tan et al., 2024)</a></li><li><a href=#16277-legallens-leveraging-llms-for-legal-violation-identification-in-unstructured-text-dor-bernsohn-et-al-2024>(16/277) LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text (Dor Bernsohn et al., 2024)</a></li><li><a href=#17277-behind-the-screen-investigating-chatgpts-dark-personality-traits-and-conspiracy-beliefs-erik-weber-et-al-2024>(17/277) Behind the Screen: Investigating ChatGPT&rsquo;s Dark Personality Traits and Conspiracy Beliefs (Erik Weber et al., 2024)</a></li><li><a href=#18277-iterative-prompt-refinement-for-radiation-oncology-symptom-extraction-using-teacher-student-large-language-models-reza-khanmohammadi-et-al-2024>(18/277) Iterative Prompt Refinement for Radiation Oncology Symptom Extraction Using Teacher-Student Large Language Models (Reza Khanmohammadi et al., 2024)</a></li><li><a href=#19277-distillm-towards-streamlined-distillation-for-large-language-models-jongwoo-ko-et-al-2024>(19/277) DistiLLM: Towards Streamlined Distillation for Large Language Models (Jongwoo Ko et al., 2024)</a></li><li><a href=#20277-soft-prompt-tuning-for-cross-lingual-transfer-when-less-is-more-fred-philippy-et-al-2024>(20/277) Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More (Fred Philippy et al., 2024)</a></li><li><a href=#21277-sentiment-enhanced-graph-based-sarcasm-explanation-in-dialogue-kun-ouyang-et-al-2024>(21/277) Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue (Kun Ouyang et al., 2024)</a></li><li><a href=#22277-partially-recentralization-softmax-loss-for-vision-language-models-robustness-hao-wang-et-al-2024>(22/277) Partially Recentralization Softmax Loss for Vision-Language Models Robustness (Hao Wang et al., 2024)</a></li><li><a href=#23277-evaluating-embeddings-for-one-shot-classification-of-doctor-ai-consultations-olumide-ebenezer-ojo-et-al-2024>(23/277) Evaluating Embeddings for One-Shot Classification of Doctor-AI Consultations (Olumide Ebenezer Ojo et al., 2024)</a></li><li><a href=#24277-the-use-of-a-large-language-model-for-cyberbullying-detection-bayode-ogunleye-et-al-2024>(24/277) The Use of a Large Language Model for Cyberbullying Detection (Bayode Ogunleye et al., 2024)</a></li><li><a href=#25277-systematic-biases-in-llm-simulations-of-debates-amir-taubenfeld-et-al-2024>(25/277) Systematic Biases in LLM Simulations of Debates (Amir Taubenfeld et al., 2024)</a></li><li><a href=#26277-beyond-lines-and-circles-unveiling-the-geometric-reasoning-gap-in-large-language-models-spyridon-mouselinos-et-al-2024>(26/277) Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models (Spyridon Mouselinos et al., 2024)</a></li><li><a href=#27277-inside-llms-internal-states-retain-the-power-of-hallucination-detection-chao-chen-et-al-2024>(27/277) INSIDE: LLMs&rsquo; Internal States Retain the Power of Hallucination Detection (Chao Chen et al., 2024)</a></li><li><a href=#28277-empowering-language-models-with-active-inquiry-for-deeper-understanding-jing-cheng-pang-et-al-2024>(28/277) Empowering Language Models with Active Inquiry for Deeper Understanding (Jing-Cheng Pang et al., 2024)</a></li><li><a href=#29277-leveraging-large-language-models-for-hybrid-workplace-decision-support-yujin-kim-et-al-2024>(29/277) Leveraging Large Language Models for Hybrid Workplace Decision Support (Yujin Kim et al., 2024)</a></li><li><a href=#30277-structured-entity-extraction-using-large-language-models-haolun-wu-et-al-2024>(30/277) Structured Entity Extraction Using Large Language Models (Haolun Wu et al., 2024)</a></li><li><a href=#31277-chatbot-meets-pipeline-augment-large-language-model-with-definite-finite-automaton-yiyou-sun-et-al-2024>(31/277) Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton (Yiyou Sun et al., 2024)</a></li><li><a href=#32277-anytool-self-reflective-hierarchical-agents-for-large-scale-api-calls-yu-du-et-al-2024>(32/277) AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls (Yu Du et al., 2024)</a></li><li><a href=#33277-albnews-a-corpus-of-headlines-for-topic-modeling-in-albanian-erion-çano-et-al-2024>(33/277) AlbNews: A Corpus of Headlines for Topic Modeling in Albanian (Erion Çano et al., 2024)</a></li><li><a href=#34277-lv-eval-a-balanced-long-context-benchmark-with-5-length-levels-up-to-256k-tao-yuan-et-al-2024>(34/277) LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K (Tao Yuan et al., 2024)</a></li><li><a href=#35277-exposing-propaganda-an-analysis-of-stylistic-cues-comparing-human-annotations-and-machine-classification-géraud-faye-et-al-2024>(35/277) Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification (Géraud Faye et al., 2024)</a></li><li><a href=#36277-linear-time-minimum-bayes-risk-decoding-with-reference-aggregation-jannis-vamvas-et-al-2024>(36/277) Linear-time Minimum Bayes Risk Decoding with Reference Aggregation (Jannis Vamvas et al., 2024)</a></li><li><a href=#37277-sparse-graph-representations-for-procedural-instructional-documents-shruti-singh-et-al-2024>(37/277) Sparse Graph Representations for Procedural Instructional Documents (Shruti Singh et al., 2024)</a></li><li><a href=#38277-stanceosaurus-20-classifying-stance-towards-russian-and-spanish-misinformation-anton-lavrouk-et-al-2024>(38/277) Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation (Anton Lavrouk et al., 2024)</a></li><li><a href=#39277-pro-han-a-heterogeneous-graph-attention-network-for-profile-based-spoken-language-understanding-dechuan-teng-et-al-2024>(39/277) Pro-HAN: A Heterogeneous Graph Attention Network for Profile-Based Spoken Language Understanding (Dechuan Teng et al., 2024)</a></li></ul></li><li><a href=#cslg-88>cs.LG (88)</a><ul><li><a href=#40277-the-hedgehog--the-porcupine-expressive-linear-attentions-with-softmax-mimicry-michael-zhang-et-al-2024>(40/277) The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry (Michael Zhang et al., 2024)</a></li><li><a href=#41277-cehr-gpt-generating-electronic-health-records-with-chronological-patient-timelines-chao-pang-et-al-2024>(41/277) CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines (Chao Pang et al., 2024)</a></li><li><a href=#42277-large-language-models-to-enhance-bayesian-optimization-tennison-liu-et-al-2024>(42/277) Large Language Models to Enhance Bayesian Optimization (Tennison Liu et al., 2024)</a></li><li><a href=#43277-fine-tuned-language-models-generate-stable-inorganic-materials-as-text-nate-gruver-et-al-2024>(43/277) Fine-Tuned Language Models Generate Stable Inorganic Materials as Text (Nate Gruver et al., 2024)</a></li><li><a href=#44277-can-mamba-learn-how-to-learn-a-comparative-study-on-in-context-learning-tasks-jongho-park-et-al-2024>(44/277) Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks (Jongho Park et al., 2024)</a></li><li><a href=#45277-understanding-the-effect-of-noise-in-llm-training-data-with-algorithmic-chains-of-thought-alex-havrilla-et-al-2024>(45/277) Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought (Alex Havrilla et al., 2024)</a></li><li><a href=#46277-pregip-watermarking-the-pretraining-of-graph-neural-networks-for-deep-intellectual-property-protection-enyan-dai-et-al-2024>(46/277) PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection (Enyan Dai et al., 2024)</a></li><li><a href=#47277-in-context-learning-agents-are-asymmetric-belief-updaters-johannes-a-schubert-et-al-2024>(47/277) In-context learning agents are asymmetric belief updaters (Johannes A. Schubert et al., 2024)</a></li><li><a href=#48277-return-aligned-decision-transformer-tsunehiko-tanaka-et-al-2024>(48/277) Return-Aligned Decision Transformer (Tsunehiko Tanaka et al., 2024)</a></li><li><a href=#49277-asymptotic-generalization-error-of-a-single-layer-graph-convolutional-network-o-duranthon-et-al-2024>(49/277) Asymptotic generalization error of a single-layer graph convolutional network (O. Duranthon et al., 2024)</a></li><li><a href=#50277-masked-graph-autoencoder-with-non-discrete-bandwidths-ziwen-zhao-et-al-2024>(50/277) Masked Graph Autoencoder with Non-discrete Bandwidths (Ziwen Zhao et al., 2024)</a></li><li><a href=#51277-weakly-supervised-anomaly-detection-via-knowledge-data-alignment-haihong-zhao-et-al-2024>(51/277) Weakly Supervised Anomaly Detection via Knowledge-Data Alignment (Haihong Zhao et al., 2024)</a></li><li><a href=#52277-similarity-based-neighbor-selection-for-graph-llms-rui-li-et-al-2024>(52/277) Similarity-based Neighbor Selection for Graph LLMs (Rui Li et al., 2024)</a></li><li><a href=#53277-musicrl-aligning-music-generation-to-human-preferences-geoffrey-cideron-et-al-2024>(53/277) MusicRL: Aligning Music Generation to Human Preferences (Geoffrey Cideron et al., 2024)</a></li><li><a href=#54277-connecting-the-dots-collaborative-fine-tuning-for-black-box-vision-language-models-zhengbo-wang-et-al-2024>(54/277) Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models (Zhengbo Wang et al., 2024)</a></li><li><a href=#55277-efficient-availability-attacks-against-supervised-and-contrastive-learning-simultaneously-yihan-wang-et-al-2024>(55/277) Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously (Yihan Wang et al., 2024)</a></li><li><a href=#56277-billm-pushing-the-limit-of-post-training-quantization-for-llms-wei-huang-et-al-2024>(56/277) BiLLM: Pushing the Limit of Post-Training Quantization for LLMs (Wei Huang et al., 2024)</a></li><li><a href=#57277-seabo-a-simple-search-based-method-for-offline-imitation-learning-jiafei-lyu-et-al-2024>(57/277) SEABO: A Simple Search-Based Method for Offline Imitation Learning (Jiafei Lyu et al., 2024)</a></li><li><a href=#58277-learning-to-generate-explainable-stock-predictions-using-self-reflective-large-language-models-kelvin-j-l-koa-et-al-2024>(58/277) Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models (Kelvin J. L. Koa et al., 2024)</a></li><li><a href=#59277-lens-a-foundation-model-for-network-traffic-in-cybersecurity-qineng-wang-et-al-2024>(59/277) Lens: A Foundation Model for Network Traffic in Cybersecurity (Qineng Wang et al., 2024)</a></li><li><a href=#60277-iot-network-traffic-analysis-with-deep-learning-mei-liu-et-al-2024>(60/277) IoT Network Traffic Analysis with Deep Learning (Mei Liu et al., 2024)</a></li><li><a href=#61277-quip-even-better-llm-quantization-with-hadamard-incoherence-and-lattice-codebooks-albert-tseng-et-al-2024>(61/277) QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks (Albert Tseng et al., 2024)</a></li><li><a href=#62277-tag-llm-repurposing-general-purpose-llms-for-specialized-domains-junhong-shen-et-al-2024>(62/277) Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains (Junhong Shen et al., 2024)</a></li><li><a href=#63277-harmbench-a-standardized-evaluation-framework-for-automated-red-teaming-and-robust-refusal-mantas-mazeika-et-al-2024>(63/277) HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal (Mantas Mazeika et al., 2024)</a></li><li><a href=#64277-improved-generalization-of-weight-space-networks-via-augmentations-aviv-shamsian-et-al-2024>(64/277) Improved Generalization of Weight Space Networks via Augmentations (Aviv Shamsian et al., 2024)</a></li><li><a href=#65277-entropy-regularized-diffusion-policy-with-q-ensembles-for-offline-reinforcement-learning-ruoqi-zhang-et-al-2024>(65/277) Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning (Ruoqi Zhang et al., 2024)</a></li><li><a href=#66277-on-dimensionality-of-feature-vectors-in-mpnns-césar-bravo-et-al-2024>(66/277) On dimensionality of feature vectors in MPNNs (César Bravo et al., 2024)</a></li><li><a href=#67277-relu2-wins-discovering-efficient-activation-functions-for-sparse-llms-zhengyan-zhang-et-al-2024>(67/277) ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs (Zhengyan Zhang et al., 2024)</a></li><li><a href=#68277-reinforcement-learning-from-bagged-reward-a-transformer-based-approach-for-instance-level-reward-redistribution-yuting-tang-et-al-2024>(68/277) Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution (Yuting Tang et al., 2024)</a></li><li><a href=#69277-fed-cvlc-compressing-federated-learning-communications-with-variable-length-codes-xiaoxin-su-et-al-2024>(69/277) Fed-CVLC: Compressing Federated Learning Communications with Variable-Length Codes (Xiaoxin Su et al., 2024)</a></li><li><a href=#70277-enhanced-sampling-of-robust-molecular-datasets-with-uncertainty-based-collective-variables-aik-rui-tan-et-al-2024>(70/277) Enhanced sampling of robust molecular datasets with uncertainty-based collective variables (Aik Rui Tan et al., 2024)</a></li><li><a href=#71277-learning-granger-causality-from-instance-wise-self-attentive-hawkes-processes-dongxia-wu-et-al-2024>(71/277) Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes (Dongxia Wu et al., 2024)</a></li><li><a href=#72277-rap-retrieval-augmented-planning-with-contextual-memory-for-multimodal-llm-agents-tomoyuki-kagaya-et-al-2024>(72/277) RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents (Tomoyuki Kagaya et al., 2024)</a></li><li><a href=#73277-scientific-language-modeling-a-quantitative-review-of-large-language-models-in-molecular-science-pengfei-liu-et-al-2024>(73/277) Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science (Pengfei Liu et al., 2024)</a></li><li><a href=#74277-the-vampprior-mixture-model-andrew-stirn-et-al-2024>(74/277) The VampPrior Mixture Model (Andrew Stirn et al., 2024)</a></li><li><a href=#75277-denoising-diffusion-probabilistic-models-in-six-simple-steps-richard-e-turner-et-al-2024>(75/277) Denoising Diffusion Probabilistic Models in Six Simple Steps (Richard E. Turner et al., 2024)</a></li><li><a href=#76277-cast-clustering-self-attention-using-surrogate-tokens-for-efficient-transformers-adjorn-van-engelenhoven-et-al-2024>(76/277) CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers (Adjorn van Engelenhoven et al., 2024)</a></li><li><a href=#77277-gradient-coding-in-decentralized-learning-for-evading-stragglers-chengxi-li-et-al-2024>(77/277) Gradient Coding in Decentralized Learning for Evading Stragglers (Chengxi Li et al., 2024)</a></li><li><a href=#78277-reinforcement-learning-with-ensemble-model-predictive-safety-certification-sven-gronauer-et-al-2024>(78/277) Reinforcement Learning with Ensemble Model Predictive Safety Certification (Sven Gronauer et al., 2024)</a></li><li><a href=#79277-provably-learning-a-multi-head-attention-layer-sitan-chen-et-al-2024>(79/277) Provably learning a multi-head attention layer (Sitan Chen et al., 2024)</a></li><li><a href=#80277-lighthgnn-distilling-hypergraph-neural-networks-into-mlps-for-100times-faster-inference-yifan-feng-et-al-2024>(80/277) LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\times$ Faster Inference (Yifan Feng et al., 2024)</a></li><li><a href=#81277-gradient-sketches-for-training-data-attribution-and-studying-the-loss-landscape-andrea-schioppa-2024>(81/277) Gradient Sketches for Training Data Attribution and Studying the Loss Landscape (Andrea Schioppa, 2024)</a></li><li><a href=#82277-a-bias-variance-decomposition-for-ensembles-over-multiple-synthetic-datasets-ossi-räisä-et-al-2024>(82/277) A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets (Ossi Räisä et al., 2024)</a></li><li><a href=#83277-discovery-of-the-hidden-world-with-large-language-models-chenxi-liu-et-al-2024>(83/277) Discovery of the Hidden World with Large Language Models (Chenxi Liu et al., 2024)</a></li><li><a href=#84277-employee-turnover-analysis-using-machine-learning-algorithms-mahyar-karimi-et-al-2024>(84/277) Employee Turnover Analysis Using Machine Learning Algorithms (Mahyar Karimi et al., 2024)</a></li><li><a href=#85277-moment-a-family-of-open-time-series-foundation-models-mononito-goswami-et-al-2024>(85/277) MOMENT: A Family of Open Time-series Foundation Models (Mononito Goswami et al., 2024)</a></li><li><a href=#86277-cascast-skillful-high-resolution-precipitation-nowcasting-via-cascaded-modelling-junchao-gong-et-al-2024>(86/277) CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling (Junchao Gong et al., 2024)</a></li><li><a href=#87277-digital-twin-mobility-profiling-a-spatio-temporal-graph-learning-approach-xin-chen-et-al-2024>(87/277) Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach (Xin Chen et al., 2024)</a></li><li><a href=#88277-modeling-spatio-temporal-dynamical-systems-with-neural-discrete-learning-and-levels-of-experts-kun-wang-et-al-2024>(88/277) Modeling Spatio-temporal Dynamical Systems with Neural Discrete Learning and Levels-of-Experts (Kun Wang et al., 2024)</a></li><li><a href=#89277-sub-play-adversarial-policies-against-partially-observed-multi-agent-reinforcement-learning-systems-oubo-ma-et-al-2024>(89/277) SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems (Oubo Ma et al., 2024)</a></li><li><a href=#90277-differentially-private-high-dimensional-bandits-apurv-shukla-2024>(90/277) Differentially Private High Dimensional Bandits (Apurv Shukla, 2024)</a></li><li><a href=#91277-clarify-improving-model-robustness-with-natural-language-corrections-yoonho-lee-et-al-2024>(91/277) Clarify: Improving Model Robustness With Natural Language Corrections (Yoonho Lee et al., 2024)</a></li><li><a href=#92277-pard-permutation-invariant-autoregressive-diffusion-for-graph-generation-lingxiao-zhao-et-al-2024>(92/277) Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation (Lingxiao Zhao et al., 2024)</a></li><li><a href=#93277-symbol-correctness-in-deep-neural-networks-containing-symbolic-layers-aaron-bembenek-et-al-2024>(93/277) Symbol Correctness in Deep Neural Networks Containing Symbolic Layers (Aaron Bembenek et al., 2024)</a></li><li><a href=#94277-transductive-reward-inference-on-graph-bohao-qu-et-al-2024>(94/277) Transductive Reward Inference on Graph (Bohao Qu et al., 2024)</a></li><li><a href=#95277-disparate-impact-on-group-accuracy-of-linearization-for-private-inference-saswat-das-et-al-2024>(95/277) Disparate Impact on Group Accuracy of Linearization for Private Inference (Saswat Das et al., 2024)</a></li><li><a href=#96277-pres-toward-scalable-memory-based-dynamic-graph-neural-networks-junwei-su-et-al-2024>(96/277) PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks (Junwei Su et al., 2024)</a></li><li><a href=#97277-exploring-higher-order-neural-network-node-interactions-with-total-correlation-thomas-kerby-et-al-2024>(97/277) Exploring higher-order neural network node interactions with total correlation (Thomas Kerby et al., 2024)</a></li><li><a href=#98277-decentralized-blockchain-based-robust-multi-agent-multi-armed-bandit-mengfan-xu-et-al-2024>(98/277) Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit (Mengfan Xu et al., 2024)</a></li><li><a href=#99277-learning-from-time-series-under-temporal-label-noise-sujay-nagaraj-et-al-2024>(99/277) Learning from Time Series under Temporal Label Noise (Sujay Nagaraj et al., 2024)</a></li><li><a href=#100277-fairwire-fair-graph-generation-o-deniz-kose-et-al-2024>(100/277) FairWire: Fair Graph Generation (O. Deniz Kose et al., 2024)</a></li><li><a href=#101277-nercc-nested-regression-coded-computing-for-resilient-distributed-prediction-serving-systems-parsa-moradi-et-al-2024>(101/277) NeRCC: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems (Parsa Moradi et al., 2024)</a></li><li><a href=#102277-scaling-laws-for-learning-with-real-and-surrogate-data-ayush-jain-et-al-2024>(102/277) Scaling laws for learning with real and surrogate data (Ayush Jain et al., 2024)</a></li><li><a href=#103277-neural-networks-learn-statistics-of-increasing-complexity-nora-belrose-et-al-2024>(103/277) Neural Networks Learn Statistics of Increasing Complexity (Nora Belrose et al., 2024)</a></li><li><a href=#104277-enhance-dnn-adversarial-robustness-and-efficiency-via-injecting-noise-to-non-essential-neurons-zhenyu-liu-et-al-2024>(104/277) Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to Non-Essential Neurons (Zhenyu Liu et al., 2024)</a></li><li><a href=#105277-informed-reinforcement-learning-for-situation-aware-traffic-rule-exceptions-daniel-bogdoll-et-al-2024>(105/277) Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions (Daniel Bogdoll et al., 2024)</a></li><li><a href=#106277-attention-with-markov-a-framework-for-principled-analysis-of-transformers-via-markov-chains-ashok-vardhan-makkuva-et-al-2024>(106/277) Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains (Ashok Vardhan Makkuva et al., 2024)</a></li><li><a href=#107277-ovor-oneprompt-with-virtual-outlier-regularization-for-rehearsal-free-class-incremental-learning-wei-cheng-huang-et-al-2024>(107/277) OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning (Wei-Cheng Huang et al., 2024)</a></li><li><a href=#108277-hierarchical-delay-attribution-classification-using-unstructured-text-in-train-management-systems-anton-borg-et-al-2024>(108/277) Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems (Anton Borg et al., 2024)</a></li><li><a href=#109277-retrieve-to-explain-evidence-driven-predictions-with-language-models-ravi-patel-et-al-2024>(109/277) Retrieve to Explain: Evidence-driven Predictions with Language Models (Ravi Patel et al., 2024)</a></li><li><a href=#110277-link-prediction-with-relational-hypergraphs-xingyue-huang-et-al-2024>(110/277) Link Prediction with Relational Hypergraphs (Xingyue Huang et al., 2024)</a></li><li><a href=#111277-more-flexible-pac-bayesian-meta-learning-by-learning-learning-algorithms-hossein-zakerinia-et-al-2024>(111/277) More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms (Hossein Zakerinia et al., 2024)</a></li><li><a href=#112277-analysis-of-linear-mode-connectivity-via-permutation-based-weight-matching-akira-ito-et-al-2024>(112/277) Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching (Akira Ito et al., 2024)</a></li><li><a href=#113277-cross-entropy-versus-label-smoothing-a-neural-collapse-perspective-li-guo-et-al-2024>(113/277) Cross Entropy versus Label Smoothing: A Neural Collapse Perspective (Li Guo et al., 2024)</a></li><li><a href=#114277-tabular-data-is-attention-all-you-need-guri-zabërgja-et-al-2024>(114/277) Tabular Data: Is Attention All You Need? (Guri Zabërgja et al., 2024)</a></li><li><a href=#115277-compound-returns-reduce-variance-in-reinforcement-learning-brett-daley-et-al-2024>(115/277) Compound Returns Reduce Variance in Reinforcement Learning (Brett Daley et al., 2024)</a></li><li><a href=#116277-a-phase-transition-between-positional-and-semantic-learning-in-a-solvable-model-of-dot-product-attention-hugo-cui-et-al-2024>(116/277) A phase transition between positional and semantic learning in a solvable model of dot-product attention (Hugo Cui et al., 2024)</a></li><li><a href=#117277-efficient-generation-of-hidden-outliers-for-improved-outlier-detection-jose-cribeiro-ramallo-et-al-2024>(117/277) Efficient Generation of Hidden Outliers for Improved Outlier Detection (Jose Cribeiro-Ramallo et al., 2024)</a></li><li><a href=#118277-estimating-barycenters-of-distributions-with-neural-optimal-transport-alexander-kolesov-et-al-2024>(118/277) Estimating Barycenters of Distributions with Neural Optimal Transport (Alexander Kolesov et al., 2024)</a></li><li><a href=#119277-expediting-in-network-federated-learning-by-voting-based-consensus-model-compression-xiaoxin-su-et-al-2024>(119/277) Expediting In-Network Federated Learning by Voting-Based Consensus Model Compression (Xiaoxin Su et al., 2024)</a></li><li><a href=#120277-no-regret-reinforcement-learning-in-smooth-mdps-davide-maran-et-al-2024>(120/277) No-Regret Reinforcement Learning in Smooth MDPs (Davide Maran et al., 2024)</a></li><li><a href=#121277-learning-a-decision-tree-algorithm-with-transformers-yufan-zhuang-et-al-2024>(121/277) Learning a Decision Tree Algorithm with Transformers (Yufan Zhuang et al., 2024)</a></li><li><a href=#122277-cross-task-linearity-emerges-in-the-pretraining-finetuning-paradigm-zhanpeng-zhou-et-al-2024>(122/277) Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm (Zhanpeng Zhou et al., 2024)</a></li><li><a href=#123277-cambranch-contrastive-learning-with-augmented-milps-for-branching-jiacheng-lin-et-al-2024>(123/277) CAMBranch: Contrastive Learning with Augmented MILPs for Branching (Jiacheng Lin et al., 2024)</a></li><li><a href=#124277-neural-network-approximators-for-marginal-map-in-probabilistic-circuits-shivvrat-arya-et-al-2024>(124/277) Neural Network Approximators for Marginal MAP in Probabilistic Circuits (Shivvrat Arya et al., 2024)</a></li><li><a href=#125277-breaking-symmetry-when-training-transformers-chunsheng-zuo-et-al-2024>(125/277) Breaking Symmetry When Training Transformers (Chunsheng Zuo et al., 2024)</a></li><li><a href=#126277-learning-metrics-that-maximise-power-for-accelerated-ab-tests-olivier-jeunen-et-al-2024>(126/277) Learning Metrics that Maximise Power for Accelerated A/B-Tests (Olivier Jeunen et al., 2024)</a></li><li><a href=#127277-adaflow-imitation-learning-with-variance-adaptive-flow-based-policies-xixi-hu-et-al-2024>(127/277) AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies (Xixi Hu et al., 2024)</a></li></ul></li><li><a href=#cscv-35>cs.CV (35)</a><ul><li><a href=#128277-exploring-low-resource-medical-image-classification-with-weakly-supervised-prompt-learning-fudan-zheng-et-al-2024>(128/277) Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning (Fudan Zheng et al., 2024)</a></li><li><a href=#129277-a-hard-to-beat-baseline-for-training-free-clip-based-adaptation-zhengbo-wang-et-al-2024>(129/277) A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation (Zhengbo Wang et al., 2024)</a></li><li><a href=#130277-vision-superalignment-weak-to-strong-generalization-for-vision-foundation-models-jianyuan-guo-et-al-2024>(130/277) Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models (Jianyuan Guo et al., 2024)</a></li><li><a href=#131277-quest-low-bit-diffusion-model-quantization-via-efficient-selective-finetuning-haoxuan-wang-et-al-2024>(131/277) QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning (Haoxuan Wang et al., 2024)</a></li><li><a href=#132277-shield--an-evaluation-benchmark-for-face-spoofing-and-forgery-detection-with-multimodal-large-language-models-yichen-shi-et-al-2024>(132/277) SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models (Yichen Shi et al., 2024)</a></li><li><a href=#133277-tuning-large-multimodal-models-for-videos-using-reinforcement-learning-from-ai-feedback-daechul-ahn-et-al-2024>(133/277) Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback (Daechul Ahn et al., 2024)</a></li><li><a href=#134277-convincing-rationales-for-visual-question-answering-reasoning-kun-li-et-al-2024>(134/277) Convincing Rationales for Visual Question Answering Reasoning (Kun Li et al., 2024)</a></li><li><a href=#135277-a-data-centric-approach-for-unsupervised-domain-generalization-via-retrieval-from-web-scale-multimodal-data-christopher-liao-et-al-2024>(135/277) A Data Centric Approach for Unsupervised Domain Generalization via Retrieval from Web Scale Multimodal Data (Christopher Liao et al., 2024)</a></li><li><a href=#136277-the-instinctive-bias-spurious-images-lead-to-hallucination-in-mllms-tianyang-han-et-al-2024>(136/277) The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs (Tianyang Han et al., 2024)</a></li><li><a href=#137277-pre-training-of-lightweight-vision-transformers-on-small-datasets-with-minimally-scaled-images-jen-hong-tan-2024>(137/277) Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images (Jen Hong Tan, 2024)</a></li><li><a href=#138277-eva-clip-18b-scaling-clip-to-18-billion-parameters-quan-sun-et-al-2024>(138/277) EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters (Quan Sun et al., 2024)</a></li><li><a href=#139277-cogcom-train-large-vision-language-models-diving-into-details-through-chain-of-manipulations-ji-qi-et-al-2024>(139/277) CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations (Ji Qi et al., 2024)</a></li><li><a href=#140277-low-rank-attention-side-tuning-for-parameter-efficient-fine-tuning-ningyuan-tang-et-al-2024>(140/277) Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning (Ningyuan Tang et al., 2024)</a></li><li><a href=#141277-yolopoint-joint-keypoint-and-object-detection-anton-backhaus-et-al-2024>(141/277) YOLOPoint Joint Keypoint and Object Detection (Anton Backhaus et al., 2024)</a></li><li><a href=#142277-energy-based-domain-adaptive-segmentation-with-depth-guidance-jinjing-zhu-et-al-2024>(142/277) Energy-based Domain-Adaptive Segmentation with Depth Guidance (Jinjing Zhu et al., 2024)</a></li><li><a href=#143277-attacknet-enhancing-biometric-security-via-tailored-convolutional-neural-network-architectures-for-liveness-detection-oleksandr-kuznetsov-et-al-2024>(143/277) AttackNet: Enhancing Biometric Security via Tailored Convolutional Neural Network Architectures for Liveness Detection (Oleksandr Kuznetsov et al., 2024)</a></li><li><a href=#144277-cat-sam-conditional-tuning-network-for-few-shot-adaptation-of-segmentation-anything-model-aoran-xiao-et-al-2024>(144/277) CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of Segmentation Anything Model (Aoran Xiao et al., 2024)</a></li><li><a href=#145277-detection-transformer-for-teeth-detection-segmentation-and-numbering-in-oral-rare-diseases-focus-on-data-augmentation-and-inpainting-techniques-hocine-kadi-et-al-2024>(145/277) Detection Transformer for Teeth Detection, Segmentation, and Numbering in Oral Rare Diseases: Focus on Data Augmentation and Inpainting Techniques (Hocine Kadi et al., 2024)</a></li><li><a href=#146277-u-shaped-vision-mamba-for-single-image-dehazing-zhuoran-zheng-et-al-2024>(146/277) U-shaped Vision Mamba for Single Image Dehazing (Zhuoran Zheng et al., 2024)</a></li><li><a href=#147277-vrmm-a-volumetric-relightable-morphable-head-model-haotian-yang-et-al-2024>(147/277) VRMM: A Volumetric Relightable Morphable Head Model (Haotian Yang et al., 2024)</a></li><li><a href=#148277-boosting-adversarial-transferability-across-model-genus-by-deformation-constrained-warping-qinliang-lin-et-al-2024>(148/277) Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping (Qinliang Lin et al., 2024)</a></li><li><a href=#149277-reviewing-fid-and-sid-metrics-on-generative-adversarial-networks-ricardo-de-deijn-et-al-2024>(149/277) Reviewing FID and SID Metrics on Generative Adversarial Networks (Ricardo de Deijn et al., 2024)</a></li><li><a href=#150277-intensive-vision-guided-network-for-radiology-report-generation-fudan-zheng-et-al-2024>(150/277) Intensive Vision-guided Network for Radiology Report Generation (Fudan Zheng et al., 2024)</a></li><li><a href=#151277-breaking-data-silos-cross-domain-learning-for-multi-agent-perception-from-independent-private-sources-jinlong-li-et-al-2024>(151/277) Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources (Jinlong Li et al., 2024)</a></li><li><a href=#152277-consisti2v-enhancing-visual-consistency-for-image-to-video-generation-weiming-ren-et-al-2024>(152/277) ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation (Weiming Ren et al., 2024)</a></li><li><a href=#153277-analysis-of-deep-image-prior-and-exploiting-self-guidance-for-image-reconstruction-shijun-liang-et-al-2024>(153/277) Analysis of Deep Image Prior and Exploiting Self-Guidance for Image Reconstruction (Shijun Liang et al., 2024)</a></li><li><a href=#154277-polyp-ddpm-diffusion-based-semantic-polyp-synthesis-for-enhanced-segmentation-zolnamar-dorjsembe-et-al-2024>(154/277) Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation (Zolnamar Dorjsembe et al., 2024)</a></li><li><a href=#155277-controllable-diverse-sampling-for-diffusion-based-motion-behavior-forecasting-yiming-xu-et-al-2024>(155/277) Controllable Diverse Sampling for Diffusion Based Motion Behavior Forecasting (Yiming Xu et al., 2024)</a></li><li><a href=#156277-eschernet-a-generative-model-for-scalable-view-synthesis-xin-kong-et-al-2024>(156/277) EscherNet: A Generative Model for Scalable View Synthesis (Xin Kong et al., 2024)</a></li><li><a href=#157277-deep-msfop-multiple-spectral-filter-operators-preservation-in-deep-functional-maps-for-unsupervised-shape-matching-feifan-luo-et-al-2024>(157/277) Deep MSFOP: Multiple Spectral filter Operators Preservation in Deep Functional Maps for Unsupervised Shape Matching (Feifan Luo et al., 2024)</a></li><li><a href=#158277-mobilevlm-v2-faster-and-stronger-baseline-for-vision-language-model-xiangxiang-chu-et-al-2024>(158/277) MobileVLM V2: Faster and Stronger Baseline for Vision Language Model (Xiangxiang Chu et al., 2024)</a></li><li><a href=#159277-attention-based-shape-and-gait-representations-learning-for-video-based-cloth-changing-person-re-identification-vuong-d-nguyen-et-al-2024>(159/277) Attention-based Shape and Gait Representations Learning for Video-based Cloth-Changing Person Re-Identification (Vuong D. Nguyen et al., 2024)</a></li><li><a href=#160277-foolsdedit-deceptively-steering-your-edits-towards-targeted-attribute-aware-distribution-qi-zhou-et-al-2024>(160/277) FoolSDEdit: Deceptively Steering Your Edits Towards Targeted Attribute-aware Distribution (Qi Zhou et al., 2024)</a></li><li><a href=#161277-beam-beta-distribution-ray-denoising-for-multi-view-3d-object-detection-feng-liu-et-al-2024>(161/277) BEAM: Beta Distribution Ray Denoising for Multi-view 3D Object Detection (Feng Liu et al., 2024)</a></li><li><a href=#162277-grasp-graph-structured-pyramidal-whole-slide-image-representation-ali-khajegili-mirabadi-et-al-2024>(162/277) GRASP: GRAph-Structured Pyramidal Whole Slide Image Representation (Ali Khajegili Mirabadi et al., 2024)</a></li></ul></li><li><a href=#statml-6>stat.ML (6)</a><ul><li><a href=#163277-pac-bayesian-adversarially-robust-generalization-bounds-for-graph-neural-network-tan-sun-et-al-2024>(163/277) PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network (Tan Sun et al., 2024)</a></li><li><a href=#164277-gaussian-process-regression-with-sliced-wasserstein-weisfeiler-lehman-graph-kernels-raphaël-carpintero-perez-et-al-2024>(164/277) Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels (Raphaël Carpintero Perez et al., 2024)</a></li><li><a href=#165277-statistical-test-for-anomaly-detections-by-variational-auto-encoders-daiki-miwa-et-al-2024>(165/277) Statistical Test for Anomaly Detections by Variational Auto-Encoders (Daiki Miwa et al., 2024)</a></li><li><a href=#166277-interpretable-multi-source-data-fusion-through-latent-variable-gaussian-process-sandipp-krishnan-ravi-et-al-2024>(166/277) Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process (Sandipp Krishnan Ravi et al., 2024)</a></li><li><a href=#167277-a-general-theory-for-kernel-packets-from-state-space-model-to-compactly-supported-basis-liang-ding-et-al-2024>(167/277) A General Theory for Kernel Packets: from state space model to compactly supported basis (Liang Ding et al., 2024)</a></li><li><a href=#168277-subsampling-is-not-magic-why-large-batch-sizes-work-for-differentially-private-stochastic-optimisation-ossi-räisä-et-al-2024>(168/277) Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation (Ossi Räisä et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#169277-reborn-reinforcement-learned-boundary-segmentation-with-iterative-training-for-unsupervised-asr-liang-hsuan-tseng-et-al-2024>(169/277) REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR (Liang-Hsuan Tseng et al., 2024)</a></li><li><a href=#170277-listen-chat-and-edit-text-guided-soundscape-modification-for-enhanced-auditory-experience-xilin-jiang-et-al-2024>(170/277) Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced Auditory Experience (Xilin Jiang et al., 2024)</a></li></ul></li><li><a href=#cscr-5>cs.CR (5)</a><ul><li><a href=#171277-explainable-adversarial-learning-framework-on-physical-layer-secret-keys-combating-malicious-reconfigurable-intelligent-surface-zhuangkun-wei-et-al-2024>(171/277) Explainable Adversarial Learning Framework on Physical Layer Secret Keys Combating Malicious Reconfigurable Intelligent Surface (Zhuangkun Wei et al., 2024)</a></li><li><a href=#172277-llm-agents-can-autonomously-hack-websites-richard-fang-et-al-2024>(172/277) LLM Agents can Autonomously Hack Websites (Richard Fang et al., 2024)</a></li><li><a href=#173277-use-of-multi-cnns-for-section-analysis-in-static-malware-detection-tony-quertier-et-al-2024>(173/277) Use of Multi-CNNs for Section Analysis in Static Malware Detection (Tony Quertier et al., 2024)</a></li><li><a href=#174277-lipstick-corruptibility-aware-and-explainable-graph-neural-network-based-oracle-less-attack-on-logic-locking-yeganeh-aghamohammadi-et-al-2024>(174/277) LIPSTICK: Corruptibility-Aware and Explainable Graph Neural Network-based Oracle-Less Attack on Logic Locking (Yeganeh Aghamohammadi et al., 2024)</a></li><li><a href=#175277-cops-a-compact-on-device-pipeline-for-real-time-smishing-detection-harichandana-b-s-s-et-al-2024>(175/277) COPS: A Compact On-device Pipeline for real-time Smishing detection (Harichandana B S S et al., 2024)</a></li></ul></li><li><a href=#csai-20>cs.AI (20)</a><ul><li><a href=#176277-self-discover-large-language-models-self-compose-reasoning-structures-pei-zhou-et-al-2024>(176/277) Self-Discover: Large Language Models Self-Compose Reasoning Structures (Pei Zhou et al., 2024)</a></li><li><a href=#177277-advancing-legal-reasoning-the-integration-of-ai-to-navigate-complexities-and-biases-in-global-jurisprudence-with-semi-automated-arbitration-processes-saaps-michael-deshazer-2024>(177/277) Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs) (Michael De&rsquo;Shazer, 2024)</a></li><li><a href=#178277-scemqa-a-scientific-college-entrance-level-multimodal-question-answering-benchmark-zhenwen-liang-et-al-2024>(178/277) SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark (Zhenwen Liang et al., 2024)</a></li><li><a href=#179277-read-to-play-r2-play-decision-transformer-with-multimodal-game-instruction-yonggang-jin-et-al-2024>(179/277) Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction (Yonggang Jin et al., 2024)</a></li><li><a href=#180277-comparing-abstraction-in-humans-and-large-language-models-using-multimodal-serial-reproduction-sreejan-kumar-et-al-2024>(180/277) Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction (Sreejan Kumar et al., 2024)</a></li><li><a href=#181277-can-generative-agents-predict-emotion-ciaran-regan-et-al-2024>(181/277) Can Generative Agents Predict Emotion? (Ciaran Regan et al., 2024)</a></li><li><a href=#182277-revorder-a-novel-method-for-enhanced-arithmetic-in-language-models-si-shen-et-al-2024>(182/277) RevOrder: A Novel Method for Enhanced Arithmetic in Language Models (Si Shen et al., 2024)</a></li><li><a href=#183277-position-paper-against-spurious-sparks---dovelating-inflated-ai-claims-patrick-altmeyer-et-al-2024>(183/277) Position Paper: Against Spurious Sparks $-$ Dovelating Inflated AI Claims (Patrick Altmeyer et al., 2024)</a></li><li><a href=#184277-cadren-contextual-anchor-driven-relational-network-for-controllable-cross-graphs-node-importance-estimation-zijie-zhong-et-al-2024>(184/277) CADReN: Contextual Anchor-Driven Relational Network for Controllable Cross-Graphs Node Importance Estimation (Zijie Zhong et al., 2024)</a></li><li><a href=#185277-quantagent-seeking-holy-grail-in-trading-by-self-improving-large-language-model-saizhuo-wang-et-al-2024>(185/277) QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model (Saizhuo Wang et al., 2024)</a></li><li><a href=#186277-limits-of-large-language-models-in-debating-humans-james-flamino-et-al-2024>(186/277) Limits of Large Language Models in Debating Humans (James Flamino et al., 2024)</a></li><li><a href=#187277-improving-contextual-congruence-across-modalities-for-effective-multimodal-marketing-using-knowledge-infused-learning-trilok-padhi-et-al-2024>(187/277) Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning (Trilok Padhi et al., 2024)</a></li><li><a href=#188277-the-essential-role-of-causality-in-foundation-world-models-for-embodied-ai-tarun-gupta-et-al-2024>(188/277) The Essential Role of Causality in Foundation World Models for Embodied AI (Tarun Gupta et al., 2024)</a></li><li><a href=#189277-counterfactual-generation-with-answer-set-programming-sopam-dasgupta-et-al-2024>(189/277) Counterfactual Generation with Answer Set Programming (Sopam Dasgupta et al., 2024)</a></li><li><a href=#190277-pedestrian-crossing-decisions-can-be-explained-by-bounded-optimal-decision-making-under-noisy-visual-perception-yueyang-wang-et-al-2024>(190/277) Pedestrian crossing decisions can be explained by bounded optimal decision-making under noisy visual perception (Yueyang Wang et al., 2024)</a></li><li><a href=#191277-task-success-is-not-enough-investigating-the-use-of-video-language-models-as-behavior-critics-for-catching-undesirable-agent-behaviors-lin-guan-et-al-2024>(191/277) &lsquo;Task Success&rsquo; is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors (Lin Guan et al., 2024)</a></li><li><a href=#192277-human-like-geometric-abstraction-in-large-pre-trained-neural-networks-declan-campbell-et-al-2024>(192/277) Human-Like Geometric Abstraction in Large Pre-trained Neural Networks (Declan Campbell et al., 2024)</a></li><li><a href=#193277-embedding-knowledge-graphs-in-degenerate-clifford-algebras-louis-mozart-kamdem-et-al-2024>(193/277) Embedding Knowledge Graphs in Degenerate Clifford Algebras (Louis Mozart Kamdem et al., 2024)</a></li><li><a href=#194277-a-call-for-embodied-ai-giuseppe-paolo-et-al-2024>(194/277) A call for embodied AI (Giuseppe Paolo et al., 2024)</a></li><li><a href=#195277-logical-specifications-guided-dynamic-task-sampling-for-reinforcement-learning-agents-yash-shukla-et-al-2024>(195/277) Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents (Yash Shukla et al., 2024)</a></li></ul></li><li><a href=#eessiv-4>eess.IV (4)</a><ul><li><a href=#196277-what-limits-performance-of-weakly-supervised-deep-learning-for-chest-ct-classification-fakrul-islam-tushar-et-al-2024>(196/277) What limits performance of weakly supervised deep learning for chest CT classification? (Fakrul Islam Tushar et al., 2024)</a></li><li><a href=#197277-deep-learning-based-correction-and-unmixing-of-hyperspectral-images-for-brain-tumor-surgery-david-black-et-al-2024>(197/277) Deep Learning-Based Correction and Unmixing of Hyperspectral Images for Brain Tumor Surgery (David Black et al., 2024)</a></li><li><a href=#198277-conunetr-a-conditional-transformer-network-for-3d-micro-ct-embryonic-cartilage-segmentation-nishchal-sapkota-et-al-2024>(198/277) ConUNETR: A Conditional Transformer Network for 3D Micro-CT Embryonic Cartilage Segmentation (Nishchal Sapkota et al., 2024)</a></li><li><a href=#199277-3d-volumetric-super-resolution-in-radiology-using-3d-rrdb-gan-juhyung-ha-et-al-2024>(199/277) 3D Volumetric Super-Resolution in Radiology Using 3D RRDB-GAN (Juhyung Ha et al., 2024)</a></li></ul></li><li><a href=#csir-6>cs.IR (6)</a><ul><li><a href=#200277-can-large-language-models-detect-rumors-on-social-media-qiang-liu-et-al-2024>(200/277) Can Large Language Models Detect Rumors on Social Media? (Qiang Liu et al., 2024)</a></li><li><a href=#201277-the-potential-of-automl-for-recommender-systems-tobias-vente-et-al-2024>(201/277) The Potential of AutoML for Recommender Systems (Tobias Vente et al., 2024)</a></li><li><a href=#202277-reliability-quality-measures-for-recommender-systems-jesús-bobadilla-et-al-2024>(202/277) Reliability quality measures for recommender systems (Jesús Bobadilla et al., 2024)</a></li><li><a href=#203277-on-practical-diversified-recommendation-with-controllable-category-diversity-framework-tao-zhang-et-al-2024>(203/277) On Practical Diversified Recommendation with Controllable Category Diversity Framework (Tao Zhang et al., 2024)</a></li><li><a href=#204277-retrieval-augmented-cross-modal-tag-recommendation-in-software-qa-sites-sijin-lu-et-al-2024>(204/277) Retrieval Augmented Cross-Modal Tag Recommendation in Software Q&amp;A Sites (Sijin Lu et al., 2024)</a></li><li><a href=#205277-understanding-and-counteracting-feature-level-bias-in-click-through-rate-prediction-jinqiu-jin-et-al-2024>(205/277) Understanding and Counteracting Feature-Level Bias in Click-Through Rate Prediction (Jinqiu Jin et al., 2024)</a></li></ul></li><li><a href=#csro-17>cs.RO (17)</a><ul><li><a href=#206277-human-observation-inspired-trajectory-prediction-for-autonomous-driving-in-mixed-autonomy-traffic-environments-haicheng-liao-et-al-2024>(206/277) Human Observation-Inspired Trajectory Prediction for Autonomous Driving in Mixed-Autonomy Traffic Environments (Haicheng Liao et al., 2024)</a></li><li><a href=#207277-reinforcement-learning-for-collision-free-flight-exploiting-deep-collision-encoding-mihir-kulkarni-et-al-2024>(207/277) Reinforcement Learning for Collision-free Flight Exploiting Deep Collision Encoding (Mihir Kulkarni et al., 2024)</a></li><li><a href=#208277-belief-scene-graphs-expanding-partial-scenes-with-objects-through-computation-of-expectation-mario-a-v-saucedo-et-al-2024>(208/277) Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation (Mario A. V. Saucedo et al., 2024)</a></li><li><a href=#209277-automatic-robotic-development-through-collaborative-framework-by-large-language-models-zhirong-luan-et-al-2024>(209/277) Automatic Robotic Development through Collaborative Framework by Large Language Models (Zhirong Luan et al., 2024)</a></li><li><a href=#210277-rl-vlm-f-reinforcement-learning-from-vision-language-foundation-model-feedback-yufei-wang-et-al-2024>(210/277) RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback (Yufei Wang et al., 2024)</a></li><li><a href=#211277-hierarchical-large-language-models-in-cloud-edge-end-architecture-for-heterogeneous-robot-cluster-control-zhirong-luan-et-al-2024>(211/277) Hierarchical Large Language Models in Cloud Edge End Architecture for Heterogeneous Robot Cluster Control (Zhirong Luan et al., 2024)</a></li><li><a href=#212277-a-survey-of-offline-and-online-learning-based-algorithms-for-multirotor-uavs-serhat-sönmez-et-al-2024>(212/277) A Survey of Offline and Online Learning-Based Algorithms for Multirotor UAVs (Serhat Sönmez et al., 2024)</a></li><li><a href=#213277-intelligent-collective-escape-of-swarm-robots-based-on-a-novel-fish-inspired-self-adaptive-approach-with-neurodynamic-models-junfei-li-et-al-2024>(213/277) Intelligent Collective Escape of Swarm Robots Based on a Novel Fish-inspired Self-adaptive Approach with Neurodynamic Models (Junfei Li et al., 2024)</a></li><li><a href=#214277-explaining-autonomy-enhancing-human-robot-interaction-through-explanation-generation-with-large-language-models-david-sobrín-hidalgo-et-al-2024>(214/277) Explaining Autonomy: Enhancing Human-Robot Interaction through Explanation Generation with Large Language Models (David Sobrín-Hidalgo et al., 2024)</a></li><li><a href=#215277-prediction-horizon-requirements-for-automated-driving-optimizing-safety-comfort-and-efficiency-manuel-muñoz-sánchez-et-al-2024>(215/277) Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency (Manuel Muñoz Sánchez et al., 2024)</a></li><li><a href=#216277-online-informative-sampling-using-semantic-features-in-underwater-environments-shrutika-vishal-thengane-et-al-2024>(216/277) Online Informative Sampling using Semantic Features in Underwater Environments (Shrutika Vishal Thengane et al., 2024)</a></li><li><a href=#217277-integration-of-4d-bim-and-robot-task-planning-creation-and-flow-of-construction-related-information-for-action-level-simulation-of-indoor-wall-frame-installation-hafiz-oyediran-et-al-2024>(217/277) Integration of 4D BIM and Robot Task Planning: Creation and Flow of Construction-Related Information for Action-Level Simulation of Indoor Wall Frame Installation (Hafiz Oyediran et al., 2024)</a></li><li><a href=#218277-enhancing-embodied-object-detection-through-language-image-pre-training-and-implicit-object-memory-nicolas-harvey-chapman-et-al-2024>(218/277) Enhancing Embodied Object Detection through Language-Image Pre-training and Implicit Object Memory (Nicolas Harvey Chapman et al., 2024)</a></li><li><a href=#219277-aed-adaptable-error-detection-for-few-shot-imitation-policy-jia-fong-yeh-et-al-2024>(219/277) AED: Adaptable Error Detection for Few-shot Imitation Policy (Jia-Fong Yeh et al., 2024)</a></li><li><a href=#220277-environment-centric-learning-approach-for-gait-synthesis-in-terrestrial-soft-robots-caitlin-freeman-et-al-2024>(220/277) Environment-Centric Learning Approach for Gait Synthesis in Terrestrial Soft Robots (Caitlin Freeman et al., 2024)</a></li><li><a href=#221277-spatial-assisted-human-drone-collaborative-navigation-and-interaction-through-immersive-mixed-reality-luca-morando-et-al-2024>(221/277) Spatial Assisted Human-Drone Collaborative Navigation and Interaction through Immersive Mixed Reality (Luca Morando et al., 2024)</a></li><li><a href=#222277-mmaud-a-comprehensive-multi-modal-anti-uav-dataset-for-modern-miniature-drone-threats-shenghai-yuan-et-al-2024>(222/277) MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats (Shenghai Yuan et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#223277-botsscl-social-bot-detection-with-self-supervised-contrastive-learning-mohammad-majid-akhtar-et-al-2024>(223/277) BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning (Mohammad Majid Akhtar et al., 2024)</a></li><li><a href=#224277-understanding-trends-patterns-and-dynamics-in-global-acquisitions-a-network-perspective-ghazal-kalhor-et-al-2024>(224/277) Understanding Trends, Patterns, and Dynamics in Global Acquisitions: A Network Perspective (Ghazal Kalhor et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-2>q-bio.QM (2)</a><ul><li><a href=#225277-moltc-towards-molecular-relational-modeling-in-language-models-junfeng-fang-et-al-2024>(225/277) MolTC: Towards Molecular Relational Modeling In Language Models (Junfeng Fang et al., 2024)</a></li><li><a href=#226277-progress-and-opportunities-of-foundation-models-in-bioinformatics-qing-li-et-al-2024>(226/277) Progress and Opportunities of Foundation Models in Bioinformatics (Qing Li et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#227277-personality-trait-recognition-using-ecg-spectrograms-and-deep-learning-muhammad-mohsin-altaf-et-al-2024>(227/277) Personality Trait Recognition using ECG Spectrograms and Deep Learning (Muhammad Mohsin Altaf et al., 2024)</a></li><li><a href=#228277-embedding-large-language-models-into-extended-reality-opportunities-and-challenges-for-inclusion-engagement-and-privacy-efe-bozkir-et-al-2024>(228/277) Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy (Efe Bozkir et al., 2024)</a></li><li><a href=#229277-genlens-a-systematic-evaluation-of-visual-genai-model-outputs-tica-lin-et-al-2024>(229/277) GenLens: A Systematic Evaluation of Visual GenAI Model Outputs (Tica Lin et al., 2024)</a></li><li><a href=#230277-human-emotions-analysis-and-recognition-using-eeg-signals-in-response-to-360circ-videos-haseeb-ur-rahman-abbasi-et-al-2024>(230/277) Human Emotions Analysis and Recognition Using EEG Signals in Response to 360$^\circ$ Videos (Haseeb ur Rahman Abbasi et al., 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#231277-arman-a-reconfigurable-monolithic-3d-accelerator-architecture-for-convolutional-neural-networks-ali-sedaghatgoo-et-al-2024>(231/277) ARMAN: A Reconfigurable Monolithic 3D Accelerator Architecture for Convolutional Neural Networks (Ali Sedaghatgoo et al., 2024)</a></li><li><a href=#232277-heam--hashed-embedding-acceleration-using-processing-in-memory-youngsuk-kim-et-al-2024>(232/277) HEAM : Hashed Embedding Acceleration using Processing-In-Memory (Youngsuk Kim et al., 2024)</a></li></ul></li><li><a href=#cscy-5>cs.CY (5)</a><ul><li><a href=#233277-the-world-of-generative-ai-deepfakes-and-large-language-models-alakananda-mitra-et-al-2024>(233/277) The World of Generative AI: Deepfakes and Large Language Models (Alakananda Mitra et al., 2024)</a></li><li><a href=#234277-measuring-implicit-bias-in-explicitly-unbiased-large-language-models-xuechunzi-bai-et-al-2024>(234/277) Measuring Implicit Bias in Explicitly Unbiased Large Language Models (Xuechunzi Bai et al., 2024)</a></li><li><a href=#235277-ai-language-models-as-role-playing-tools-not-human-participants-zhicheng-lin-2024>(235/277) AI language models as role-playing tools, not human participants (Zhicheng Lin, 2024)</a></li><li><a href=#236277-prioritizing-safeguarding-over-autonomy-risks-of-llm-agents-for-science-xiangru-tang-et-al-2024>(236/277) Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science (Xiangru Tang et al., 2024)</a></li><li><a href=#237277-measuring-machine-learning-harms-from-stereotypes-requires-understanding-who-is-being-harmed-by-which-errors-in-what-ways-angelina-wang-et-al-2024>(237/277) Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways (Angelina Wang et al., 2024)</a></li></ul></li><li><a href=#csit-6>cs.IT (6)</a><ul><li><a href=#238277-sensing-mutual-information-with-random-signals-in-gaussian-channels-bridging-sensing-and-communication-metrics-lei-xie-et-al-2024>(238/277) Sensing Mutual Information with Random Signals in Gaussian Channels: Bridging Sensing and Communication Metrics (Lei Xie et al., 2024)</a></li><li><a href=#239277-on-learning-spatial-provenance-in-privacy-constrained-wireless-networks-manish-bansal-et-al-2024>(239/277) On Learning Spatial Provenance in Privacy-Constrained Wireless Networks (Manish Bansal et al., 2024)</a></li><li><a href=#240277-batch-universal-prediction-marco-bondaschi-et-al-2024>(240/277) Batch Universal Prediction (Marco Bondaschi et al., 2024)</a></li><li><a href=#241277-vector-approximate-message-passing-with-arbitrary-iid-noise-priors-mohamed-akrout-et-al-2024>(241/277) Vector Approximate Message Passing With Arbitrary I.I.D. Noise Priors (Mohamed Akrout et al., 2024)</a></li><li><a href=#242277-tail-erasure-correcting-codes-boaz-moav-et-al-2024>(242/277) Tail-Erasure-Correcting Codes (Boaz Moav et al., 2024)</a></li><li><a href=#243277-fundamental-limits-of-two-hop-mimo-channels-an-asymptotic-approach-zeyan-zhuang-et-al-2024>(243/277) Fundamental Limits of Two-Hop MIMO Channels: An Asymptotic Approach (Zeyan Zhuang et al., 2024)</a></li></ul></li><li><a href=#csse-7>cs.SE (7)</a><ul><li><a href=#244277-assured-llm-based-software-engineering-nadia-alshahwan-et-al-2024>(244/277) Assured LLM-Based Software Engineering (Nadia Alshahwan et al., 2024)</a></li><li><a href=#245277-multi-line-ai-assisted-code-authoring-omer-dunay-et-al-2024>(245/277) Multi-line AI-assisted Code Authoring (Omer Dunay et al., 2024)</a></li><li><a href=#246277-automated-description-generation-for-software-patches-thanh-trong-vu-et-al-2024>(246/277) Automated Description Generation for Software Patches (Thanh Trong Vu et al., 2024)</a></li><li><a href=#247277-improving-automated-code-reviews-learning-from-experience-hong-yi-lin-et-al-2024>(247/277) Improving Automated Code Reviews: Learning from Experience (Hong Yi Lin et al., 2024)</a></li><li><a href=#248277-investigating-the-utility-of-chatgpt-in-the-issue-tracking-system-an-exploratory-study-joy-krishan-das-et-al-2024>(248/277) Investigating the Utility of ChatGPT in the Issue Tracking System: An Exploratory Study (Joy Krishan Das et al., 2024)</a></li><li><a href=#249277-enhancing-llm-based-coding-tools-through-native-integration-of-ide-derived-static-context-yichen-li-et-al-2024>(249/277) Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context (Yichen Li et al., 2024)</a></li><li><a href=#250277-studying-vulnerable-code-entities-in-r-zixiao-zhao-et-al-2024>(250/277) Studying Vulnerable Code Entities in R (Zixiao Zhao et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#251277-resource-aware-hierarchical-federated-learning-in-wireless-video-caching-networks-md-ferdous-pervej-et-al-2024>(251/277) Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks (Md Ferdous Pervej et al., 2024)</a></li><li><a href=#252277-demarking-a-defense-for-network-flow-watermarking-in-real-time-yali-yuan-et-al-2024>(252/277) DeMarking: A Defense for Network Flow Watermarking in Real-Time (Yali Yuan et al., 2024)</a></li></ul></li><li><a href=#eesssy-5>eess.SY (5)</a><ul><li><a href=#253277-a-digital-twin-design-methodology-for-control-simulation-and-monitoring-of-fluidic-circuits-veyis-gunes-2024>(253/277) A Digital Twin Design Methodology for Control, Simulation, and Monitoring of Fluidic Circuits (Veyis Gunes, 2024)</a></li><li><a href=#254277-pmsm-transient-response-optimization-by-end-to-end-optimal-control-yuta-kawachi-et-al-2024>(254/277) PMSM transient response optimization by end-to-end optimal control (Yuta Kawachi et al., 2024)</a></li><li><a href=#255277-pso-based-adaptive-nmpc-for-uranium-extraction-scrubbing-operation-in-spent-nuclear-fuel-treatment-process-duc-tri-vo-et-al-2024>(255/277) PSO-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process (Duc-Tri Vo et al., 2024)</a></li><li><a href=#256277-equitable-networked-microgrid-topology-reconfiguration-for-wildfire-risk-mitigation-yuqi-zhou-et-al-2024>(256/277) Equitable Networked Microgrid Topology Reconfiguration for Wildfire Risk Mitigation (Yuqi Zhou et al., 2024)</a></li><li><a href=#257277-design-and-implementation-of-a-real-time-onboard-system-for-a-stratospheric-balloon-mission-using-commercial-off-the-self-components-and-a-model-based-approach-angel-grover-perez-munoz-et-al-2024>(257/277) Design and implementation of a real-time onboard system for a stratospheric balloon mission using commercial off-the-self components and a model-based approach (Angel-Grover Perez-Munoz et al., 2024)</a></li></ul></li><li><a href=#cssy-1>cs.SY (1)</a><ul><li><a href=#258277-robust-data-enabled-predictive-leading-cruise-control-via-reachability-analysis-shuai-li-et-al-2024>(258/277) Robust Data-EnablEd Predictive Leading Cruise Control via Reachability Analysis (Shuai Li et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#259277-finite-volumes-for-the-gross-pitaevskii-equation-quentin-chauleur-2024>(259/277) Finite volumes for the Gross-Pitaevskii equation (Quentin Chauleur, 2024)</a></li><li><a href=#260277-strong-approximation-of-the-time-fractional-cahn--hilliard-equation-driven-by-a-fractionally-integrated-additive-noise-mariam-al-maskari-et-al-2024>(260/277) Strong approximation of the time-fractional Cahn&ndash;Hilliard equation driven by a fractionally integrated additive noise (Mariam Al-Maskari et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#261277-association-between-prefrontal-fnirs-signals-during-cognitive-tasks-and-college-scholastic-ability-test-csat-scores-analysis-using-a-quantum-annealing-approach-yeaju-kim-et-al-2024>(261/277) Association between Prefrontal fNIRS signals during Cognitive tasks and College scholastic ability test (CSAT) scores: Analysis using a quantum annealing approach (Yeaju Kim et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#262277-gambling-based-confidence-sequences-for-bounded-random-vectors-j-jon-ryu-et-al-2024>(262/277) Gambling-Based Confidence Sequences for Bounded Random Vectors (J. Jon Ryu et al., 2024)</a></li></ul></li><li><a href=#csdc-3>cs.DC (3)</a><ul><li><a href=#263277-argo-an-auto-tuning-runtime-system-for-scalable-gnn-training-on-multi-core-processor-yi-chien-lin-et-al-2024>(263/277) ARGO: An Auto-Tuning Runtime System for Scalable GNN Training on Multi-Core Processor (Yi-Chien Lin et al., 2024)</a></li><li><a href=#264277-adaptive-blockwise-task-interleaved-pipeline-parallelism-ding-tang-et-al-2024>(264/277) Adaptive Blockwise Task-interleaved Pipeline Parallelism (Ding Tang et al., 2024)</a></li><li><a href=#265277-agent-based-triangle-counting-and-its-applications-in-anonymous-graphs-prabhat-kumar-chand-et-al-2024>(265/277) Agent-Based Triangle Counting and its Applications in Anonymous Graphs (Prabhat Kumar Chand et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#266277-interpretable-domain-knowledge-enhanced-machine-learning-framework-on-axial-capacity-prediction-of-circular-cfst-columns-dian-wang-et-al-2024>(266/277) Interpretable domain knowledge enhanced machine learning framework on axial capacity prediction of circular CFST columns (Dian Wang et al., 2024)</a></li></ul></li><li><a href=#csgt-3>cs.GT (3)</a><ul><li><a href=#267277-fair-interval-scheduling-of-indivisible-chores-sarfaraz-equbal-et-al-2024>(267/277) Fair Interval Scheduling of Indivisible Chores (Sarfaraz Equbal et al., 2024)</a></li><li><a href=#268277-approximating-the-core-via-iterative-coalition-sampling-ian-gemp-et-al-2024>(268/277) Approximating the Core via Iterative Coalition Sampling (Ian Gemp et al., 2024)</a></li><li><a href=#269277-rlas-for-2-seat-stv-elections-revisited-michelle-blom-et-al-2024>(269/277) RLAs for 2-Seat STV Elections: Revisited (Michelle Blom et al., 2024)</a></li></ul></li><li><a href=#cspf-1>cs.PF (1)</a><ul><li><a href=#270277-acceleration-and-energy-consumption-optimization-in-cascading-classifiers-for-face-detection-on-low-cost-arm-biglittle-asymmetric-architectures-alberto-corpas-et-al-2024>(270/277) Acceleration and energy consumption optimization in cascading classifiers for face detection on low-cost ARM big.LITTLE asymmetric architectures (Alberto Corpas et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#271277-on-convergence-of-adam-for-stochastic-optimization-under-relaxed-assumptions-yusu-hong-et-al-2024>(271/277) On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions (Yusu Hong et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#272277-joint-intrinsic-motivation-for-coordinated-exploration-in-multi-agent-deep-reinforcement-learning-maxime-toquebiau-et-al-2024>(272/277) Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning (Maxime Toquebiau et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#273277-using-metaheuristics-for-the-location-of-bicycle-stations-christian-cintrano-et-al-2024>(273/277) Using metaheuristics for the location of bicycle stations (Christian Cintrano et al., 2024)</a></li></ul></li><li><a href=#cond-matmes-hall-1>cond-mat.mes-hall (1)</a><ul><li><a href=#274277-fully-autonomous-tuning-of-a-spin-qubit-jonas-schuff-et-al-2024>(274/277) Fully autonomous tuning of a spin qubit (Jonas Schuff et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#275277-geometric-quantum-machine-learning-of-bqpa-protocols-and-latent-graph-classifiers-chukwudubem-umeano-et-al-2024>(275/277) Geometric quantum machine learning of BQP$^A$ protocols and latent graph classifiers (Chukwudubem Umeano et al., 2024)</a></li></ul></li><li><a href=#q-finrm-1>q-fin.RM (1)</a><ul><li><a href=#276277-explainable-automated-machine-learning-for-credit-decisions-enhancing-human-artificial-intelligence-collaboration-in-financial-engineering-marc-schmitt-2024>(276/277) Explainable Automated Machine Learning for Credit Decisions: Enhancing Human Artificial Intelligence Collaboration in Financial Engineering (Marc Schmitt, 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#277277-an-effective-branch-and-bound-algorithm-with-new-bounding-methods-for-the-maximum-s-bundle-problem-jinghui-xue-et-al-2024>(277/277) An Effective Branch-and-Bound Algorithm with New Bounding Methods for the Maximum $s$-Bundle Problem (Jinghui Xue et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>