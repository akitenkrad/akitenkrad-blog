<!doctype html><html><head><title>arXiv @ 2024.02.07</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.07"><meta property="og:description" content="Primary Categories astro-ph.IM (1) astro-ph.SR (1) cs.AI (11) cs.AR (2) cs.CE (3) cs.CL (48) cs.CR (6) cs.CV (41) cs.CY (1) cs.DB (1) cs.DC (2) cs.DS (1) cs.GR (1) cs.HC (4) cs.IR (12) cs.IT (9) cs.LG (79) cs.LO (2) cs.MA (2) cs.MM (1) cs.NE (1) cs.NI (2) cs.PL (1) cs.RO (2) cs.SD (6) cs.SE (4) cs.SI (3) eess.AS (2) eess.IV (10) eess.SY (4) math.AT (1) math.NA (3) math.OC (2) physics.chem-ph (1) q-bio."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240207000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-07T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-07T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.07"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240207000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Wednesday, Feb 7, 2024</p></div><div class=title><h1>arXiv @ 2024.02.07</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#astro-phsr-1>astro-ph.SR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csai-11>cs.AI (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csce-3>cs.CE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#cscl-48>cs.CL (48)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#cscr-6>cs.CR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#cscv-41>cs.CV (41)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csir-12>cs.IR (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csit-9>cs.IT (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#cslg-79>cs.LG (79)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#cslo-2>cs.LO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csro-2>cs.RO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#cssd-6>cs.SD (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#csse-4>cs.SE (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#cssi-3>cs.SI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#eessiv-10>eess.IV (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#eesssy-4>eess.SY (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#mathat-1>math.AT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#mathna-3>math.NA (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#physicschem-ph-1>physics.chem-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#quant-ph-3>quant-ph (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/#statml-9>stat.ML (9)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>category</th><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.IR</th><th>cs.LG</th><th>eess.IV</th></tr></thead><tbody><tr><th></th><td>Active Learning</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Adversarial Attack</td><td>1</td><td>1</td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Adversarial Learning</td><td></td><td>1</td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Aspect-based Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Autoencoder</td><td></td><td></td><td></td><td></td><td>2</td><td>1</td></tr><tr><th></th><td>Automatic Speech Recognition</td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>BERT</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Bandit Algorithm</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>ChatGPT</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Code Generation</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Continual Learning</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Contrastive Learning</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Convolution</td><td></td><td></td><td>5</td><td></td><td>4</td><td>4</td></tr><tr><th></th><td>Convolutional Neural Network</td><td></td><td></td><td>8</td><td></td><td>2</td><td>6</td></tr><tr><th></th><td>Counter-factual</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Dense Retrieval</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Domain Adaptation</td><td></td><td></td><td>2</td><td>1</td><td>1</td><td></td></tr><tr><th></th><td>Event Argument Extraction</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Explainable AI</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Fact Verification</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Fairness</td><td></td><td>2</td><td></td><td>1</td><td>7</td><td></td></tr><tr><th></th><td>Few-shot</td><td></td><td>1</td><td>2</td><td></td><td>2</td><td></td></tr><tr><th></th><td>Few-shot Learning</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Fine-tuning</td><td></td><td>9</td><td>11</td><td>1</td><td>5</td><td></td></tr><tr><th></th><td>Foundation Model</td><td></td><td>1</td><td>5</td><td></td><td>1</td><td></td></tr><tr><th></th><td>GPT</td><td>1</td><td>5</td><td></td><td>1</td><td>1</td><td></td></tr><tr><th></th><td>GPT-2</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>GPT-3</td><td></td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>GPT-3.5</td><td></td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>GPT-4</td><td>1</td><td>3</td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><th></th><td>Generative AI</td><td>1</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Generative Adversarial Network</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Graph Attention Networks</td><td></td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><th></th><td>Graph Convolutional Network</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Graph Neural Network</td><td></td><td>2</td><td>2</td><td></td><td>8</td><td></td></tr><tr><th></th><td>Grounding</td><td>2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Hallucination Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>High-Resource</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Image2text</td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><th></th><td>In-context Learning</td><td></td><td>5</td><td></td><td></td><td>9</td><td></td></tr><tr><th></th><td>Information Retrieval</td><td></td><td>2</td><td></td><td>2</td><td></td><td></td></tr><tr><th></th><td>Instruction Following</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Instruction Tuning</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Intent Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Knowledge Based Question Answering</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Knowledge Distillation</td><td></td><td>1</td><td>6</td><td>3</td><td>4</td><td></td></tr><tr><th></th><td>LLaMA</td><td>1</td><td>5</td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Label Smoothing</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Large Language Model</td><td>12</td><td>53</td><td>2</td><td>4</td><td>19</td><td></td></tr><tr><th></th><td>Low-Resource</td><td></td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>Masked Language Model</td><td></td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Mathematical Reasoning</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Meta Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Model Compression</td><td></td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><th></th><td>Multi-modal</td><td></td><td>6</td><td>8</td><td></td><td>3</td><td>1</td></tr><tr><th></th><td>Mutual Information</td><td></td><td>1</td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Named Entity Recognition</td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Natural Language Inference</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Natural Language Understanding</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Object Detection</td><td></td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><th></th><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Out-of-distribution</td><td></td><td></td><td>1</td><td></td><td>5</td><td></td></tr><tr><th></th><td>Out-of-domain</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>PaLM</td><td>1</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Perplexity</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Pre-trained Language Model</td><td></td><td>2</td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Prompt</td><td>2</td><td>11</td><td>5</td><td>1</td><td>5</td><td></td></tr><tr><th></th><td>Pruning</td><td></td><td></td><td></td><td></td><td>5</td><td></td></tr><tr><th></th><td>Quantization</td><td></td><td>2</td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Question Answering</td><td></td><td>3</td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Reasoning</td><td>4</td><td>4</td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Recommendation</td><td></td><td>1</td><td>1</td><td>8</td><td>2</td><td></td></tr><tr><th></th><td>Recommender System</td><td></td><td></td><td></td><td>6</td><td></td><td></td></tr><tr><th></th><td>Reconstruction Loss</td><td></td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><th></th><td>Recurrent Neural Network</td><td></td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><th></th><td>Reinforcement Learning</td><td>1</td><td>1</td><td></td><td></td><td>23</td><td></td></tr><tr><th></th><td>Rerank</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>Rouge</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Sample Size</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Self-supervised Learning</td><td></td><td></td><td>5</td><td></td><td>2</td><td></td></tr><tr><th></th><td>Semantic Parsing</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Sentence Embedding</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Sentiment Analysis</td><td></td><td>3</td><td></td><td>1</td><td></td><td></td></tr><tr><th></th><td>Simulation</td><td></td><td>1</td><td>2</td><td></td><td>5</td><td>1</td></tr><tr><th></th><td>Simulator</td><td></td><td>1</td><td>2</td><td></td><td>5</td><td>1</td></tr><tr><th></th><td>Stance Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Stemming</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><th></th><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Style Transfer</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><th></th><td>Summarization</td><td></td><td>3</td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Supervised Learning</td><td></td><td>4</td><td>6</td><td>1</td><td>5</td><td></td></tr><tr><th></th><td>Text Classification</td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Text Embedding</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Text Generation</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><th></th><td>Text Summarization</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Text2image</td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><th></th><td>Tokenization</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><th></th><td>Topic Model</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Topic Modeling</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Topic Segmentation</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Transfer Learning</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><th></th><td>Transformer</td><td>2</td><td>8</td><td>5</td><td>1</td><td>9</td><td>1</td></tr><tr><th></th><td>Unsupervised Learning</td><td></td><td></td><td>2</td><td></td><td>6</td><td>2</td></tr><tr><th></th><td>Variational Autoencoder</td><td></td><td></td><td></td><td></td><td>2</td><td>1</td></tr><tr><th></th><td>Vision-and-Language</td><td></td><td></td><td>4</td><td></td><td>2</td><td></td></tr><tr><th></th><td>Visual Question Answering</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><th></th><td>Weakly Supervised Learning</td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><th></th><td>Weakly-supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td><td>2</td></tr><tr><th></th><td>Zero-shot</td><td>1</td><td>5</td><td>1</td><td></td><td>3</td><td></td></tr><tr><th></th><td>falcon</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-48>cs.CL (48)</h2><h3 id=1285-illuminate-a-novel-approach-for-depression-detection-with-explainable-analysis-and-proactive-therapy-using-prompt-engineering-aryan-agrawal-2024>(1/285) Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering (Aryan Agrawal, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aryan Agrawal. (2024)<br><strong>Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering</strong><br><button class=copy-to-clipboard title="Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 110<br>Keywords: Few-shot, Fine-tuning, Recommendation, GPT, GPT-4, LLaMA, Transformer, Large Language Model, Large Language Model, Prompt, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05127v1.pdf filename=2402.05127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel paradigm for depression detection and treatment using advanced <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs):</b> Generative Pre-trained <b>Transformer</b> 4 <b>(GPT-4),</b> <b>Llama</b> 2 chat, and Gemini. These <b>LLMs</b> are <b>fine-tuned</b> with specialized <b>prompts</b> to diagnose, explain, and suggest therapeutic interventions for depression. A unique <b>few-shot</b> <b>prompting</b> method enhances the models&rsquo; ability to analyze and explain depressive symptoms based on the DSM-5 criteria. In the interaction phase, the models engage in empathetic dialogue management, drawing from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide, fostering supportive interactions with individuals experiencing major depressive disorders. Additionally, the research introduces the Illuminate Database, enriched with various CBT modules, aiding in personalized therapy <b>recommendations.</b> The study evaluates <b>LLM</b> performance using metrics such as F1 scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy for Gisting Evaluation <b>(ROUGE)</b> across different test sets, demonstrating their effectiveness. This comprehensive approach blends cutting-edge AI with established psychological methods, offering new possibilities in mental health care and showcasing the potential of <b>LLMs</b> in revolutionizing depression diagnosis and treatment strategies.</p></p class="citation"></blockquote><h3 id=2285-enhancing-textbook-question-answering-task-with-large-language-models-and-retrieval-augmented-generation-hessa-abdulrahman-alawwad-et-al-2024>(2/285) Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation (Hessa Abdulrahman Alawwad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal. (2024)<br><strong>Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation</strong><br><button class=copy-to-clipboard title="Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 96<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Out-of-domain, Supervised Learning, Transfer Learning, LLaMA, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05128v1.pdf filename=2402.05128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Textbook <b>question</b> <b>answering</b> (TQA) is a challenging task in artificial intelligence due to the complex nature of context and <b>multimodal</b> data. Although previous research has significantly improved the task, there are still some limitations including the models&rsquo; weak <b>reasoning</b> and inability to capture contextual information in the lengthy context. The introduction of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has revolutionized the field of AI, however, directly applying <b>LLMs</b> often leads to inaccurate answers. This paper proposes a methodology that handle the <b>out-of-domain</b> scenario in TQA where concepts are spread across different lessons by incorporating the retrieval augmented generation (RAG) technique and utilize <b>transfer</b> <b>learning</b> to handle the long context and enhance <b>reasoning</b> abilities. Through <b>supervised</b> <b>fine-tuning</b> of the <b>LLM</b> model <b>Llama-2</b> and the incorporation of RAG, our architecture outperforms the baseline, achieving a 4.12% accuracy improvement on validation set and 9.84% on test set for non-diagram multiple-choice questions.</p></p class="citation"></blockquote><h3 id=3285-lb-kbqa-large-language-model-and-bert-based-knowledge-based-question-and-answering-system-yan-zhao-et-al-2024>(3/285) LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System (Yan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Zhao, Zhongyun Li, Yushan Pan, Jiaxing Wang, Yihong Wang. (2024)<br><strong>LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System</strong><br><button class=copy-to-clipboard title="LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Generative AI, BERT, Intent Detection, Knowledge Based Question Answering, Natural Language Understanding, Question Answering, Semantic Parsing, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05130v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05130v2.pdf filename=2402.05130v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Artificial</b> Intelligence (AI), because of its emergent abilities, has empowered various fields, one typical of which is <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> One of the typical application fields of <b>Generative</b> <b>AI</b> is <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> and the <b>natural</b> <b>language</b> <b>understanding</b> capability of <b>LLM</b> is dramatically improved when compared with conventional AI-based methods. The <b>natural</b> <b>language</b> <b>understanding</b> capability has always been a barrier to the <b>intent</b> <b>recognition</b> performance of the Knowledge-Based-Question-and-Answer <b>(KBQA)</b> system, which arises from linguistic diversity and the newly appeared <b>intent.</b> <b>Conventional</b> AI-based methods for <b>intent</b> <b>recognition</b> can be divided into <b>semantic</b> <b>parsing-based</b> and model-based approaches. However, both of the methods suffer from limited resources in <b>intent</b> <b>recognition.</b> To address this issue, we propose a novel <b>KBQA</b> system based on a <b>Large</b> <b>Language</b> <b>Model(LLM)</b> and <b>BERT</b> (LB-KBQA). With the help of <b>generative</b> <b>AI,</b> our proposed method could detect newly appeared <b>intent</b> <b>and</b> acquire new knowledge. In experiments on financial domain <b>question</b> <b>answering,</b> our model has demonstrated superior effectiveness.</p></p class="citation"></blockquote><h3 id=4285-constrained-decoding-for-cross-lingual-label-projection-duong-minh-le-et-al-2024>(4/285) Constrained Decoding for Cross-lingual Label Projection (Duong Minh Le et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duong Minh Le, Yang Chen, Alan Ritter, Wei Xu. (2024)<br><strong>Constrained Decoding for Cross-lingual Label Projection</strong><br><button class=copy-to-clipboard title="Constrained Decoding for Cross-lingual Label Projection" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, High-Resource, Low-Resource, Supervised Learning, Transfer Learning, Zero-shot, Event Argument Extraction, Named Entity Recognition, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03131v1.pdf filename=2402.03131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> cross-lingual <b>transfer</b> <b>utilizing</b> multilingual <b>LLMs</b> has become a popular learning paradigm for <b>low-resource</b> languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of <b>zero-shot</b> cross-lingual <b>transfer</b> <b>learning</b> lags far behind <b>supervised</b> <b>fine-tuning</b> methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a <b>high-resource</b> language (e.g., English) together with the gold labels into <b>low-resource</b> languages, and/or (2) translating test data in <b>low-resource</b> languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we explore a new direction that leverages constrained decoding for label projection to overcome the aforementioned issues. Our new method not only can preserve the quality of translated texts but also has the versatility of being applicable to both translating training and translating test data strategies. This versatility is crucial as our experiments reveal that translating test data can lead to a considerable boost in performance compared to translating only training data. We evaluate on two cross-lingual <b>transfer</b> <b>tasks,</b> namely <b>Named</b> <b>Entity</b> <b>Recognition</b> and <b>Event</b> <b>Argument</b> <b>Extraction,</b> spanning 20 languages. The results demonstrate that our approach outperforms the state-of-the-art marker-based method by a large margin and also shows better performance than other label projection methods that rely on external word alignment.</p></p class="citation"></blockquote><h3 id=5285-arabic-synonym-bert-based-adversarial-examples-for-text-classification-norah-alshahrani-et-al-2024>(5/285) Arabic Synonym BERT-based Adversarial Examples for Text Classification (Norah Alshahrani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Norah Alshahrani, Saied Alshahrani, Esma Wali, Jeanna Matthews. (2024)<br><strong>Arabic Synonym BERT-based Adversarial Examples for Text Classification</strong><br><button class=copy-to-clipboard title="Arabic Synonym BERT-based Adversarial Examples for Text Classification" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Adversarial Learning, Fine-tuning, BERT, Text Classification, Masked Language Model, Masked Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03477v1.pdf filename=2402.03477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text</b> <b>classification</b> systems have been proven vulnerable to <b>adversarial</b> <b>text</b> <b>examples,</b> modified versions of the original <b>text</b> <b>examples</b> that are often unnoticed by human eyes, yet can force <b>text</b> <b>classification</b> models to alter their classification. Often, research works quantifying the impact of <b>adversarial</b> <b>text</b> <b>attacks</b> have been applied only to models trained in English. In this paper, we introduce the first word-level study of <b>adversarial</b> <b>attacks</b> in Arabic. Specifically, we use a synonym (word-level) attack using a <b>Masked</b> <b>Language</b> <b>Modeling</b> <b>(MLM)</b> task with a <b>BERT</b> model in a black-box setting to assess the robustness of the state-of-the-art <b>text</b> <b>classification</b> models to <b>adversarial</b> <b>attacks</b> in Arabic. To evaluate the grammatical and semantic similarities of the newly produced <b>adversarial</b> <b>examples</b> using our synonym <b>BERT-based</b> attack, we invite four human evaluators to assess and compare the produced <b>adversarial</b> <b>examples</b> with their original examples. We also study the transferability of these newly produced Arabic <b>adversarial</b> <b>examples</b> to various models and investigate the effectiveness of defense mechanisms against these <b>adversarial</b> <b>examples</b> on the <b>BERT</b> models. We find that <b>fine-tuned</b> <b>BERT</b> models were more susceptible to our synonym attacks than the other Deep Neural Networks (DNN) models like WordCNN and WordLSTM we trained. We also find that <b>fine-tuned</b> <b>BERT</b> models were more susceptible to transferred attacks. We, lastly, find that <b>fine-tuned</b> <b>BERT</b> models successfully regain at least 2% in accuracy after applying <b>adversarial</b> <b>training</b> as an initial defense mechanism.</p></p class="citation"></blockquote><h3 id=6285-multi-multimodal-understanding-leaderboard-with-text-and-images-zichen-zhu-et-al-2024>(6/285) Multi: Multimodal Understanding Leaderboard with Text and Images (Zichen Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zichen Zhu, Yang Xu, Lu Chen, Jingkai Yang, Yichuan Ma, Yiming Sun, Hailin Wen, Jiaqi Liu, Jinyu Cai, Yingzi Ma, Situo Zhang, Zihan Zhao, Liangtai Sun, Kai Yu. (2024)<br><strong>Multi: Multimodal Understanding Leaderboard with Text and Images</strong><br><button class=copy-to-clipboard title="Multi: Multimodal Understanding Leaderboard with Text and Images" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 66<br>Keywords: Multi-modal, Multi-modal, GPT, Question Answering, Reasoning, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03173v1.pdf filename=2402.03173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rapid progress in <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides <b>multimodal</b> inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality <b>reasoning.</b> Multi includes over 18,000 questions, with a focus on science-based <b>QA</b> in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances <b>In-Context</b> <b>Learning</b> research with more than 4,500 knowledge pieces. Our evaluation indicates significant potential for MLLM advancement, with <b>GPT-4V</b> achieving a 63.7% accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and 53.7%. Multi serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI.</p></p class="citation"></blockquote><h3 id=7285-swag-storytelling-with-action-guidance-zeeshan-patel-et-al-2024>(7/285) SWAG: Storytelling With Action Guidance (Zeeshan Patel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeeshan Patel, Karim El-Refai, Jonathan Pei, Tianle Li. (2024)<br><strong>SWAG: Storytelling With Action Guidance</strong><br><button class=copy-to-clipboard title="SWAG: Storytelling With Action Guidance" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03483v1.pdf filename=2402.03483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated long-form story generation typically employs long-context <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for one-shot creation, which can produce cohesive but not necessarily engaging content. We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with <b>LLMs.</b> Our approach reduces story writing to a search problem through a two-model feedback loop: one <b>LLM</b> generates story content, and another auxiliary <b>LLM</b> is used to choose the next best &ldquo;action&rdquo; to steer the story&rsquo;s future direction. Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by <b>GPT-4</b> and through human evaluation, and our SWAG pipeline using only open-source models surpasses <b>GPT-3.5-Turbo.</b></p></p class="citation"></blockquote><h3 id=8285-llm-agents-in-interaction-measuring-personality-consistency-and-linguistic-alignment-in-interacting-populations-of-large-language-models-ivar-frisch-et-al-2024>(8/285) LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models (Ivar Frisch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ivar Frisch, Mario Giulianelli. (2024)<br><strong>LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models</strong><br><button class=copy-to-clipboard title="LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-MA, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02896v1.pdf filename=2402.02896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While both agent interaction and personalisation are vibrant topics in research on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned <b>LLM</b> agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition <b>GPT-3.5</b> on personality profiles through <b>prompting</b> and create a two-group population of <b>LLM</b> agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between <b>LLMs</b> and highlights the need for new approaches to crafting robust, more human-like <b>LLM</b> personas for interactive environments.</p></p class="citation"></blockquote><h3 id=9285-kivi-a-tuning-free-asymmetric-2bit-quantization-for-kv-cache-zirui-liu-et-al-2024>(9/285) KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache (Zirui Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, Xia Hu. (2024)<br><strong>KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache</strong><br><button class=copy-to-clipboard title="KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs-PF, cs.CL<br>Keyword Score: 60<br>Keywords: Quantization, Quantization, LLaMA, falcon, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02750v1.pdf filename=2402.02750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficiently serving <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU&rsquo;s SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is <b>quantization,</b> which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache <b>quantization.</b> To fill the gap, we conducted a comprehensive study on the element distribution in KV cache of popular <b>LLMs.</b> Our findings indicate that the key cache should be <b>quantized</b> per-channel, i.e., group elements along the channel dimension and <b>quantize</b> them together. In contrast, the value cache should be <b>quantized</b> per-token. From this analysis, we developed a tuning-free 2bit KV cache <b>quantization</b> algorithm, named KIVI. With the hardware-friendly implementation, KIVI can enable <b>Llama</b> <b>(Llama-2),</b> <b>Falcon,</b> and Mistral models to maintain almost the same quality while using $\mathbf{2.6\times}$ less peak memory usage (including the model weight). This reduction in memory usage enables up to $\mathbf{4\times}$ larger batch size, bringing $\mathbf{2.35\times \sim 3.47\times}$ throughput on real <b>LLM</b> inference workload. The source code is available at <a href=https://github.com/jy-yuan/KIVI>https://github.com/jy-yuan/KIVI</a>.</p></p class="citation"></blockquote><h3 id=10285-graph-neural-network-and-ner-based-text-summarization-imaad-zaffar-khan-et-al-2024>(10/285) Graph Neural Network and NER-Based Text Summarization (Imaad Zaffar Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Imaad Zaffar Khan, Amaan Aijaz Sheikh, Utkarsh Sinha. (2024)<br><strong>Graph Neural Network and NER-Based Text Summarization</strong><br><button class=copy-to-clipboard title="Graph Neural Network and NER-Based Text Summarization" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Graph Neural Network, Graph Neural Network, Named Entity Recognition, Named Entity Recognition, Text Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05126v1.pdf filename=2402.05126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the abundance of data and information in todays time, it is nearly impossible for man, or, even machine, to go through all of the data line by line. What one usually does is to try to skim through the lines and retain the absolutely important information, that in a more formal term is called <b>summarization.</b> <b>Text</b> <b>summarization</b> is an important task that aims to compress lengthy documents or articles into shorter, coherent representations while preserving the core information and meaning. This project introduces an innovative approach to <b>text</b> <b>summarization,</b> leveraging the capabilities of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> and <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> systems. <b>GNNs,</b> with their exceptional ability to capture and process the relational data inherent in textual information, are adept at understanding the complex structures within large documents. Meanwhile, <b>NER</b> systems contribute by identifying and emphasizing key entities, ensuring that the <b>summarization</b> process maintains a focus on the most critical aspects of the <b>text.</b> <b>By</b> integrating these two technologies, our method aims to enhances the efficiency of <b>summarization</b> and also tries to ensures a high degree relevance in the condensed content. This project, therefore, offers a promising direction for handling the ever increasing volume of textual data in an information-saturated world.</p></p class="citation"></blockquote><h3 id=11285-resolving-transcription-ambiguity-in-spanish-a-hybrid-acoustic-lexical-system-for-punctuation-restoration-xiliang-zhu-et-al-2024>(11/285) Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration (Xiliang Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiliang Zhu, Chia-Tien Chang, Shayna Gardiner, David Rossouw, Jonas Robertson. (2024)<br><strong>Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration</strong><br><button class=copy-to-clipboard title="Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03519v1.pdf filename=2402.03519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Punctuation restoration is a crucial step after <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR)</b> systems to enhance transcript readability and facilitate subsequent NLP tasks. Nevertheless, conventional lexical-based approaches are inadequate for solving the punctuation restoration task in Spanish, where ambiguity can be often found between unpunctuated declaratives and questions. In this study, we propose a novel hybrid acoustic-lexical punctuation restoration system for Spanish transcription, which consolidates acoustic and lexical signals through a modular process. Our experiment results show that the proposed system can effectively improve F1 score of question marks and overall punctuation restoration on both public and internal Spanish conversational datasets. Additionally, benchmark comparison against <b>LLMs</b> <b>(Large</b> <b>Language</b> <b>Model)</b> indicates the superiority of our approach in accuracy, reliability and latency. Furthermore, we demonstrate that the Word Error Rate (WER) of the <b>ASR</b> module also benefits from our proposed system.</p></p class="citation"></blockquote><h3 id=12285-nevermind-instruction-override-and-moderation-in-large-language-models-edward-kim-2024>(12/285) Nevermind: Instruction Override and Moderation in Large Language Models (Edward Kim, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edward Kim. (2024)<br><strong>Nevermind: Instruction Override and Moderation in Large Language Models</strong><br><button class=copy-to-clipboard title="Nevermind: Instruction Override and Moderation in Large Language Models" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Instruction Following, Large Language Model, Large Language Model, Perplexity, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03303v1.pdf filename=2402.03303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the impressive capabilities of recent <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit <b>instruction</b> <b>following</b> in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the <b>prompt,</b> and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve <b>instruction</b> <b>following</b> - larger models perform the best in following <b>instructions</b> <b>that</b> override internal and contextual <b>instructions,</b> <b>and</b> are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the <b>perplexity</b> cliff in order to maintain <b>instruction</b> <b>following</b> capabilities. Finally, we observe improving <b>instruction</b> <b>following,</b> and subsequently <b>instruction</b> <b>overrides/jailbreaks,</b> is fundamentally at odds with the ability of a language model to follow given safety filters or guidelines. Thus, we postulate the most effective approach for safe, trustworthy AI should be dealt external to the <b>LLM</b> itself.</p></p class="citation"></blockquote><h3 id=13285-uncertainty-of-thoughts-uncertainty-aware-planning-enhances-information-seeking-in-large-language-models-zhiyuan-hu-et-al-2024>(13/285) Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models (Zhiyuan Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, Bryan Hooi. (2024)<br><strong>Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models</strong><br><button class=copy-to-clipboard title="Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03271v1.pdf filename=2402.03271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment <b>large</b> <b>language</b> <b>models</b> with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware <b>simulation</b> approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the &lsquo;20 Questions&rsquo; game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple <b>LLMs</b> compared with direct <b>prompting,</b> and also improves efficiency (i.e., the number of questions needed to complete the task).</p></p class="citation"></blockquote><h3 id=14285-english-prompts-are-better-for-nli-based-zero-shot-emotion-classification-than-target-language-prompts-patrick-barreiß-et-al-2024>(14/285) English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts (Patrick Barreiß et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Barreiß, Roman Klinger, Jeremy Barnes. (2024)<br><strong>English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts</strong><br><button class=copy-to-clipboard title="English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Zero-shot, Natural Language Inference, Natural Language Inference, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03223v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03223v1.pdf filename=2402.03223v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emotion classification in text is a challenging and subjective task, due to the involved cognitive inference processes that are required to interpret a textual stimulus. In addition, the set of emotion categories is highly domain-specific. For instance, literature analysis might require the use of aesthetic emotions (e.g., finding something beautiful), and social media analysis could benefit from fine-grained sets (e.g., separating anger from annoyance) in contrast to basic emotion categories. This renders the task an interesting field for <b>zero-shot</b> classifications, in which the label set is not known at model development time. Unfortunately, most resources for emotion analysis are English, and therefore, most studies on emotion analysis have been performed in English, including those that involve <b>prompting</b> language models for text labels. This leaves us with a research gap that we address in this paper: In which language should we <b>prompt</b> for emotion labels on non-English texts? This is particularly of interest when we have access to a multilingual <b>large</b> <b>language</b> <b>model,</b> because we could request labels with English <b>prompts</b> even for non-English data. Our experiments with <b>natural</b> <b>language</b> <b>inference-based</b> language models show that it is consistently better to use English <b>prompts</b> even if the data is in a different language.</p></p class="citation"></blockquote><h3 id=15285-how-do-large-language-models-learn-in-context-query-and-key-matrices-of-in-context-heads-are-two-towers-for-metric-learning-zeping-yu-et-al-2024>(15/285) How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning (Zeping Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeping Yu, Sophia Ananiadou. (2024)<br><strong>How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning</strong><br><button class=copy-to-clipboard title="How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: GPT-2, LLaMA, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02872v1.pdf filename=2402.02872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore the mechanism of <b>in-context</b> <b>learning</b> and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, <b>in-context</b> <b>heads</b> make great contributions. In each <b>in-context</b> <b>head,</b> the value-output matrix extracts the labels&rsquo; features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on <b>GPT2</b> <b>large,</b> <b>Llama</b> <b>7B,</b> 13B and 30B. The results can support our analysis. Overall, our study provides a new method and a reasonable hypothesis for understanding the mechanism of <b>in-context</b> <b>learning.</b> Our code will be released on github.</p></p class="citation"></blockquote><h3 id=16285-large-language-models-are-geographically-biased-rohin-manvi-et-al-2024>(16/285) Large Language Models are Geographically Biased (Rohin Manvi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohin Manvi, Samar Khanna, Marshall Burke, David Lobell, Stefano Ermon. (2024)<br><strong>Large Language Models are Geographically Biased</strong><br><button class=copy-to-clipboard title="Large Language Models are Geographically Biased" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fairness, Foundation Model, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02680v1.pdf filename=2402.02680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these <b>foundation</b> <b>models</b> grows, understanding and evaluating their biases becomes crucial to achieving <b>fairness</b> and accuracy. We propose to study what <b>LLMs</b> know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that <b>LLMs</b> are capable of making accurate <b>zero-shot</b> geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman&rsquo;s $\rho$ of up to 0.89). We then show that <b>LLMs</b> exhibit common biases across a range of objective and subjective topics. In particular, <b>LLMs</b> are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman&rsquo;s $\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=17285-chain-of-feedback-mitigating-the-effects-of-inconsistency-in-responses-jinwoo-ahn-2024>(17/285) Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses (Jinwoo Ahn, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinwoo Ahn. (2024)<br><strong>Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses</strong><br><button class=copy-to-clipboard title="Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: ChatGPT, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02648v1.pdf filename=2402.02648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the <b>LLMs</b> to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like <b>ChatGPT</b> by showing how Chain-of-Feedback (CoF) triggers <b>LLMs</b> to deviate more from the actual answer and 2) suggest a novel <b>prompting</b> method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the other hand, to mitigate the effects of the aforementioned inconsistencies, we present a novel method of recursively revising the initial incorrect <b>reasoning</b> provided by the <b>LLM</b> by repetitively breaking down each incorrect step into smaller individual problems.</p></p class="citation"></blockquote><h3 id=18285-evaluating-the-factuality-of-zero-shot-summarizers-across-varied-domains-sanjana-ramprasad-et-al-2024>(18/285) Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains (Sanjana Ramprasad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanjana Ramprasad, Kundan Krishna, Zachary C Lipton, Byron C Wallace. (2024)<br><strong>Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains</strong><br><button class=copy-to-clipboard title="Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03509v1.pdf filename=2402.03509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has shown that <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are capable of generating summaries <b>zero-shot</b> (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article <b>summarization.</b> How do <b>zero-shot</b> summarizers perform in other (potentially more specialized) domains? In this work we evaluate <b>zero-shot</b> generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotations to facilitate additional research toward measuring and realizing factually accurate <b>summarization,</b> beyond news articles. The dataset can be downloaded from <a href=https://github.com/sanjanaramprasad/zero_shot_faceval_domains>https://github.com/sanjanaramprasad/zero_shot_faceval_domains</a></p></p class="citation"></blockquote><h3 id=19285-psychological-assessments-with-large-language-models-a-privacy-focused-and-cost-effective-approach-sergi-blanco-cuaresma-2024>(19/285) Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach (Sergi Blanco-Cuaresma, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sergi Blanco-Cuaresma. (2024)<br><strong>Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach</strong><br><button class=copy-to-clipboard title="Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs.CL<br>Keyword Score: 40<br>Keywords: Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03435v1.pdf filename=2402.03435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to analyze text comments from Reddit users, aiming to achieve two primary objectives: firstly, to pinpoint critical excerpts that support a predefined psychological assessment of suicidal risk; and secondly, to <b>summarize</b> the material to substantiate the preassigned suicidal risk level. The work is circumscribed to the use of &ldquo;open-source&rdquo; <b>LLMs</b> that can be run locally, thereby enhancing data privacy. Furthermore, it prioritizes models with low computational requirements, making it accessible to both individuals and institutions operating on limited computing budgets. The implemented strategy only relies on a carefully crafted <b>prompt</b> and a grammar to guide the <b>LLM&rsquo;s</b> text completion. Despite its simplicity, the evaluation metrics show outstanding results, making it a valuable privacy-focused and cost-effective approach. This work is part of the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared task.</p></p class="citation"></blockquote><h3 id=20285-deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models-zhihong-shao-et-al-2024>(20/285) DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Zhihong Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, Daya Guo. (2024)<br><strong>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</strong><br><button class=copy-to-clipboard title="DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Mathematical Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03300v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03300v2.pdf filename=2402.03300v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Mathematical</b> <b>reasoning</b> poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and <b>GPT-4.</b> Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The <b>mathematical</b> <b>reasoning</b> capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances <b>mathematical</b> <b>reasoning</b> abilities while concurrently optimizing the memory usage of PPO.</p></p class="citation"></blockquote><h3 id=21285-cidar-culturally-relevant-instruction-dataset-for-arabic-zaid-alyafeai-et-al-2024>(21/285) CIDAR: Culturally Relevant Instruction Dataset For Arabic (Zaid Alyafeai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zaid Alyafeai, Khalid Almubarak, Ahmed Ashraf, Deema Alnuhait, Saied Alshahrani, Gubran A. Q. Abdulrahman, Gamil Ahmed, Qais Gawah, Zead Saleh, Mustafa Ghaleb, Yousef Ali, Maged S. Al-Shaibani. (2024)<br><strong>CIDAR: Culturally Relevant Instruction Dataset For Arabic</strong><br><button class=copy-to-clipboard title="CIDAR: Culturally Relevant Instruction Dataset For Arabic" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03177v1.pdf filename=2402.03177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> has emerged as a prominent methodology for teaching <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to follow <b>instructions.</b> <b>However,</b> current <b>instruction</b> <b>datasets</b> predominantly cater to English or are derived from English-dominated <b>LLMs,</b> resulting in inherent biases toward Western culture. This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region. This paper addresses this limitation by introducing CIDAR: <a href=https://hf.co/datasets/arbml/CIDAR>https://hf.co/datasets/arbml/CIDAR</a>, the first open Arabic <b>instruction-tuning</b> <b>dataset</b> culturally-aligned by human reviewers. CIDAR contains 10,000 <b>instruction</b> <b>and</b> output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to other models <b>fine-tuned</b> on other datasets. Our experiments show that CIDAR can help enrich research efforts in aligning <b>LLMs</b> with the Arabic culture. All the code is available at <a href=https://github.com/ARBML/CIDAR>https://github.com/ARBML/CIDAR</a>.</p></p class="citation"></blockquote><h3 id=22285-multilingual-transformer-and-bertopic-for-short-text-topic-modeling-the-case-of-serbian-darija-medvecki-et-al-2024>(22/285) Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian (Darija Medvecki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Darija Medvecki, Bojana Bašaragin, Adela Ljajić, Nikola Milošević. (2024)<br><strong>Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian</strong><br><button class=copy-to-clipboard title="Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Low-Resource, Topic Model, Transformer, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03067v1.pdf filename=2402.03067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the results of the first application of BERTopic, a state-of-the-art <b>topic</b> <b>modeling</b> technique, to short text written in a morphologi-cally rich language. We applied BERTopic with three multilingual embed-ding models on two levels of text preprocessing (partial and full) to evalu-ate its performance on partially preprocessed short text in Serbian. We also compared it to LDA and NMF on fully preprocessed text. The experiments were conducted on a dataset of tweets expressing hesitancy toward COVID-19 vaccination. Our results show that with adequate parameter setting, BERTopic can yield informative <b>topics</b> <b>even</b> when applied to partially pre-processed short text. When the same parameters are applied in both prepro-cessing scenarios, the performance drop on partially preprocessed text is minimal. Compared to LDA and NMF, judging by the keywords, BERTopic offers more informative <b>topics</b> <b>and</b> gives novel insights when the number of <b>topics</b> <b>is</b> not limited. The findings of this paper can be significant for re-searchers working with other morphologically rich <b>low-resource</b> languages and short text.</p></p class="citation"></blockquote><h3 id=23285-easyinstruct-an-easy-to-use-instruction-processing-framework-for-large-language-models-yixin-ou-et-al-2024>(23/285) EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models (Yixin Ou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen. (2024)<br><strong>EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models</strong><br><button class=copy-to-clipboard title="EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-IR, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03049v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03049v2.pdf filename=2402.03049v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>instruction</b> <b>tuning</b> has gained increasing attention and emerged as a crucial technique to enhance the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> To construct high-quality <b>instruction</b> <b>datasets,</b> many <b>instruction</b> <b>processing</b> approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various <b>instruction</b> <b>processing</b> methods, there is no standard open-source <b>instruction</b> <b>processing</b> implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate <b>instruction</b> <b>processing</b> research and development, we present EasyInstruct, an easy-to-use <b>instruction</b> <b>processing</b> framework for <b>LLMs,</b> which modularizes <b>instruction</b> <b>generation,</b> selection, and <b>prompting,</b> while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at <a href=https://github.com/zjunlp/EasyInstruct>https://github.com/zjunlp/EasyInstruct</a>, along with a running demo App at <a href=https://huggingface.co/spaces/zjunlp/EasyInstruct>https://huggingface.co/spaces/zjunlp/EasyInstruct</a> for quick-start, calling for broader research centered on <b>instruction</b> <b>data.</b></p></p class="citation"></blockquote><h3 id=24285-unimem-towards-a-unified-view-of-long-context-large-language-models-junjie-fang-et-al-2024>(24/285) UniMem: Towards a Unified View of Long-Context Large Language Models (Junjie Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjie Fang, Likai Tang, Hongzhe Bi, Yujia Qin, Si Sun, Zhenyu Li, Haolun Li, Yongjian Li, Xin Cong, Yukun Yan, Xiaodong Shi, Sen Song, Yankai Lin, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>UniMem: Towards a Unified View of Long-Context Large Language Models</strong><br><button class=copy-to-clipboard title="UniMem: Towards a Unified View of Long-Context Large Language Models" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Transformer, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03009v1.pdf filename=2402.03009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-context processing is a critical ability that constrains the applicability of <b>large</b> <b>language</b> <b>models.</b> Although there exist various methods devoted to enhancing the long-context processing ability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments. In this paper, we introduce UniMem, a unified framework that reformulates existing long-context methods from the view of memory augmentation of <b>LLMs.</b> UniMem is characterized by four key dimensions: Memory Management, Memory Writing, Memory Reading, and Memory Injection, providing a systematic theory for understanding various long-context methods. We reformulate 16 existing methods based on UniMem and analyze four representative methods: <b>Transformer-XL,</b> Memorizing <b>Transformer,</b> RMT, and Longformer into equivalent UniMem forms to reveal their design principles and strengths. Based on these analyses, we propose UniMix, an innovative approach that integrates the strengths of these algorithms. Experimental results show that UniMix achieves superior performance in handling long contexts with significantly lower <b>perplexity</b> than baselines.</p></p class="citation"></blockquote><h3 id=25285-putting-context-in-context-the-impact-of-discussion-structure-on-text-classification-nicolò-penzo-et-al-2024>(25/285) Putting Context in Context: the Impact of Discussion Structure on Text Classification (Nicolò Penzo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolò Penzo, Antonio Longa, Bruno Lepri, Sara Tonelli, Marco Guerini. (2024)<br><strong>Putting Context in Context: the Impact of Discussion Structure on Text Classification</strong><br><button class=copy-to-clipboard title="Putting Context in Context: the Impact of Discussion Structure on Text Classification" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Transformer, Stance Detection, Text Classification, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02975v1.pdf filename=2402.02975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>text</b> <b>classification</b> approaches usually focus on the content to be classified. Contextual aspects (both linguistic and extra-linguistic) are usually neglected, even in tasks based on online discussions. Still in many cases the multi-party and multi-turn nature of the context from which these elements are selected can be fruitfully exploited. In this work, we propose a series of experiments on a large dataset for <b>stance</b> <b>detection</b> in English, in which we evaluate the contribution of different types of contextual information, i.e. linguistic, structural and temporal, by feeding them as natural language input into a <b>transformer-based</b> model. We also experiment with different amounts of training data and analyse the topology of local discussion networks in a privacy-compliant way. Results show that structural information can be highly beneficial to <b>text</b> <b>classification</b> but only under certain circumstances (e.g. depending on the amount of training data and on discussion chain complexity). Indeed, we show that contextual information on smaller datasets from other classification tasks does not yield significant improvements. Our framework, based on local discussion networks, allows the integration of structural information, while minimising user profiling, thus preserving their privacy.</p></p class="citation"></blockquote><h3 id=26285-zero-shot-clinical-trial-patient-matching-with-llms-michael-wornow-et-al-2024>(26/285) Zero-Shot Clinical Trial Patient Matching with LLMs (Michael Wornow et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Wornow, Alejandro Lozano, Dev Dash, Jenelle Jindal, Kenneth W. Mahaffey, Nigam H. Shah. (2024)<br><strong>Zero-Shot Clinical Trial Patient Matching with LLMs</strong><br><button class=copy-to-clipboard title="Zero-Shot Clinical Trial Patient Matching with LLMs" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05125v1.pdf filename=2402.05125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial&rsquo;s eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> offer a promising solution. In this work, we explore their application to trial matching. First, we design an <b>LLM-based</b> system which, given a patient&rsquo;s medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our <b>zero-shot</b> system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a <b>prompting</b> strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of tokens processed by up to a third while retaining high performance. Third, we evaluate the interpretability of our system by having clinicians evaluate the natural language justifications generated by the <b>LLM</b> for each eligibility decision, and show that it can output coherent explanations for 97% of its correct decisions and 75% of its incorrect ones. Our results establish the feasibility of using <b>LLMs</b> to accelerate clinical trial operations.</p></p class="citation"></blockquote><h3 id=27285-texshape-information-theoretic-sentence-embedding-for-language-models-h-kaan-kale-et-al-2024>(27/285) TexShape: Information Theoretic Sentence Embedding for Language Models (H. Kaan Kale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>H. Kaan Kale, Homa Esfahanizadeh, Noel Elias, Oguzhan Baser, Muriel Medard, Sriram Vishwanath. (2024)<br><strong>TexShape: Information Theoretic Sentence Embedding for Language Models</strong><br><button class=copy-to-clipboard title="TexShape: Information Theoretic Sentence Embedding for Language Models" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IT, cs.CL, math-IT<br>Keyword Score: 30<br>Keywords: Fairness, Mutual Information, Sentence Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05132v1.pdf filename=2402.05132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the exponential growth in data volume and the emergence of data-intensive applications, particularly in the field of machine learning, concerns related to resource utilization, privacy, and <b>fairness</b> have become paramount. This paper focuses on the textual domain of data and addresses challenges regarding encoding <b>sentences</b> <b>to</b> their optimized representations through the lens of information-theory. In particular, we use empirical estimates of <b>mutual</b> <b>information,</b> using the Donsker-Varadhan definition of Kullback-Leibler divergence. Our approach leverages this estimation to train an information-theoretic <b>sentence</b> <b>embedding,</b> called TexShape, for (task-based) data compression or for filtering out sensitive information, enhancing privacy and <b>fairness.</b> In this study, we employ a benchmark language model for initial text representation, complemented by neural networks for information-theoretic compression and <b>mutual</b> <b>information</b> estimations. Our experiments demonstrate significant advancements in preserving maximal targeted information and minimal sensitive information over adverse compression ratios, in terms of predictive accuracy of downstream models that are trained using the compressed data.</p></p class="citation"></blockquote><h3 id=28285-deal-or-no-deal-or-who-knows-forecasting-uncertainty-in-conversations-using-large-language-models-anthony-sicilia-et-al-2024>(28/285) Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models (Anthony Sicilia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anthony Sicilia, Hyunwoo Kim, Khyathi Raghavi Chandu, Malihe Alikhani, Jack Hessel. (2024)<br><strong>Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models</strong><br><button class=copy-to-clipboard title="Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Reinforcement Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03284v1.pdf filename=2402.03284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing &ldquo;conversation forecasting&rdquo; task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose <b>fine-tuning</b> strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed <b>fine-tuning</b> strategies (a traditional supervision strategy and an off-policy <b>reinforcement</b> <b>learning</b> strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their size.</p></p class="citation"></blockquote><h3 id=29285-jobskape-a-framework-for-generating-synthetic-job-postings-to-enhance-skill-matching-antoine-magron-et-al-2024>(29/285) JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching (Antoine Magron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antoine Magron, Anna Dai, Mike Zhang, Syrielle Montariol, Antoine Bosselut. (2024)<br><strong>JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching</strong><br><button class=copy-to-clipboard title="JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03242v1.pdf filename=2402.03242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent approaches in skill matching, employing synthetic training data for classification or similarity model training, have shown promising results, reducing the need for time-consuming and expensive annotations. However, previous synthetic datasets have limitations, such as featuring only one skill per sentence and generally comprising short sentences. In this paper, we introduce JobSkape, a framework to generate synthetic data that tackles these limitations, specifically designed to enhance skill-to-taxonomy matching. Within this framework, we create SkillSkape, a comprehensive open-source synthetic dataset of job postings tailored for skill-matching tasks. We introduce several offline metrics that show that our dataset resembles real-world data. Additionally, we present a multi-step pipeline for skill extraction and matching tasks using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> benchmarking against known <b>supervised</b> methodologies. We outline that the downstream evaluation results on real-world data can beat baselines, underscoring its efficacy and adaptability.</p></p class="citation"></blockquote><h3 id=30285-bge-m3-embedding-multi-lingual-multi-functionality-multi-granularity-text-embeddings-through-self-knowledge-distillation-jianlv-chen-et-al-2024>(30/285) BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation (Jianlv Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu. (2024)<br><strong>BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation</strong><br><button class=copy-to-clipboard title="BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Dense Retrieval, Knowledge Distillation, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03216v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03216v2.pdf filename=2402.03216v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: <b>dense</b> <b>retrieval,</b> multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge <b>distillation</b> approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, enabling a large batch size and high training throughput to ensure the discriminativeness of embeddings. To the best of our knowledge, M3-Embedding is the first embedding model which realizes such a strong versatility. The model and code will be publicly available at <a href=https://github.com/FlagOpen/FlagEmbedding>https://github.com/FlagOpen/FlagEmbedding</a>.</p></p class="citation"></blockquote><h3 id=31285-homograph-attacks-on-maghreb-sentiment-analyzers-fatima-zahra-qachfar-et-al-2024>(31/285) Homograph Attacks on Maghreb Sentiment Analyzers (Fatima Zahra Qachfar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatima Zahra Qachfar, Rakesh M. Verma. (2024)<br><strong>Homograph Attacks on Maghreb Sentiment Analyzers</strong><br><button class=copy-to-clipboard title="Homograph Attacks on Maghreb Sentiment Analyzers" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Transformer, Sentiment Analysis, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03171v1.pdf filename=2402.03171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We examine the impact of homograph attacks on the <b>Sentiment</b> <b>Analysis</b> (SA) task of different Arabic dialects from the Maghreb North-African countries. Homograph attacks result in a 65.3% decrease in <b>transformer</b> classification from an F1-score of 0.95 to 0.33 when data is written in &ldquo;Arabizi&rdquo;. The goal of this study is to highlight <b>LLMs</b> weaknesses&rsquo; and to prioritize ethical and responsible Machine Learning.</p></p class="citation"></blockquote><h3 id=32285-linguistic-features-for-sentence-difficulty-prediction-in-absa-adrian-gabriel-chifu-et-al-2024>(32/285) Linguistic features for sentence difficulty prediction in ABSA (Adrian-Gabriel Chifu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian-Gabriel Chifu, Sébastien Fournier. (2024)<br><strong>Linguistic features for sentence difficulty prediction in ABSA</strong><br><button class=copy-to-clipboard title="Linguistic features for sentence difficulty prediction in ABSA" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 30<br>Keywords: Aspect-based Sentiment Analysis, Natural Language Understanding, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03163v1.pdf filename=2402.03163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the challenges of <b>natural</b> <b>language</b> <b>understanding</b> is to deal with the subjectivity of sentences, which may express opinions and emotions that add layers of complexity and nuance. <b>Sentiment</b> <b>analysis</b> is a field that aims to extract and analyze these subjective elements from text, and it can be applied at different levels of granularity, such as document, paragraph, sentence, or aspect. <b>Aspect-based</b> <b>sentiment</b> <b>analysis</b> is a well-studied topic with many available data sets and models. However, there is no clear definition of what makes a sentence difficult for <b>aspect-based</b> <b>sentiment</b> <b>analysis.</b> In this paper, we explore this question by conducting an experiment with three data sets: &ldquo;Laptops&rdquo;, &ldquo;Restaurants&rdquo;, and &ldquo;MTSC&rdquo; (Multi-Target-dependent <b>Sentiment</b> <b>Classification),</b> and a merged version of these three datasets. We study the impact of domain diversity and syntactic diversity on difficulty. We use a combination of classifiers to identify the most difficult sentences and analyze their characteristics. We employ two ways of defining sentence difficulty. The first one is binary and labels a sentence as difficult if the classifiers fail to correctly predict the <b>sentiment</b> <b>polarity.</b> The second one is a six-level scale based on how many of the top five best-performing classifiers can correctly predict the <b>sentiment</b> <b>polarity.</b> We also define 9 linguistic features that, combined, aim at estimating the difficulty at sentence level.</p></p class="citation"></blockquote><h3 id=33285-best-practices-for-text-annotation-with-large-language-models-petter-törnberg-2024>(33/285) Best Practices for Text Annotation with Large Language Models (Petter Törnberg, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Petter Törnberg. (2024)<br><strong>Best Practices for Text Annotation with Large Language Models</strong><br><button class=copy-to-clipboard title="Best Practices for Text Annotation with Large Language Models" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05129v1.pdf filename=2402.05129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have ushered in a new era of text annotation, as their ease-of-use, high accuracy, and relatively low costs have meant that their use has exploded in recent months. However, the rapid growth of the field has meant that <b>LLM-based</b> annotation has become something of an academic Wild West: the lack of established practices and standards has led to concerns about the quality and validity of research. Researchers have warned that the ostensible simplicity of <b>LLMs</b> can be misleading, as they are prone to bias, misunderstandings, and unreliable results. Recognizing the transformative potential of <b>LLMs,</b> this paper proposes a comprehensive set of standards and best practices for their reliable, reproducible, and ethical use. These guidelines span critical areas such as model selection, <b>prompt</b> engineering, structured <b>prompting,</b> <b>prompt</b> stability analysis, rigorous model validation, and the consideration of ethical and legal implications. The paper emphasizes the need for a structured, directed, and formalized approach to using <b>LLMs,</b> aiming to ensure the integrity and robustness of text annotation practices, and advocates for a nuanced and critical engagement with <b>LLMs</b> in social scientific research.</p></p class="citation"></blockquote><h3 id=34285-intent-based-prompt-calibration-enhancing-prompt-optimization-with-synthetic-boundary-cases-elad-levi-et-al-2024>(34/285) Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases (Elad Levi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elad Levi, Eli Brosh, Matan Friedmann. (2024)<br><strong>Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases</strong><br><button class=copy-to-clipboard title="Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03099v1.pdf filename=2402.03099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt</b> engineering is a challenging and important task due to the high sensitivity of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to the given <b>prompt</b> and the inherent ambiguity of a textual task instruction. Automatic <b>prompt</b> engineering is essential to achieve optimized performance from <b>LLMs.</b> Recent studies have demonstrated the capabilities of <b>LLMs</b> to automatically conduct <b>prompt</b> engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved <b>prompt.</b> However, this requires a high-quality benchmark to compare different <b>prompts,</b> which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic <b>prompt</b> engineering, using a calibration process that iteratively refines the <b>prompt</b> to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the <b>prompt</b> according to the generated dataset. We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation. Our method outperforms state-of-the-art methods with a limited number of annotated samples. Furthermore, we validate the advantages of each one of the system&rsquo;s key components. Our system is built in a modular way, facilitating easy adaptation to other tasks. The code is available $\href{https://github.com/Eladlev/AutoPrompt}{here}$.</p></p class="citation"></blockquote><h3 id=35285-multi-lingual-malaysian-embedding-leveraging-large-language-models-for-semantic-representations-husein-zolkepli-et-al-2024>(35/285) Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations (Husein Zolkepli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan. (2024)<br><strong>Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations</strong><br><button class=copy-to-clipboard title="Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03053v1.pdf filename=2402.03053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present a comprehensive exploration of <b>finetuning</b> Malaysian language models, specifically Llama2 and Mistral, on embedding tasks involving negative and positive pairs. We release two distinct models tailored for Semantic Similarity and Retrieval-Augmented Generation (RAG). For Semantic Similarity, our 600 million parameter Llama2 model outperforms OpenAI <b>text-embedding-ada-002</b> <b>across</b> all recall@k metrics for b.cari.com.my, c.cari.com.my, Malay news, and Malaysian Twitter test sets. In the realm of RAG models, our approach proves competitive with OpenAI <b>text-embedding-ada-002</b> <b>in</b> the Malaysian context. Notably, our 2 billion parameter Llama2 model achieves superior Recall@5, Recall@10 for the &ldquo;Melayu&rdquo; keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10 for the lom.agc.gov.my dataset. These findings underscore the effectiveness of our <b>finetuning</b> strategy and highlight the performance gains in both Semantic Similarity and RAG tasks. All models released at <a href=https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99>https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99</a></p></p class="citation"></blockquote><h3 id=36285-ks-lottery-finding-certified-lottery-tickets-for-multilingual-language-models-fei-yuan-et-al-2024>(36/285) KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models (Fei Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Yuan, Chang Ma, Shuai Yuan, Qiushi Sun, Lei Li. (2024)<br><strong>KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models</strong><br><button class=copy-to-clipboard title="KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, LLaMA, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02801v1.pdf filename=2402.02801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The lottery ticket hypothesis posits the existence of ``winning tickets&rsquo;&rsquo; within a randomly initialized neural network. Do winning tickets exist for <b>LLMs</b> in <b>fine-tuning</b> scenarios? How can we find such winning tickets? In this paper, we propose KS-Lottery, a method to identify a small subset of <b>LLM</b> parameters highly effective in multilingual <b>fine-tuning.</b> Our key idea is to use Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before and after <b>fine-tuning.</b> We further theoretically prove that KS-Lottery can find the certified winning tickets in the embedding layer, <b>fine-tuning</b> on the found parameters is guaranteed to perform as well as full <b>fine-tuning.</b> Comparing KS-Lottery with other parameter-efficient tuning algorithms on translation tasks, the experimental results show that KS-Lottery finds a much smaller set of parameters for <b>fine-tuning</b> while achieving the comparable performance as full <b>fine-tuning</b> <b>LLM.</b> Surprisingly, we find that <b>fine-tuning</b> 18 tokens&rsquo; embedding of <b>LLaMA</b> suffices to reach the <b>fine-tuning</b> translation performance. Code and model will be released to the public.</p></p class="citation"></blockquote><h3 id=37285-unified-hallucination-detection-for-multimodal-large-language-models-xiang-chen-et-al-2024>(37/285) Unified Hallucination Detection for Multimodal Large Language Models (Xiang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Jinjie Gu, Huajun Chen. (2024)<br><strong>Unified Hallucination Detection for Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="Unified Hallucination Detection for Multimodal Large Language Models" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs-MM, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Hallucination Detection, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03190v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03190v1.pdf filename=2402.03190v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite significant strides in <b>multimodal</b> tasks, <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) are plagued by the critical issue of <b>hallucination.</b> <b>The</b> reliable detection of such <b>hallucinations</b> <b>in</b> MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of <b>hallucination</b> <b>categories</b> addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of <b>hallucination</b> <b>detection.</b> We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in <b>hallucination</b> <b>detection</b> methods. Additionally, we unveil a novel unified <b>multimodal</b> <b>hallucination</b> <b>detection</b> framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of <b>hallucinations</b> <b>robustly.</b> We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.</p></p class="citation"></blockquote><h3 id=38285-sociolinguistically-informed-interpretability-a-case-study-on-hinglish-emotion-classification-kushal-tatariya-et-al-2024>(38/285) Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification (Kushal Tatariya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kushal Tatariya, Heather Lent, Johannes Bjerva, Miryam de Lhoneux. (2024)<br><strong>Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification</strong><br><button class=copy-to-clipboard title="Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03137v1.pdf filename=2402.03137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emotion classification is a challenging task in NLP due to the inherent idiosyncratic and subjective nature of linguistic expression, especially with code-mixed data. <b>Pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> have achieved high performance for many tasks and languages, but it remains to be seen whether these models learn and are robust to the differences in emotional expression across languages. Sociolinguistic studies have shown that Hinglish speakers switch to Hindi when expressing negative emotions and to English when expressing positive emotions. To understand if language models can learn these associations, we study the effect of language on emotion prediction across 3 <b>PLMs</b> on a Hinglish emotion classification dataset. Using LIME and token level language ID, we find that models do learn these associations between language choice and emotional expression. Moreover, having code-mixed data present in the pre-training can augment that learning when task-specific data is scarce. We also conclude from the misclassifications that the models may overgeneralise this heuristic to other infrequent examples where this sociolinguistic phenomenon does not apply.</p></p class="citation"></blockquote><h3 id=39285-sidu-txt-an-xai-algorithm-for-nlp-with-a-holistic-assessment-approach-mohammad-n-s-jahromi-et-al-2024>(39/285) SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach (Mohammad N. S. Jahromi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad N. S. Jahromi, Satya. M. Muddamsetty, Asta Sofie Stage Jarlner, Anna Murphy Høgenhaug, Thomas Gammeltoft-Hansen, Thomas B. Moeslund. (2024)<br><strong>SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach</strong><br><button class=copy-to-clipboard title="SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Explainable AI, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03043v1.pdf filename=2402.03043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Explainable</b> <b>AI</b> (XAI) aids in deciphering &lsquo;black-box&rsquo; models. While several methods have been proposed and evaluated primarily in the image domain, the exploration of explainability in the text domain remains a growing research area. In this paper, we delve into the applicability of XAI methods for the text domain. In this context, the &lsquo;Similarity Difference and Uniqueness&rsquo; (SIDU) XAI method, recognized for its superior capability in localizing entire salient regions in image-based classification is extended to textual data. The extended method, SIDU-TXT, utilizes feature activation maps from &lsquo;black-box&rsquo; models to generate heatmaps at a granular, word-based level, thereby providing explanations that highlight contextually significant textual elements crucial for model predictions. Given the absence of a unified standard for assessing XAI methods, this study applies a holistic three-tiered comprehensive evaluation framework: Functionally-Grounded, Human-Grounded and Application-Grounded, to assess the effectiveness of the proposed SIDU-TXT across various experiments. We find that, in <b>sentiment</b> <b>analysis</b> task of a movie review dataset, SIDU-TXT excels in both functionally and human-grounded evaluations, demonstrating superior performance through quantitative and qualitative analyses compared to benchmarks like Grad-CAM and LIME. In the application-grounded evaluation within the sensitive and complex legal domain of asylum decision-making, SIDU-TXT and Grad-CAM demonstrate comparable performances, each with its own set of strengths and weaknesses. However, both methods fall short of entirely fulfilling the sophisticated criteria of expert expectations, highlighting the imperative need for additional research in XAI methods suitable for such domains.</p></p class="citation"></blockquote><h3 id=40285-automated-cognate-detection-as-a-supervised-link-prediction-task-with-cognate-transformer-v-s-d-s-mahesh-akavarapu-et-al-2024>(40/285) Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer (V. S. D. S. Mahesh Akavarapu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>V. S. D. S. Mahesh Akavarapu, Arnab Bhattacharya. (2024)<br><strong>Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer</strong><br><button class=copy-to-clipboard title="Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs-LG, cs-SI, cs.CL<br>Keyword Score: 20<br>Keywords: Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02926v1.pdf filename=2402.02926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identification of cognates across related languages is one of the primary problems in historical linguistics. Automated cognate identification is helpful for several downstream tasks including identifying sound correspondences, proto-language reconstruction, phylogenetic classification, etc. Previous state-of-the-art methods for cognate identification are mostly based on distributions of phonemes computed across multilingual wordlists and make little use of the cognacy labels that define links among cognate clusters. In this paper, we present a <b>transformer-based</b> architecture inspired by computational biology for the task of automated cognate detection. Beyond a certain amount of supervision, this method performs better than the existing methods, and shows steady improvement with further increase in supervision, thereby proving the efficacy of utilizing the labeled information. We also demonstrate that accepting multiple sequence alignments as input and having an end-to-end architecture with link prediction head saves much computation time while simultaneously yielding superior performance.</p></p class="citation"></blockquote><h3 id=41285-approximate-attributions-for-off-the-shelf-siamese-transformers-lucas-möller-et-al-2024>(41/285) Approximate Attributions for Off-the-Shelf Siamese Transformers (Lucas Möller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Möller, Dmitry Nikolaev, Sebastian Padó. (2024)<br><strong>Approximate Attributions for Off-the-Shelf Siamese Transformers</strong><br><button class=copy-to-clipboard title="Approximate Attributions for Off-the-Shelf Siamese Transformers" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02883v1.pdf filename=2402.02883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Siamese encoders such as sentence <b>transformers</b> are among the least understood deep models. Established attribution methods cannot tackle this model class since it compares two inputs rather than processing a single one. To address this gap, we have recently proposed an attribution method specifically for Siamese encoders (M"oller et al., 2023). However, it requires models to be adjusted and <b>fine-tuned</b> and therefore cannot be directly applied to off-the-shelf models. In this work, we reassess these restrictions and propose (i) a model with exact attribution ability that retains the original model&rsquo;s predictive performance and (ii) a way to compute approximate attributions for off-the-shelf models. We extensively compare approximate and exact attributions and use them to analyze the models&rsquo; attendance to different linguistic aspects. We gain insights into which syntactic roles Siamese <b>transformers</b> attend to, confirm that they mostly ignore negation, explore how they judge semantically opposite adjectives, and find that they exhibit lexical bias.</p></p class="citation"></blockquote><h3 id=42285-comparing-knowledge-sources-for-open-domain-scientific-claim-verification-juraj-vladika-et-al-2024>(42/285) Comparing Knowledge Sources for Open-Domain Scientific Claim Verification (Juraj Vladika et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juraj Vladika, Florian Matthes. (2024)<br><strong>Comparing Knowledge Sources for Open-Domain Scientific Claim Verification</strong><br><button class=copy-to-clipboard title="Comparing Knowledge Sources for Open-Domain Scientific Claim Verification" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 20<br>Keywords: Fact Verification, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02844v1.pdf filename=2402.02844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing rate at which scientific knowledge is discovered and health claims shared online has highlighted the importance of developing efficient <b>fact-checking</b> <b>systems</b> for scientific claims. The usual setting for this task in the literature assumes that the documents containing the evidence for claims are already provided and annotated or contained in a limited corpus. This renders the systems unrealistic for real-world settings where knowledge sources with potentially millions of documents need to be queried to find relevant evidence. In this paper, we perform an array of experiments to test the performance of open-domain claim verification systems. We test the final verdict prediction of systems on four datasets of biomedical and health claims in different settings. While keeping the pipeline&rsquo;s evidence selection and verdict prediction parts constant, document retrieval is performed over three common knowledge sources (PubMed, Wikipedia, Google) and using two different <b>information</b> <b>retrieval</b> techniques. We show that PubMed works better with specialized biomedical claims, while Wikipedia is more suited for everyday health concerns. Likewise, BM25 excels in retrieval precision, while semantic search in recall of relevant evidence. We discuss the results, outline frequent retrieval patterns and challenges, and provide promising future directions.</p></p class="citation"></blockquote><h3 id=43285-rethinking-optimization-and-architecture-for-tiny-language-models-yehui-tang-et-al-2024>(43/285) Rethinking Optimization and Architecture for Tiny Language Models (Yehui Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, Yunhe Wang. (2024)<br><strong>Rethinking Optimization and Architecture for Tiny Language Models</strong><br><button class=copy-to-clipboard title="Rethinking Optimization and Architecture for Tiny Language Models" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02791v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02791v2.pdf filename=2402.02791v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The power of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, \ie, neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingual corpora, following the established formulas. Experimental results demonstrate the improved optimization and architecture yield a notable average improvement of 8.87 on benchmark evaluation sets for PanGu-$\pi$-1B Pro. Besides, PanGu-$\pi$-1.5B Pro surpasses a range of SOTA models with larger model sizes, validating its superior performance. The code is available at <a href=https://github.com/YuchuanTian/RethinkTinyLM>https://github.com/YuchuanTian/RethinkTinyLM</a>.</p></p class="citation"></blockquote><h3 id=44285-racer-an-llm-powered-methodology-for-scalable-analysis-of-semi-structured-mental-health-interviews-satpreet-harcharan-singh-et-al-2024>(44/285) RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews (Satpreet Harcharan Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Satpreet Harcharan Singh, Kevin Jiang, Kanchan Bhasin, Ashutosh Sabharwal, Nidal Moukaddam, Ankit B Patel. (2024)<br><strong>RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews</strong><br><button class=copy-to-clipboard title="RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, q-bio-QM<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02656v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02656v1.pdf filename=2402.02656v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semi-structured interviews (SSIs) are a commonly employed data-collection method in healthcare research, offering in-depth qualitative insights into subject experiences. Despite their value, the manual analysis of SSIs is notoriously time-consuming and labor-intensive, in part due to the difficulty of extracting and categorizing emotional responses, and challenges in scaling human evaluation for <b>large</b> <b>populations.</b> <b>In</b> this study, we develop RACER, a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> based expert-guided automated pipeline that efficiently converts raw interview transcripts into insightful domain-relevant themes and sub-themes. We used RACER to analyze SSIs conducted with 93 healthcare professionals and trainees to assess the broad personal and professional mental health impacts of the COVID-19 crisis. RACER achieves moderately high agreement with two human evaluators (72%), which approaches the human inter-rater agreement (77%). Interestingly, <b>LLMs</b> and humans struggle with similar content involving nuanced emotional, ambivalent/dialectical, and psychological statements. Our study highlights the opportunities and challenges in using <b>LLMs</b> to improve research efficiency and opens new avenues for scalable analysis of SSIs in healthcare research.</p></p class="citation"></blockquote><h3 id=45285-financial-report-chunking-for-effective-retrieval-augmented-generation-antonio-jimeno-yepes-et-al-2024>(45/285) Financial Report Chunking for Effective Retrieval Augmented Generation (Antonio Jimeno Yepes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, Leah Li. (2024)<br><strong>Financial Report Chunking for Effective Retrieval Augmented Generation</strong><br><button class=copy-to-clipboard title="Financial Report Chunking for Effective Retrieval Augmented Generation" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05131v1.pdf filename=2402.05131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chunking <b>information</b> <b>is</b> a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraph-level chunking. This approach treats all texts as equal and neglects the <b>information</b> <b>contained</b> in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the <b>information</b> <b>retrieved.</b> We also demonstrate how this approach impacts RAG assisted Question & Answer task performance. Our research includes a comprehensive analysis of various element types, their role in effective <b>information</b> <b>retrieval,</b> and the impact they have on the quality of RAG outputs. Findings support that element type based chunking largely improve RAG results on financial reporting. Through this research, we are also able to answer how to uncover highly accurate RAG.</p></p class="citation"></blockquote><h3 id=46285-accurate-and-well-calibrated-icd-code-assignment-through-attention-over-diverse-label-embeddings-gonçalo-gomes-et-al-2024>(46/285) Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings (Gonçalo Gomes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gonçalo Gomes, Isabel Coutinho, Bruno Martins. (2024)<br><strong>Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings</strong><br><button class=copy-to-clipboard title="Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03172v1.pdf filename=2402.03172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although the International Classification of Diseases (ICD) has been adopted worldwide, manually assigning ICD codes to clinical text is time-consuming, error-prone, and expensive, motivating the development of automated approaches. This paper describes a novel approach for automated ICD coding, combining several ideas from previous related work. We specifically employ a strong <b>Transformer-based</b> model as a text encoder and, to handle lengthy clinical narratives, we explored either (a) adapting the base encoder model into a Longformer, or (b) dividing the text into chunks and processing each chunk independently. The representations produced by the encoder are combined with a label embedding mechanism that explores diverse ICD code synonyms. Experiments with different splits of the MIMIC-III dataset show that the proposed approach outperforms the current state-of-the-art models in ICD coding, with the label embeddings significantly contributing to the good performance. Our approach also leads to properly calibrated classification results, which can effectively inform downstream tasks such as quantification.</p></p class="citation"></blockquote><h3 id=47285-eevee-an-easy-annotation-tool-for-natural-language-processing-axel-sorensen-et-al-2024>(47/285) EEVEE: An Easy Annotation Tool for Natural Language Processing (Axel Sorensen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Axel Sorensen, Siyao Peng, Barbara Plank, Rob van der Goot. (2024)<br><strong>EEVEE: An Easy Annotation Tool for Natural Language Processing</strong><br><button class=copy-to-clipboard title="EEVEE: An Easy Annotation Tool for Natural Language Processing" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 10<br>Keywords: Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02864v1.pdf filename=2402.02864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Annotation tools are the starting point for creating Natural Language Processing (NLP) datasets. There is a wide variety of tools available; setting up these tools is however a hindrance. We propose EEVEE, an annotation tool focused on simplicity, efficiency, and ease of use. It can run directly in the browser (no setup required) and uses tab-separated files (as opposed to character offsets or task-specific formats) for annotation. It allows for annotation of multiple tasks on a single dataset and supports four task-types: sequence labeling, span labeling, <b>text</b> <b>classification</b> and seq2seq.</p></p class="citation"></blockquote><h3 id=48285-with-a-little-help-from-my-linguistic-friends-topic-segmentation-of-multi-party-casual-conversations-amandine-decker-et-al-2024>(48/285) With a Little Help from my (Linguistic) Friends: Topic Segmentation of Multi-party Casual Conversations (Amandine Decker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amandine Decker, Maxime Amblard. (2024)<br><strong>With a Little Help from my (Linguistic) Friends: Topic Segmentation of Multi-party Casual Conversations</strong><br><button class=copy-to-clipboard title="With a Little Help from my (Linguistic) Friends: Topic Segmentation of Multi-party Casual Conversations" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Topic Segmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02837v1.pdf filename=2402.02837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Topics</b> <b>play</b> an important role in the global organisation of a conversation as what is currently discussed constrains the possible contributions of the participant. Understanding the way <b>topics</b> <b>are</b> organised in interaction would provide insight on the structure of dialogue beyond the sequence of utterances. However, studying this high-level structure is a complex task that we try to approach by first segmenting dialogues into smaller topically coherent sets of utterances. Understanding the interactions between these segments would then enable us to propose a model of <b>topic</b> <b>organisation</b> at a dialogue level. In this paper we work with open-domain conversations and try to reach a comparable level of accuracy as recent machine learning based <b>topic</b> <b>segmentation</b> models but with a formal approach. The features we identify as meaningful for this task help us understand better the <b>topical</b> <b>structure</b> of a conversation.</p></p class="citation"></blockquote><h2 id=csir-12>cs.IR (12)</h2><h3 id=49285-large-language-model-distilling-medication-recommendation-model-qidong-liu-et-al-2024>(49/285) Large Language Model Distilling Medication Recommendation Model (Qidong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Zijian Zhang, Feng Tian, Yefeng Zheng. (2024)<br><strong>Large Language Model Distilling Medication Recommendation Model</strong><br><button class=copy-to-clipboard title="Large Language Model Distilling Medication Recommendation Model" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs-LG, cs.IR<br>Keyword Score: 80<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Recommendation, Recommender System, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02803v1.pdf filename=2402.02803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>recommendation</b> of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient&rsquo;s specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Our research aims to transform existing medication <b>recommendation</b> methodologies using <b>LLMs.</b> In this paper, we introduce a novel approach called <b>Large</b> <b>Language</b> <b>Model</b> <b>Distilling</b> Medication <b>Recommendation</b> (LEADER). We begin by creating appropriate <b>prompt</b> templates that enable <b>LLMs</b> to suggest medications effectively. However, the straightforward integration of <b>LLMs</b> into <b>recommender</b> <b>systems</b> leads to an out-of-corpus issue specific to drugs. We handle it by adapting the <b>LLMs</b> with a novel output layer and a refined tuning loss function. Although <b>LLM-based</b> models exhibit remarkable capabilities, they are plagued by high computational costs during inference, which is impractical for the healthcare sector. To mitigate this, we have developed a feature-level <b>knowledge</b> <b>distillation</b> technique, which transfers the <b>LLM&rsquo;s</b> proficiency to a more compact model. Extensive experiments conducted on two real-world datasets, MIMIC-III and MIMIC-IV, demonstrate that our proposed model not only delivers effective results but also is efficient. To ease the reproducibility of our experiments, we release the implementation code online.</p></p class="citation"></blockquote><h3 id=50285-harnessing-pubmed-user-query-logs-for-post-hoc-explanations-of-recommended-similar-articles-ashley-shin-et-al-2024>(50/285) Harnessing PubMed User Query Logs for Post Hoc Explanations of Recommended Similar Articles (Ashley Shin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashley Shin, Qiao Jin, James Anibal, Zhiyong Lu. (2024)<br><strong>Harnessing PubMed User Query Logs for Post Hoc Explanations of Recommended Similar Articles</strong><br><button class=copy-to-clipboard title="Harnessing PubMed User Query Logs for Post Hoc Explanations of Recommended Similar Articles" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Recommendation, GPT, GPT-3, GPT-3.5, GPT-4, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03484v1.pdf filename=2402.03484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Searching for a related article based on a reference article is an integral part of scientific research. PubMed, like many academic search engines, has a &ldquo;similar articles&rdquo; feature that recommends articles relevant to the current article viewed by a user. Explaining recommended items can be of great utility to users, particularly in the literature search process. With more than a million biomedical papers being published each year, explaining the recommended similar articles would facilitate researchers and clinicians in searching for related articles. Nonetheless, the majority of current literature <b>recommendation</b> systems lack explanations for their suggestions. We employ a post hoc approach to explaining <b>recommendations</b> by identifying relevant tokens in the titles of similar articles. Our major contribution is building PubCLogs by repurposing 5.6 million pairs of coclicked articles from PubMed&rsquo;s user query logs. Using our PubCLogs dataset, we train the Highlight Similar Article Title (HSAT), a <b>transformer-based</b> model designed to select the most relevant parts of the title of a similar article, based on the title and abstract of a seed article. HSAT demonstrates strong performance in our empirical evaluations, achieving an F1 score of 91.72 percent on the PubCLogs test set, considerably outperforming several baselines including BM25 (70.62), MPNet (67.11), MedCPT (62.22), <b>GPT-3.5</b> (46.00), and <b>GPT-4</b> (64.89). Additional evaluations on a separate, manually annotated test set further verifies HSAT&rsquo;s performance. Moreover, participants of our user study indicate a preference for HSAT, due to its superior balance between conciseness and comprehensiveness. Our study suggests that repurposing user query logs of academic search engines can be a promising way to train state-of-the-art models for explaining literature <b>recommendation.</b></p></p class="citation"></blockquote><h3 id=51285-list-aware-reranking-truncation-joint-model-for-search-and-retrieval-augmented-generation-shicheng-xu-et-al-2024>(51/285) List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation (Shicheng Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, Xueqi Cheng. (2024)<br><strong>List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation</strong><br><button class=copy-to-clipboard title="List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Rerank, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02764v1.pdf filename=2402.02764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The results of <b>information</b> <b>retrieval</b> (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented generation for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including <b>reranking</b> and truncation. <b>Reranking</b> finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share the contextual <b>information</b> <b>of</b> the ranking list between the two tasks. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the <b>reranking</b> stage can largely affect the truncation stage. To solve these problems, we propose a <b>Reranking-Truncation</b> joint model (GenRT) that can perform the two tasks concurrently. GenRT integrates <b>reranking</b> and truncation via generative paradigm based on encoder-decoder architecture. We also design the novel loss functions for joint optimization to make the model learn both tasks. Sharing parameters by the joint model is conducive to making full use of the common modeling <b>information</b> <b>of</b> the two tasks. Besides, the two tasks are performed concurrently and co-optimized to solve the error accumulation problem between separate stages. Experiments on public learning-to-rank benchmarks and open-domain Q&amp;A tasks show that our method achieves SOTA performance on both <b>reranking</b> and truncation tasks for web search and retrieval-augmented <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=52285-finest-stabilizing-recommendations-by-rank-preserving-fine-tuning-sejoon-oh-et-al-2024>(52/285) FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning (Sejoon Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sejoon Oh, Berk Ustun, Julian McAuley, Srijan Kumar. (2024)<br><strong>FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning</strong><br><button class=copy-to-clipboard title="FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs-SI, cs.IR<br>Keyword Score: 30<br>Keywords: Fine-tuning, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03481v1.pdf filename=2402.03481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>recommender</b> <b>systems</b> may output considerably different <b>recommendations</b> due to small perturbations in the training data. Changes in the data from a single user will alter the <b>recommendations</b> as well as the <b>recommendations</b> of other users. In applications like healthcare, housing, and finance, this sensitivity can have adverse effects on user experience. We propose a method to stabilize a given <b>recommender</b> <b>system</b> against such perturbations. This is a challenging task due to (1) the lack of a ``reference&rsquo;&rsquo; rank list that can be used to anchor the outputs; and (2) the computational challenges in ensuring the stability of rank lists with respect to all possible perturbations of training data. Our method, FINEST, overcomes these challenges by obtaining reference rank lists from a given <b>recommendation</b> model and then <b>fine-tuning</b> the model under simulated perturbation scenarios with rank-preserving regularization on sampled items. Our experiments on real-world datasets demonstrate that FINEST can ensure that <b>recommender</b> <b>models</b> output stable <b>recommendations</b> under a wide range of different perturbations without compromising next-item prediction accuracy.</p></p class="citation"></blockquote><h3 id=53285-domain-adaptation-of-multilingual-semantic-search----literature-review-anna-bringmann-et-al-2024>(53/285) Domain Adaptation of Multilingual Semantic Search &ndash; Literature Review (Anna Bringmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Bringmann, Anastasia Zhukova. (2024)<br><strong>Domain Adaptation of Multilingual Semantic Search &ndash; Literature Review</strong><br><button class=copy-to-clipboard title="Domain Adaptation of Multilingual Semantic Search -- Literature Review" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 30<br>Keywords: Low-Resource, Information Retrieval, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02932v1.pdf filename=2402.02932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This literature review gives an overview of current approaches to perform <b>domain</b> <b>adaptation</b> in a <b>low-resource</b> and approaches to perform multilingual semantic search in a <b>low-resource</b> setting. We developed a new typology to cluster <b>domain</b> <b>adaptation</b> approaches based on the part of dense textual <b>information</b> <b>retrieval</b> systems, which they adapt, focusing on how to combine them efficiently. We also explore the possibilities of combining multilingual semantic search with <b>domain</b> <b>adaptation</b> approaches for dense retrievers in a <b>low-resource</b> setting.</p></p class="citation"></blockquote><h3 id=54285-intersectional-two-sided-fairness-in-recommendation-yifan-wang-et-al-2024>(54/285) Intersectional Two-sided Fairness in Recommendation (Yifan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Wang, Peijie Sun, Weizhi Ma, Min Zhang, Yuan Zhang, Peng Jiang, Shaoping Ma. (2024)<br><strong>Intersectional Two-sided Fairness in Recommendation</strong><br><button class=copy-to-clipboard title="Intersectional Two-sided Fairness in Recommendation" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CY, cs-IR, cs-LG, cs.IR<br>Keyword Score: 30<br>Keywords: Fairness, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02816v1.pdf filename=2402.02816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fairness</b> of <b>recommender</b> <b>systems</b> (RS) has attracted increasing attention recently. Based on the involved stakeholders, the <b>fairness</b> of RS can be divided into user <b>fairness,</b> item <b>fairness,</b> and two-sided <b>fairness</b> which considers both user and item <b>fairness</b> simultaneously. However, we argue that the intersectional two-sided unfairness may still exist even if the RS is two-sided fair, which is observed and shown by empirical studies on real-world data in this paper, and has not been well-studied previously. To mitigate this problem, we propose a novel approach called Intersectional Two-sided <b>Fairness</b> <b>Recommendation</b> (ITFR). Our method utilizes a sharpness-aware loss to perceive disadvantaged groups, and then uses collaborative loss balance to develop consistent distinguishing abilities for different intersectional groups. Additionally, predicted score normalization is leveraged to align positive predicted scores to fairly treat positives in different intersectional groups. Extensive experiments and analyses on three public datasets show that our proposed approach effectively alleviates the intersectional two-sided unfairness and consistently outperforms previous state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=55285-event-based-product-carousel-recommendation-with-query-click-graph-luyi-ma-et-al-2024>(55/285) Event-based Product Carousel Recommendation with Query-Click Graph (Luyi Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luyi Ma, Nimesh Sinha, Parth Vajge, Jason HD Cho, Sushant Kumar, Kannan Achan. (2024)<br><strong>Event-based Product Carousel Recommendation with Query-Click Graph</strong><br><button class=copy-to-clipboard title="Event-based Product Carousel Recommendation with Query-Click Graph" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03277v1.pdf filename=2402.03277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many current <b>recommender</b> <b>systems</b> mainly focus on the product-to-product <b>recommendations</b> and user-to-product <b>recommendations</b> even during the time of events rather than modeling the typical <b>recommendations</b> for the target event (e.g., festivals, seasonal activities, or social activities) without addressing the multiple aspects of the shopping demands for the target event. Product <b>recommendations</b> for the multiple aspects of the target event are usually generated by human curators who manually identify the aspects and select a list of aspect-related products (i.e., product carousel) for each aspect as <b>recommendations.</b> However, building a <b>recommender</b> <b>system</b> with machine learning is non-trivial due to the lack of both the ground truth of event-related aspects and the aspect-related products. To fill this gap, we define the novel problem as the event-based product carousel <b>recommendations</b> in e-commerce and propose an effective <b>recommender</b> <b>system</b> based on the query-click bipartite graph. We apply the iterative clustering algorithm over the query-click bipartite graph and infer the event-related aspects by the clusters of queries. The aspect-related <b>recommendations</b> are powered by the click-through rate of products regarding each aspect. We show through experiments that this approach effectively mines product carousels for the target event.</p></p class="citation"></blockquote><h3 id=56285-comparison-of-topic-modelling-approaches-in-the-banking-context-bayode-ogunleye-et-al-2024>(56/285) Comparison of Topic Modelling Approaches in the Banking Context (Bayode Ogunleye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bayode Ogunleye, Tonderai Maswera, Laurence Hirsch, Jotham Gaudoin, Teresa Brunsdon. (2024)<br><strong>Comparison of Topic Modelling Approaches in the Banking Context</strong><br><button class=copy-to-clipboard title="Comparison of Topic Modelling Approaches in the Banking Context" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: H-3-3, cs-AI, cs-IR, cs-LG, cs.IR, stat-CO<br>Keyword Score: 20<br>Keywords: Recommendation, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03176v1.pdf filename=2402.03176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Topic modelling is a prominent task for automatic topic extraction in many applications such as <b>sentiment</b> <b>analysis</b> and <b>recommendation</b> systems. The approach is vital for service industries to monitor their customer discussions. The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document. Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture. We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches. Our findings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463.</p></p class="citation"></blockquote><h3 id=57285-understanding-and-guiding-weakly-supervised-entity-alignment-with-potential-isomorphism-propagation-yuanyi-wang-et-al-2024>(57/285) Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation (Yuanyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanyi Wang, Wei Tang, Haifeng Sun, Zirui Zhuang, Xiaoyuan Fu, Jingyu Wang, Qi Qi, Jianxin Liao. (2024)<br><strong>Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation</strong><br><button class=copy-to-clipboard title="Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03025v1.pdf filename=2402.03025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Weakly <b>Supervised</b> Entity Alignment (EA) is the task of identifying equivalent entities across diverse knowledge graphs (KGs) using only a limited number of seed alignments. Despite substantial advances in aggregation-based weakly <b>supervised</b> EA, the underlying mechanisms in this setting remain unexplored. In this paper, we present a propagation perspective to analyze weakly <b>supervised</b> EA and explain the existing aggregation-based EA models. Our theoretical analysis reveals that these models essentially seek propagation operators for pairwise entity similarities. We further prove that, despite the structural heterogeneity of different KGs, the potentially aligned entities within aggregation-based EA models have isomorphic subgraphs, which is the core premise of EA but has not been investigated. Leveraging this insight, we introduce a potential isomorphism propagation operator to enhance the propagation of neighborhood information across KGs. We develop a general EA framework, PipEA, incorporating this operator to improve the accuracy of every type of aggregation-based model without altering the learning process. Extensive experiments substantiate our theoretical findings and demonstrate PipEA&rsquo;s significant performance gains over state-of-the-art weakly <b>supervised</b> EA methods. Our work not only advances the field but also enhances our comprehension of aggregation-based weakly <b>supervised</b> EA.</p></p class="citation"></blockquote><h3 id=58285-dynamic-sparse-learning-a-novel-paradigm-for-efficient-recommendation-shuyao-wang-et-al-2024>(58/285) Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation (Shuyao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuyao Wang, Yongduo Sui, Jiancan Wu, Zhi Zheng, Hui Xiong. (2024)<br><strong>Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation</strong><br><button class=copy-to-clipboard title="Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Model Compression, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02855v1.pdf filename=2402.02855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of deep learning-based <b>recommendation</b> systems, the increasing computational demands, driven by the growing number of users and items, pose a significant challenge to practical deployment. This challenge is primarily twofold: reducing the <b>model</b> <b>size</b> while effectively learning user and item representations for efficient <b>recommendations.</b> Despite considerable advancements in <b>model</b> <b>compression</b> and architecture search, prevalent approaches face notable constraints. These include substantial additional computational costs from pre-training/re-training in <b>model</b> <b>compression</b> and an extensive search space in architecture design. Additionally, managing complexity and adhering to memory constraints is problematic, especially in scenarios with strict time or space limitations. Addressing these issues, this paper introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored for <b>recommendation</b> <b>models.</b> <b>DSL</b> innovatively trains a lightweight sparse <b>model</b> <b>from</b> scratch, periodically evaluating and dynamically adjusting each weight&rsquo;s significance and the <b>model&rsquo;s</b> <b>sparsity</b> distribution during the training. This approach ensures a consistent and minimal parameter budget throughout the full learning lifecycle, paving the way for &ldquo;end-to-end&rdquo; efficiency from training to inference. Our extensive experimental results underline DSL&rsquo;s effectiveness, significantly reducing training and inference costs while delivering comparable <b>recommendation</b> performance.</p></p class="citation"></blockquote><h3 id=59285-denoising-time-cycle-modeling-for-recommendation-sicong-xie-et-al-2024>(59/285) Denoising Time Cycle Modeling for Recommendation (Sicong Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sicong Xie, Qunwei Li, Weidi Xu, Kaiming Shen, Shaohu Chen, Wenliang Zhong. (2024)<br><strong>Denoising Time Cycle Modeling for Recommendation</strong><br><button class=copy-to-clipboard title="Denoising Time Cycle Modeling for Recommendation" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02718v1.pdf filename=2402.02718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, modeling temporal patterns of user-item interactions have attracted much attention in <b>recommender</b> <b>systems.</b> We argue that existing methods ignore the variety of temporal patterns of user behaviors. We define the subset of user behaviors that are irrelevant to the target item as noises, which limits the performance of target-related time cycle modeling and affect the <b>recommendation</b> performance. In this paper, we propose Denoising Time Cycle Modeling (DiCycle), a novel approach to denoise user behaviors and select the subset of user behaviors that are highly related to the target item. DiCycle is able to explicitly model diverse time cycle patterns for <b>recommendation.</b> Extensive experiments are conducted on both public benchmarks and a real-world dataset, demonstrating the superior performance of DiCycle over the state-of-the-art <b>recommendation</b> methods.</p></p class="citation"></blockquote><h3 id=60285-trinity-syncretizing-multi-long-taillong-term-interests-all-in-one-jing-yan-et-al-2024>(60/285) Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One (Jing Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Yan, Liu Jiang, Jianfei Cui, Zhichen Zhao, Xingyan Bin, Feng Zhang, Zuotao Liu. (2024)<br><strong>Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One</strong><br><button class=copy-to-clipboard title="Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 10<br>Keywords: Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02842v1.pdf filename=2402.02842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interest modeling in <b>recommender</b> <b>system</b> has been a constant topic for improving user experience, and typical interest modeling tasks (e.g. multi-interest, long-tail interest and long-term interest) have been investigated in many existing works. However, most of them only consider one interest in isolation, while neglecting their interrelationships. In this paper, we argue that these tasks suffer from a common &ldquo;interest amnesia&rdquo; problem, and a solution exists to mitigate it simultaneously. We figure that long-term cues can be the cornerstone since they reveal multi-interest and clarify long-tail interest. Inspired by the observation, we propose a novel and unified framework in the retrieval stage, &ldquo;Trinity&rdquo;, to solve interest amnesia problem and improve multiple interest modeling tasks. We construct a real-time clustering system that enables us to project items into enumerable clusters, and calculate statistical interest histograms over these clusters. Based on these histograms, Trinity recognizes underdelivered themes and remains stable when facing emerging hot topics. Trinity is more appropriate for large-scale industry scenarios because of its modest computational overheads. Its derived retrievers have been deployed on the <b>recommender</b> <b>system</b> of Douyin, significantly improving user experience and retention. We believe that such practical experience can be well generalized to other scenarios.</p></p class="citation"></blockquote><h2 id=cslg-79>cs.LG (79)</h2><h3 id=61285-iced-zero-shot-transfer-in-reinforcement-learning-via-in-context-environment-design-samuel-garcin-et-al-2024>(61/285) ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design (Samuel Garcin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Garcin, James Doran, Shangmin Guo, Christopher G. Lucas, Stefano V. Albrecht. (2024)<br><strong>ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design</strong><br><button class=copy-to-clipboard title="ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Autoencoder, Mutual Information, Reinforcement Learning, Unsupervised Learning, Variational Autoencoder, Zero-shot, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03479v1.pdf filename=2402.03479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous agents trained using deep <b>reinforcement</b> <b>learning</b> (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the <b>zero-shot</b> generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the <b>mutual</b> <b>information</b> between the agent&rsquo;s internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to <b>unsupervised</b> environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance. To prevent both overfitting and distributional shift, we introduce <b>in-context</b> environment design (ICED). ICED generates levels using a <b>variational</b> <b>autoencoder</b> trained over an initial set of level parameters, reducing distributional shift, and achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods.</p></p class="citation"></blockquote><h3 id=62285-automatic-combination-of-sample-selection-strategies-for-few-shot-learning-branislav-pecher-et-al-2024>(62/285) Automatic Combination of Sample Selection Strategies for Few-Shot Learning (Branislav Pecher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Branislav Pecher, Ivan Srba, Maria Bielikova, Joaquin Vanschoren. (2024)<br><strong>Automatic Combination of Sample Selection Strategies for Few-Shot Learning</strong><br><button class=copy-to-clipboard title="Automatic Combination of Sample Selection Strategies for Few-Shot Learning" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Meta Learning, Supervised Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03038v1.pdf filename=2402.03038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>few-shot</b> <b>learning,</b> such as <b>meta-learning,</b> <b>few-shot</b> <b>fine-tuning</b> or <b>in-context</b> <b>learning,</b> the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of <b>few-shot</b> <b>learning</b> is not extensively known, as most of them have been so far evaluated in typical <b>supervised</b> settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 <b>few-shot</b> <b>learning</b> approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for <b>in-context</b> <b>learning.</b> We also show a strong modality, dataset and approach dependence for the majority of strategies as well as their dependence on the number of shots - demonstrating that the sample selection strategies play a significant role for lower number of shots, but regresses to random selection at higher number of shots.</p></p class="citation"></blockquote><h3 id=63285-make-every-move-count-llm-based-high-quality-rtl-code-generation-using-mcts-matthew-delorenzo-et-al-2024>(63/285) Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS (Matthew DeLorenzo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew DeLorenzo, Animesh Basak Chowdhury, Vasudev Gohil, Shailja Thakur, Ramesh Karri, Siddharth Garg, Jeyavijayan Rajendran. (2024)<br><strong>Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS</strong><br><button class=copy-to-clipboard title="Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-AR, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, Transformer, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03289v1.pdf filename=2402.03289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for register transfer level <b>code</b> <b>generation</b> face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency. This is due to the lack of PPA awareness in conventional <b>transformer</b> decoding algorithms. In response, we present an automated <b>transformer</b> decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the <b>transformer</b> to produce compilable, functionally correct, and PPA-optimized <b>code.</b> <b>Empirical</b> evaluation with a <b>fine-tuned</b> language model on RTL codesets shows that our proposed technique consistently generates functionally correct <b>code</b> <b>compared</b> to <b>prompting-only</b> methods and effectively addresses the PPA-unawareness drawback of naive <b>large</b> <b>language</b> <b>models.</b> For the largest design generated by the state-of-the-art <b>LLM</b> (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product.</p></p class="citation"></blockquote><h3 id=64285-empowering-time-series-analysis-with-large-language-models-a-survey-yushan-jiang-et-al-2024>(64/285) Empowering Time Series Analysis with Large Language Models: A Survey (Yushan Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, Dongjin Song. (2024)<br><strong>Empowering Time Series Analysis with Large Language Models: A Survey</strong><br><button class=copy-to-clipboard title="Empowering Time Series Analysis with Large Language Models: A Survey" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, Tokenization, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03182v1.pdf filename=2402.03182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, remarkable progress has been made over <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a <b>large</b> <b>general-purpose</b> <b>model</b> from the scratch is challenging for time series analysis, due to the <b>large</b> <b>volumes</b> <b>and</b> varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained <b>LLMs</b> can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage <b>LLMs</b> for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of <b>LLMs.</b> Next, we <b>summarize</b> the general pipeline for <b>LLM-based</b> time series analysis, categorize existing methods into different groups (i.e., direct query, <b>tokenization,</b> <b>prompt</b> design, <b>fine-tune,</b> and model integration), and highlight the key ideas within each group. We also discuss the applications of <b>LLMs</b> for both general and spatial-temporal time series data, tailored to specific domains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=65285-the-matrix-a-bayesian-learning-model-for-llms-siddhartha-dalal-et-al-2024>(65/285) The Matrix: A Bayesian learning model for LLMs (Siddhartha Dalal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siddhartha Dalal, Vishal Misra. (2024)<br><strong>The Matrix: A Bayesian learning model for LLMs</strong><br><button class=copy-to-clipboard title="The Matrix: A Bayesian learning model for LLMs" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-7, cs-AI, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Text Generation, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03175v1.pdf filename=2402.03175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a Bayesian learning model to understand the behavior of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> We explore the optimization metric of <b>LLMs,</b> which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative <b>text</b> <b>model</b> represented by a multinomial transition probability matrix with a prior, and we examine how <b>LLMs</b> approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how <b>text</b> <b>generation</b> by <b>LLMs</b> aligns with Bayesian learning principles and delve into the implications for <b>in-context</b> <b>learning,</b> specifically explaining why <b>in-context</b> <b>learning</b> emerges in larger models where <b>prompts</b> are considered as samples to be updated. Our findings indicate that the behavior of <b>LLMs</b> is consistent with Bayesian Learning, offering new insights into their functioning and potential applications.</p></p class="citation"></blockquote><h3 id=66285-a-survey-on-transformer-compression-yehui-tang-et-al-2024>(66/285) A Survey on Transformer Compression (Yehui Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhijun Tu, Kai Han, Hailin Hu, Dacheng Tao. (2024)<br><strong>A Survey on Transformer Compression</strong><br><button class=copy-to-clipboard title="A Survey on Transformer Compression" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Knowledge Distillation, Model Compression, Pruning, Quantization, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05964v1.pdf filename=2402.05964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>models</b> <b>based</b> on the <b>Transformer</b> architecture play increasingly vital roles in artificial intelligence, particularly within the realms of natural language processing (NLP) and computer vision (CV). <b>Model</b> <b>compression</b> methods reduce their memory and computational cost, which is a necessary step to implement the <b>transformer</b> <b>models</b> <b>on</b> practical devices. Given the unique architecture of <b>transformer,</b> featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are required. The efficiency of these compression methods is also paramount, as it is usually impractical to retrain large <b>models</b> <b>on</b> the entire training dataset.This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to <b>transformer</b> <b>models.</b> <b>The</b> compression methods are primarily categorized into <b>pruning,</b> <b>quantization,</b> <b>knowledge</b> <b>distillation,</b> and efficient architecture design. In each category, we discuss compression methods for both CV and NLP tasks, highlighting common underlying principles. At last, we delve into the relation between various compression methods, and discuss the further directions in this domain.</p></p class="citation"></blockquote><h3 id=67285-guard-role-playing-to-generate-natural-language-jailbreakings-to-test-guideline-adherence-of-large-language-models-haibo-jin-et-al-2024>(67/285) GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models (Haibo Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang. (2024)<br><strong>GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models</strong><br><button class=copy-to-clipboard title="GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: ChatGPT, LLaMA, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03299v1.pdf filename=2402.03299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The discovery of &ldquo;jailbreaks&rdquo; to bypass safety filters of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the <b>LLMs</b> with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user <b>LLMs</b> to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing <b>LLMs</b> to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether <b>LLMs</b> follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced <b>LLMs</b> (Vicuna-13B, LongChat-7B, and <b>Llama-2-7B),</b> as well as a widely-utilized commercial <b>LLM</b> <b>(ChatGPT).</b> Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD&rsquo;s versatility and contributing valuable insights for the development of safer, more reliable <b>LLM-based</b> applications across diverse modalities.</p></p class="citation"></blockquote><h3 id=68285-shortened-llama-a-simple-depth-pruning-for-large-language-models-bo-kyeong-kim-et-al-2024>(68/285) Shortened LLaMA: A Simple Depth Pruning for Large Language Models (Bo-Kyeong Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, Hyoung-Kyu Song. (2024)<br><strong>Shortened LLaMA: A Simple Depth Pruning for Large Language Models</strong><br><button class=copy-to-clipboard title="Shortened LLaMA: A Simple Depth Pruning for Large Language Models" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Pruning, Zero-shot, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02834v1.pdf filename=2402.02834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structured <b>pruning</b> of modern <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has emerged as a way of decreasing their high computational needs. Width <b>pruning</b> reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth <b>pruning,</b> in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth <b>pruning,</b> with little comparative analysis between the two units (width vs. depth) concerning their impact on <b>LLM</b> inference efficiency. In this work, we show that a simple depth <b>pruning</b> approach can compete with recent width <b>pruning</b> methods in terms of <b>zero-shot</b> task performance. Our <b>pruning</b> method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running <b>LLMs,</b> where width <b>pruning</b> is ineffective. We hope this work can help deploy <b>LLMs</b> on local and edge devices.</p></p class="citation"></blockquote><h3 id=69285-diffusion-world-model-zihan-ding-et-al-2024>(69/285) Diffusion World Model (Zihan Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Ding, Amy Zhang, Yuandong Tian, Qinqing Zheng. (2024)<br><strong>Diffusion World Model</strong><br><button class=copy-to-clipboard title="Diffusion World Model" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03570v1.pdf filename=2402.03570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of <b>offline</b> <b>reinforcement</b> <b>learning,</b> DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables <b>offline</b> <b>Q-learning</b> <b>with</b> synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon <b>simulation.</b> In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44%$ performance gain, and achieves state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=70285-path-signatures-and-graph-neural-networks-for-slow-earthquake-analysis-better-together-hans-riess-et-al-2024>(70/285) Path Signatures and Graph Neural Networks for Slow Earthquake Analysis: Better Together? (Hans Riess et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hans Riess, Manolis Veveakis, Michael M. Zavlanos. (2024)<br><strong>Path Signatures and Graph Neural Networks for Slow Earthquake Analysis: Better Together?</strong><br><button class=copy-to-clipboard title="Path Signatures and Graph Neural Networks for Slow Earthquake Analysis: Better Together?" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-geo-ph<br>Keyword Score: 40<br>Keywords: Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03558v1.pdf filename=2402.03558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The path signature, having enjoyed recent success in the machine learning community, is a theoretically-driven method for engineering features from irregular paths. On the other hand, <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNN),</b> neural architectures for processing data on <b>graphs,</b> <b>excel</b> <b>on</b> tasks with irregular domains, such as sensor networks. In this paper, we introduce a novel approach, Path Signature <b>Graph</b> <b>Convolutional</b> <b>Neural</b> <b>Networks</b> (PS-GCNN), integrating path signatures into <b>graph</b> <b>convolutional</b> <b>neural</b> <b>networks</b> (GCNN), and leveraging the strengths of both path signatures, for feature extraction, and GCNNs, for handling spatial interactions. We apply our method to analyze slow earthquake sequences, also called slow slip events (SSE), utilizing data from GPS timeseries, with a case study on a GPS sensor network on the east coast of New Zealand&rsquo;s north island. We also establish benchmarks for our method on simulated stochastic differential equations, which model similar reaction-diffusion phenomenon. Our methodology shows promise for future advancement in earthquake prediction and sensor network analysis.</p></p class="citation"></blockquote><h3 id=71285-fairness-and-privacy-guarantees-in-federated-contextual-bandits-sambhav-solanki-et-al-2024>(71/285) Fairness and Privacy Guarantees in Federated Contextual Bandits (Sambhav Solanki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sambhav Solanki, Shweta Jain, Sujit Gujar. (2024)<br><strong>Fairness and Privacy Guarantees in Federated Contextual Bandits</strong><br><button class=copy-to-clipboard title="Fairness and Privacy Guarantees in Federated Contextual Bandits" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Bandit Algorithm, Fairness, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03531v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03531v1.pdf filename=2402.03531v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers the contextual multi-armed <b>bandit</b> (CMAB) problem with <b>fairness</b> and privacy guarantees in a federated environment. We consider merit-based exposure as the desired fair outcome, which provides exposure to each action in proportion to the reward associated. We model the algorithm&rsquo;s effectiveness using <b>fairness</b> regret, which captures the difference between fair optimal policy and the policy output by the algorithm. Applying fair CMAB algorithm to each agent individually leads to <b>fairness</b> regret linear in the number of agents. We propose that collaborative &ndash; federated learning can be more effective and provide the algorithm Fed-FairX-LinUCB that also ensures differential privacy. The primary challenge in extending the existing privacy framework is designing the communication protocol for communicating required information across agents. A naive protocol can either lead to weaker privacy guarantees or higher regret. We design a novel communication protocol that allows for (i) Sub-linear theoretical bounds on <b>fairness</b> regret for Fed-FairX-LinUCB and comparable bounds for the private counterpart, Priv-FairX-LinUCB (relative to single-agent learning), (ii) Effective use of privacy budget in Priv-FairX-LinUCB. We demonstrate the efficacy of our proposed algorithm with extensive <b>simulations-based</b> experiments. We show that both Fed-FairX-LinUCB and Priv-FairX-LinUCB achieve near-optimal <b>fairness</b> regret.</p></p class="citation"></blockquote><h3 id=72285-skill-set-optimization-reinforcing-language-model-behavior-via-transferable-skills-kolby-nottingham-et-al-2024>(72/285) Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills (Kolby Nottingham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kolby Nottingham, Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Sameer Singh, Peter Clark, Roy Fox. (2024)<br><strong>Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills</strong><br><button class=copy-to-clipboard title="Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Pruning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03244v1.pdf filename=2402.03244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual <b>LLM</b> actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving <b>LLM</b> actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the <b>LLM</b> actor <b>in-context</b> to reinforce behaviors with high rewards. Then, SSO further refines the skill set by <b>pruning</b> skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO&rsquo;s ability to optimize a set of skills and perform <b>in-context</b> policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%.</p></p class="citation"></blockquote><h3 id=73285-is-mamba-capable-of-in-context-learning-riccardo-grazzi-et-al-2024>(73/285) Is Mamba Capable of In-Context Learning? (Riccardo Grazzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, Frank Hutter. (2024)<br><strong>Is Mamba Capable of In-Context Learning?</strong><br><button class=copy-to-clipboard title="Is Mamba Capable of In-Context Learning?" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Transformer, In-context Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03170v1.pdf filename=2402.03170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work provides empirical evidence that Mamba, a newly proposed selective structured state space model, has similar <b>in-context</b> <b>learning</b> <b>(ICL)</b> capabilities as <b>transformers.</b> We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems. Our results demonstrate that across both categories of tasks, Mamba matches the performance of <b>transformer</b> models for <b>ICL.</b> Further analysis reveals that like <b>transformers,</b> Mamba appears to solve <b>ICL</b> problems by incrementally optimizing its internal representations. Overall, our work suggests that Mamba can be an efficient alternative to <b>transformers</b> for <b>ICL</b> tasks involving longer input sequences.</p></p class="citation"></blockquote><h3 id=74285-text-guided-image-clustering-andreas-stephan-et-al-2024>(74/285) Text-Guided Image Clustering (Andreas Stephan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Stephan, Lukas Miklautz, Kevin Sidak, Jan Philip Wahle, Bela Gipp, Claudia Plant, Benjamin Roth. (2024)<br><strong>Text-Guided Image Clustering</strong><br><button class=copy-to-clipboard title="Text-Guided Image Clustering" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Question Answering, Visual Question Answering, Visual Question Answering, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02996v1.pdf filename=2402.02996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations. Those are usually in the form of text, begging the <b>question</b> <b>of</b> using text as an abstraction for image clustering. Current image clustering methods, however, neglect the use of generated textual descriptions. We, therefore, propose Text-Guided Image Clustering, i.e., generating text using image captioning and <b>visual</b> <b>question-answering</b> <b>(VQA)</b> models and subsequently clustering the generated text. Further, we introduce a novel approach to inject task- or domain knowledge for clustering by <b>prompting</b> <b>VQA</b> models. Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features. Additionally, we propose a counting-based cluster explainability method. Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests. Overall, this research challenges traditional approaches and paves the way for a paradigm shift in image clustering, using generated text.</p></p class="citation"></blockquote><h3 id=75285-powergraph-a-power-grid-benchmark-dataset-for-graph-neural-networks-anna-varbella-et-al-2024>(75/285) PowerGraph: A power grid benchmark dataset for graph neural networks (Anna Varbella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Varbella, Kenza Amara, Blazhe Gjorgiev, Giovanni Sansavini. (2024)<br><strong>PowerGraph: A power grid benchmark dataset for graph neural networks</strong><br><button class=copy-to-clipboard title="PowerGraph: A power grid benchmark dataset for graph neural networks" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 40<br>Keywords: Graph Neural Network, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02827v1.pdf filename=2402.02827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Public <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNN)</b> benchmark datasets facilitate the use of <b>GNN</b> and enhance <b>GNN</b> applicability to diverse disciplines. The community currently lacks public datasets of electrical power grids for <b>GNN</b> applications. Indeed, <b>GNNs</b> can potentially capture complex power grid phenomena over alternative machine learning techniques. Power grids are complex engineered networks that are naturally amenable to <b>graph</b> <b>representations.</b> <b>Therefore,</b> <b>GNN</b> have the potential for capturing the behavior of power grids over alternative machine learning techniques. To this aim, we develop a <b>graph</b> <b>dataset</b> <b>for</b> cascading failure events, which are the major cause of blackouts in electric power grids. Historical blackout datasets are scarce and incomplete. The assessment of vulnerability and the identification of critical components are usually conducted via computationally expensive offline <b>simulations</b> of cascading failures. Instead, we propose using machine learning models for the online detection of cascading failures leveraging the knowledge of the system state at the onset of the cascade. We develop PowerGraph, a <b>graph</b> <b>dataset</b> <b>modeling</b> cascading failures in power grids, designed for two purposes, namely, i) training <b>GNN</b> models for different <b>graph-level</b> <b>tasks</b> <b>including</b> multi-class classification, binary classification, and regression, and ii) explaining <b>GNN</b> models. The dataset generated via a physics-based cascading failure model ensures the generality of the operating and environmental conditions by spanning diverse failure scenarios. In addition, we foster the use of the dataset to benchmark <b>GNN</b> explainability methods by assigning ground-truth edge-level explanations. PowerGraph helps the development of better <b>GNN</b> models for <b>graph-level</b> <b>tasks</b> <b>and</b> explainability, critical in many domains ranging from chemistry to biology, where the systems and processes can be described as graphs.</p></p class="citation"></blockquote><h3 id=76285-vision-language-models-provide-promptable-representations-for-reinforcement-learning-william-chen-et-al-2024>(76/285) Vision-Language Models Provide Promptable Representations for Reinforcement Learning (William Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Chen, Oier Mees, Aviral Kumar, Sergey Levine. (2024)<br><strong>Vision-Language Models Provide Promptable Representations for Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Vision-Language Models Provide Promptable Representations for Reinforcement Learning" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Instruction Following, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02651v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02651v1.pdf filename=2402.02651v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with <b>reinforcement</b> <b>learning</b> (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in <b>vision-language</b> models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM&rsquo;s internal knowledge, as elicited through <b>prompts</b> that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms <b>instruction-following</b> <b>methods</b> and performs comparably to domain-specific embeddings.</p></p class="citation"></blockquote><h3 id=77285-assessing-the-impact-of-distribution-shift-on-reinforcement-learning-performance-ted-fujimoto-et-al-2024>(77/285) Assessing the Impact of Distribution Shift on Reinforcement Learning Performance (Ted Fujimoto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ted Fujimoto, Joshua Suetterlein, Samrat Chatterjee, Auroop Ganguly. (2024)<br><strong>Assessing the Impact of Distribution Shift on Reinforcement Learning Performance</strong><br><button class=copy-to-clipboard title="Assessing the Impact of Distribution Shift on Reinforcement Learning Performance" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs.LG<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Recommendation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03590v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03590v1.pdf filename=2402.03590v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research in machine learning is making progress in fixing its own reproducibility crisis. <b>Reinforcement</b> <b>learning</b> (RL), in particular, faces its own set of unique challenges. Comparison of point estimates, and plots that show successful convergence to the optimal policy during training, may obfuscate overfitting or dependence on the experimental setup. Although researchers in RL have proposed reliability metrics that account for uncertainty to better understand each algorithm&rsquo;s strengths and weaknesses, the <b>recommendations</b> of past work do not assume the presence of <b>out-of-distribution</b> observations. We propose a set of evaluation methods that measure the robustness of RL algorithms under distribution shifts. The tools presented here argue for the need to account for performance over time while the agent is acting in its environment. In particular, we recommend time series analysis as a method of observational RL evaluation. We also show that the unique properties of RL and simulated dynamic environments allow us to make stronger assumptions to justify the measurement of causal impact in our evaluations. We then apply these tools to single-agent and multi-agent environments to show the impact of introducing distribution shifts during test time. We present this methodology as a first step toward rigorous RL evaluation in the presence of distribution shifts.</p></p class="citation"></blockquote><h3 id=78285-effective-acquisition-functions-for-active-correlation-clustering-linus-aronsson-et-al-2024>(78/285) Effective Acquisition Functions for Active Correlation Clustering (Linus Aronsson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linus Aronsson, Morteza Haghir Chehreghani. (2024)<br><strong>Effective Acquisition Functions for Active Correlation Clustering</strong><br><button class=copy-to-clipboard title="Effective Acquisition Functions for Active Correlation Clustering" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Active Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03587v1.pdf filename=2402.03587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Correlation clustering is a powerful <b>unsupervised</b> <b>learning</b> paradigm that supports positive and negative similarities. In this paper, we assume the similarities are not known in advance. Instead, we employ <b>active</b> <b>learning</b> to iteratively query similarities in a cost-efficient way. In particular, we develop three effective acquisition functions to be used in this setting. One is based on the notion of inconsistency (i.e., when similarities violate the transitive property). The remaining two are based on information-theoretic quantities, i.e., entropy and information gain.</p></p class="citation"></blockquote><h3 id=79285-distinguishing-the-knowable-from-the-unknowable-with-language-models-gustaf-ahdritz-et-al-2024>(79/285) Distinguishing the Knowable from the Unknowable with Language Models (Gustaf Ahdritz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, Benjamin L. Edelman. (2024)<br><strong>Distinguishing the Knowable from the Unknowable with Language Models</strong><br><button class=copy-to-clipboard title="Distinguishing the Knowable from the Unknowable with Language Models" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Unsupervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03563v1.pdf filename=2402.03563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given <b>LLM&rsquo;s</b> uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully <b>unsupervised</b> method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that <b>LLMs</b> naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings.</p></p class="citation"></blockquote><h3 id=80285-single-gpu-gnn-systems-traps-and-pitfalls-yidong-gong-et-al-2024>(80/285) Single-GPU GNN Systems: Traps and Pitfalls (Yidong Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yidong Gong, Arnab Tarafder, Saima Afrin, Pradeep Kumar. (2024)<br><strong>Single-GPU GNN Systems: Traps and Pitfalls</strong><br><button class=copy-to-clipboard title="Single-GPU GNN Systems: Traps and Pitfalls" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Graph Neural Network, Graph Neural Network, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03548v1.pdf filename=2402.03548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The current <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> systems have established a clear trend of not showing training accuracy results, and directly or indirectly relying on smaller datasets for evaluations majorly. Our in-depth analysis shows that it leads to a chain of pitfalls in the system design and evaluation process, questioning the practicality of many of the proposed system optimizations, and affecting conclusions and lessons learned. We analyze many single-GPU systems and show the fundamental impact of these pitfalls. We further develop hypotheses, <b>recommendations,</b> and evaluation methodologies, and provide future directions. Finally, a new reference system is developed to establish a new line of optimizations rooted in solving the system-design pitfalls efficiently and practically. The proposed design can productively be integrated into prior works, thereby truly advancing the state-of-the-art.</p></p class="citation"></blockquote><h3 id=81285-hamlet-graph-transformer-neural-operator-for-partial-differential-equations-andrey-bryutkin-et-al-2024>(81/285) HAMLET: Graph Transformer Neural Operator for Partial Differential Equations (Andrey Bryutkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrey Bryutkin, Jiahao Huang, Zhongying Deng, Guang Yang, Carola-Bibiane Schönlieb, Angelica Aviles-Rivero. (2024)<br><strong>HAMLET: Graph Transformer Neural Operator for Partial Differential Equations</strong><br><button class=copy-to-clipboard title="HAMLET: Graph Transformer Neural Operator for Partial Differential Equations" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03541v1.pdf filename=2402.03541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel graph <b>transformer</b> framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks. The framework uses graph <b>transformers</b> with modular input encoders to directly incorporate differential equation information into the solution process. This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats. Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness. HAMLET is not just tailored to a single type of physical <b>simulation,</b> but can be applied across various domains. Moreover, it boosts model resilience and performance, especially in scenarios with limited data. We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs.</p></p class="citation"></blockquote><h3 id=82285-can-we-remove-the-square-root-in-adaptive-gradient-methods-a-second-order-perspective-wu-lin-et-al-2024>(82/285) Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective (Wu Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E. Turner, Alireza Makhzani. (2024)<br><strong>Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective</strong><br><button class=copy-to-clipboard title="Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 30<br>Keywords: Convolution, Stochastic Gradient Descent, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03496v1.pdf filename=2402.03496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as <b>transformers.</b> Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to <b>SGD</b> on <b>convolutional</b> architectures, while maintaining their root-based counterpart&rsquo;s performance on <b>transformers.</b> The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square roots and therefore work well in low precision, which we demonstrate empirically. This raises important questions regarding the currently overlooked role of adaptivity for the success of adaptive methods.</p></p class="citation"></blockquote><h3 id=83285-understanding-the-reasoning-ability-of-language-models-from-the-perspective-of-reasoning-paths-aggregation-xinyi-wang-et-al-2024>(83/285) Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation (Xinyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, William Yang Wang. (2024)<br><strong>Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation</strong><br><button class=copy-to-clipboard title="Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Reasoning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03268v1.pdf filename=2402.03268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>language</b> <b>models</b> (LMs) are able to perform complex <b>reasoning</b> without explicit <b>fine-tuning.</b> To understand how pre-training with a next-token prediction objective contributes to the emergence of such <b>reasoning</b> capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect <b>reasoning</b> paths seen at pre-training time. We found this perspective effective in two important cases of <b>reasoning:</b> logic <b>reasoning</b> with knowledge graphs (KGs) and math <b>reasoning</b> with math word problems (MWPs). More specifically, we formalize the <b>reasoning</b> paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk <b>reasoning</b> paths can improve real-world multi-step <b>reasoning</b> performance.</p></p class="citation"></blockquote><h3 id=84285-mobilitygpt-enhanced-human-mobility-modeling-with-a-gpt-model-ammar-haydari-et-al-2024>(84/285) MobilityGPT: Enhanced Human Mobility Modeling with a GPT model (Ammar Haydari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ammar Haydari, Dongjie Chen, Zhengfeng Lai, Chen-Nee Chuah. (2024)<br><strong>MobilityGPT: Enhanced Human Mobility Modeling with a GPT model</strong><br><button class=copy-to-clipboard title="MobilityGPT: Enhanced Human Mobility Modeling with a GPT model" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, GPT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03264v1.pdf filename=2402.03264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits. To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained <b>Transformer</b> <b>(GPT).</b> To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT. We propose a gravity-based sampling method to train a <b>transformer</b> for semantic sequence similarity. Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits. Lastly, we constructed a <b>Reinforcement</b> <b>Learning</b> from Trajectory Feedback (RLTF) to minimize the travel distance between training and the synthetically generated trajectories. Our experiments on real-world datasets demonstrate that MobilityGPT outperforms state-of-the-art methods in generating high-quality mobility trajectories that are closest to real data in terms of origin-destination similarity, trip length, travel radius, link, and gravity distributions.</p></p class="citation"></blockquote><h3 id=85285-less-is-ken-a-universal-and-simple-non-parametric-pruning-algorithm-for-large-language-models-michele-mastromattei-et-al-2024>(85/285) Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models (Michele Mastromattei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michele Mastromattei, Fabio Massimo Zanzotto. (2024)<br><strong>Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models</strong><br><button class=copy-to-clipboard title="Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Pruning, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03142v1.pdf filename=2402.03142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural network <b>pruning</b> has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing <b>pruning</b> algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on complex calculations, rendering them impractical for real-world applications. In this paper, we propose KEN: a straightforward, universal and unstructured <b>pruning</b> algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized <b>transformer</b> models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings. Extensive evaluations on seven <b>transformer</b> models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other <b>pruning</b> and PEFT algorithms confirm KEN effectiveness. Furthermore, we introduce KEN_viz, an explainable tool that visualizes the optimized model composition and the subnetwork selected by KEN.</p></p class="citation"></blockquote><h3 id=86285-fine-tuning-reinforcement-learning-models-is-secretly-a-forgetting-mitigation-problem-maciej-wołczyk-et-al-2024>(86/285) Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem (Maciej Wołczyk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maciej Wołczyk, Bartłomiej Cupiał, Mateusz Ostaszewski, Michał Bortkiewicz, Michał Zając, Razvan Pascanu, Łukasz Kuciński, Piotr Miłoś. (2024)<br><strong>Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem</strong><br><button class=copy-to-clipboard title="Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Foundation Model, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02868v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02868v1.pdf filename=2402.02868v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of <b>foundation</b> <b>models.</b> However, <b>fine-tuning</b> <b>reinforcement</b> <b>learning</b> (RL) models remains a challenge. This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: forgetting of pre-trained capabilities. Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of <b>fine-tuning,</b> on which the model behaved well due to pre-training. This way, we lose the anticipated transfer benefits. We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic. Through a detailed empirical analysis of the challenging NetHack and Montezuma&rsquo;s Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the pre-trained capabilities. In particular, in NetHack, we achieve a new state-of-the-art for neural models, improving the previous best score from $5$K to over $10$K points in the Human Monk scenario.</p></p class="citation"></blockquote><h3 id=87285-revisiting-vae-for-unsupervised-time-series-anomaly-detection-a-frequency-perspective-zexin-wang-et-al-2024>(87/285) Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective (Zexin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zexin Wang, Changhua Pei, Minghua Ma, Xin Wang, Zhihan Li, Dan Pei, Saravan Rajmohan, Dongmei Zhang, Qingwei Lin, Haiming Zhang, Jianhui Li, Gaogang Xie. (2024)<br><strong>Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective</strong><br><button class=copy-to-clipboard title="Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Autoencoder, Unsupervised Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02820v1.pdf filename=2402.02820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series Anomaly Detection (AD) plays a crucial role for web systems. Various web systems rely on time series data to monitor and identify anomalies in real time, as well as to initiate diagnosis and remediation procedures. <b>Variational</b> <b>Autoencoders</b> (VAEs) have gained popularity in recent decades due to their superior de-noising capabilities, which are useful for anomaly detection. However, our study reveals that VAE-based methods face challenges in capturing long-periodic heterogeneous patterns and detailed short-periodic trends simultaneously. To address these challenges, we propose Frequency-enhanced Conditional <b>Variational</b> <b>Autoencoder</b> (FCVAE), a novel <b>unsupervised</b> AD method for univariate time series. To ensure an accurate AD, FCVAE exploits an innovative approach to concurrently integrate both the global and local frequency features into the condition of Conditional <b>Variational</b> <b>Autoencoder</b> (CVAE) to significantly increase the accuracy of reconstructing the normal data. Together with a carefully designed &ldquo;target attention&rdquo; mechanism, our approach allows the model to pick the most useful information from the frequency domain for better short-periodic trend construction. Our FCVAE has been evaluated on public datasets and a large-scale cloud system, and the results demonstrate that it outperforms state-of-the-art methods. This confirms the practical applicability of our approach in addressing the limitations of current VAE-based anomaly detection models.</p></p class="citation"></blockquote><h3 id=88285-position-paper-what-can-large-language-models-tell-us-about-time-series-analysis-ming-jin-et-al-2024>(88/285) Position Paper: What Can Large Language Models Tell Us about Time Series Analysis (Ming Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang, Shirui Pan, Qingsong Wen. (2024)<br><strong>Position Paper: What Can Large Language Models Tell Us about Time Series Analysis</strong><br><button class=copy-to-clipboard title="Position Paper: What Can Large Language Models Tell Us about Time Series Analysis" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02713v1.pdf filename=2402.02713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series analysis is essential for comprehending the complexities inherent in various real-world systems and applications. Although <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current <b>LLMs</b> have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including modality switching and time series <b>question</b> <b>answering.</b> We encourage researchers and practitioners to recognize the potential of <b>LLMs</b> in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing <b>LLM</b> technologies and outline promising avenues for future research.</p></p class="citation"></blockquote><h3 id=89285-beyond-expectations-learning-with-stochastic-dominance-made-practical-shicong-cen-et-al-2024>(89/285) Beyond Expectations: Learning with Stochastic Dominance Made Practical (Shicong Cen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shicong Cen, Jincheng Mei, Hanjun Dai, Dale Schuurmans, Yuejie Chi, Bo Dai. (2024)<br><strong>Beyond Expectations: Learning with Stochastic Dominance Made Practical</strong><br><button class=copy-to-clipboard title="Beyond Expectations: Learning with Stochastic Dominance Made Practical" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02698v1.pdf filename=2402.02698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stochastic dominance models risk-averse preferences for decision making with uncertain outcomes, which naturally captures the intrinsic structure of the underlying uncertainty, in contrast to simply resorting to the expectations. Despite theoretically appealing, the application of stochastic dominance in machine learning has been scarce, due to the following challenges: $\textbf{i)}$, the original concept of stochastic dominance only provides a $\textit{partial order}$, therefore, is not amenable to serve as an optimality criterion; and $\textbf{ii)}$, an efficient computational recipe remains lacking due to the continuum nature of evaluating stochastic dominance.%, which barriers its application for machine learning. In this work, we make the first attempt towards establishing a general framework of learning with stochastic dominance. We first generalize the stochastic dominance concept to enable feasible comparisons between any arbitrary pair of random variables. We next develop a simple and computationally efficient approach for finding the optimal solution in terms of stochastic dominance, which can be seamlessly plugged into many learning tasks. Numerical experiments demonstrate that the proposed method achieves comparable performance as standard risk-neutral strategies and obtains better trade-offs against risk across a variety of applications including <b>supervised</b> <b>learning,</b> <b>reinforcement</b> <b>learning,</b> and portfolio optimization.</p></p class="citation"></blockquote><h3 id=90285-statistical-guarantees-for-link-prediction-using-graph-neural-networks-alan-chung-et-al-2024>(90/285) Statistical Guarantees for Link Prediction using Graph Neural Networks (Alan Chung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alan Chung, Amin Saberi, Morgane Austern. (2024)<br><strong>Statistical Guarantees for Link Prediction using Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Statistical Guarantees for Link Prediction using Graph Neural Networks" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 30<br>Keywords: Graph Convolutional Network, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02692v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02692v2.pdf filename=2402.02692v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper derives statistical guarantees for the performance of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> in link prediction tasks on <b>graphs</b> <b>generated</b> <b>by</b> a graphon. We propose a linear <b>GNN</b> architecture (LG-GNN) that produces consistent estimators for the underlying edge probabilities. We establish a bound on the mean squared error and give guarantees on the ability of LG-GNN to detect high-probability edges. Our guarantees hold for both sparse and dense <b>graphs.</b> <b>Finally,</b> <b>we</b> demonstrate some of the shortcomings of the classical <b>GCN</b> architecture, as well as verify our results on real and synthetic datasets.</p></p class="citation"></blockquote><h3 id=91285-fusemoe-mixture-of-experts-transformers-for-fleximodal-fusion-xing-han-et-al-2024>(91/285) FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion (Xing Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xing Han, Huy Nguyen, Carl Harris, Nhat Ho, Suchi Saria. (2024)<br><strong>FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion</strong><br><button class=copy-to-clipboard title="FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Graph Attention Networks, Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03226v1.pdf filename=2402.03226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As machine learning models in critical fields increasingly grapple with <b>multimodal</b> data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models&rsquo; predictive performance. We introduce ``FuseMoE&rsquo;&rsquo;, a mixture-of-experts framework incorporated with an innovative <b>gating</b> function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique <b>gating</b> function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.</p></p class="citation"></blockquote><h3 id=92285-continual-domain-adversarial-adaptation-via-double-head-discriminators-yan-shen-et-al-2024>(92/285) Continual Domain Adversarial Adaptation via Double-Head Discriminators (Yan Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Shen, Zhanghexuan Ji, Chunwei Ma, Mingchen Gao. (2024)<br><strong>Continual Domain Adversarial Adaptation via Double-Head Discriminators</strong><br><button class=copy-to-clipboard title="Continual Domain Adversarial Adaptation via Double-Head Discriminators" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Continual Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03588v1.pdf filename=2402.03588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Domain</b> <b>adversarial</b> adaptation in a <b>continual</b> <b>setting</b> poses a significant challenge due to the limitations on accessing previous source <b>domain</b> <b>data.</b> Despite extensive research in <b>continual</b> <b>learning,</b> the task of adversarial adaptation cannot be effectively accomplished using only a small number of stored source <b>domain</b> <b>data,</b> which is a standard setting in memory replay approaches. This limitation arises from the erroneous empirical estimation of $\gH$-divergence with few source <b>domain</b> <b>samples.</b> To tackle this problem, we propose a double-head discriminator algorithm, by introducing an addition source-only <b>domain</b> <b>discriminator</b> that are trained solely on source learning phase. We prove that with the introduction of a pre-trained source-only <b>domain</b> <b>discriminator,</b> the empirical estimation error of $\gH$-divergence related adversarial loss is reduced from the source <b>domain</b> <b>side.</b> Further experiments on existing <b>domain</b> <b>adaptation</b> benchmark show that our proposed algorithm achieves more than 2$%$ improvement on all categories of target <b>domain</b> <b>adaptation</b> task while significantly mitigating the forgetting on source domain.</p></p class="citation"></blockquote><h3 id=93285-generalization-properties-of-adversarial-training-for-ell_0-bounded-adversarial-attacks-payam-delgosha-et-al-2024>(93/285) Generalization Properties of Adversarial Training for $\ell_0$-Bounded Adversarial Attacks (Payam Delgosha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Payam Delgosha, Hamed Hassani, Ramtin Pedarsani. (2024)<br><strong>Generalization Properties of Adversarial Training for $\ell_0$-Bounded Adversarial Attacks</strong><br><button class=copy-to-clipboard title="Generalization Properties of Adversarial Training for $\ell_0$-Bounded Adversarial Attacks" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03576v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03576v1.pdf filename=2402.03576v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We have widely observed that neural networks are vulnerable to small additive perturbations to the input causing misclassification. In this paper, we focus on the $\ell_0$-bounded <b>adversarial</b> <b>attacks,</b> and aim to theoretically characterize the performance of <b>adversarial</b> <b>training</b> for an important class of truncated classifiers. Such classifiers are shown to have strong performance empirically, as well as theoretically in the Gaussian mixture model, in the $\ell_0$-adversarial setting. The main contribution of this paper is to prove a novel generalization bound for the binary classification setting with $\ell_0$-bounded <b>adversarial</b> <b>perturbation</b> that is distribution-independent. Deriving a generalization bound in this setting has two main challenges: (i) the truncated inner product which is highly non-linear; and (ii) maximization over the $\ell_0$ ball due to <b>adversarial</b> <b>training</b> is non-convex and highly non-smooth. To tackle these challenges, we develop new coding techniques for bounding the combinatorial dimension of the truncated hypothesis class.</p></p class="citation"></blockquote><h3 id=94285-online-feature-updates-improve-online-generalized-label-shift-adaptation-ruihan-wu-et-al-2024>(94/285) Online Feature Updates Improve Online (Generalized) Label Shift Adaptation (Ruihan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruihan Wu, Siddhartha Datta, Yi Su, Dheeraj Baby, Yu-Xiang Wang, Kilian Q. Weinberger. (2024)<br><strong>Online Feature Updates Improve Online (Generalized) Label Shift Adaptation</strong><br><button class=copy-to-clipboard title="Online Feature Updates Improve Online (Generalized) Label Shift Adaptation" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03545v1.pdf filename=2402.03545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages <b>self-supervised</b> <b>learning</b> to refine the feature extraction process, thereby improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on <b>self-supervised</b> <b>learning</b> for feature refinement. Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts.</p></p class="citation"></blockquote><h3 id=95285-early-prediction-of-onset-of-sepsis-in-clinical-setting-fahim-mohammad-et-al-2024>(95/285) Early prediction of onset of sepsis in Clinical Setting (Fahim Mohammad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fahim Mohammad, Lakshmi Arunachalam, Samanway Sadhu, Boudewijn Aasman, Shweta Garg, Adil Ahmed, Silvie Colman, Meena Arunachalam, Sudhir Kulkarni, Parsa Mirhaji. (2024)<br><strong>Early prediction of onset of sepsis in Clinical Setting</strong><br><button class=copy-to-clipboard title="Early prediction of onset of sepsis in Clinical Setting" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03486v1.pdf filename=2402.03486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study proposes the use of Machine Learning models to predict the early onset of sepsis using deidentified clinical data from Montefiore Medical Center in Bronx, NY, USA. A <b>supervised</b> <b>learning</b> approach was adopted, wherein an XGBoost model was trained utilizing 80% of the train dataset, encompassing 107 features (including the original and derived features). Subsequently, the model was evaluated on the remaining 20% of the test data. The model was validated on prospective data that was entirely unseen during the training phase. To assess the model&rsquo;s performance at the individual patient level and timeliness of the prediction, a normalized utility score was employed, a widely recognized scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis Challenge paper. Metrics such as F1 Score, Sensitivity, Specificity, and Flag Rate were also devised. The model achieved a normalized utility score of 0.494 on test data and 0.378 on prospective data at threshold 0.3. The F1 scores were 80.8% and 67.1% respectively for the test data and the prospective data for the same threshold, highlighting its potential to be integrated into clinical decision-making processes effectively. These results bear testament to the model&rsquo;s robust predictive capabilities and its potential to substantially impact clinical decision-making processes.</p></p class="citation"></blockquote><h3 id=96285-zero-shot-object-level-ood-detection-with-context-aware-inpainting-quang-huy-nguyen-et-al-2024>(96/285) Zero-shot Object-Level OOD Detection with Context-Aware Inpainting (Quang-Huy Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quang-Huy Nguyen, Jin Peng Zhou, Zhenzhen Liu, Khanh-Huyen Bui, Kilian Q. Weinberger, Dung D. Le. (2024)<br><strong>Zero-shot Object-Level OOD Detection with Context-Aware Inpainting</strong><br><button class=copy-to-clipboard title="Zero-shot Object-Level OOD Detection with Context-Aware Inpainting" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Out-of-distribution, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03292v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03292v2.pdf filename=2402.03292v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of <b>zero-shot</b> <b>out-of-distribution</b> (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier&rsquo;s label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in <b>zero-shot</b> and non-zero-shot settings.</p></p class="citation"></blockquote><h3 id=97285-a-framework-for-partially-observed-reward-states-in-rlhf-chinmaya-kausik-et-al-2024>(97/285) A Framework for Partially Observed Reward-States in RLHF (Chinmaya Kausik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chinmaya Kausik, Mirco Mutti, Aldo Pacchiano, Ambuj Tewari. (2024)<br><strong>A Framework for Partially Observed Reward-States in RLHF</strong><br><button class=copy-to-clipboard title="A Framework for Partially Observed Reward-States in RLHF" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03282v1.pdf filename=2402.03282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of <b>reinforcement</b> <b>learning</b> from human feedback (RLHF) has gained prominence in recent years due to its role in the development of <b>LLMs.</b> Neuroscience research shows that human responses to stimuli are known to depend on partially-observed &ldquo;internal states.&rdquo; Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as <b>reinforcement</b> <b>learning</b> with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regret. We then present the first explicit reduction that converts guarantees for cardinal regret to dueling regret. We show that our models and guarantees in both settings generalize and extend existing ones. Finally, we identify a recursive structure on our model that could improve the statistical and computational tractability of PORRL, giving examples from past work on RLHF as well as learning perfect reward machines, which PORRL subsumes.</p></p class="citation"></blockquote><h3 id=98285-rethink-model-re-basin-and-the-linear-mode-connectivity-xingyu-qu-et-al-2024>(98/285) Rethink Model Re-Basin and the Linear Mode Connectivity (Xingyu Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Qu, Samuel Horvath. (2024)<br><strong>Rethink Model Re-Basin and the Linear Mode Connectivity</strong><br><button class=copy-to-clipboard title="Rethink Model Re-Basin and the Linear Mode Connectivity" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Pruning, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05966v1.pdf filename=2402.05966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies suggest that with sufficiently wide models, most <b>SGD</b> solutions can, up to permutation, converge into the same basin. This phenomenon, known as the model re-basin regime, has significant implications for model averaging. However, current re-basin strategies are limited in effectiveness due to a lack of comprehensive understanding of underlying mechanisms. Addressing this gap, our work revisits standard practices and uncovers the frequent inadequacies of existing matching algorithms, which we show can be mitigated through proper re-normalization. By introducing a more direct analytical approach, we expose the interaction between matching algorithms and re-normalization processes. This perspective not only clarifies and refines previous findings but also facilitates novel insights. For instance, it connects the linear mode connectivity to <b>pruning,</b> motivating a lightweight yet effective post-pruning plug-in that can be directly merged with any existing <b>pruning</b> techniques. Our implementation is available at <a href=https://github.com/XingyuQu/rethink-re-basin>https://github.com/XingyuQu/rethink-re-basin</a>.</p></p class="citation"></blockquote><h3 id=99285-toward-green-and-human-like-artificial-intelligence-a-complete-survey-on-contemporary-few-shot-learning-approaches-georgios-tsoumplekas-et-al-2024>(99/285) Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches (Georgios Tsoumplekas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georgios Tsoumplekas, Vladislav Li, Vasileios Argyriou, Anastasios Lytos, Eleftherios Fountoukidis, Sotirios K. Goudos, Ioannis D. Moscholios, Panagiotis Sarigiannidis. (2024)<br><strong>Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches</strong><br><button class=copy-to-clipboard title="Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03017v1.pdf filename=2402.03017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite deep learning&rsquo;s widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications. <b>Few-Shot</b> <b>Learning</b> (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years. This survey provides a comprehensive overview of the field&rsquo;s latest advancements. Initially, FSL is formally defined, and its relationship with different learning fields is presented. A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described. Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed.</p></p class="citation"></blockquote><h3 id=100285-on-the-development-of-a-practical-bayesian-optimisation-algorithm-for-expensive-experiments-and-simulations-with-changing-environmental-conditions-mike-diessner-et-al-2024>(100/285) On the development of a practical Bayesian optimisation algorithm for expensive experiments and simulations with changing environmental conditions (Mike Diessner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mike Diessner, Kevin J. Wilson, Richard D. Whalley. (2024)<br><strong>On the development of a practical Bayesian optimisation algorithm for expensive experiments and simulations with changing environmental conditions</strong><br><button class=copy-to-clipboard title="On the development of a practical Bayesian optimisation algorithm for expensive experiments and simulations with changing environmental conditions" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03006v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03006v1.pdf filename=2402.03006v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Experiments in engineering are typically conducted in controlled environments where parameters can be set to any desired value. This assumes that the same applies in a real-world setting &ndash; an assumption that is often incorrect as many experiments are influenced by uncontrollable environmental conditions such as temperature, humidity and wind speed. When optimising such experiments, the focus should lie on finding optimal values conditionally on these uncontrollable variables. This article extends Bayesian optimisation to the optimisation of systems in changing environments that include controllable and uncontrollable parameters. The extension fits a global surrogate model over all controllable and environmental variables but optimises only the controllable parameters conditional on measurements of the uncontrollable variables. The method is validated on two synthetic test functions and the effects of the noise level, the number of the environmental parameters, the parameter fluctuation, the variability of the uncontrollable parameters, and the effective domain size are investigated. ENVBO, the proposed algorithm resulting from this investigation, is applied to a wind farm simulator with eight controllable and one environmental parameter. ENVBO finds solutions for the full domain of the environmental variable that outperforms results from optimisation algorithms that only focus on a fixed environmental value in all but one case while using a fraction of their evaluation budget. This makes the proposed approach very sample-efficient and cost-effective. An off-the-shelf open-source version of ENVBO is available via the NUBO Python package.</p></p class="citation"></blockquote><h3 id=101285-ds-ms-tcn-otago-exercises-recognition-with-a-dual-scale-multi-stage-temporal-convolutional-network-meng-shang-et-al-2024>(101/285) DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network (Meng Shang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Shang, Lenore Dedeyne, Jolan Dupont, Laura Vercauteren, Nadjia Amini, Laurence Lapauw, Evelien Gielen, Sabine Verschueren, Carolina Varon, Walter De Raedt, Bart Vanrumste. (2024)<br><strong>DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network</strong><br><button class=copy-to-clipboard title="DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02910v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02910v2.pdf filename=2402.02910v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength. Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness. This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives. A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments. The study proposes a Dual-Scale Multi-Stage Temporal <b>Convolutional</b> <b>Network</b> (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function. In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels). Subsequent stages extend the recognition to encompass the complete range of exercises (macro labels). The DS-MS-TCN model surpasses existing state-of-the-art deep learning models, achieving f1-scores exceeding 80% and Intersection over Union (IoU) f1-scores surpassing 60% for all four exercises evaluated. Notably, the model outperforms the prior study utilizing the sliding window technique, eliminating the need for post-processing stages and window size tuning. To our knowledge, we are the first to present a novel perspective on enhancing Human Activity Recognition (HAR) systems through the recognition of each repetition of activities.</p></p class="citation"></blockquote><h3 id=102285-deep-autoregressive-density-nets-vs-neural-ensembles-for-model-based-offline-reinforcement-learning-abdelhakim-benechehab-et-al-2024>(102/285) Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning (Abdelhakim Benechehab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdelhakim Benechehab, Albert Thomas, Balázs Kégl. (2024)<br><strong>Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning</strong><br><button class=copy-to-clipboard title="Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02858v1.pdf filename=2402.02858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of <b>offline</b> <b>reinforcement</b> <b>learning</b> where only a set of system transitions is made available for policy optimization. Following recent advances in the field, we consider a model-based <b>reinforcement</b> <b>learning</b> algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts. This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system. The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain. We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark. We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent.</p></p class="citation"></blockquote><h3 id=103285-contrastive-diffuser-planning-towards-high-return-states-via-contrastive-learning-yixiang-shan-et-al-2024>(103/285) Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning (Yixiang Shan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixiang Shan, Zhengbang Zhu, Ting Long, Qifan Liang, Yi Chang, Weinan Zhang, Liang Yin. (2024)<br><strong>Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning</strong><br><button class=copy-to-clipboard title="Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02772v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02772v2.pdf filename=2402.02772v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Applying diffusion models in <b>reinforcement</b> <b>learning</b> for long-term planning has gained much attention recently. Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions. These methods generate subsequent trajectories for planning and have demonstrated significant improvement. However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns. They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset. As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution. Even equipped with the guidance model, the performance is still suppressed. To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return contrast mechanism to pull the states in generated trajectories towards high-return states while pushing them away from low-return states to improve the base distribution. Experiments on 14 commonly used D4RL benchmarks demonstrate the effectiveness of our proposed method.</p></p class="citation"></blockquote><h3 id=104285-counterfactual-fairness-is-not-demographic-parity-and-other-observations-ricardo-silva-2024>(104/285) Counterfactual Fairness Is Not Demographic Parity, and Other Observations (Ricardo Silva, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ricardo Silva. (2024)<br><strong>Counterfactual Fairness Is Not Demographic Parity, and Other Observations</strong><br><button class=copy-to-clipboard title="Counterfactual Fairness Is Not Demographic Parity, and Other Observations" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Counter-factual, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02663v1.pdf filename=2402.02663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Blanket statements of equivalence between causal concepts and purely probabilistic concepts should be approached with care. In this short note, I examine a recent claim that <b>counterfactual</b> <b>fairness</b> is equivalent to demographic parity. The claim fails to hold up upon closer examination. I will take the opportunity to address some broader misunderstandings about <b>counterfactual</b> <b>fairness.</b></p></p class="citation"></blockquote><h3 id=105285-learning-with-mixture-of-prototypes-for-out-of-distribution-detection-haodong-lu-et-al-2024>(105/285) Learning with Mixture of Prototypes for Out-of-Distribution Detection (Haodong Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haodong Lu, Dong Gong, Shuo Wang, Jason Xue, Lina Yao, Kristen Moore. (2024)<br><strong>Learning with Mixture of Prototypes for Out-of-Distribution Detection</strong><br><button class=copy-to-clipboard title="Learning with Mixture of Prototypes for Out-of-Distribution Detection" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Out-of-distribution, PaLM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02653v1.pdf filename=2402.02653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world. Distance-based OOD detection methods have emerged with enhanced deep representation learning. They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes. However, existing approaches learn the representation relying on oversimplified data assumptions, e.g, modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data. Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance. To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes <b>(PALM)</b> which models each class with multiple prototypes to capture the sample diversities, and learns more faithful and compact samples embeddings to enhance OOD detection. Our method automatically identifies and dynamically updates prototypes, assigning each sample to a subset of prototypes via reciprocal neighbor soft assignment weights. <b>PALM</b> optimizes a maximum likelihood estimation (MLE) loss to encourage the sample embeddings to be compact around the associated prototypes, as well as a contrastive loss on all prototypes to enhance intra-class compactness and inter-class discrimination at the prototype level. Moreover, the automatic estimation of prototypes enables our approach to be extended to the challenging OOD detection task with unlabelled ID data. Extensive experiments demonstrate the superiority of <b>PALM,</b> achieving state-of-the-art average AUROC performance of 93.82 on the challenging CIFAR-100 benchmark. Code is available at <a href=https://github.com/jeff024/PALM>https://github.com/jeff024/PALM</a>.</p></p class="citation"></blockquote><h3 id=106285-how-good-is-a-single-basin-kai-lion-et-al-2024>(106/285) How Good is a Single Basin? (Kai Lion et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Lion, Lorenzo Noci, Thomas Hofmann, Gregor Bachmann. (2024)<br><strong>How Good is a Single Basin?</strong><br><button class=copy-to-clipboard title="How Good is a Single Basin?" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Knowledge Distillation, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03187v1.pdf filename=2402.03187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>multi-modal</b> nature of neural loss landscapes is often considered to be the main driver behind the empirical success of deep ensembles. In this work, we probe this belief by constructing various &ldquo;connected&rdquo; ensembles which are restricted to lie in the same basin. Through our experiments, we demonstrate that increased connectivity indeed negatively impacts performance. However, when incorporating the knowledge from other basins implicitly through <b>distillation,</b> we show that the gap in performance can be mitigated by re-discovering (multi-basin) deep ensembles within a single basin. Thus, we conjecture that while the extra-basin knowledge is at least partially present in any given basin, it cannot be easily harnessed without learning it from other basins.</p></p class="citation"></blockquote><h3 id=107285-a-reinforcement-learning-approach-for-dynamic-rebalancing-in-bike-sharing-system-jiaqi-liang-et-al-2024>(107/285) A Reinforcement Learning Approach for Dynamic Rebalancing in Bike-Sharing System (Jiaqi Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Liang, Sanjay Dominik Jena, Defeng Liu, Andrea Lodi. (2024)<br><strong>A Reinforcement Learning Approach for Dynamic Rebalancing in Bike-Sharing System</strong><br><button class=copy-to-clipboard title="A Reinforcement Learning Approach for Dynamic Rebalancing in Bike-Sharing System" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03589v1.pdf filename=2402.03589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bike-Sharing Systems provide eco-friendly urban mobility, contributing to the alleviation of traffic congestion and to healthier lifestyles. Efficiently operating such systems and maintaining high customer satisfaction is challenging due to the stochastic nature of trip demand, leading to full or empty stations. Devising effective rebalancing strategies using vehicles to redistribute bikes among stations is therefore of uttermost importance for operators. As a promising alternative to classical mathematical optimization, <b>reinforcement</b> <b>learning</b> is gaining ground to solve sequential decision-making problems. This paper introduces a spatio-temporal <b>reinforcement</b> <b>learning</b> algorithm for the dynamic rebalancing problem with multiple vehicles. We first formulate the problem as a Multi-agent Markov Decision Process in a continuous time framework. This allows for independent and cooperative vehicle rebalancing, eliminating the impractical restriction of time-discretized models where vehicle departures are synchronized. A comprehensive simulator under the first-arrive-first-serve rule is then developed to facilitate the learning process by computing immediate rewards under diverse demand scenarios. To estimate the value function and learn the rebalancing policy, various Deep Q-Network configurations are tested, minimizing the lost demand. Experiments are carried out on various datasets generated from historical data, affected by both temporal and weather factors. The proposed algorithms outperform benchmarks, including a multi-period Mixed-Integer Programming model, in terms of lost demand. Once trained, it yields immediate decisions, making it suitable for real-time applications. Our work offers practical insights for operators and enriches the integration of <b>reinforcement</b> <b>learning</b> into dynamic rebalancing problems, paving the way for more intelligent and robust urban mobility solutions.</p></p class="citation"></blockquote><h3 id=108285-deconstructing-the-goldilocks-zone-of-neural-network-initialization-artem-vysogorets-et-al-2024>(108/285) Deconstructing the Goldilocks Zone of Neural Network Initialization (Artem Vysogorets et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Artem Vysogorets, Anna Dawid, Julia Kempe. (2024)<br><strong>Deconstructing the Goldilocks Zone of Neural Network Initialization</strong><br><button class=copy-to-clipboard title="Deconstructing the Goldilocks Zone of Neural Network Initialization" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03579v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03579v1.pdf filename=2402.03579v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The second-order properties of the training loss have a massive impact on the optimization dynamics of deep learning models. Fort & Scherlis (2019) discovered that a high positive curvature and local convexity of the loss Hessian are associated with highly trainable initial points located in a region coined the &ldquo;Goldilocks zone&rdquo;. Only a handful of subsequent studies touched upon this relationship, so it remains largely unexplained. In this paper, we present a rigorous and comprehensive analysis of the Goldilocks zone for homogeneous neural networks. In particular, we derive the fundamental condition resulting in non-zero positive curvature of the loss Hessian and argue that it is only incidentally related to the initialization norm, contrary to prior beliefs. Further, we relate high positive curvature to model confidence, low initial loss, and a previously unknown type of vanishing cross-entropy loss gradient. To understand the importance of positive curvature for trainability of deep networks, we optimize both fully-connected and <b>convolutional</b> architectures outside the Goldilocks zone and analyze the emergent behaviors. We find that strong model performance is not necessarily aligned with the Goldilocks zone, which questions the practical significance of this concept.</p></p class="citation"></blockquote><h3 id=109285-revisiting-the-dataset-bias-problem-from-a-statistical-perspective-kien-do-et-al-2024>(109/285) Revisiting the Dataset Bias Problem from a Statistical Perspective (Kien Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kien Do, Dung Nguyen, Hung Le, Thao Le, Dang Nguyen, Haripriya Harikumar, Truyen Tran, Santu Rana, Svetha Venkatesh. (2024)<br><strong>Revisiting the Dataset Bias Problem from a Statistical Perspective</strong><br><button class=copy-to-clipboard title="Revisiting the Dataset Bias Problem from a Statistical Perspective" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03577v1.pdf filename=2402.03577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the &ldquo;dataset bias&rdquo; problem from a statistical standpoint, and identify the main cause of the problem as the strong correlation between a class attribute u and a non-class attribute b in the input x, represented by p(u|b) differing significantly from p(u). Since p(u|b) appears as part of the sampling distributions in the standard maximum log-likelihood (MLL) objective, a model trained on a biased dataset via MLL inherently incorporates such correlation into its parameters, leading to poor generalization to unbiased test data. From this observation, we propose to mitigate dataset bias via either weighting the objective of each sample n by \frac{1}{p(u_{n}|b_{n})} or sampling that sample with a weight proportional to \frac{1}{p(u_{n}|b_{n})}. While both methods are statistically equivalent, the former proves more stable and effective in practice. Additionally, we establish a connection between our debiasing approach and causal <b>reasoning,</b> reinforcing our method&rsquo;s theoretical foundation. However, when the bias label is unavailable, computing p(u|b) exactly is difficult. To overcome this challenge, we propose to approximate \frac{1}{p(u|b)} using a biased classifier trained with &ldquo;bias amplification&rdquo; losses. Extensive experiments on various biased datasets demonstrate the superiority of our method over existing debiasing techniques in most settings, validating our theoretical analysis.</p></p class="citation"></blockquote><h3 id=110285-regulation-games-for-trustworthy-machine-learning-mohammad-yaghini-et-al-2024>(110/285) Regulation Games for Trustworthy Machine Learning (Mohammad Yaghini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Yaghini, Patty Liu, Franziska Boenisch, Nicolas Papernot. (2024)<br><strong>Regulation Games for Trustworthy Machine Learning</strong><br><button class=copy-to-clipboard title="Regulation Games for Trustworthy Machine Learning" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-GT, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03540v1.pdf filename=2402.03540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing work on trustworthy machine learning (ML) often concentrates on individual aspects of trust, such as <b>fairness</b> or privacy. Additionally, many techniques overlook the distinction between those who train ML models and those responsible for assessing their trustworthiness. To address these issues, we propose a framework that views trustworthy ML as a multi-objective multi-agent optimization problem. This naturally lends itself to a game-theoretic formulation we call regulation games. We illustrate a particular game instance, the SpecGame in which we model the relationship between an ML model builder and <b>fairness</b> and privacy regulators. Regulators wish to design penalties that enforce compliance with their specification, but do not want to discourage builders from participation. Seeking such socially optimal (i.e., efficient for all agents) solutions to the game, we introduce ParetoPlay. This novel equilibrium search algorithm ensures that agents remain on the Pareto frontier of their objectives and avoids the inefficiencies of other equilibria. Simulating SpecGame through ParetoPlay can provide policy guidance for ML Regulation. For instance, we show that for a gender classification application, regulators can enforce a differential privacy budget that is on average 4.0 lower if they take the initiative to specify their desired guarantee first.</p></p class="citation"></blockquote><h3 id=111285-deep-reinforcement-learning-for-picker-routing-problem-in-warehousing-george-dunn-et-al-2024>(111/285) Deep Reinforcement Learning for Picker Routing Problem in Warehousing (George Dunn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George Dunn, Hadi Charkhgard, Ali Eshragh, Sasan Mahmoudinazlou, Elizabeth Stojanovski. (2024)<br><strong>Deep Reinforcement Learning for Picker Routing Problem in Warehousing</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning for Picker Routing Problem in Warehousing" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03525v1.pdf filename=2402.03525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Order Picker Routing is a critical issue in Warehouse Operations Management. Due to the complexity of the problem and the need for quick solutions, suboptimal algorithms are frequently employed in practice. However, <b>Reinforcement</b> <b>Learning</b> offers an appealing alternative to traditional heuristics, potentially outperforming existing methods in terms of speed and accuracy. We introduce an attention based neural network for modeling picker tours, which is trained using <b>Reinforcement</b> <b>Learning.</b> Our method is evaluated against existing heuristics across a range of problem parameters to demonstrate its efficacy. A key advantage of our proposed method is its ability to offer an option to reduce the perceived complexity of routes.</p></p class="citation"></blockquote><h3 id=112285-how-does-unlabeled-data-provably-help-out-of-distribution-detection-xuefeng-du-et-al-2024>(112/285) How Does Unlabeled Data Provably Help Out-of-Distribution Detection? (Xuefeng Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuefeng Du, Zhen Fang, Ilias Diakonikolas, Yixuan Li. (2024)<br><strong>How Does Unlabeled Data Provably Help Out-of-Distribution Detection?</strong><br><button class=copy-to-clipboard title="How Does Unlabeled Data Provably Help Out-of-Distribution Detection?" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03502v1.pdf filename=2402.03502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting <b>out-of-distribution</b> (OOD) data. Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data. This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier. Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection. This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness. The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data. Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm. Our theory shows that SAL can separate the candidate outliers with small error rates, which leads to a generalization guarantee for the learned OOD classifier. Empirically, SAL achieves state-of-the-art performance on common benchmarks, reinforcing our theoretical insights. Code is publicly available at <a href=https://github.com/deeplearning-wisc/sal>https://github.com/deeplearning-wisc/sal</a>.</p></p class="citation"></blockquote><h3 id=113285-fair-active-ranking-from-pairwise-preferences-sruthi-gorantla-et-al-2024>(113/285) Fair Active Ranking from Pairwise Preferences (Sruthi Gorantla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sruthi Gorantla, Sara Ahmadian. (2024)<br><strong>Fair Active Ranking from Pairwise Preferences</strong><br><button class=copy-to-clipboard title="Fair Active Ranking from Pairwise Preferences" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03252v1.pdf filename=2402.03252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the problem of probably approximately correct and fair (PACF) ranking of items by adaptively evoking pairwise comparisons. Given a set of $n$ items that belong to disjoint groups, our goal is to find an $(\epsilon, \delta)$-PACF-Ranking according to a fair objective function that we propose. We assume access to an oracle, wherein, for each query, the learner can choose a pair of items and receive stochastic winner feedback from the oracle. Our proposed objective function asks to minimize the $\ell_q$ norm of the error of the groups, where the error of a group is the $\ell_p$ norm of the error of all the items within that group, for $p, q \geq 1$. This generalizes the objective function of $\epsilon$-Best-Ranking, proposed by Saha & Gopalan (2019). By adopting our objective function, we gain the flexibility to explore fundamental <b>fairness</b> concepts like equal or proportionate errors within a unified framework. Adjusting parameters $p$ and $q$ allows tailoring to specific <b>fairness</b> preferences. We present both group-blind and group-aware algorithms and analyze their sample complexity. We provide matching lower bounds up to certain logarithmic factors for group-blind algorithms. For a restricted class of group-aware algorithms, we show that we can get reasonable lower bounds. We conduct comprehensive experiments on both real-world and synthetic datasets to complement our theoretical findings.</p></p class="citation"></blockquote><h3 id=114285-the-last-dance--robust-backdoor-attack-via-diffusion-models-and-bayesian-approach-orson-mengara-2024>(114/285) The last Dance : Robust backdoor attack via diffusion models and bayesian approach (Orson Mengara, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Orson Mengara. (2024)<br><strong>The last Dance : Robust backdoor attack via diffusion models and bayesian approach</strong><br><button class=copy-to-clipboard title="The last Dance : Robust backdoor attack via diffusion models and bayesian approach" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05967v1.pdf filename=2402.05967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular <b>transformer-based</b> artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called <code>BacKBayDiffMod</code>) on audio <b>transformers</b> derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model&rsquo;s training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.</p></p class="citation"></blockquote><h3 id=115285-optimal-and-near-optimal-adaptive-vector-quantization-ran-ben-basat-et-al-2024>(115/285) Optimal and Near-Optimal Adaptive Vector Quantization (Ran Ben-Basat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ran Ben-Basat, Yaniv Ben-Itzhak, Michael Mitzenmacher, Shay Vargaftik. (2024)<br><strong>Optimal and Near-Optimal Adaptive Vector Quantization</strong><br><button class=copy-to-clipboard title="Optimal and Near-Optimal Adaptive Vector Quantization" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-IT, cs-LG, cs-NI, cs.LG, math-IT<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03158v1.pdf filename=2402.03158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Quantization</b> is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of <b>quantization</b> is \emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive <b>quantization</b> methods are considered infeasible in terms of both their runtime and memory requirements. We revisit the Adaptive Vector <b>Quantization</b> (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.</p></p class="citation"></blockquote><h3 id=116285-a-multi-step-loss-function-for-robust-learning-of-the-dynamics-in-model-based-reinforcement-learning-abdelhakim-benechehab-et-al-2024>(116/285) A Multi-step Loss Function for Robust Learning of the Dynamics in Model-based Reinforcement Learning (Abdelhakim Benechehab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdelhakim Benechehab, Albert Thomas, Giuseppe Paolo, Maurizio Filippone, Balázs Kégl. (2024)<br><strong>A Multi-step Loss Function for Robust Learning of the Dynamics in Model-based Reinforcement Learning</strong><br><button class=copy-to-clipboard title="A Multi-step Loss Function for Robust Learning of the Dynamics in Model-based Reinforcement Learning" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03146v1.pdf filename=2402.03146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In model-based <b>reinforcement</b> <b>learning,</b> most algorithms rely on simulating trajectories from one-step models of the dynamics learned on data. A critical challenge of this approach is the compounding of one-step prediction errors as the length of the trajectory grows. In this paper we tackle this issue by using a multi-step objective to train one-step models. Our objective is a weighted sum of the mean squared error (MSE) loss at various future horizons. We find that this new loss is particularly useful when the data is noisy (additive Gaussian noise in the observations), which is often the case in real-life environments. To support the multi-step loss, first we study its properties in two tractable cases: i) uni-dimensional linear system, and ii) two-parameter non-linear system. Second, we show in a variety of tasks (environments or datasets) that the models learned with this loss achieve a significant improvement in terms of the averaged R2-score on future prediction horizons. Finally, in the pure batch <b>reinforcement</b> <b>learning</b> setting, we demonstrate that one-step models serve as strong baselines when dynamics are deterministic, while multi-step models would be more advantageous in the presence of noise, highlighting the potential of our approach in real-world applications.</p></p class="citation"></blockquote><h3 id=117285-boosting-long-delayed-reinforcement-learning-with-auxiliary-short-delayed-task-qingyuan-wu-et-al-2024>(117/285) Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task (Qingyuan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingyuan Wu, Simon Sinong Zhan, Yixuan Wang, Chung-Wei Lin, Chen Lv, Qi Zhu, Chao Huang. (2024)<br><strong>Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task</strong><br><button class=copy-to-clipboard title="Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03141v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03141v1.pdf filename=2402.03141v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays. State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments. To address these challenges, our novel Auxiliary-Delayed <b>Reinforcement</b> <b>Learning</b> (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments. Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task. We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task. On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficiency and policy performance.</p></p class="citation"></blockquote><h3 id=118285-just-cluster-it-an-approach-for-exploration-in-high-dimensions-using-clustering-and-pre-trained-representations-stefan-sylvius-wagner-et-al-2024>(118/285) Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations (Stefan Sylvius Wagner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Sylvius Wagner, Stefan Harmeling. (2024)<br><strong>Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations</strong><br><button class=copy-to-clipboard title="Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03138v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03138v1.pdf filename=2402.03138v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we adopt a representation-centric perspective on exploration in <b>reinforcement</b> <b>learning,</b> viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-trained biases into exploration. We evaluate our approach on the VizDoom and Habitat environments, demonstrating that our method surpasses other well-known exploration methods in these settings.</p></p class="citation"></blockquote><h3 id=119285-non-stationary-latent-auto-regressive-bandits-anna-l-trella-et-al-2024>(119/285) Non-Stationary Latent Auto-Regressive Bandits (Anna L. Trella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna L. Trella, Walter Dempsey, Finale Doshi-Velez, Susan A. Murphy. (2024)<br><strong>Non-Stationary Latent Auto-Regressive Bandits</strong><br><button class=copy-to-clipboard title="Non-Stationary Latent Auto-Regressive Bandits" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03110v1.pdf filename=2402.03110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the stochastic multi-armed <b>bandit</b> problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR <b>bandit.</b> Different forms of the latent AR <b>bandit</b> appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.</p></p class="citation"></blockquote><h3 id=120285-probabilistic-actor-critic-learning-to-explore-with-pac-bayes-uncertainty-bahareh-tasdighi-et-al-2024>(120/285) Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty (Bahareh Tasdighi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bahareh Tasdighi, Nicklas Werge, Yi-Shan Wu, Melih Kandemir. (2024)<br><strong>Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty</strong><br><button class=copy-to-clipboard title="Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03055v1.pdf filename=2402.03055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Probabilistic Actor-Critic (PAC), a novel <b>reinforcement</b> <b>learning</b> algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor&rsquo;s decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep <b>reinforcement</b> <b>learning.</b> We report empirical evaluations demonstrating PAC&rsquo;s enhanced stability and improved performance over the state of the art in diverse continuous control problems.</p></p class="citation"></blockquote><h3 id=121285-open-rl-benchmark-comprehensive-tracked-experiments-for-reinforcement-learning-shengyi-huang-et-al-2024>(121/285) Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning (Shengyi Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengyi Huang, Quentin Gallouédec, Florian Felten, Antonin Raffin, Rousslan Fernand Julien Dossa, Yanxiao Zhao, Ryan Sullivan, Viktor Makoviychuk, Denys Makoviichuk, Mohamad H. Danesh, Cyril Roumégous, Jiayi Weng, Chufan Chen, Md Masudur Rahman, João G. M. Araújo, Guorui Quan, Daniel Tan, Timo Klein, Rujikorn Charakorn, Mark Towers, Yann Berthelot, Kinal Mehta, Dipam Chakraborty, Arjun KG, Valentin Charraut, Chang Ye, Zichen Liu, Lucas N. Alegre, Alexander Nikulin, Xiao Hu, Tianlin Liu, Jongwook Choi, Brent Yi. (2024)<br><strong>Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03046v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03046v1.pdf filename=2402.03046v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many <b>Reinforcement</b> <b>Learning</b> (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms. However, the complete raw data of the learning curves are rarely available. As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone. We present Open RL Benchmark, a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics. Open RL Benchmark is community-driven: anyone can download, use, and contribute to the data. At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years. Open RL Benchmark covers a wide range of RL libraries and reference implementations. Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it. In addition, Open RL Benchmark comes with a command-line interface (CLI) for easy fetching and generating figures to present the results. In this document, we include two case studies to demonstrate the usefulness of Open RL Benchmark in practice. To the best of our knowledge, Open RL Benchmark is the first RL benchmark of its kind, and the authors hope that it will improve and facilitate the work of researchers in the field.</p></p class="citation"></blockquote><h3 id=122285-whom-to-trust-elective-learning-for-distributed-gaussian-process-regression-zewen-yang-et-al-2024>(122/285) Whom to Trust? Elective Learning for Distributed Gaussian Process Regression (Zewen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zewen Yang, Xiaobing Dai, Akshat Dubey, Sandra Hirche, Georges Hattab. (2024)<br><strong>Whom to Trust? Elective Learning for Distributed Gaussian Process Regression</strong><br><button class=copy-to-clipboard title="Whom to Trust? Elective Learning for Distributed Gaussian Process Regression" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03014v1.pdf filename=2402.03014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces an innovative approach to enhance distributed cooperative learning using <b>Gaussian</b> <b>process</b> (GP) regression in multi-agent systems (MASs). The key contribution of this work is the development of an elective learning algorithm, namely prior-aware elective distributed GP (Pri-GP), which empowers agents with the capability to selectively request predictions from neighboring agents based on their trustworthiness. The proposed Pri-GP effectively improves individual prediction accuracy, especially in cases where the prior knowledge of an agent is incorrect. Moreover, it eliminates the need for computationally intensive variance calculations for determining aggregation weights in distributed GP. Furthermore, we establish a prediction error bound within the Pri-GP framework, ensuring the reliability of predictions, which is regarded as a crucial property in safety-critical MAS applications.</p></p class="citation"></blockquote><h3 id=123285-on-the-impact-of-output-perturbation-on-fairness-in-binary-linear-classification-vitalii-emelianov-et-al-2024>(123/285) On the Impact of Output Perturbation on Fairness in Binary Linear Classification (Vitalii Emelianov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vitalii Emelianov, Michaël Perrot. (2024)<br><strong>On the Impact of Output Perturbation on Fairness in Binary Linear Classification</strong><br><button class=copy-to-clipboard title="On the Impact of Output Perturbation on Fairness in Binary Linear Classification" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03011v1.pdf filename=2402.03011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We theoretically study how differential privacy interacts with both individual and group <b>fairness</b> in binary linear classification. More precisely, we focus on the output perturbation mechanism, a classic approach in privacy-preserving machine learning. We derive high-probability bounds on the level of individual and group <b>fairness</b> that the perturbed models can achieve compared to the original model. Hence, for individual <b>fairness,</b> we prove that the impact of output perturbation on the level of <b>fairness</b> is bounded but grows with the dimension of the model. For group <b>fairness,</b> we show that this impact is determined by the distribution of so-called angular margins, that is signed margins of the non-private model re-scaled by the norm of each example.</p></p class="citation"></blockquote><h3 id=124285-decoding-time-realignment-of-language-models-tianlin-liu-et-al-2024>(124/285) Decoding-time Realignment of Language Models (Tianlin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianlin Liu, Shangmin Guo, Leonardo Bianco, Daniele Calandriello, Quentin Berthet, Felipe Llinares, Jessica Hoffmann, Lucas Dixon, Michal Valko, Mathieu Blondel. (2024)<br><strong>Decoding-time Realignment of Language Models</strong><br><button class=copy-to-clipboard title="Decoding-time Realignment of Language Models" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02992v1.pdf filename=2402.02992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aligning language models with human preferences is crucial for reducing errors and biases in these models. Alignment techniques, such as <b>reinforcement</b> <b>learning</b> from human feedback (RLHF), are typically cast as optimizing a tradeoff between human preference rewards and a proximity regularization term that encourages staying close to the unaligned model. Selecting an appropriate level of regularization is critical: insufficient regularization can lead to reduced model capabilities due to reward hacking, whereas excessive regularization hinders alignment. Traditional methods for finding the optimal regularization level require retraining multiple models with varying regularization strengths. This process, however, is resource-intensive, especially for large models. To address this challenge, we propose decoding-time realignment (DeRa), a simple method to explore and evaluate different regularization strengths in aligned models without retraining. DeRa enables control over the degree of alignment, allowing users to smoothly transition between unaligned and aligned models. It also enhances the efficiency of hyperparameter tuning by enabling the identification of effective regularization strengths using a validation dataset.</p></p class="citation"></blockquote><h3 id=125285-kernel-pca-for-out-of-distribution-detection-kun-fang-et-al-2024>(125/285) Kernel PCA for Out-of-Distribution Detection (Kun Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Fang, Qinghua Tao, Kexin Lv, Mingzhen He, Xiaolin Huang, Jie Yang. (2024)<br><strong>Kernel PCA for Out-of-Distribution Detection</strong><br><button class=copy-to-clipboard title="Kernel PCA for Out-of-Distribution Detection" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02949v1.pdf filename=2402.02949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-Distribution</b> (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs). Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper nonlinear mappings. In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD and InD features are allocated with significantly different patterns. We devise two feature mappings that induce non-linear kernels in KPCA to advocate the separability between InD and OoD data in the subspace spanned by the principal components. Given any test sample, the reconstruction error in such subspace is then used to efficiently obtain the detection result with $\mathcal{O}(1)$ time complexity in inference. Extensive empirical results on multiple OoD data sets and network structures verify the superiority of our KPCA-based detector in efficiency and efficacy with state-of-the-art OoD detection performances.</p></p class="citation"></blockquote><h3 id=126285-frugal-actor-critic-sample-efficient-off-policy-deep-reinforcement-learning-using-unique-experiences-nikhil-kumar-singh-et-al-2024>(126/285) Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning Using Unique Experiences (Nikhil Kumar Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhil Kumar Singh, Indranil Saha. (2024)<br><strong>Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning Using Unique Experiences</strong><br><button class=copy-to-clipboard title="Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning Using Unique Experiences" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05963v1.pdf filename=2402.05963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient utilization of the replay buffer plays a significant role in the off-policy actor-critic <b>reinforcement</b> <b>learning</b> (RL) algorithms used for model-free control policy synthesis for complex dynamical systems. We propose a method for achieving sample efficiency, which focuses on selecting unique samples and adding them to the replay buffer during the exploration with the goal of reducing the buffer size and maintaining the independent and identically distributed (IID) nature of the samples. Our method is based on selecting an important subset of the set of state variables from the experiences encountered during the initial phase of random exploration, partitioning the state space into a set of abstract states based on the selected important state variables, and finally selecting the experiences with unique state-reward combination by using a kernel density estimator. We formally prove that the off-policy actor-critic algorithm incorporating the proposed method for unique experience accumulation converges faster than the vanilla off-policy actor-critic algorithm. Furthermore, we evaluate our method by comparing it with two state-of-the-art actor-critic RL algorithms on several continuous control benchmarks available in the Gym environment. Experimental results demonstrate that our method achieves a significant reduction in the size of the replay buffer for all the benchmarks while achieving either faster convergent or better reward accumulation compared to the baseline algorithms.</p></p class="citation"></blockquote><h3 id=127285-evading-data-contamination-detection-for-language-models-is-too-easy-jasper-dekoninck-et-al-2024>(127/285) Evading Data Contamination Detection for Language Models is (too) Easy (Jasper Dekoninck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jasper Dekoninck, Mark Niklas Müller, Maximilian Baader, Marc Fischer, Martin Vechev. (2024)<br><strong>Evading Data Contamination Detection for Language Models is (too) Easy</strong><br><button class=copy-to-clipboard title="Evading Data Contamination Detection for Language Models is (too) Easy" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02823v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02823v1.pdf filename=2402.02823v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.</p></p class="citation"></blockquote><h3 id=128285-learning-from-teaching-regularization-generalizable-correlations-should-be-easy-to-imitate-can-jin-et-al-2024>(128/285) Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate (Can Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Can Jin, Tong Che, Hongwu Peng, Yiyuan Li, Marco Pavone. (2024)<br><strong>Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate</strong><br><button class=copy-to-clipboard title="Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02769v1.pdf filename=2402.02769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and <b>Reinforcement</b> <b>Learning,</b> demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into the swamp of complex patterns in data, making LoT a valuable addition to the current machine learning frameworks.</p></p class="citation"></blockquote><h3 id=129285-exgc-bridging-efficiency-and-explainability-in-graph-condensation-junfeng-fang-et-al-2024>(129/285) EXGC: Bridging Efficiency and Explainability in Graph Condensation (Junfeng Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junfeng Fang, Xinglin Li, Yongduo Sui, Yuan Gao, Guibin Zhang, Kun Wang, Xiang Wang, Xiangnan He. (2024)<br><strong>EXGC: Bridging Efficiency and Explainability in Graph Condensation</strong><br><button class=copy-to-clipboard title="EXGC: Bridging Efficiency and Explainability in Graph Condensation" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05962v1.pdf filename=2402.05962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Graph representation learning on vast datasets, like web data, has made significant strides. However, the associated computational and storage overheads raise concerns. In sight of this, Graph condensation (GCond) has been introduced to <b>distill</b> these large real datasets into a more concise yet information-rich synthetic graph. Despite acceleration efforts, existing GCond methods mainly grapple with efficiency, especially on expansive web data graphs. Hence, in this work, we pinpoint two major inefficiencies of current paradigms: (1) the concurrent updating of a vast parameter set, and (2) pronounced parameter redundancy. To counteract these two limitations correspondingly, we first (1) employ the Mean-Field variational approximation for convergence acceleration, and then (2) propose the objective of Gradient Information Bottleneck (GDIB) to prune redundancy. By incorporating the leading explanation techniques (e.g., GNNExplainer and GSAT) to instantiate the GDIB, our EXGC, the Efficient and eXplainable Graph Condensation method is proposed, which can markedly boost efficiency and inject explainability. Our extensive evaluations across eight datasets underscore EXGC&rsquo;s superiority and relevance. Code is available at <a href=https://github.com/MangoKiller/EXGC>https://github.com/MangoKiller/EXGC</a>.</p></p class="citation"></blockquote><h3 id=130285-standard-gaussian-process-is-all-you-need-for-high-dimensional-bayesian-optimization-zhitong-xu-et-al-2024>(130/285) Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization (Zhitong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhitong Xu, Shandian Zhe. (2024)<br><strong>Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization</strong><br><button class=copy-to-clipboard title="Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02746v1.pdf filename=2402.02746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard <b>Gaussian</b> <b>process</b> (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO with standard GP not only excels in high-dimensional optimization but also proves robust in accommodating various structures within the target functions. Furthermore, with standard GP, achieving promising optimization performance is possible by only using maximum likelihood estimation, eliminating the need for expensive Markov-Chain Monte Carlo (MCMC) sampling that might be required by more complex surrogate models. We thus advocate for a re-evaluation and in-depth study of the potential of standard BO in addressing high-dimensional problems.</p></p class="citation"></blockquote><h3 id=131285-representation-surgery-for-multi-task-model-merging-enneng-yang-et-al-2024>(131/285) Representation Surgery for Multi-Task Model Merging (Enneng Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enneng Yang, Li Shen, Zhenyi Wang, Guibing Guo, Xiaojun Chen, Xingwei Wang, Dacheng Tao. (2024)<br><strong>Representation Surgery for Multi-Task Model Merging</strong><br><button class=copy-to-clipboard title="Representation Surgery for Multi-Task Model Merging" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02705v1.pdf filename=2402.02705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-task learning (MTL) compresses the information from multiple tasks into a unified backbone to improve computational efficiency and generalization. Recent work directly merges multiple independently trained models to perform MTL instead of collecting their raw data for joint training, greatly expanding the application scenarios of MTL. However, by visualizing the representation distribution of existing model merging schemes, we find that the merged model often suffers from the dilemma of representation bias. That is, there is a significant discrepancy in the representation distribution between the merged and individual models, resulting in poor performance of merged MTL. In this paper, we propose a representation surgery solution called &ldquo;Surgery&rdquo; to reduce representation bias in the merged model. Specifically, Surgery is a lightweight task-specific module that takes the representation of the merged model as input and attempts to output the biases contained in the representation from the merged model. We then designed an <b>unsupervised</b> optimization objective that updates the Surgery module by minimizing the distance between the merged model&rsquo;s representation and the individual model&rsquo;s representation. Extensive experiments demonstrate significant MTL performance improvements when our Surgery module is applied to state-of-the-art (SOTA) model merging schemes.</p></p class="citation"></blockquote><h3 id=132285-understanding-what-affects-generalization-gap-in-visual-reinforcement-learning-theory-and-empirical-evidence-jiafei-lyu-et-al-2024>(132/285) Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence (Jiafei Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiafei Lyu, Le Wan, Xiu Li, Zongqing Lu. (2024)<br><strong>Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence</strong><br><button class=copy-to-clipboard title="Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02701v1.pdf filename=2402.02701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there are many efforts attempting to learn useful policies for continuous control in visual <b>reinforcement</b> <b>learning</b> (RL). In this scenario, it is important to learn a generalizable policy, as the testing environment may differ from the training environment, e.g., there exist distractors during deployment. Many practical algorithms are proposed to handle this problem. However, to the best of our knowledge, none of them provide a theoretical understanding of what affects the generalization gap and why their proposed methods work. In this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors. Our theories indicate that minimizing the representation distance between training and testing environments, which aligns with human intuition, is the most critical for the benefit of reducing the generalization gap. Our theoretical results are supported by the empirical evidence in the DMControl Generalization Benchmark (DMC-GB).</p></p class="citation"></blockquote><h3 id=133285-sample-complexity-characterization-for-linear-contextual-mdps-junze-deng-et-al-2024>(133/285) Sample Complexity Characterization for Linear Contextual MDPs (Junze Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junze Deng, Yuan Cheng, Shaofeng Zou, Yingbin Liang. (2024)<br><strong>Sample Complexity Characterization for Linear Contextual MDPs</strong><br><button class=copy-to-clipboard title="Sample Complexity Characterization for Linear Contextual MDPs" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02700v1.pdf filename=2402.02700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contextual Markov decision processes (CMDPs) describe a class of <b>reinforcement</b> <b>learning</b> problems in which the transition kernels and reward functions can change over time with different MDPs indexed by a context variable. While CMDPs serve as an important framework to model many real-world applications with time-varying environments, they are largely unexplored from theoretical perspective. In this paper, we study CMDPs under two linear function approximation models: Model I with context-varying representations and common linear weights for all contexts; and Model II with common representations for all contexts and context-varying linear weights. For both models, we propose novel model-based algorithms and show that they enjoy guaranteed $\epsilon$-suboptimality gap with desired polynomial sample complexity. In particular, instantiating our result for the first model to the tabular CMDP improves the existing result by removing the reachability assumption. Our result for the second model is the first-known result for such a type of function approximation models. Comparison between our results for the two models further indicates that having context-varying features leads to much better sample efficiency than having common representations for all contexts under linear CMDPs.</p></p class="citation"></blockquote><h3 id=134285-causal-feature-selection-for-responsible-machine-learning-raha-moraffah-et-al-2024>(134/285) Causal Feature Selection for Responsible Machine Learning (Raha Moraffah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raha Moraffah, Paras Sheth, Saketh Vishnubhatla, Huan Liu. (2024)<br><strong>Causal Feature Selection for Responsible Machine Learning</strong><br><button class=copy-to-clipboard title="Causal Feature Selection for Responsible Machine Learning" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02696v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02696v1.pdf filename=2402.02696v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine Learning (ML) has become an integral aspect of many real-world applications. As a result, the need for responsible machine learning has emerged, focusing on aligning ML models to ethical and social values, while enhancing their reliability and trustworthiness. Responsible ML involves many issues. This survey addresses four main issues: interpretability, <b>fairness,</b> adversarial robustness, and domain generalization. Feature selection plays a pivotal role in the responsible ML tasks. However, building upon statistical correlations between variables can lead to spurious patterns with biases and compromised performance. This survey focuses on the current study of causal feature selection: what it is and how it can reinforce the four aspects of responsible ML. By identifying features with causal impacts on outcomes and distinguishing causality from correlation, causal feature selection is posited as a unique approach to ensuring ML models to be ethically and socially responsible in high-stakes applications.</p></p class="citation"></blockquote><h3 id=135285-poisson-process-for-bayesian-optimization-xiaoxing-wang-et-al-2024>(135/285) Poisson Process for Bayesian Optimization (Xiaoxing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoxing Wang, Jiaxing Li, Chao Xue, Wei Liu, Weifeng Liu, Xiaokang Yang, Junchi Yan, Dacheng Tao. (2024)<br><strong>Poisson Process for Bayesian Optimization</strong><br><button class=copy-to-clipboard title="Poisson Process for Bayesian Optimization" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02687v1.pdf filename=2402.02687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>BayesianOptimization(BO) is a sample-efficient black-box optimizer, and extensive methods have been proposed to build the absolute function response of the black-box function through a probabilistic surrogate model, including Tree-structured Parzen Estimator (TPE), random forest (SMAC), and <b>Gaussian</b> <b>process</b> (GP). However, few methods have been explored to estimate the relative rankings of candidates, which can be more robust to noise and have better practicality than absolute function responses, especially when the function responses are intractable but preferences can be acquired. To this end, we propose a novel ranking-based surrogate model based on the Poisson process and introduce an efficient BO framework, namely Poisson Process Bayesian Optimization (PoPBO). Two tailored acquisition functions are further derived from classic LCB and EI to accommodate it. Compared to the classic GP-BO method, our PoPBO has lower computation costs and better robustness to noise, which is verified by abundant experiments. The results on both simulated and real-world benchmarks, including hyperparameter optimization (HPO) and neural architecture search (NAS), show the effectiveness of PoPBO.</p></p class="citation"></blockquote><h3 id=136285-counterfactual-explanations-of-black-box-machine-learning-models-using-causal-discovery-with-applications-to-credit-rating-daisuke-takahashi-et-al-2024>(136/285) Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating (Daisuke Takahashi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daisuke Takahashi, Shohei Shimizu, Takuma Tanaka. (2024)<br><strong>Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating</strong><br><button class=copy-to-clipboard title="Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02678v1.pdf filename=2402.02678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explainable artificial intelligence (XAI) has helped elucidate the internal mechanisms of machine learning algorithms, bolstering their reliability by demonstrating the basis of their predictions. Several XAI models consider causal relationships to explain models by examining the input-output relationships of prediction models and the dependencies between features. The majority of these models have been based their explanations on <b>counterfactual</b> probabilities, assuming that the causal graph is known. However, this assumption complicates the application of such models to real data, given that the causal relationships between features are unknown in most cases. Thus, this study proposed a novel XAI framework that relaxed the constraint that the causal graph is known. This framework leveraged <b>counterfactual</b> probabilities and additional prior information on causal structure, facilitating the integration of a causal graph estimated through causal discovery methods and a black-box classification model. Furthermore, explanatory scores were estimated based on <b>counterfactual</b> probabilities. Numerical experiments conducted employing artificial data confirmed the possibility of estimating the explanatory score more accurately than in the absence of a causal graph. Finally, as an application to real data, we constructed a classification model of credit ratings assigned by Shiga Bank, Shiga prefecture, Japan. We demonstrated the effectiveness of the proposed method in cases where the causal graph is unknown.</p></p class="citation"></blockquote><h3 id=137285-verifiable-evaluations-of-machine-learning-models-using-zksnarks-tobin-south-et-al-2024>(137/285) Verifiable evaluations of machine learning models using zkSNARKs (Tobin South et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobin South, Alexander Camuto, Shrey Jain, Shayla Nguyen, Robert Mahari, Christian Paquin, Jason Morton, Alex &lsquo;Sandy&rsquo; Pentland. (2024)<br><strong>Verifiable evaluations of machine learning models using zkSNARKs</strong><br><button class=copy-to-clipboard title="Verifiable evaluations of machine learning models using zkSNARKs" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T01, cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02675v1.pdf filename=2402.02675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a world of increasing closed-source commercial machine learning models, model evaluations from developers must be taken at face value. These benchmark results, whether over task accuracy, bias evaluations, or safety checks, are traditionally impossible to verify by a model end-user without the costly or impossible process of re-performing the benchmark on black-box model outputs. This work presents a method of verifiable model evaluation using model inference through zkSNARKs. The resulting zero-knowledge computational proofs of model outputs over datasets can be packaged into verifiable evaluation attestations showing that models with fixed private weights achieve stated performance or <b>fairness</b> metrics over public inputs. These verifiable attestations can be performed on any standard neural network model with varying compute requirements. For the first time, we demonstrate this across a sample of real-world models and highlight key challenges and design solutions. This presents a new transparency paradigm in the verifiable evaluation of private models.</p></p class="citation"></blockquote><h3 id=138285-utility-based-reinforcement-learning-unifying-single-objective-and-multi-objective-reinforcement-learning-peter-vamplew-et-al-2024>(138/285) Utility-Based Reinforcement Learning: Unifying Single-objective and Multi-objective Reinforcement Learning (Peter Vamplew et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Vamplew, Cameron Foale, Conor F. Hayes, Patrick Mannion, Enda Howley, Richard Dazeley, Scott Johnson, Johan Källström, Gabriel Ramos, Roxana Rădulescu, Willem Röpke, Diederik M. Roijers. (2024)<br><strong>Utility-Based Reinforcement Learning: Unifying Single-objective and Multi-objective Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Utility-Based Reinforcement Learning: Unifying Single-objective and Multi-objective Reinforcement Learning" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02665v1.pdf filename=2402.02665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research in multi-objective <b>reinforcement</b> <b>learning</b> (MORL) has introduced the utility-based paradigm, which makes use of both environmental rewards and a function that defines the utility derived by the user from those rewards. In this paper we extend this paradigm to the context of single-objective <b>reinforcement</b> <b>learning</b> (RL), and outline multiple potential benefits including the ability to perform multi-policy learning across tasks relating to uncertain objectives, risk-aware RL, discounting, and safe RL. We also examine the algorithmic implications of adopting a utility-based approach.</p></p class="citation"></blockquote><h3 id=139285-boosting-voting-classifiers-and-randomized-sample-compression-schemes-arthur-da-cunha-et-al-2024>(139/285) Boosting, Voting Classifiers and Randomized Sample Compression Schemes (Arthur da Cunha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arthur da Cunha, Kasper Green Larsen, Martin Ritzert. (2024)<br><strong>Boosting, Voting Classifiers and Randomized Sample Compression Schemes</strong><br><button class=copy-to-clipboard title="Boosting, Voting Classifiers and Randomized Sample Compression Schemes" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02976v1.pdf filename=2402.02976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In boosting, we aim to leverage multiple weak learners to produce a strong learner. At the center of this paradigm lies the concept of building the strong learner as a voting classifier, which outputs a weighted majority vote of the weak learners. While many successful boosting algorithms, such as the iconic AdaBoost, produce voting classifiers, their theoretical performance has long remained sub-optimal: the best known bounds on the number of training examples necessary for a voting classifier to obtain a given accuracy has so far always contained at least two logarithmic factors above what is known to be achievable by general weak-to-strong learners. In this work, we break this barrier by proposing a randomized boosting algorithm that outputs voting classifiers whose generalization error contains a single logarithmic dependency on the <b>sample</b> <b>size.</b> We obtain this result by building a general framework that extends <b>sample</b> <b>compression</b> methods to support randomized learning algorithms based on sub-sampling.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=140285-enhancing-the-stability-of-llm-based-speech-generation-systems-through-self-supervised-representations-álvaro-martín-cortinas-et-al-2024>(140/285) Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations (Álvaro Martín-Cortinas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Álvaro Martín-Cortinas, Daniel Sáez-Trigueros, Iván Vallés-Pérez, Biel Tura-Vecino, Piotr Biliński, Mateusz Lajszczak, Grzegorz Beringer, Roberto Barra-Chicote, Jaime Lorenzo-Trueba. (2024)<br><strong>Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations</strong><br><button class=copy-to-clipboard title="Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-LG, eess-AS, eess.AS<br>Keyword Score: 70<br>Keywords: Self-supervised Learning, Text-to-speech, Text-to-speech, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03407v1.pdf filename=2402.03407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are one of the most promising technologies for the next era of speech generation systems, due to their scalability and <b>in-context</b> <b>learning</b> capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new <b>self-supervised</b> Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train <b>LLMs</b> for <b>text-to-speech</b> <b>(TTS)</b> allows the <b>LLM</b> to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that <b>LLMs</b> trained over speaker-disentangled <b>self-supervised</b> representations provide an improvement of 4.7pp in speaker similarity over SOTA entangled representations, and a word error rate (WER) 5.4pp lower. Furthermore, they achieve higher naturalness than human recordings of the LibriTTS test-other dataset. Finally, we show that using explicit reference embedding negatively impacts intelligibility (stability), with WER increasing by 14pp compared to the model that only uses text to infer the style.</p></p class="citation"></blockquote><h3 id=141285-description-on-ieee-icme-2024-grand-challenge-semi-supervised-acoustic-scene-classification-under-domain-shift-jisheng-bai-et-al-2024>(141/285) Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic Scene Classification under Domain Shift (Jisheng Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jisheng Bai, Mou Wang, Haohe Liu, Han Yin, Yafei Jia, Siwei Huang, Yutong Du, Dongzhe Zhang, Mark D. Plumbley, Dongyuan Shi, Woon-Seng Gan, Susanto Rahardja, Bin Xiang, Jianfeng Chen. (2024)<br><strong>Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic Scene Classification under Domain Shift</strong><br><button class=copy-to-clipboard title="Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic Scene Classification under Domain Shift" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 10<br>Keywords: Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02694v1.pdf filename=2402.02694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acoustic scene classification (ASC) is a crucial research problem in computational auditory scene analysis, and it aims to recognize the unique acoustic characteristics of an environment. One of the challenges of the ASC task is domain shift caused by a distribution gap between training and testing data. Since 2018, ASC challenges have focused on the generalization of ASC models across different recording devices. Although this task in recent years has achieved substantial progress in device generalization, the challenge of domain shift between different regions, involving characteristics such as time, space, culture, and language, remains insufficiently explored at present. In addition, considering the abundance of unlabeled acoustic scene data in the real world, it is important to study the possible ways to utilize these unlabelled data. Therefore, we introduce the task <b>Semi-supervised</b> <b>Acoustic</b> Scene Classification under Domain Shift in the ICME 2024 Grand Challenge. We encourage participants to innovate with <b>semi-supervised</b> <b>learning</b> techniques, aiming to develop more robust ASC models under domain shift.</p></p class="citation"></blockquote><h2 id=csai-11>cs.AI (11)</h2><h3 id=142285-graph-enhanced-large-language-models-in-asynchronous-plan-reasoning-fangru-lin-et-al-2024>(142/285) Graph-enhanced Large Language Models in Asynchronous Plan Reasoning (Fangru Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangru Lin, Emanuele La Malfa, Valentin Hofmann, Elle Michelle Yang, Anthony Cohn, Janet B. Pierrehumbert. (2024)<br><strong>Graph-enhanced Large Language Models in Asynchronous Plan Reasoning</strong><br><button class=copy-to-clipboard title="Graph-enhanced Large Language Models in Asynchronous Plan Reasoning" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 70<br>Keywords: GPT, GPT-4, LLaMA, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02805v1.pdf filename=2402.02805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reasoning</b> about asynchronous plans is challenging since it requires sequential and parallel planning to optimize time costs. Can <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> succeed at this task? Here, we present the first <b>large-scale</b> <b>study</b> <b>investigating</b> this question. We find that a representative set of closed and open-source <b>LLMs,</b> including <b>GPT-4</b> and <b>LLaMA-2,</b> behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language <b>prompts</b> and achieves state-of-the-art results. We show that although PLaG can boost model performance, <b>LLMs</b> still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing <b>LLMs</b> for simulating digital devices. We see our study as an exciting step towards using <b>LLMs</b> as efficient autonomous agents.</p></p class="citation"></blockquote><h3 id=143285-c-rag-certified-generation-risks-for-retrieval-augmented-language-models-mintong-kang-et-al-2024>(143/285) C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models (Mintong Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mintong Kang, Nezihe Merve Gürel, Ning Yu, Dawn Song, Bo Li. (2024)<br><strong>C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models</strong><br><button class=copy-to-clipboard title="C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-IR, cs.AI<br>Keyword Score: 40<br>Keywords: Transformer, Grounding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03181v1.pdf filename=2402.03181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the impressive capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by <b>grounding</b> external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla <b>LLMs,</b> and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single <b>LLM</b> when the quality of the retrieval model and <b>transformer</b> is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.</p></p class="citation"></blockquote><h3 id=144285-deal-decoding-time-alignment-for-large-language-models-james-y-huang-et-al-2024>(144/285) DeAL: Decoding-time Alignment for Large Language Models (James Y. Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James Y. Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchoff, Dan Roth. (2024)<br><strong>DeAL: Decoding-time Alignment for Large Language Models</strong><br><button class=copy-to-clipboard title="DeAL: Decoding-time Alignment for Large Language Models" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06147v1.pdf filename=2402.06147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are nowadays expected to generate content aligned with human preferences. Current work focuses on alignment at model training time, through techniques such as <b>Reinforcement</b> <b>Learning</b> with Human Feedback (RLHF). However, it is unclear if such methods are an effective choice to teach alignment objectives to the model. First, the inability to incorporate multiple, custom rewards and reliance on a model developer&rsquo;s view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training). To address these, we propose DeAL, a framework that allows the user to customize reward functions and enables Decoding-time Alignment of <b>LLMs</b> (DeAL). At its core, we view decoding as a heuristic-guided search process and facilitate the use of a wide variety of alignment objectives. Our experiments with programmatic constraints such as keyword and length constraints (studied widely in the pre-LLM era) and abstract objectives such as harmlessness and helpfulness (proposed in the post-LLM era) show that we can DeAL with fine-grained trade-offs, improve adherence to alignment objectives, and address residual gaps in <b>LLMs.</b> Lastly, while DeAL can be effectively paired with RLHF and <b>prompting</b> techniques, its generality makes decoding slower, an optimization we leave for future work.</p></p class="citation"></blockquote><h3 id=145285-neural-networks-for-abstraction-and-reasoning-towards-broad-generalization-in-machines-mikel-bober-irizar-et-al-2024>(145/285) Neural networks for abstraction and reasoning: Towards broad generalization in machines (Mikel Bober-Irizar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mikel Bober-Irizar, Soumya Banerjee. (2024)<br><strong>Neural networks for abstraction and reasoning: Towards broad generalization in machines</strong><br><button class=copy-to-clipboard title="Neural networks for abstraction and reasoning: Towards broad generalization in machines" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03507v1.pdf filename=2402.03507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and <b>reasoning</b> - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.In this work, we look at several novel approaches for solving the Abstraction & <b>Reasoning</b> Corpus (ARC), a dataset of abstract visual <b>reasoning</b> tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task. First, we adapt the DreamCoder neurosymbolic <b>reasoning</b> solver to ARC. DreamCoder automatically writes programs in a bespoke domain-specific language to perform <b>reasoning,</b> using a neural network to mimic human intuition. We present the Perceptual Abstraction and <b>Reasoning</b> Language (PeARL) language, which allows DreamCoder to solve ARC tasks, and propose a new recognition model that allows us to significantly improve on the previous best implementation.We also propose a new encoding and augmentation scheme that allows <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to solve ARC tasks, and find that the largest models can solve some ARC tasks. <b>LLMs</b> are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches. We perform an ensemble analysis, combining models to achieve better results than any system alone. Finally, we publish the arckit Python library to make future research on ARC easier.</p></p class="citation"></blockquote><h3 id=146285-beyond-text-improving-llms-decision-making-for-robot-navigation-via-vocal-cues-xingpeng-sun-et-al-2024>(146/285) Beyond Text: Improving LLM&rsquo;s Decision Making for Robot Navigation via Vocal Cues (Xingpeng Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingpeng Sun, Haoming Meng, Souradip Chakraborty, Amrit Singh Bedi, Aniket Bera. (2024)<br><strong>Beyond Text: Improving LLM&rsquo;s Decision Making for Robot Navigation via Vocal Cues</strong><br><button class=copy-to-clipboard title="Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-RO, cs.AI<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03494v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03494v1.pdf filename=2402.03494v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work highlights a critical shortcoming in text-based <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While <b>LLMs</b> excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present &ldquo;Beyond Text&rdquo;; an approach that improves <b>LLM</b> decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations. This approach not only achieves a 70.26% winning rate, outperforming existing <b>LLMs</b> by 48.30%, but also enhances robustness against token manipulation <b>adversarial</b> <b>attacks,</b> highlighted by a 22.44% less decrease ratio than the text-only language model in winning rate. &ldquo;Beyond Text&rdquo; marks an advancement in social robot navigation and broader Human-Robot interactions, seamlessly integrating text-based guidance with human-audio-informed language models.</p></p class="citation"></blockquote><h3 id=147285-toward-human-ai-alignment-in-large-scale-multi-player-games-sugandha-sharma-et-al-2024>(147/285) Toward Human-AI Alignment in Large-Scale Multi-Player Games (Sugandha Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sugandha Sharma, Guy Davidson, Khimya Khetarpal, Anssi Kanervisto, Udit Arora, Katja Hofmann, Ida Momennejad. (2024)<br><strong>Toward Human-AI Alignment in Large-Scale Multi-Player Games</strong><br><button class=copy-to-clipboard title="Toward Human-AI Alignment in Large-Scale Multi-Player Games" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs.AI<br>Keyword Score: 20<br>Keywords: Generative AI, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03575v1.pdf filename=2402.03575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay. We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies. Our approach has three components. First, we analyze extensive human gameplay data from Xbox&rsquo;s Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space. This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent. Second, we train an AI agent to play Bleeding Edge using a <b>Generative</b> <b>Pretrained</b> Causal <b>Transformer</b> and measure its behavior. Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast. This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight and explore-exploit behavior, AI players tend towards uniformity. Furthermore, AI agents predominantly engage in solo play, while humans often engage in cooperative and competitive multi-agent patterns. These stark differences underscore the need for interpretable evaluation, design, and integration of AI in human-aligned applications. Our study advances the alignment discussion in AI and especially <b>generative</b> <b>AI</b> research, offering a measurable framework for interpretable human-agent alignment in multiplayer gaming.</p></p class="citation"></blockquote><h3 id=148285-understanding-the-planning-of-llm-agents-a-survey-xu-huang-et-al-2024>(148/285) Understanding the planning of LLM agents: A survey (Xu Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen. (2024)<br><strong>Understanding the planning of LLM agents: A survey</strong><br><button class=copy-to-clipboard title="Understanding the planning of LLM agents: A survey" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02716v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02716v1.pdf filename=2402.02716v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown significant intelligence, the progress to leverage <b>LLMs</b> as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of <b>LLM-based</b> agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on <b>LLM-Agent</b> planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.</p></p class="citation"></blockquote><h3 id=149285-multi-step-problem-solving-through-a-verifier-an-empirical-analysis-on-model-induced-process-supervision-zihan-wang-et-al-2024>(149/285) Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision (Zihan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, Jingbo Shang. (2024)<br><strong>Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision</strong><br><button class=copy-to-clipboard title="Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: PaLM, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02658v1.pdf filename=2402.02658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the <b>reasoning</b> model, and obtaining an accuracy defined as the proportion of correct completions. Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work. Our approach significantly improves the performance of <b>PaLM</b> 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output supervision trained verifier). Additionally, our study demonstrates that the verifier exhibits strong generalization ability across different <b>reasoning</b> models.</p></p class="citation"></blockquote><h3 id=150285-v-irl-grounding-virtual-intelligence-in-real-life-jihan-yang-et-al-2024>(150/285) V-IRL: Grounding Virtual Intelligence in Real Life (Jihan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, Saining Xie. (2024)<br><strong>V-IRL: Grounding Virtual Intelligence in Real Life</strong><br><button class=copy-to-clipboard title="V-IRL: Grounding Virtual Intelligence in Real Life" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs.AI<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03310v1.pdf filename=2402.03310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.</p></p class="citation"></blockquote><h3 id=151285-decidable-reasoning-about-time-in-finite-domain-situation-calculus-theories-till-hofmann-et-al-2024>(151/285) Decidable Reasoning About Time in Finite-Domain Situation Calculus Theories (Till Hofmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Till Hofmann, Stefan Schupp, Gerhard Lakemeyer. (2024)<br><strong>Decidable Reasoning About Time in Finite-Domain Situation Calculus Theories</strong><br><button class=copy-to-clipboard title="Decidable Reasoning About Time in Finite-Domain Situation Calculus Theories" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LO, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03164v1.pdf filename=2402.03164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Representing time is crucial for cyber-physical systems and has been studied extensively in the Situation Calculus. The most commonly used approach represents time by adding a real-valued fluent $\mathit{time}(a)$ that attaches a time point to each action and consequently to each situation. We show that in this approach, checking whether there is a reachable situation that satisfies a given formula is undecidable, even if the domain of discourse is restricted to a finite set of objects. We present an alternative approach based on well-established results from timed automata theory by introducing clocks as real-valued fluents with restricted successor state axioms and comparison operators. %that only allow comparisons against fixed rationals. With this restriction, we can show that the reachability problem for finite-domain basic action theories is decidable. Finally, we apply our results on Golog program realization by presenting a decidable procedure for determining an action sequence that is a successful execution of a given program.</p></p class="citation"></blockquote><h3 id=152285-mastering-zero-shot-interactions-in-cooperative-and-competitive-simultaneous-games-yannik-mahlau-et-al-2024>(152/285) Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games (Yannik Mahlau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yannik Mahlau, Frederik Schubert, Bodo Rosenhahn. (2024)<br><strong>Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games</strong><br><button class=copy-to-clipboard title="Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-GT, cs.AI<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03136v1.pdf filename=2402.03136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The combination of self-play and planning has achieved great successes in sequential games, for instance in Chess and Go. However, adapting algorithms such as AlphaZero to simultaneous games poses a new challenge. In these games, missing information about concurrent actions of other agents is a limiting factor as they may select different Nash equilibria or do not play optimally at all. Thus, it is vital to model the behavior of the other agents when interacting with them in simultaneous games. To this end, we propose Albatross: AlphaZero for Learning Bounded-rational Agents and Temperature-based Response Optimization using Simulated Self-play. Albatross learns to play the novel equilibrium concept of a Smooth Best Response Logit Equilibrium (SBRLE), which enables cooperation and competition with agents of any playing strength. We perform an extensive evaluation of Albatross on a set of cooperative and competitive simultaneous perfect-information games. In contrast to AlphaZero, Albatross is able to exploit weak agents in the competitive game of Battlesnake. Additionally, it yields an improvement of 37.6% compared to previous state of the art in the cooperative Overcooked benchmark.</p></p class="citation"></blockquote><h2 id=cscr-6>cs.CR (6)</h2><h3 id=153285-detecting-scams-using-large-language-models-liming-jiang-2024>(153/285) Detecting Scams Using Large Language Models (Liming Jiang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liming Jiang. (2024)<br><strong>Detecting Scams Using Large Language Models</strong><br><button class=copy-to-clipboard title="Detecting Scams Using Large Language Models" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 60<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03147v1.pdf filename=2402.03147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have gained prominence in various applications, including security. This paper explores the utility of <b>LLMs</b> in scam detection, a critical aspect of cybersecurity. Unlike traditional applications, we propose a novel use case for <b>LLMs</b> to identify scams, such as phishing, advance fee fraud, and romance scams. We present notable security applications of <b>LLMs</b> and discuss the unique challenges posed by scams. Specifically, we outline the key steps involved in building an effective scam detector using <b>LLMs,</b> emphasizing data collection, preprocessing, model selection, training, and integration into target systems. Additionally, we conduct a preliminary evaluation using <b>GPT-3.5</b> and <b>GPT-4</b> on a duplicated email, highlighting their proficiency in identifying common signs of phishing or scam emails. The results demonstrate the models&rsquo; effectiveness in recognizing suspicious elements, but we emphasize the need for a comprehensive assessment across various language tasks. The paper concludes by underlining the importance of ongoing refinement and collaboration with cybersecurity experts to adapt to evolving threats.</p></p class="citation"></blockquote><h3 id=154285-conversation-reconstruction-attack-against-gpt-models-junjie-chu-et-al-2024>(154/285) Conversation Reconstruction Attack Against GPT Models (Junjie Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjie Chu, Zeyang Sha, Michael Backes, Yang Zhang. (2024)<br><strong>Conversation Reconstruction Attack Against GPT Models</strong><br><button class=copy-to-clipboard title="Conversation Reconstruction Attack Against GPT Models" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 60<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02987v1.pdf filename=2402.02987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent times, significant advancements have been made in the field of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> represented by <b>GPT</b> series models. To optimize task execution, users often engage in multi-round conversations with <b>GPT</b> models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting <b>GPT</b> models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when <b>GPT</b> models are subjected to the proposed attack. However, <b>GPT-4</b> demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous conversations, specifically the UNR attack and the PBU attack. Our experimental findings indicate that the PBU attack yields substantial performance across all models, achieving semantic similarity scores exceeding 0.60, while the UNR attack is effective solely on <b>GPT-3.5.</b> Our results reveal the concern about privacy risks associated with conversations involving <b>GPT</b> models and aim to draw the community&rsquo;s attention to prevent the potential misuse of these models&rsquo; remarkable capabilities. We will responsibly disclose our findings to the suppliers of related <b>large</b> <b>language</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=155285-unihenn-designing-more-versatile-homomorphic-encryption-based-cnns-without-im2col-hyunmin-choi-et-al-2024>(155/285) UniHENN: Designing More Versatile Homomorphic Encryption-based CNNs without im2col (Hyunmin Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyunmin Choi, Jihun Kim, Seungho Kim, Seonhye Park, Jeongyong Park, Wonbin Choi, Hyoungshick Kim. (2024)<br><strong>UniHENN: Designing More Versatile Homomorphic Encryption-based CNNs without im2col</strong><br><button class=copy-to-clipboard title="UniHENN: Designing More Versatile Homomorphic Encryption-based CNNs without im2col" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03060v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03060v1.pdf filename=2402.03060v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Homomorphic encryption enables computations on encrypted data without decryption, which is crucial for privacy-preserving cloud services. However, deploying <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> with homomorphic encryption encounters significant challenges, particularly in converting input data into a two-dimensional matrix for <b>convolution,</b> typically achieved using the im2col technique. While efficient, this method limits the variety of deployable <b>CNN</b> models due to compatibility constraints with the encrypted data structure. UniHENN, a homomorphic encryption-based <b>CNN</b> architecture, eliminates the need for im2col, ensuring compatibility with a diverse range of <b>CNN</b> models using homomorphic encryption. Our experiments demonstrate that UniHENN surpasses the leading 2D <b>CNN</b> inference architecture, PyCrCNN, in inference time, as evidenced by its performance on the LeNet-1 dataset, where it averages 30.090 seconds&ndash;significantly faster than PyCrCNN&rsquo;s 794.064 seconds. Furthermore, UniHENN outperforms TenSEAL, which employs im2col, in processing concurrent images, an essential feature for high-demand cloud applications. The versatility of UniHENN is proven across various <b>CNN</b> architectures, including 1D and six different 2D <b>CNNs,</b> highlighting its flexibility and efficiency. These qualities establish UniHENN as a promising solution for privacy-preserving, cloud-based <b>CNN</b> services, addressing the increasing demand for scalable, secure, and efficient deep learning in cloud computing environments.</p></p class="citation"></blockquote><h3 id=156285-unraveling-the-key-of-machine-learning-solutions-for-android-malware-detection-jiahao-liu-et-al-2024>(156/285) Unraveling the Key of Machine Learning Solutions for Android Malware Detection (Jiahao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Liu, Jun Zeng, Fabio Pierazzi, Lorenzo Cavallaro, Zhenkai Liang. (2024)<br><strong>Unraveling the Key of Machine Learning Solutions for Android Malware Detection</strong><br><button class=copy-to-clipboard title="Unraveling the Key of Machine Learning Solutions for Android Malware Detection" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Recommendation, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02953v1.pdf filename=2402.02953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Android malware detection serves as the front line against malicious apps. With the rapid advancement of machine learning (ML), ML-based Android malware detection has attracted increasing attention due to its capability of automatically capturing malicious patterns from Android APKs. These learning-driven methods have reported promising results in detecting malware. However, the absence of an in-depth analysis of current research progress makes it difficult to gain a holistic picture of the state of the art in this area. This paper presents a comprehensive investigation to date into ML-based Android malware detection with empirical and quantitative analysis. We first survey the literature, categorizing contributions into a taxonomy based on the Android feature engineering and ML modeling pipeline. Then, we design a general-propose framework for ML-based Android malware detection, re-implement 12 representative approaches from different research communities, and evaluate them from three primary dimensions, i.e., effectiveness, robustness, and efficiency. The evaluation reveals that ML-based approaches still face open challenges and provides insightful findings like more powerful ML models are not the silver bullet for designing better malware detectors. We further <b>summarize</b> our findings and put forth <b>recommendations</b> to guide future research.</p></p class="citation"></blockquote><h3 id=157285-towards-eliminating-hard-label-constraints-in-gradient-inversion-attacks-yanbo-wang-et-al-2024>(157/285) Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks (Yanbo Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanbo Wang, Jian Liang, Ran He. (2024)<br><strong>Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks</strong><br><button class=copy-to-clipboard title="Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Label Smoothing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03124v1.pdf filename=2402.03124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework. Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard <b>label</b> <b>constraints.</b> Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft <b>labels.</b> <b>In</b> this work, we change the focus from enlarging batchsize to investigating the hard <b>label</b> <b>constraints,</b> considering a more realistic circumstance where <b>label</b> <b>smoothing</b> and mixup techniques are used in the training process. In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented <b>label</b> <b>and</b> the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based <b>label</b> <b>recovery</b> methods. Extensive experiments testify to the <b>label</b> <b>recovery</b> accuracy, as well as the benefits to the following image reconstruction. We believe soft <b>labels</b> <b>in</b> classification tasks are worth further attention in gradient inversion attacks.</p></p class="citation"></blockquote><h3 id=158285-disdet-exploring-detectability-of-backdoor-attack-on-diffusion-models-yang-sui-et-al-2024>(158/285) DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models (Yang Sui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi, Yan Wang, Yingying Chen, Bo Yuan. (2024)<br><strong>DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models</strong><br><button class=copy-to-clipboard title="DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02739v1.pdf filename=2402.02739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the exciting <b>generative</b> <b>AI</b> era, the diffusion model has emerged as a very powerful and widely adopted content generation and editing tool for various data modalities, making the study of their potential security risks very necessary and critical. Very recently, some pioneering works have shown the vulnerability of the diffusion model against backdoor attacks, calling for in-depth analysis and investigation of the security challenges of this popular and fundamental AI technique. In this paper, for the first time, we systematically explore the detectability of the poisoned noise input for the backdoored diffusion models, an important performance metric yet little explored in the existing works. Starting from the perspective of a defender, we first analyze the properties of the trigger pattern in the existing diffusion backdoor attacks, discovering the important role of distribution discrepancy in Trojan detection. Based on this finding, we propose a low-cost trigger detection mechanism that can effectively identify the poisoned input noise. We then take a further step to study the same problem from the attack side, proposing a backdoor attack strategy that can learn the unnoticeable trigger to evade our proposed detection scheme. Empirical evaluations across various diffusion models and datasets demonstrate the effectiveness of the proposed trigger detection and detection-evading attack strategy. For trigger detection, our distribution discrepancy-based solution can achieve a 100% detection rate for the Trojan triggers used in the existing works. For evading trigger detection, our proposed stealthy trigger design approach performs end-to-end learning to make the distribution of poisoned noise input approach that of benign noise, enabling nearly 100% detection pass rate with very high attack and benign performance for the backdoored diffusion models.</p></p class="citation"></blockquote><h2 id=statml-9>stat.ML (9)</h2><h3 id=159285-minimum-description-length-and-generalization-guarantees-for-representation-learning-milad-sefidgaran-et-al-2024>(159/285) Minimum Description Length and Generalization Guarantees for Representation Learning (Milad Sefidgaran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Milad Sefidgaran, Abdellatif Zaidi, Piotr Krasnowski. (2024)<br><strong>Minimum Description Length and Generalization Guarantees for Representation Learning</strong><br><button class=copy-to-clipboard title="Minimum Description Length and Generalization Guarantees for Representation Learning" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-IT, cs-LG, math-IT, stat-ML, stat.ML<br>Keyword Score: 50<br>Keywords: Mutual Information, Simulation, Simulator, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03254v1.pdf filename=2402.03254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A major challenge in designing efficient statistical <b>supervised</b> <b>learning</b> algorithms is finding representations that perform well not only on available training samples but also on unseen data. While the study of representation learning has spurred much interest, most existing such approaches are heuristic; and very little is known about theoretical generalization guarantees. In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm in terms of the &ldquo;Minimum Description Length&rdquo; (MDL) of the labels or the latent variables (representations). Rather than the <b>mutual</b> <b>information</b> between the encoder&rsquo;s input and the representation, which is often believed to reflect the algorithm&rsquo;s generalization capability in the related literature but in fact, falls short of doing so, our new bounds involve the &ldquo;multi-letter&rdquo; relative entropy between the distribution of the representations (or labels) of the training and test sets and a fixed prior. In particular, these new bounds reflect the structure of the encoder and are not vacuous for deterministic algorithms. Our compressibility approach, which is information-theoretic in nature, builds upon that of Blum-Langford for PAC-MDL bounds and introduces two essential ingredients: block-coding and lossy-compression. The latter allows our approach to subsume the so-called geometrical compressibility as a special case. To the best knowledge of the authors, the established generalization bounds are the first of their kind for Information Bottleneck (IB) type encoders and representation learning. Finally, we partly exploit the theoretical results by introducing a new data-dependent prior. Numerical <b>simulations</b> illustrate the advantages of well-chosen such priors over classical priors used in IB.</p></p class="citation"></blockquote><h3 id=160285-towards-understanding-the-word-sensitivity-of-attention-layers-a-study-via-random-features-simone-bombari-et-al-2024>(160/285) Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features (Simone Bombari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simone Bombari, Marco Mondelli. (2024)<br><strong>Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features</strong><br><button class=copy-to-clipboard title="Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CL, cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: BERT, Transformer, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02969v1.pdf filename=2402.02969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unveiling the reasons behind the exceptional success of <b>transformers</b> requires a better understanding of why attention layers are suitable for NLP tasks. In particular, such tasks require predictive models to capture contextual meaning which often depends on one or few <b>words,</b> <b>even</b> if the sentence is long. Our work studies this key property, dubbed <b>word</b> <b>sensitivity</b> (WS), in the prototypical setting of random features. We show that attention layers enjoy high WS, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map. The argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., ReLU). In contrast, the WS of standard random features is of order $1/\sqrt{n}$, $n$ being the number of <b>words</b> <b>in</b> the textual sample, and thus it decays with the length of the context. We then translate these results on the <b>word</b> <b>sensitivity</b> into generalization bounds: due to their low WS, random features provably cannot learn to distinguish between two sentences that differ only in a single <b>word;</b> <b>in</b> contrast, due to their high WS, random attention features have higher generalization capabilities. We validate our theoretical results with experimental evidence over the <b>BERT-Base</b> <b>word</b> <b>embeddings</b> of the imdb review dataset.</p></p class="citation"></blockquote><h3 id=161285-graph-neural-machine-a-new-model-for-learning-with-tabular-data-giannis-nikolentzos-et-al-2024>(161/285) Graph Neural Machine: A New Model for Learning with Tabular Data (Giannis Nikolentzos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giannis Nikolentzos, Siyun Wang, Johannes Lutzeyer, Michalis Vazirgiannis. (2024)<br><strong>Graph Neural Machine: A New Model for Learning with Tabular Data</strong><br><button class=copy-to-clipboard title="Graph Neural Machine: A New Model for Learning with Tabular Data" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Message-Passing, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02862v1.pdf filename=2402.02862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been a growing interest in mapping data from different domains to <b>graph</b> <b>structures.</b> <b>Among</b> others, neural network models such as the multi-layer perceptron (MLP) can be modeled as <b>graphs.</b> <b>In</b> <b>fact,</b> MLPs can be represented as directed acyclic <b>graphs.</b> <b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have recently become the standard tool for performing machine learning tasks on <b>graphs.</b> <b>In</b> <b>this</b> work, we show that an MLP is equivalent to an asynchronous message passing <b>GNN</b> model which operates on the MLP&rsquo;s <b>graph</b> <b>representation.</b> <b>We</b> then propose a new machine learning model for tabular data, the so-called <b>Graph</b> <b>Neural</b> <b>Machine</b> (GNM), which replaces the MLP&rsquo;s directed acyclic <b>graph</b> <b>with</b> <b>a</b> nearly complete <b>graph</b> <b>and</b> <b>which</b> employs a synchronous message passing scheme. We show that a single GNM model can simulate multiple MLP models. We evaluate the proposed model in several classification and regression datasets. In most cases, the GNM model outperforms the MLP architecture.</p></p class="citation"></blockquote><h3 id=162285-non-asymptotic-analysis-of-biased-adaptive-stochastic-approximation-sobihan-surendran-et-al-2024>(162/285) Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation (Sobihan Surendran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sobihan Surendran, Antoine Godichon-Baggioni, Adeline Fermanian, Sylvain Le Corff. (2024)<br><strong>Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation</strong><br><button class=copy-to-clipboard title="Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02857v1.pdf filename=2402.02857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Stochastic</b> <b>Gradient</b> <b>Descent</b> <b>(SGD)</b> with adaptive steps is now widely used for training deep neural networks. Most theoretical results assume access to unbiased gradient estimators, which is not the case in several recent deep learning and <b>reinforcement</b> <b>learning</b> applications that use Monte Carlo methods. This paper provides a comprehensive non-asymptotic analysis of <b>SGD</b> with biased gradients and adaptive steps for convex and non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias and Mean Squared Error (MSE) of the gradient estimator. In particular, we establish that Adagrad and RMSProp with biased gradients converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning.</p></p class="citation"></blockquote><h3 id=163285-challenges-in-variable-importance-ranking-under-correlation-annie-liang-et-al-2024>(163/285) Challenges in Variable Importance Ranking Under Correlation (Annie Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Annie Liang, Thomas Jemielita, Andy Liaw, Vladimir Svetnik, Lingkang Huang, Richard Baumgartner, Jason M. Klusowski. (2024)<br><strong>Challenges in Variable Importance Ranking Under Correlation</strong><br><button class=copy-to-clipboard title="Challenges in Variable Importance Ranking Under Correlation" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03447v1.pdf filename=2402.03447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Variable importance plays a pivotal role in interpretable machine learning as it helps measure the impact of factors on the output of the prediction model. Model agnostic methods based on the generation of &ldquo;null&rdquo; features via permutation (or related approaches) can be applied. Such analysis is often utilized in pharmaceutical applications due to its ability to interpret black-box models, including tree-based ensembles. A major challenge and significant confounder in variable importance estimation however is the presence of between-feature correlation. Recently, several adjustments to marginal permutation utilizing feature knockoffs were proposed to address this issue, such as the variable importance measure known as conditional predictive impact (CPI). Assessment and evaluation of such approaches is the focus of our work. We first present a comprehensive <b>simulation</b> study investigating the impact of feature correlation on the assessment of variable importance. We then theoretically prove the limitation that highly correlated features pose for the CPI through the knockoff construction. While we expect that there is always no correlation between knockoff variables and its corresponding predictor variables, we prove that the correlation increases linearly beyond a certain correlation threshold between the predictor variables. Our findings emphasize the absence of free lunch when dealing with high feature correlation, as well as the necessity of understanding the utility and limitations behind methods in variable importance estimation.</p></p class="citation"></blockquote><h3 id=164285-diffusive-gibbs-sampling-wenlin-chen-et-al-2024>(164/285) Diffusive Gibbs Sampling (Wenlin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenlin Chen, Mingtian Zhang, Brooks Paige, José Miguel Hernández-Lobato, David Barber. (2024)<br><strong>Diffusive Gibbs Sampling</strong><br><button class=copy-to-clipboard title="Diffusive Gibbs Sampling" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-CO, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Convolution, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03008v1.pdf filename=2402.03008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for <b>multi-modal</b> distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian <b>convolution</b> to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. Our approach exhibits a better mixing property for sampling <b>multi-modal</b> distributions than state-of-the-art methods such as parallel tempering. We demonstrate that our sampler attains substantially improved results across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.</p></p class="citation"></blockquote><h3 id=165285-attention-meets-post-hoc-interpretability-a-mathematical-perspective-gianluigi-lopardo-et-al-2024>(165/285) Attention Meets Post-hoc Interpretability: A Mathematical Perspective (Gianluigi Lopardo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianluigi Lopardo, Frederic Precioso, Damien Garreau. (2024)<br><strong>Attention Meets Post-hoc Interpretability: A Mathematical Perspective</strong><br><button class=copy-to-clipboard title="Attention Meets Post-hoc Interpretability: A Mathematical Perspective" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CL, cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03485v1.pdf filename=2402.03485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Attention-based architectures, in particular <b>transformers,</b> are at the heart of a technological revolution. Interestingly, in addition to helping obtain state-of-the-art results on a wide range of applications, the attention mechanism intrinsically provides meaningful insights on the internal behavior of the model. Can these insights be used as explanations? Debate rages on. In this paper, we mathematically study a simple attention-based architecture and pinpoint the differences between post-hoc and attention-based explanations. We show that they provide quite different results, and that, despite their limitations, post-hoc methods are capable of capturing more useful insights than merely examining the attention weights.</p></p class="citation"></blockquote><h3 id=166285-on-least-squares-estimation-in-softmax-gating-mixture-of-experts-huy-nguyen-et-al-2024>(166/285) On Least Squares Estimation in Softmax Gating Mixture of Experts (Huy Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huy Nguyen, Nhat Ho, Alessandro Rinaldo. (2024)<br><strong>On Least Squares Estimation in Softmax Gating Mixture of Experts</strong><br><button class=copy-to-clipboard title="On Least Squares Estimation in Softmax Gating Mixture of Experts" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Graph Attention Networks<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02952v1.pdf filename=2402.02952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mixture of experts (MoE) model is a statistical machine learning design that aggregates multiple expert networks using a softmax <b>gating</b> function in order to form a more intricate and expressive model. Despite being commonly used in several applications owing to their scalability, the mathematical and statistical properties of MoE models are complex and difficult to analyze. As a result, previous theoretical works have primarily focused on probabilistic MoE models by imposing the impractical assumption that the data are generated from a Gaussian MoE model. In this work, we investigate the performance of the least squares estimators (LSE) under a deterministic MoE model where the data are sampled according to a regression model, a setting that has remained largely unexplored. We establish a condition called strong identifiability to characterize the convergence behavior of various types of expert functions. We demonstrate that the rates for estimating strongly identifiable experts, namely the widely used feed forward networks with activation functions $\mathrm{sigmoid}(\cdot)$ and $\tanh(\cdot)$, are substantially faster than those of polynomial experts, which we show to exhibit a surprising slow estimation rate. Our findings have important practical implications for expert selection.</p></p class="citation"></blockquote><h3 id=167285-bayes-optimal-fair-classification-with-linear-disparity-constraints-via-pre--in--and-post-processing-xianli-zeng-et-al-2024>(167/285) Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing (Xianli Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianli Zeng, Guang Cheng, Edgar Dobriban. (2024)<br><strong>Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing</strong><br><button class=copy-to-clipboard title="Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CY, cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02817v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02817v2.pdf filename=2402.02817v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning algorithms may have disparate impacts on protected groups. To address this, we develop methods for Bayes-optimal fair classification, aiming to minimize classification error subject to given group <b>fairness</b> constraints. We introduce the notion of \emph{linear disparity measures}, which are linear functions of a probabilistic classifier; and \emph{bilinear disparity measures}, which are also linear in the group-wise regression functions. We show that several popular disparity measures &ndash; the deviations from demographic parity, equality of opportunity, and predictive equality &ndash; are bilinear. We find the form of Bayes-optimal fair classifiers under a single linear disparity measure, by uncovering a connection with the Neyman-Pearson lemma. For bilinear disparity measures, Bayes-optimal fair classifiers become group-wise thresholding rules. Our approach can also handle multiple <b>fairness</b> constraints (such as equalized odds), and the common scenario when the protected attribute cannot be used at the prediction phase. Leveraging our theoretical results, we design methods that learn fair Bayes-optimal classifiers under bilinear disparity constraints. Our methods cover three popular approaches to <b>fairness-aware</b> classification, via pre-processing (Fair Up- and Down-Sampling), in-processing (Fair Cost-Sensitive Classification) and post-processing (a Fair Plug-In Rule). Our methods control disparity directly while achieving near-optimal <b>fairness-accuracy</b> tradeoffs. We show empirically that our methods compare favorably to existing algorithms.</p></p class="citation"></blockquote><h2 id=cscv-41>cs.CV (41)</h2><h3 id=168285-cross-domain-few-shot-object-detection-via-enhanced-open-set-object-detector-yuqian-fu-et-al-2024>(168/285) Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector (Yuqian Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan, Tong Liu, Lingjie Kong, Yanwei Fu, Luc Van Gool, Xingqun Jiang. (2024)<br><strong>Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector</strong><br><button class=copy-to-clipboard title="Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Object Detection, Few-shot, Fine-tuning, Out-of-domain, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03094v1.pdf filename=2402.03094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenge of cross-domain <b>few-shot</b> <b>object</b> <b>detection</b> (CD-FSOD), aiming to develop an accurate <b>object</b> <b>detector</b> for novel domains with minimal labeled examples. While <b>transformer-based</b> open-set detectors e.g., DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary <b>object</b> <b>detection</b> and traditional <b>few-shot</b> <b>object</b> <b>detection,</b> detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set <b>object</b> <b>detection</b> methods are evaluated on this benchmark, with evident performance degradation observed across <b>out-of-domain</b> datasets. This indicates the failure of adopting open-set detectors directly for CD-FSOD. Sequentially, to overcome the performance degradation issue and also to answer the second proposed question, we endeavor to enhance the vanilla DE-ViT. With several novel components including <b>finetuning,</b> a learnable prototype module, and a lightweight attention module, we present an improved Cross-Domain Vision <b>Transformer</b> for CD-FSOD (CD-ViTO). Experiments show that our CD-ViTO achieves impressive results on both <b>out-of-domain</b> and in-domain target datasets, establishing new SOTAs for both CD-FSOD and FSOD. All the datasets, codes, and models will be released to the community.</p></p class="citation"></blockquote><h3 id=169285-unsupervised-semantic-segmentation-of-high-resolution-uav-imagery-for-road-scene-parsing-zihan-ma-et-al-2024>(169/285) Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing (Zihan Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Ma, Yongshang Li, Ronggui Ma, Chen Liang. (2024)<br><strong>Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing</strong><br><button class=copy-to-clipboard title="Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Foundation Model, Self-supervised Learning, Supervised Learning, Unsupervised Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02985v1.pdf filename=2402.02985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Two challenges are presented when parsing road scenes in UAV images. First, the high resolution of UAV images makes processing difficult. Second, <b>supervised</b> deep learning methods require a large amount of manual annotations to train robust and accurate models. In this paper, an <b>unsupervised</b> road parsing framework that leverages recent advances in vision language models and fundamental computer vision model is introduced.Initially, a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images. Subsequently, the vision <b>foundation</b> <b>model</b> SAM is utilized to generate masks for the road regions without category information. Following that, a <b>self-supervised</b> representation learning network extracts feature representations from all masked regions. Finally, an <b>unsupervised</b> clustering algorithm is applied to cluster these feature representations and assign IDs to each cluster. The masked regions are combined with the corresponding IDs to generate initial pseudo-labels, which initiate an iterative self-training process for regular semantic segmentation. The proposed method achieves an impressive 89.96% mIoU on the development dataset without relying on any manual annotation. Particularly noteworthy is the extraordinary flexibility of the proposed method, which even goes beyond the limitations of human-defined categories and is able to acquire knowledge of new categories from the dataset itself.</p></p class="citation"></blockquote><h3 id=170285-exploring-the-synergies-of-hybrid-cnns-and-vits-architectures-for-computer-vision-a-survey-haruna-yunusa-et-al-2024>(170/285) Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey (Haruna Yunusa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haruna Yunusa, Shiyin Qin, Abdulrahman Hamman Adama Chukkol, Abdulganiyu Abdu Yusuf, Isah Bello, Adamu Lawan. (2024)<br><strong>Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey</strong><br><button class=copy-to-clipboard title="Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Recommendation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02941v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02941v1.pdf filename=2402.02941v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The hybrid of <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> and Vision <b>Transformers</b> (ViT) architectures has emerged as a groundbreaking approach, pushing the boundaries of computer vision (CV). This comprehensive review provides a thorough examination of the literature on state-of-the-art hybrid <b>CNN-ViT</b> architectures, exploring the synergies between these two approaches. The main content of this survey includes: (1) a background on the vanilla <b>CNN</b> and ViT, (2) systematic review of various taxonomic hybrid designs to explore the synergy achieved through merging <b>CNNs</b> and ViTs models, (3) comparative analysis and application task-specific synergy between different hybrid architectures, (4) challenges and future directions for hybrid models, (5) lastly, the survey concludes with a summary of key findings and <b>recommendations.</b> Through this exploration of hybrid CV architectures, the survey aims to serve as a guiding resource, fostering a deeper understanding of the intricate dynamics between <b>CNNs</b> and ViTs and their collective impact on shaping the future of CV architectures.</p></p class="citation"></blockquote><h3 id=171285-video-lavit-unified-video-language-pre-training-with-decoupled-visual-motional-tokenization-yang-jin-et-al-2024>(171/285) Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization (Yang Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, Yadong Mu. (2024)<br><strong>Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization</strong><br><button class=copy-to-clipboard title="Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, Image2text, Tokenization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03161v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03161v2.pdf filename=2402.03161v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In light of recent advances in <b>multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> there is increasing attention to scaling them from <b>image-text</b> data to more informative real-world videos. Compared to static images, video poses unique challenges for effective <b>large-scale</b> <b>pre-training</b> <b>due</b> to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an <b>LLM</b> using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the <b>LLM</b> are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 <b>multimodal</b> benchmarks in image and video understanding and generation. Our code and models will be available at <a href=https://video-lavit.github.io>https://video-lavit.github.io</a>.</p></p class="citation"></blockquote><h3 id=172285-nnmamba-3d-biomedical-image-segmentation-classification-and-landmark-detection-with-state-space-model-haifan-gong-et-al-2024>(172/285) nnMamba: 3D Biomedical Image Segmentation, Classification and Landmark Detection with State Space Model (Haifan Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haifan Gong, Luoyao Kang, Yitao Wang, Xiang Wan, Haofeng Li. (2024)<br><strong>nnMamba: 3D Biomedical Image Segmentation, Classification and Landmark Detection with State Space Model</strong><br><button class=copy-to-clipboard title="nnMamba: 3D Biomedical Image Segmentation, Classification and Landmark Detection with State Space Model" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03526v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03526v1.pdf filename=2402.03526v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of biomedical image analysis, the quest for architectures capable of effectively capturing long-range dependencies is paramount, especially when dealing with 3D image segmentation, classification, and landmark detection. Traditional <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> struggle with locality respective field, and <b>Transformers</b> have a heavy computational load when applied to high-dimensional medical images. In this paper, we introduce nnMamba, a novel architecture that integrates the strengths of <b>CNNs</b> and the advanced long-range modeling capabilities of State Space Sequence Models (SSMs). nnMamba adds the SSMs to the <b>convolutional</b> <b>residual-block</b> <b>to</b> extract local features and model complex dependencies. For diffirent tasks, we build different blocks to learn the features. Extensive experiments demonstrate nnMamba&rsquo;s superiority over state-of-the-art methods in a suite of challenging tasks, including 3D image segmentation, classification, and landmark detection. nnMamba emerges as a robust solution, offering both the local representation ability of <b>CNNs</b> and the efficient global context processing of SSMs, setting a new standard for long-range dependency modeling in medical image analysis. Code is available at <a href=https://github.com/lhaof/nnMamba>https://github.com/lhaof/nnMamba</a></p></p class="citation"></blockquote><h3 id=173285-physics-encoded-graph-neural-networks-for-deformation-prediction-under-contact-mahdi-saleh-et-al-2024>(173/285) Physics-Encoded Graph Neural Networks for Deformation Prediction under Contact (Mahdi Saleh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahdi Saleh, Michael Sommersperger, Nassir Navab, Federico Tombari. (2024)<br><strong>Physics-Encoded Graph Neural Networks for Deformation Prediction under Contact</strong><br><button class=copy-to-clipboard title="Physics-Encoded Graph Neural Networks for Deformation Prediction under Contact" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CG, cs-CV, cs-RO, cs.CV<br>Keyword Score: 40<br>Keywords: Graph Neural Network, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03466v1.pdf filename=2402.03466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In robotics, it&rsquo;s crucial to understand object deformation during tactile interactions. A precise understanding of deformation can elevate robotic <b>simulations</b> and have broad implications across different industries. We introduce a method using Physics-Encoded <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> for such predictions. Similar to robotic grasping and manipulation scenarios, we focus on modeling the dynamics between a rigid mesh contacting a deformable mesh under external forces. Our approach represents both the soft body and the rigid body within <b>graph</b> <b>structures,</b> <b>where</b> nodes hold the physical states of the meshes. We also incorporate cross-attention mechanisms to capture the interplay between the objects. By jointly learning geometry and physics, our model reconstructs consistent and detailed deformations. We&rsquo;ve made our code and dataset public to advance research in robotic <b>simulation</b> and grasping.</p></p class="citation"></blockquote><h3 id=174285-hassod-hierarchical-adaptive-self-supervised-object-detection-shengcao-cao-et-al-2024>(174/285) HASSOD: Hierarchical Adaptive Self-Supervised Object Detection (Shengcao Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, Yu-Xiong Wang. (2024)<br><strong>HASSOD: Hierarchical Adaptive Self-Supervised Object Detection</strong><br><button class=copy-to-clipboard title="HASSOD: Hierarchical Adaptive Self-Supervised Object Detection" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Self-supervised Learning, Self-supervised Learning, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03311v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03311v1.pdf filename=2402.03311v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of <b>objects.</b> <b>Drawing</b> inspiration from these two abilities, we propose Hierarchical Adaptive <b>Self-Supervised</b> <b>Object</b> <b>Detection</b> (HASSOD), a novel approach that learns to detect <b>objects</b> <b>and</b> understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into <b>object</b> <b>masks</b> based on <b>self-supervised</b> <b>visual</b> representations, adaptively determining the number of <b>objects</b> <b>per</b> image. Furthermore, HASSOD identifies the hierarchical levels of <b>objects</b> <b>in</b> terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional <b>self-supervised</b> <b>learning</b> task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from <b>semi-supervised</b> <b>learning,</b> which leads to a smoother and more efficient training process. Through extensive experiments on prevalent image datasets, we demonstrate the superiority of HASSOD over existing methods, thereby advancing the state of the art in <b>self-supervised</b> <b>object</b> <b>detection.</b> Notably, we improve Mask AR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page: <a href=https://HASSOD-NeurIPS23.github.io>https://HASSOD-NeurIPS23.github.io</a>.</p></p class="citation"></blockquote><h3 id=175285-clip-can-understand-depth-dunam-kim-et-al-2024>(175/285) CLIP Can Understand Depth (Dunam Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dunam Kim, Seokju Lee. (2024)<br><strong>CLIP Can Understand Depth</strong><br><button class=copy-to-clipboard title="CLIP Can Understand Depth" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Foundation Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03251v1.pdf filename=2402.03251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related <b>prompts.</b> In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without <b>fine-tuning</b> its original <b>vision-language</b> alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static <b>prompt</b> for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any <b>prompt</b> written in a human way. This research demonstrates that through minimal adjustments, the prior knowledge of <b>vision-language</b> <b>foundation</b> <b>models,</b> such as CLIP, can be generalized even to domains where learning during pretraining is challenging. We facilitate future works focused on methods to adjust suboptimal prior knowledge of <b>vision-language</b> models using non-human language <b>prompts,</b> achieving performance on par with task-specific state-of-the-art methodologies.</p></p class="citation"></blockquote><h3 id=176285-froster-frozen-clip-is-a-strong-teacher-for-open-vocabulary-action-recognition-xiaohu-huang-et-al-2024>(176/285) FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition (Xiaohu Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohu Huang, Hao Zhou, Kun Yao, Kai Han. (2024)<br><strong>FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition</strong><br><button class=copy-to-clipboard title="FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Knowledge Distillation, Image2text, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03241v1.pdf filename=2402.03241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce FROSTER, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability <b>stemming</b> from pretaining on massive <b>image-text</b> pairs. However, applying CLIP directly to the open-vocabulary action recognition task is challenging due to the absence of temporal information in CLIP&rsquo;s pretraining. Further, <b>fine-tuning</b> CLIP on action recognition datasets may lead to overfitting and hinder its generalizability, resulting in unsatisfactory results when dealing with unseen actions. To address these issues, FROSTER employs a residual feature <b>distillation</b> approach to ensure that CLIP retains its generalization capability while effectively adapting to the action recognition task. Specifically, the residual feature <b>distillation</b> treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features to bridge the gap between images and videos. Meanwhile, it uses a residual sub-network for feature <b>distillation</b> to reach a balance between the two distinct objectives of learning generalizable and video-specific features. We extensively evaluate FROSTER on open-vocabulary action recognition benchmarks under both base-to-novel and cross-dataset settings. FROSTER consistently achieves state-of-the-art performance on all datasets across the board. Project page: <a href=https://visual-ai.github.io/froster>https://visual-ai.github.io/froster</a>.</p></p class="citation"></blockquote><h3 id=177285-adatreeformer-few-shot-domain-adaptation-for-tree-counting-from-a-single-high-resolution-image-hamed-amini-amirkolaee-et-al-2024>(177/285) AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image (Hamed Amini Amirkolaee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamed Amini Amirkolaee, Miaojing Shi, Lianghua He, Mark Mulligan. (2024)<br><strong>AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image</strong><br><button class=copy-to-clipboard title="AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Few-shot, Knowledge Distillation, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02956v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02956v1.pdf filename=2402.02956v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source <b>domain</b> <b>with</b> sufficient labeled trees and is adapted to the target <b>domain</b> <b>with</b> only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target <b>domains.</b> <b>It</b> also consists of three subnets: two for extracting self-domain attention maps from source and target <b>domains</b> <b>respectively</b> and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to <b>distill</b> relevant information from different <b>domains</b> <b>while</b> generating tree density maps; a hierarchical cross-domain feature alignment scheme is proposed that progressively aligns the features from the source and target <b>domains.</b> <b>We</b> also adopt <b>adversarial</b> <b>learning</b> into the framework to further reduce the gap between source and target <b>domains.</b> <b>Our</b> AdaTreeFormer is evaluated on six designed <b>domain</b> <b>adaptation</b> tasks using three tree counting datasets, ie Jiangsu, Yosemite, and London; and outperforms the state of the art methods significantly.</p></p class="citation"></blockquote><h3 id=178285-delving-into-multi-modal-multi-task-foundation-models-for-road-scene-understanding-from-learning-paradigm-perspectives-sheng-luo-et-al-2024>(178/285) Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives (Sheng Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheng Luo, Wei Chen, Wanxin Tian, Rui Liu, Luanxuan Hou, Xiubao Zhang, Haifeng Shen, Ruiqi Wu, Shuyi Geng, Yi Zhou, Ling Shao, Yi Yang, Bojun Gao, Qun Li, Guobin Wu. (2024)<br><strong>Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives</strong><br><button class=copy-to-clipboard title="Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Continual Learning, Foundation Model, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02968v1.pdf filename=2402.02968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of <b>foundation</b> <b>models</b> has proven to be transformative, offering notable advancements in visual understanding. Equipped with <b>multi-modal</b> and multi-task learning capabilities, <b>multi-modal</b> multi-task visual understanding <b>foundation</b> <b>models</b> (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified <b>multi-modal</b> models, unified multi-task models, and <b>foundation</b> <b>model</b> <b>prompting</b> techniques, but also to highlight their advanced capabilities in diverse learning paradigms. These paradigms include open-world understanding, efficient transfer for road scenes, <b>continual</b> <b>learning,</b> interactive and generative capability. Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models. To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at <a href=https://github.com/rolsheng/MM-VUFM4DS>https://github.com/rolsheng/MM-VUFM4DS</a></p></p class="citation"></blockquote><h3 id=179285-vln-video-utilizing-driving-videos-for-outdoor-vision-and-language-navigation-jialu-li-et-al-2024>(179/285) VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation (Jialu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialu Li, Aishwarya Padmakumar, Gaurav Sukhatme, Mohit Bansal. (2024)<br><strong>VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation</strong><br><button class=copy-to-clipboard title="VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Masked Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03561v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03561v2.pdf filename=2402.03561v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Outdoor <b>Vision-and-Language</b> Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with three proxy tasks: <b>Masked</b> <b>Language</b> <b>Modeling,</b> Instruction and Trajectory Matching, and Next Action Prediction, so as to learn temporally-aware and visually-aligned instruction representations. The learned instruction representation is adapted to the state-of-the-art navigator when <b>fine-tuning</b> on the Touchdown dataset. Empirical results demonstrate that VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in task completion rate, achieving a new state-of-the-art on the Touchdown dataset.</p></p class="citation"></blockquote><h3 id=180285-constrained-multiview-representation-for-self-supervised-contrastive-learning-siyuan-dai-et-al-2024>(180/285) Constrained Multiview Representation for Self-supervised Contrastive Learning (Siyuan Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Dai, Kai Ye, Kun Zhao, Ge Cui, Haoteng Tang, Liang Zhan. (2024)<br><strong>Constrained Multiview Representation for Self-supervised Contrastive Learning</strong><br><button class=copy-to-clipboard title="Constrained Multiview Representation for Self-supervised Contrastive Learning" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Mutual Information, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03456v1.pdf filename=2402.03456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Representation learning constitutes a pivotal cornerstone in contemporary deep learning paradigms, offering a conduit to elucidate distinctive features within the latent space and interpret the deep models. Nevertheless, the inherent complexity of anatomical patterns and the random nature of lesion distribution in medical image segmentation pose significant challenges to the disentanglement of representations and the understanding of salient features. Methods guided by the maximization of <b>mutual</b> <b>information,</b> particularly within the framework of <b>contrastive</b> <b>learning,</b> have demonstrated remarkable success and superiority in decoupling densely intertwined representations. However, the effectiveness of <b>contrastive</b> <b>learning</b> highly depends on the quality of the positive and negative sample pairs, i.e. the unselected average <b>mutual</b> <b>information</b> among multi-views would obstruct the learning strategy so the selection of the views is vital. In this work, we introduce a novel approach predicated on representation distance-based <b>mutual</b> <b>information</b> (MI) maximization for measuring the significance of different views, aiming at conducting more efficient <b>contrastive</b> <b>learning</b> and representation disentanglement. Additionally, we introduce an MI re-ranking strategy for representation selection, benefiting both the continuous MI estimating and representation significance distance measuring. Specifically, we harness multi-view representations extracted from the frequency domain, re-evaluating their significance based on <b>mutual</b> <b>information</b> across varying frequencies, thereby facilitating a multifaceted <b>contrastive</b> <b>learning</b> approach to bolster semantic comprehension. The statistical results under the five metrics demonstrate that our proposed framework proficiently constrains the MI maximization-driven representation selection and steers the multi-view <b>contrastive</b> <b>learning</b> process.</p></p class="citation"></blockquote><h3 id=181285-training-free-consistent-text-to-image-generation-yoad-tewel-et-al-2024>(181/285) Training-Free Consistent Text-to-Image Generation (Yoad Tewel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, Yuval Atzmon. (2024)<br><strong>Training-Free Consistent Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Training-Free Consistent Text-to-Image Generation" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03286v1.pdf filename=2402.03286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse <b>prompts</b> remains challenging. Existing approaches <b>fine-tune</b> the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text <b>prompts</b> and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.</p></p class="citation"></blockquote><h3 id=182285-organic-or-diffused-can-we-distinguish-human-art-from-ai-generated-images-anna-yoo-jeong-ha-et-al-2024>(182/285) Organic or Diffused: Can We Distinguish Human Art from AI-generated Images? (Anna Yoo Jeong Ha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan, Reid Southen, Haitao Zheng, Ben Y. Zhao. (2024)<br><strong>Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?</strong><br><button class=copy-to-clipboard title="Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Generative AI, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03214v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03214v2.pdf filename=2402.03214v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>generative</b> <b>AI</b> images has completely disrupted the art world. Distinguishing AI generated images from human art is a challenging problem whose impact is growing over time. A failure to address this problem allows bad actors to defraud individuals paying a premium for human art and companies whose stated policies forbid AI imagery. It is also critical for content owners to establish copyright, and for model trainers interested in curating training data in order to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by <b>supervised</b> <b>learning,</b> research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today&rsquo;s modern <b>generative</b> <b>models</b> in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 <b>generative</b> <b>models,</b> and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness.</p></p class="citation"></blockquote><h3 id=183285-good-teachers-explain-explanation-enhanced-knowledge-distillation-amin-parchami-araghi-et-al-2024>(183/285) Good Teachers Explain: Explanation-Enhanced Knowledge Distillation (Amin Parchami-Araghi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Parchami-Araghi, Moritz Böhle, Sukrut Rao, Bernt Schiele. (2024)<br><strong>Good Teachers Explain: Explanation-Enhanced Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Good Teachers Explain: Explanation-Enhanced Knowledge Distillation" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03119v1.pdf filename=2402.03119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student&rsquo;s and teacher&rsquo;s functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the &lsquo;right features&rsquo; from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic <b>KD</b> loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed &rsquo;explanation-enhanced&rsquo; <b>KD</b> (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and to give similar explanations, and (3) is robust with respect to the model architectures, the amount of training data, and even works with &lsquo;approximate&rsquo;, pre-computed explanations.</p></p class="citation"></blockquote><h3 id=184285-time--memory--and-parameter-efficient-visual-adaptation-otniel-bogdan-mercea-et-al-2024>(184/285) Time-, Memory- and Parameter-Efficient Visual Adaptation (Otniel-Bogdan Mercea et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, Anurag Arnab. (2024)<br><strong>Time-, Memory- and Parameter-Efficient Visual Adaptation</strong><br><button class=copy-to-clipboard title="Time-, Memory- and Parameter-Efficient Visual Adaptation" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Foundation Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02887v1.pdf filename=2402.02887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>foundation</b> <b>models</b> become more popular, there is a growing need to efficiently <b>finetune</b> them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly. We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the training efficiency and scalability of our method by adapting a vision <b>transformer</b> backbone of 4 billion parameters for the computationally demanding task of video classification, without any intricate model parallelism. Here, we outperform a prior adaptor-based method which could only scale to a 1 billion parameter backbone, or fully-finetuning a smaller backbone, with the same GPU and less training time.</p></p class="citation"></blockquote><h3 id=185285-enhancing-compositional-generalization-via-compositional-feature-alignment-haoxiang-wang-et-al-2024>(185/285) Enhancing Compositional Generalization via Compositional Feature Alignment (Haoxiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxiang Wang, Haozhe Si, Huajie Shao, Han Zhao. (2024)<br><strong>Enhancing Compositional Generalization via Compositional Feature Alignment</strong><br><button class=copy-to-clipboard title="Enhancing Compositional Generalization via Compositional Feature Alignment" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, stat-ML<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02851v1.pdf filename=2402.02851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on <b>foundational</b> <b>models,</b> such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage <b>finetuning</b> technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) <b>fine-tunes</b> the encoder with the newly learned head frozen. We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models. We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision <b>foundation</b> <b>models.</b> Experiment results show that CFA outperforms common <b>finetuning</b> techniques in compositional generalization, corroborating CFA&rsquo;s efficacy in compositional feature learning.</p></p class="citation"></blockquote><h3 id=186285-joint-attention-guided-feature-fusion-network-for-saliency-detection-of-surface-defects-xiaoheng-jiang-et-al-2024>(186/285) Joint Attention-Guided Feature Fusion Network for Saliency Detection of Surface Defects (Xiaoheng Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoheng Jiang, Feng Yan, Yang Lu, Ke Wang, Shuai Guo, Tianzhu Zhang, Yanwei Pang, Jianwei Niu, Mingliang Xu. (2024)<br><strong>Joint Attention-Guided Feature Fusion Network for Saliency Detection of Surface Defects</strong><br><button class=copy-to-clipboard title="Joint Attention-Guided Feature Fusion Network for Saliency Detection of Surface Defects" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02797v1.pdf filename=2402.02797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Surface defect inspection plays an important role in the process of industrial manufacture and production. Though <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> based defect inspection methods have made huge leaps, they still confront a lot of challenges such as defect scale variation, complex background, low contrast, and so on. To address these issues, we propose a joint attention-guided feature fusion network (JAFFNet) for saliency detection of surface defects based on the encoder-decoder network. JAFFNet mainly incorporates a joint attention-guided feature fusion (JAFF) module into decoding stages to adaptively fuse low-level and high-level features. The JAFF module learns to emphasize defect features and suppress background noise during feature fusion, which is beneficial for detecting low-contrast defects. In addition, JAFFNet introduces a dense receptive field (DRF) module following the encoder to capture features with rich context information, which helps detect defects of different scales. The JAFF module mainly utilizes a learned joint channel-spatial attention map provided by high-level semantic features to guide feature fusion. The attention map makes the model pay more attention to defect features. The DRF module utilizes a sequence of multi-receptive-field (MRF) units with each taking as inputs all the preceding MRF feature maps and the original input. The obtained DRF features capture rich context information with a large range of receptive fields. Extensive experiments conducted on SD-saliency-900, Magnetic tile, and DAGM 2007 indicate that our method achieves promising performance in comparison with other state-of-the-art methods. Meanwhile, our method reaches a real-time defect detection speed of 66 FPS.</p></p class="citation"></blockquote><h3 id=187285-image-caption-encoding-for-improving-zero-shot-generalization-eric-yang-yu-et-al-2024>(187/285) Image-Caption Encoding for Improving Zero-Shot Generalization (Eric Yang Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Yang Yu, Christopher Liao, Sathvik Ravi, Theodoros Tsiligkaridis, Brian Kulis. (2024)<br><strong>Image-Caption Encoding for Improving Zero-Shot Generalization</strong><br><button class=copy-to-clipboard title="Image-Caption Encoding for Improving Zero-Shot Generalization" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02662v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02662v1.pdf filename=2402.02662v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>vision-language</b> models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like <b>zero-shot</b> image classification. However, a persistent issue of these models for image classification is their <b>out-of-distribution</b> (OOD) generalization capabilities. We first show that when an OOD data point is misclassified, the correct class can be typically found in the Top-K predicted classes. In order to steer the model prediction toward the correct class within the top predicted classes, we propose the Image-Caption Encoding (ICE) method, a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only. Intuitively, we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-K predicted classes. We show that our method can be easily combined with other SOTA methods to enhance Top-1 OOD accuracies by 0.5% on average and up to 3% on challenging datasets. Our code: <a href=https://github.com/Chris210634/ice>https://github.com/Chris210634/ice</a></p></p class="citation"></blockquote><h3 id=188285-aoneus-a-neural-rendering-framework-for-acoustic-optical-sensor-fusion-mohamad-qadri-et-al-2024>(188/285) AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion (Mohamad Qadri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamad Qadri, Kevin Zhang, Akshay Hinduja, Michael Kaess, Adithya Pediredla, Christopher A. Metzler. (2024)<br><strong>AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion</strong><br><button class=copy-to-clipboard title="AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03309v1.pdf filename=2402.03309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based <b>multimodal</b> acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive <b>simulations</b> and in-lab experiments, we demonstrate that AONeuS dramatically outperforms recent RGB-only and sonar-only inverse-differentiable-rendering&ndash;based surface reconstruction methods. A website visualizing the results of our paper is located at this address: <a href=https://aoneus.github.io/>https://aoneus.github.io/</a></p></p class="citation"></blockquote><h3 id=189285-activeanno3d----an-active-learning-framework-for-multi-modal-3d-object-detection-ahmed-ghita-et-al-2024>(189/285) ActiveAnno3D &ndash; An Active Learning Framework for Multi-Modal 3D Object Detection (Ahmed Ghita et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed Ghita, Bjørk Antoniussen, Walter Zimmer, Ross Greer, Christian Creß, Andreas Møgelmose, Mohan M. Trivedi, Alois C. Knoll. (2024)<br><strong>ActiveAnno3D &ndash; An Active Learning Framework for Multi-Modal 3D Object Detection</strong><br><button class=copy-to-clipboard title="ActiveAnno3D -- An Active Learning Framework for Multi-Modal 3D Object Detection" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Active Learning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03235v1.pdf filename=2402.03235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The curation of large-scale datasets is still costly and requires much time and resources. Data is often manually labeled, and the challenge of creating high-quality datasets remains. In this work, we fill the research gap using <b>active</b> <b>learning</b> for <b>multi-modal</b> 3D <b>object</b> <b>detection.</b> We propose ActiveAnno3D, an <b>active</b> <b>learning</b> framework to select data samples for labeling that are of maximum informativeness for training. We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance. Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset. We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset. BEVFusion achieved an mAP of 64.31 when using half of the training data and 75.0 mAP when using the complete nuScenes dataset. We integrate our <b>active</b> <b>learning</b> framework into the proAnno labeling tool to enable AI-assisted data selection and labeling and minimize the labeling costs. Finally, we provide code, weights, and visualization results on our website: <a href=https://active3d-framework.github.io/active3d-framework>https://active3d-framework.github.io/active3d-framework</a>.</p></p class="citation"></blockquote><h3 id=190285-iguane-a-3d-generalizable-cyclegan-for-multicenter-harmonization-of-brain-mr-images-vincent-roca-et-al-2024>(190/285) IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images (Vincent Roca et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincent Roca, Grégory Kuchcinski, Jean-Pierre Pruvo, Dorian Manouvriez, Renaud Lopes. (2024)<br><strong>IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images</strong><br><button class=copy-to-clipboard title="IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Sample Size, Image2text, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03227v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03227v2.pdf filename=2402.03227v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances <b>sample</b> <b>size</b> but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for <b>image</b> <b>translation</b> have emerged as a solution for harmonizing MR <b>images</b> <b>across</b> sites. In this study, we introduce IGUANe <b>(Image</b> <b>Generation</b> with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of <b>style</b> <b>transfer</b> methods for multicenter brain MR <b>image</b> <b>harmonization.</b> IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any <b>image,</b> <b>even</b> from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted <b>images</b> <b>from</b> 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the transformation of MR <b>images</b> <b>with</b> traveling subjects, the preservation of pairwise distances between MR <b>images</b> <b>within</b> domains, the evolution of volumetric patterns related to age and Alzheimer$^\prime$s disease (AD), and the performance in age regression and patient classification tasks. Comparisons with other harmonization and normalization methods suggest that IGUANe better preserves individual information in MR <b>images</b> <b>and</b> is more suitable for maintaining and reinforcing variabilities related to age and AD. Future studies may further assess IGUANe in other multicenter contexts, either using the same model or retraining it for applications to different <b>image</b> <b>modalities.</b></p></p class="citation"></blockquote><h3 id=191285-one-shot-neural-face-reenactment-via-finding-directions-in-gans-latent-space-stella-bounareli-et-al-2024>(191/285) One-shot Neural Face Reenactment via Finding Directions in GAN&rsquo;s Latent Space (Stella Bounareli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos. (2024)<br><strong>One-shot Neural Face Reenactment via Finding Directions in GAN&rsquo;s Latent Space</strong><br><button class=copy-to-clipboard title="One-shot Neural Face Reenactment via Finding Directions in GAN's Latent Space" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03553v1.pdf filename=2402.03553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present our framework for neural face/head reenactment whose goal is to transfer the 3D head orientation and expression of a target face to a source face. Previous methods focus on learning embedding networks for identity and head pose/expression disentanglement which proves to be a rather hard task, degrading the quality of the generated images. We take a different approach, bypassing the training of such networks, by using <b>(fine-tuned)</b> pre-trained <b>GANs</b> which have been shown capable of producing high-quality facial images. Because <b>GANs</b> are characterized by weak controllability, the core of our approach is a method to discover which directions in latent <b>GAN</b> space are responsible for controlling head pose and expression variations. We present a simple pipeline to learn such directions with the aid of a 3D shape model which, by construction, inherently captures disentangled directions for head pose, identity, and expression. Moreover, we show that by embedding real images in the <b>GAN</b> latent space, our method can be successfully used for the reenactment of real-world faces. Our method features several favorable properties including using a single source image (one-shot) and enabling cross-person reenactment. Extensive qualitative and quantitative results show that our approach typically produces reenacted faces of notably higher quality than those produced by state-of-the-art methods for the standard benchmarks of VoxCeleb1 & 2.</p></p class="citation"></blockquote><h3 id=192285-a-computer-vision-based-approach-for-stalking-detection-using-a-cnn-lstm-mlp-hybrid-fusion-model-murad-hasan-et-al-2024>(192/285) A Computer Vision Based Approach for Stalking Detection Using a CNN-LSTM-MLP Hybrid Fusion Model (Murad Hasan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Murad Hasan, Shahriar Iqbal, Md. Billal Hossain Faisal, Md. Musnad Hossin Neloy, Md. Tonmoy Kabir, Md. Tanzim Reza, Md. Golam Rabiul Alam, Md Zia Uddin. (2024)<br><strong>A Computer Vision Based Approach for Stalking Detection Using a CNN-LSTM-MLP Hybrid Fusion Model</strong><br><button class=copy-to-clipboard title="A Computer Vision Based Approach for Stalking Detection Using a CNN-LSTM-MLP Hybrid Fusion Model" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03417v1.pdf filename=2402.03417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Criminal and suspicious activity detection has become a popular research topic in recent years. The rapid growth of computer vision technologies has had a crucial impact on solving this issue. However, physical stalking detection is still a less explored area despite the evolution of modern technology. Nowadays, stalking in public places has become a common occurrence with women being the most affected. Stalking is a visible action that usually occurs before any criminal activity begins as the stalker begins to follow, loiter, and stare at the victim before committing any criminal activity such as assault, kidnapping, rape, and so on. Therefore, it has become a necessity to detect stalking as all of these criminal activities can be stopped in the first place through stalking detection. In this research, we propose a novel deep learning-based hybrid fusion model to detect potential stalkers from a single video with a minimal number of frames. We extract multiple relevant features, such as facial landmarks, head pose estimation, and relative distance, as numerical values from video frames. This data is fed into a multilayer perceptron (MLP) to perform a classification task between a stalking and a non-stalking scenario. Simultaneously, the video frames are fed into a combination of <b>convolutional</b> and LSTM models to extract the spatio-temporal features. We use a fusion of these numerical and spatio-temporal features to build a classifier to detect stalking incidents. Additionally, we introduce a dataset consisting of stalking and non-stalking videos gathered from various feature films and television series, which is also used to train the model. The experimental results show the efficiency and dynamism of our proposed stalker detection system, achieving 89.58% testing accuracy with a significant improvement as compared to the state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=193285-transcending-adversarial-perturbations-manifold-aided-adversarial-examples-with-legitimate-semantics-shuai-li-et-al-2024>(193/285) Transcending Adversarial Perturbations: Manifold-Aided Adversarial Examples with Legitimate Semantics (Shuai Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Li, Xiaoyu Jiang, Xiaoguang Ma. (2024)<br><strong>Transcending Adversarial Perturbations: Manifold-Aided Adversarial Examples with Legitimate Semantics</strong><br><button class=copy-to-clipboard title="Transcending Adversarial Perturbations: Manifold-Aided Adversarial Examples with Legitimate Semantics" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03095v1.pdf filename=2402.03095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks were significantly vulnerable to <b>adversarial</b> <b>examples</b> manipulated by malicious tiny perturbations. Although most conventional <b>adversarial</b> <b>attacks</b> ensured the visual imperceptibility between <b>adversarial</b> <b>examples</b> and corresponding raw images by minimizing their geometric distance, these constraints on geometric distance led to limited attack transferability, inferior visual quality, and human-imperceptible interpretability. In this paper, we proposed a <b>supervised</b> semantic-transformation generative model to generate <b>adversarial</b> <b>examples</b> with real and legitimate semantics, wherein an unrestricted <b>adversarial</b> <b>manifold</b> containing continuous semantic variations was constructed for the first time to realize a legitimate transition from non-adversarial examples to <b>adversarial</b> <b>ones.</b> Comprehensive experiments on MNIST and industrial defect datasets showed that our <b>adversarial</b> <b>examples</b> not only exhibited better visual quality but also achieved superior attack transferability and more effective explanations for model vulnerabilities, indicating their great potential as generic <b>adversarial</b> <b>examples.</b> The code and pre-trained models were available at <a href=https://github.com/shuaili1027/MAELS.git>https://github.com/shuaili1027/MAELS.git</a>.</p></p class="citation"></blockquote><h3 id=194285-taylor-videos-for-action-recognition-lei-wang-et-al-2024>(194/285) Taylor Videos for Action Recognition (Lei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Wang, Xiuyuan Yuan, Tom Gedeon, Liang Zheng. (2024)<br><strong>Taylor Videos for Action Recognition</strong><br><button class=copy-to-clipboard title="Taylor Videos for Action Recognition" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03019v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03019v3.pdf filename=2402.03019v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effectively extracting motions from video is a critical and long-standing problem for action recognition. This problem is very challenging because motions (i) do not have an explicit form, (ii) have various concepts such as displacement, velocity, and acceleration, and (iii) often contain noise caused by unstable pixels. Addressing these challenges, we propose the Taylor video, a new video format that highlights the dominate motions (e.g., a waving hand) in each of its frames named the Taylor frame. Taylor video is named after Taylor series, which approximates a function at a given point using important terms. In the scenario of videos, we define an implicit motion-extraction function which aims to extract motions from video temporal block. In this block, using the frames, the difference frames, and higher-order difference frames, we perform Taylor expansion to approximate this function at the starting frame. We show the summation of the higher-order terms in the Taylor series gives us dominant motion patterns, where static objects, small and unstable motions are removed. Experimentally we show that Taylor videos are effective inputs to popular architectures including 2D <b>CNNs,</b> 3D <b>CNNs,</b> and <b>transformers.</b> When used individually, Taylor videos yield competitive action recognition accuracy compared to RGB videos and optical flow. When fused with RGB or optical flow videos, further accuracy improvement is achieved.</p></p class="citation"></blockquote><h3 id=195285-retrieval-augmented-score-distillation-for-text-to-3d-generation-junyoung-seo-et-al-2024>(195/285) Retrieval-Augmented Score Distillation for Text-to-3D Generation (Junyoung Seo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyoung Seo, Susung Hong, Wooseok Jang, Inès Hyeonsu Kim, Minseop Kwak, Doyup Lee, Seungryong Kim. (2024)<br><strong>Retrieval-Augmented Score Distillation for Text-to-3D Generation</strong><br><button class=copy-to-clipboard title="Retrieval-Augmented Score Distillation for Text-to-3D Generation" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02972v1.pdf filename=2402.02972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, <b>fine-tuning</b> the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score <b>distillation,</b> dubbed RetDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model&rsquo;s 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes. We conduct extensive experiments to demonstrate that RetDream exhibits superior quality with increased geometric consistency. Project page is available at <a href=https://ku-cvlab.github.io/RetDream/>https://ku-cvlab.github.io/RetDream/</a>.</p></p class="citation"></blockquote><h3 id=196285-toonaging-face-re-aging-upon-artistic-portrait-style-transfer-bumsoo-kim-et-al-2024>(196/285) ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer (Bumsoo Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bumsoo Kim, Abdul Muqeet, Kyuchul Lee, Sanghyun Seo. (2024)<br><strong>ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer</strong><br><button class=copy-to-clipboard title="ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02733v1.pdf filename=2402.02733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait <b>style</b> <b>transfer,</b> executed in a single generative step. We leverage existing face re-aging and <b>style</b> <b>transfer</b> networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attributes and NPR appearance. Adopting an exemplar-based approach, our method offers greater flexibility than domain-level <b>fine-tuning</b> approaches, which typically require separate training or <b>fine-tuning</b> for each domain. This effectively addresses the limitation of requiring paired datasets for re-aging and domain-level, data-driven approaches for stylization. Our experiments show that our model can effortlessly generate re-aged images while simultaneously transferring the <b>style</b> <b>of</b> examples, maintaining both natural appearance and controllability.</p></p class="citation"></blockquote><h3 id=197285-interactivevideo-user-centric-controllable-video-generation-with-synergistic-multimodal-instructions-yiyuan-zhang-et-al-2024>(197/285) InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions (Yiyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyuan Zhang, Yuhao Kang, Zhixin Zhang, Xiaohan Ding, Sanyuan Zhao, Xiangyu Yue. (2024)<br><strong>InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions</strong><br><button class=copy-to-clipboard title="InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs-MM, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03040v1.pdf filename=2402.03040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce $\textit{InteractiveVideo}$, a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image <b>prompts,</b> painting, drag-and-drop, etc. We propose a Synergistic <b>Multimodal</b> Instruction mechanism, designed to seamlessly integrate users&rsquo; <b>multimodal</b> instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With $\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions until their requirements are fully met. Code, models, and demo are available at <a href=https://github.com/invictus717/InteractiveVideo>https://github.com/invictus717/InteractiveVideo</a></p></p class="citation"></blockquote><h3 id=198285-decoder-only-image-registration-xi-jia-et-al-2024>(198/285) Decoder-Only Image Registration (Xi Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xi Jia, Wenqi Lu, Xinxing Cheng, Jinming Duan. (2024)<br><strong>Decoder-Only Image Registration</strong><br><button class=copy-to-clipboard title="Decoder-Only Image Registration" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03585v1.pdf filename=2402.03585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>unsupervised</b> medical image registration, the predominant approaches involve the utilization of a encoder-decoder network architecture, allowing for precise prediction of dense, full-resolution displacement fields from given paired images. Despite its widespread use in the literature, we argue for the necessity of making both the encoder and decoder learnable in such an architecture. For this, we propose a novel network architecture, termed LessNet in this paper, which contains only a learnable decoder, while entirely omitting the utilization of a learnable encoder. LessNet substitutes the learnable encoder with simple, handcrafted features, eliminating the need to learn (optimize) network parameters in the encoder altogether. Consequently, this leads to a compact, efficient, and decoder-only architecture for 3D medical image registration. Evaluated on two publicly available brain MRI datasets, we demonstrate that our decoder-only LessNet can effectively and efficiently learn both dense displacement and diffeomorphic deformation fields in 3D. Furthermore, our decoder-only LessNet can achieve comparable registration performance to state-of-the-art methods such as VoxelMorph and TransMorph, while requiring significantly fewer computational resources. Our code and pre-trained models are available at <a href=https://github.com/xi-jia/LessNet>https://github.com/xi-jia/LessNet</a>.</p></p class="citation"></blockquote><h3 id=199285-an-inpainting-infused-pipeline-for-attire-and-background-replacement-felipe-rodrigues-perche-mahlow-et-al-2024>(199/285) An Inpainting-Infused Pipeline for Attire and Background Replacement (Felipe Rodrigues Perche-Mahlow et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felipe Rodrigues Perche-Mahlow, André Felipe-Zanella, William Alberto Cruz-Castañeda, Marcellus Amadeus. (2024)<br><strong>An Inpainting-Infused Pipeline for Attire and Background Replacement</strong><br><button class=copy-to-clipboard title="An Inpainting-Infused Pipeline for Attire and Background Replacement" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03501v1.pdf filename=2402.03501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, groundbreaking advancements in Generative Artificial Intelligence (GenAI) have triggered a transformative paradigm shift, significantly influencing various domains. In this work, we specifically explore an integrated approach, leveraging advanced techniques in GenAI and computer vision emphasizing image manipulation. The methodology unfolds through several stages, including depth estimation, the creation of inpaint masks based on depth information, the generation and replacement of backgrounds utilizing Stable Diffusion in conjunction with Latent Consistency Models (LCMs), and the subsequent replacement of clothes and application of aesthetic changes through an inpainting pipeline. Experiments conducted in this study underscore the methodology&rsquo;s efficacy, highlighting its potential to produce visually captivating content. The convergence of these advanced techniques allows users to input photographs of individuals and manipulate them to modify clothing and background based on specific <b>prompts</b> without manually input inpainting masks, effectively placing the subjects within the vast landscape of creative imagination.</p></p class="citation"></blockquote><h3 id=200285-test-time-adaptation-for-depth-completion-hyoungseob-park-et-al-2024>(200/285) Test-Time Adaptation for Depth Completion (Hyoungseob Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyoungseob Park, Anjali Gupta, Alex Wong. (2024)<br><strong>Test-Time Adaptation for Depth Completion</strong><br><button class=copy-to-clipboard title="Test-Time Adaptation for Depth Completion" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03312v1.pdf filename=2402.03312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a <b>domain</b> <b>gap</b> between them. Existing methods for bridging this gap, such as <b>domain</b> <b>adaptation</b> (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the <b>domain</b> <b>shift</b> in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source <b>domain</b> <b>that</b> preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During test time, sparse depth features are projected using this map as a proxy for source <b>domain</b> <b>features</b> and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test <b>domain</b> <b>to</b> that of the source <b>domain.</b> <b>We</b> evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%.</p></p class="citation"></blockquote><h3 id=201285-instancediffusion-instance-level-control-for-image-generation-xudong-wang-et-al-2024>(201/285) InstanceDiffusion: Instance-level Control for Image Generation (Xudong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra. (2024)<br><strong>InstanceDiffusion: Instance-level Control for Image Generation</strong><br><button class=copy-to-clipboard title="InstanceDiffusion: Instance-level Control for Image Generation" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03290v1.pdf filename=2402.03290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to <b>text-to-image</b> diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to <b>text-to-image</b> models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for <b>text-to-image</b> models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, and 25.4% IoU for mask inputs.</p></p class="citation"></blockquote><h3 id=202285-direct-a-video-customized-video-generation-with-user-directed-camera-movement-and-object-motion-shiyuan-yang-et-al-2024>(202/285) Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion (Shiyuan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, Jing Liao. (2024)<br><strong>Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion</strong><br><button class=copy-to-clipboard title="Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03162v1.pdf filename=2402.03162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent text-to-video diffusion models have achieved impressive progress. In practice, users often desire the ability to control object motion and camera movement independently for customized video creation. However, current methods lack the focus on separately controlling object motion and camera movement in a decoupled manner, which limits the controllability and flexibility of text-to-video models. In this paper, we introduce Direct-a-Video, a system that allows users to independently specify motions for one or multiple objects and/or camera movements, as if directing a video. We propose a simple yet effective strategy for the decoupled control of object motion and camera movement. Object motion is controlled through spatial cross-attention modulation using the model&rsquo;s inherent priors, requiring no additional optimization. For camera movement, we introduce new temporal cross-attention layers to interpret quantitative camera movement parameters. We further employ an augmentation-based approach to train these layers in a <b>self-supervised</b> manner on a small-scale dataset, eliminating the need for explicit motion annotation. Both components operate independently, allowing individual or combined control, and can generalize to open-domain scenarios. Extensive experiments demonstrate the superiority and effectiveness of our method. Project page: <a href=https://direct-a-video.github.io/>https://direct-a-video.github.io/</a>.</p></p class="citation"></blockquote><h3 id=203285-visual-text-meets-low-level-vision-a-comprehensive-survey-on-visual-text-processing-yan-shu-et-al-2024>(203/285) Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing (Yan Shu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Shu, Weichao Zeng, Zhenhang Li, Fangmin Zhao, Yu Zhou. (2024)<br><strong>Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing</strong><br><button class=copy-to-clipboard title="Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03082v1.pdf filename=2402.03082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual text, a pivotal element in both document and scene images, speaks volumes and attracts significant attention in the computer vision domain. Beyond visual text detection and recognition, the field of visual text processing has experienced a surge in research, driven by the advent of fundamental generative models. However, challenges persist due to the unique properties and features that distinguish text from general objects. Effectively leveraging these unique textual characteristics is crucial in visual text processing, as observed in our study. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in this field. Initially, we introduce a hierarchical taxonomy encompassing areas ranging from text image enhancement and restoration to text image manipulation, followed by different learning paradigms. Subsequently, we conduct an in-depth discussion of how specific textual features such as structure, stroke, semantics, style, and spatial context are seamlessly integrated into various tasks. Furthermore, we explore available public datasets and benchmark the reviewed methods on several widely-used datasets. Finally, we identify principal challenges and potential avenues for future research. Our aim is to establish this survey as a fundamental resource, fostering continued exploration and innovation in the dynamic area of visual text processing.</p></p class="citation"></blockquote><h3 id=204285-pfdm-parser-free-virtual-try-on-via-diffusion-model-yunfang-niu-et-al-2024>(204/285) PFDM: Parser-Free Virtual Try-on via Diffusion Model (Yunfang Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunfang Niu, Dong Yi, Lingxiang Wu, Zhiwei Liu, Pengxiang Cai, Jinqiao Wang. (2024)<br><strong>PFDM: Parser-Free Virtual Try-on via Diffusion Model</strong><br><button class=copy-to-clipboard title="PFDM: Parser-Free Virtual Try-on via Diffusion Model" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03047v1.pdf filename=2402.03047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision. However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling. To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM). Given two images, PFDM can &ldquo;wear&rdquo; garments on the target person seamlessly by implicitly warping without any other information. To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons. <b>Supervised</b> by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and outperform both state-of-the-art parser-free and parser-based models.</p></p class="citation"></blockquote><h3 id=205285-houghtoradon-transform-new-neural-network-layer-for-features-improvement-in-projection-space-alexandra-zhabitskaya-et-al-2024>(205/285) HoughToRadon Transform: New Neural Network Layer for Features Improvement in Projection Space (Alexandra Zhabitskaya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandra Zhabitskaya, Alexander Sheshkus, Vladimir L. Arlazarov. (2024)<br><strong>HoughToRadon Transform: New Neural Network Layer for Features Improvement in Projection Space</strong><br><button class=copy-to-clipboard title="HoughToRadon Transform: New Neural Network Layer for Features Improvement in Projection Space" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-NE, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02946v1.pdf filename=2402.02946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce HoughToRadon Transform layer, a novel layer designed to improve the speed of neural networks incorporated with Hough Transform to solve semantic image segmentation problems. By placing it after a Hough Transform layer, &ldquo;inner&rdquo; <b>convolutions</b> receive modified feature maps with new beneficial properties, such as a smaller area of processed images and parameter space linearity by angle and shift. These properties were not presented in Hough Transform alone. Furthermore, HoughToRadon Transform layer allows us to adjust the size of intermediate feature maps using two new parameters, thus allowing us to balance the speed and quality of the resulting neural network. Our experiments on the open MIDV-500 dataset show that this new approach leads to time savings in document segmentation tasks and achieves state-of-the-art 97.7% accuracy, outperforming HoughEncoder with larger computational complexity.</p></p class="citation"></blockquote><h3 id=206285-pixel-wise-color-constancy-via-smoothness-techniques-in-multi-illuminant-scenes-umut-cem-entok-et-al-2024>(206/285) Pixel-Wise Color Constancy via Smoothness Techniques in Multi-Illuminant Scenes (Umut Cem Entok et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Umut Cem Entok, Firas Laakom, Farhad Pakdaman, Moncef Gabbouj. (2024)<br><strong>Pixel-Wise Color Constancy via Smoothness Techniques in Multi-Illuminant Scenes</strong><br><button class=copy-to-clipboard title="Pixel-Wise Color Constancy via Smoothness Techniques in Multi-Illuminant Scenes" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Label Smoothing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02922v1.pdf filename=2402.02922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most scenes are illuminated by several light sources, where the traditional assumption of uniform illumination is invalid. This issue is ignored in most color constancy methods, primarily due to the complex spatial impact of multiple light sources on the image. Moreover, most existing multi-illuminant methods fail to preserve the smooth change of illumination, which stems from spatial dependencies in natural images. Motivated by this, we propose a novel multi-illuminant color constancy method, by learning pixel-wise illumination maps caused by multiple light sources. The proposed method enforces smoothness within neighboring pixels, by regularizing the training with the total variation loss. Moreover, a bilateral filter is provisioned further to enhance the natural appearance of the estimated images, while preserving the edges. Additionally, we propose a <b>label-smoothing</b> <b>technique</b> that enables the model to generalize well despite the uncertainties in ground truth. Quantitative and qualitative experiments demonstrate that the proposed method outperforms the state-of-the-art.</p></p class="citation"></blockquote><h3 id=207285-improving-robustness-of-lidar-camera-fusion-model-against-weather-corruption-from-fusion-strategy-perspective-yihao-huang-et-al-2024>(207/285) Improving Robustness of LiDAR-Camera Fusion Model against Weather Corruption from Fusion Strategy Perspective (Yihao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihao Huang, Kaiyuan Yu, Qing Guo, Felix Juefei-Xu, Xiaojun Jia, Tianlin Li, Geguang Pu, Yang Liu. (2024)<br><strong>Improving Robustness of LiDAR-Camera Fusion Model against Weather Corruption from Fusion Strategy Perspective</strong><br><button class=copy-to-clipboard title="Improving Robustness of LiDAR-Camera Fusion Model against Weather Corruption from Fusion Strategy Perspective" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02738v1.pdf filename=2402.02738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, LiDAR-camera fusion models have markedly advanced 3D <b>object</b> <b>detection</b> tasks in autonomous driving. However, their robustness against common weather corruption such as fog, rain, snow, and sunlight in the intricate physical world remains underexplored. In this paper, we evaluate the robustness of fusion models from the perspective of fusion strategies on the corrupted dataset. Based on the evaluation, we further propose a concise yet practical fusion strategy to enhance the robustness of the fusion models, namely flexibly weighted fusing features from LiDAR and camera sources to adapt to varying weather scenarios. Experiments conducted on four types of fusion models, each with two distinct lightweight implementations, confirm the broad applicability and effectiveness of the approach.</p></p class="citation"></blockquote><h3 id=208285-using-motion-cues-to-supervise-single-frame-body-pose-and-shape-estimation-in-low-data-regimes-andrey-davydov-et-al-2024>(208/285) Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation in Low Data Regimes (Andrey Davydov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrey Davydov, Alexey Sidnev, Artsiom Sanakoyeu, Yuhua Chen, Mathieu Salzmann, Pascal Fua. (2024)<br><strong>Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation in Low Data Regimes</strong><br><button class=copy-to-clipboard title="Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation in Low Data Regimes" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02736v1.pdf filename=2402.02736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When enough annotated training data is available, <b>supervised</b> deep-learning algorithms excel at estimating human body pose and shape using a single camera. The effects of too little such data being available can be mitigated by using other information sources, such as databases of body shapes, to learn priors. Unfortunately, such sources are not always available either. We show that, in such cases, easy-to-obtain unannotated videos can be used instead to provide the required supervisory signals. Given a trained model using too little annotated data, we compute poses in consecutive frames along with the optical flow between them. We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next. This provides enough additional supervision to effectively refine the network weights and to perform on par with methods trained using far more annotated data.</p></p class="citation"></blockquote><h2 id=csit-9>cs.IT (9)</h2><h3 id=209285-fast-and-accurate-cooperative-radio-map-estimation-enabled-by-gan-zezhong-zhang-et-al-2024>(209/285) Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN (Zezhong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zezhong Zhang, Guangxu Zhu, Junting Chen, Shuguang Cui. (2024)<br><strong>Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN</strong><br><button class=copy-to-clipboard title="Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-CV, cs-IT, cs-LG, cs.IT, eess-IV, math-IT<br>Keyword Score: 50<br>Keywords: Generative AI, Generative Adversarial Network, Generative Adversarial Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02729v1.pdf filename=2402.02729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the 6G era, real-time radio resource monitoring and management are urged to support diverse wireless-empowered applications. This calls for fast and accurate estimation on the distribution of the radio resources, which is usually represented by the spatial signal power strength over the geographical environment, known as a radio map. In this paper, we present a cooperative radio map estimation (CRME) approach enabled by the <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN),</b> called as <b>GAN-CRME,</b> which features fast and accurate radio map estimation without the transmitters&rsquo; information. The radio map is inferred by exploiting the interaction between distributed received signal strength (RSS) measurements at mobile users and the geographical map using a deep neural network estimator, resulting in low data-acquisition cost and computational complexity. Moreover, a <b>GAN-based</b> learning algorithm is proposed to boost the inference capability of the deep neural network estimator by exploiting the power of <b>generative</b> <b>AI.</b> <b>Simulation</b> results showcase that the proposed <b>GAN-CRME</b> is even capable of coarse error-correction when the geographical map information is inaccurate.</p></p class="citation"></blockquote><h3 id=210285-multi-agent-reinforcement-learning-for-energy-saving-in-multi-cell-massive-mimo-systems-tianzhang-cai-et-al-2024>(210/285) Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems (Tianzhang Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianzhang Cai, Qichen Wang, Shuai Zhang, Özlem Tuğfe Demir, Cicek Cavdar. (2024)<br><strong>Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems</strong><br><button class=copy-to-clipboard title="Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-AI, cs-IT, cs-LG, cs.IT, math-IT<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03204v1.pdf filename=2402.03204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop a multi-agent <b>reinforcement</b> <b>learning</b> (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference. A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy. To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed. <b>Simulation</b> results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies. Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor policy reduces power consumption by approximately 8.7% during low-traffic hours and improves energy efficiency by approximately 19% during high-traffic hours, respectively.</p></p class="citation"></blockquote><h3 id=211285-rejection-sampled-universal-quantization-for-smaller-quantization-errors-chih-wei-ling-et-al-2024>(211/285) Rejection-Sampled Universal Quantization for Smaller Quantization Errors (Chih Wei Ling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chih Wei Ling, Cheuk Ting Li. (2024)<br><strong>Rejection-Sampled Universal Quantization for Smaller Quantization Errors</strong><br><button class=copy-to-clipboard title="Rejection-Sampled Universal Quantization for Smaller Quantization Errors" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Quantization, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03030v1.pdf filename=2402.03030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We construct a randomized vector quantizer which has a smaller maximum error compared to all known lattice quantizers with the same entropy for dimensions 5, 6, &mldr;, 48, and also has a smaller mean squared error compared to known lattice quantizers with the same entropy for dimensions 35, &mldr;, 48, in the high resolution limit. Moreover, our randomized quantizer has a desirable property that the <b>quantization</b> error is always uniform over the ball and independent of the input. Our construction is based on applying rejection sampling on universal <b>quantization,</b> which allows us to shape the error distribution to be any continuous distribution, not only uniform distributions over basic cells of a lattice as in conventional dithered <b>quantization.</b> We also characterize the high SNR limit of one-shot channel <b>simulation</b> for any additive noise channel under a mild assumption (e.g., the AWGN channel), up to an additive constant of 1.45 bits.</p></p class="citation"></blockquote><h3 id=212285-on-the-performance-of-ris-aided-spatial-modulation-for-downlink-transmission-xusheng-zhu-et-al-2024>(212/285) On the Performance of RIS-Aided Spatial Modulation for Downlink Transmission (Xusheng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xusheng Zhu, Qingqing Wu, Wen Chen. (2024)<br><strong>On the Performance of RIS-Aided Spatial Modulation for Downlink Transmission</strong><br><button class=copy-to-clipboard title="On the Performance of RIS-Aided Spatial Modulation for Downlink Transmission" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02893v1.pdf filename=2402.02893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we explore the performance of a reconfigurable reflecting surface (RIS)-assisted transmit spatial modulation (SM) system for downlink transmission, wherein the deployment of RIS serves the purpose of blind area coverage within the channel. At the receiving end, we present three detectors, i.e., maximum likelihood (ML) detector, two-stage ML detection, and greedy detector to recover the transmitted signal. By utilizing the ML detector, we initially derive the conditional pair error probability expression for the proposed scheme. Subsequently, we leverage the central limit theorem (CLT) to obtain the probability density function of the combined channel. Following this, the Gaussian-Chebyshev quadrature method is applied to derive a closed-form expression for the unconditional pair error probability and establish the union tight upper bound for the average bit error probability (ABEP). Furthermore, we derive a closed-form expression for the ergodic capacity of the proposed RIS-SM scheme. Monte Carlo <b>simulations</b> are conducted not only to assess the complexity and reliability of the three detection algorithms but also to validate the results obtained through theoretical derivation results.</p></p class="citation"></blockquote><h3 id=213285-joint-beamforming-design-for-the-star-ris-enabled-isac-systems-with-multiple-targets-and-multiple-users-shuang-zhang-et-al-2024>(213/285) Joint Beamforming Design for the STAR-RIS-Enabled ISAC Systems with Multiple Targets and Multiple Users (Shuang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuang Zhang, Wanming Hao, Gangcan Sun, Zhengyu Zhu, Xingwang Li, Qingqing Wu. (2024)<br><strong>Joint Beamforming Design for the STAR-RIS-Enabled ISAC Systems with Multiple Targets and Multiple Users</strong><br><button class=copy-to-clipboard title="Joint Beamforming Design for the STAR-RIS-Enabled ISAC Systems with Multiple Targets and Multiple Users" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-ET, cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03949v1.pdf filename=2402.03949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, the sensing beam pattern gain under simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RIS)-enabled integrated sensing and communications (ISAC) systems is investigated, in which multiple targets and multiple users exist. However, multiple targets detection introduces new challenges, since the STAR-RIS cannot directly send sensing beams and detect targets, the dual-functional base station (DFBS) is required to analyze the echoes of the targets. While the echoes reflected by different targets through STAR-RIS come from the same direction for the DFBS, making it impossible to distinguish them. To address the issue, we first introduce the signature sequence (SS) modulation scheme to the ISAC system, and thus, the DFBS can detect different targets by the SS-modulated sensing beams. Next, via the joint beamforming design of DFBS and STAR-RIS, we develop a maxmin sensing beam pattern gain problem, and meanwhile, considering the communication quality requirements, the interference limitations of other targets and users, the passive nature constraint of STAR-RIS, and the total transmit power limitation. Then, to tackle the complex non-convex problem, we propose an alternating optimization method to divide it into two quadratic semidefinite program subproblems and decouple the coupled variables. Drawing on mathematical transformation, semidefinite programming, as well as semidefinite relaxation techniques, these two subproblems are iteratively sloved until convergence, and the ultimate solutions are obtained. Finally, <b>simulation</b> results are conducted to validate the benefits and efficiency of our proposed scheme.</p></p class="citation"></blockquote><h3 id=214285-successive-bayesian-reconstructor-for-fas-channel-estimation-zijian-zhang-et-al-2024>(214/285) Successive Bayesian Reconstructor for FAS Channel Estimation (Zijian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Zhang, Jieao Zhu, Linglong Dai, Robert W. Heath Jr. (2024)<br><strong>Successive Bayesian Reconstructor for FAS Channel Estimation</strong><br><button class=copy-to-clipboard title="Successive Bayesian Reconstructor for FAS Channel Estimation" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-SY, cs.IT, eess-SP, eess-SY, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02688v1.pdf filename=2402.02688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fluid antenna systems (FASs) can reconfigure their locations freely within a spatially continuous space. To keep favorable antenna positions, the channel state information (CSI) acquisition for FASs is essential. While some techniques have been proposed, most existing FAS channel estimators require several channel assumptions, such as slow variation and angular-domain sparsity. When these assumptions are not reasonable, the model mismatch may lead to unpredictable performance loss. In this paper, we propose the successive Bayesian reconstructor (S-BAR) as a general solution to estimate FAS channels. Unlike model-based estimators, the proposed S-BAR is prior-aided, which builds the experiential kernel for CSI acquisition. Inspired by Bayesian regression, the key idea of S-BAR is to model the FAS channels as a stochastic process, whose uncertainty can be successively eliminated by kernel-based sampling and regression. In this way, the predictive mean of the regressed stochastic process can be viewed as the maximum a posterior (MAP) estimator of FAS channels. <b>Simulation</b> results verify that, in both model-mismatched and model-matched cases, the proposed S-BAR can achieve higher estimation accuracy than the existing schemes.</p></p class="citation"></blockquote><h3 id=215285-explicit-formula-for-partial-information-decomposition-aobo-lyu-et-al-2024>(215/285) Explicit Formula for Partial Information Decomposition (Aobo Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aobo Lyu, Andrew Clark, Netanel Raviv. (2024)<br><strong>Explicit Formula for Partial Information Decomposition</strong><br><button class=copy-to-clipboard title="Explicit Formula for Partial Information Decomposition" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT, math-PR<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03554v1.pdf filename=2402.03554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Mutual</b> <b>information</b> between two random variables is a well-studied notion, whose understanding is fairly complete. <b>Mutual</b> <b>information</b> between one random variable and a pair of other random variables, however, is a far more involved notion. Specifically, Shannon&rsquo;s <b>mutual</b> <b>information</b> does not capture fine-grained interactions between those three variables, resulting in limited insights in complex systems. To capture these fine-grained interactions, in 2010 Williams and Beer proposed to decompose this <b>mutual</b> <b>information</b> to information atoms, called unique, redundant, and synergistic, and proposed several operational axioms that these atoms must satisfy. In spite of numerous efforts, a general formula which satisfies these axioms has yet to be found. Inspired by Judea Pearl&rsquo;s do-calculus, we resolve this open problem by introducing the do-operation, an operation over the variable system which sets a certain marginal to a desired value, which is distinct from any existing approaches. Using this operation, we provide the first explicit formula for calculating the information atoms so that Williams and Beer&rsquo;s axioms are satisfied, as well as additional properties from subsequent studies in the field.</p></p class="citation"></blockquote><h3 id=216285-algorithms-for-computing-the-free-distance-of-convolutional-codes-zita-abreu-et-al-2024>(216/285) Algorithms for Computing the Free Distance of Convolutional Codes (Zita Abreu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zita Abreu, Joachim Rosenthal, Michael Schaller. (2024)<br><strong>Algorithms for Computing the Free Distance of Convolutional Codes</strong><br><button class=copy-to-clipboard title="Algorithms for Computing the Free Distance of Convolutional Codes" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02982v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02982v1.pdf filename=2402.02982v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The free distance of a <b>convolutional</b> code is a reliable indicator of its performance. However its computation is not an easy task. In this paper, we present some algorithms to compute the free distance with good efficiency that work for <b>convolutional</b> codes of all rates and over any field. Furthermore we discuss why an algorithm which is claimed to be very efficient is incorrect.</p></p class="citation"></blockquote><h3 id=217285-code-based-single-server-private-information-retrieval-circumventing-the-sub-query-attack-neehar-verma-et-al-2024>(217/285) Code-Based Single-Server Private Information Retrieval: Circumventing the Sub-Query Attack (Neehar Verma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neehar Verma, Camilla Hollanti. (2024)<br><strong>Code-Based Single-Server Private Information Retrieval: Circumventing the Sub-Query Attack</strong><br><button class=copy-to-clipboard title="Code-Based Single-Server Private Information Retrieval: Circumventing the Sub-Query Attack" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-CR, cs-IT, cs.IT, math-CO, math-IT<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02871v1.pdf filename=2402.02871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Private <b>information</b> <b>retrieval</b> from a single server is considered, utilizing random linear codes. Presented is a modified version of the first code-based single-server computational PIR scheme proposed by Holzbaur, Hollanti, and Wachter-Zeh in [Holzbaur et al., &ldquo;Computational Code-Based Single-Server Private <b>Information</b> <b>Retrieval&rdquo;,</b> 2020 IEEE ISIT]. The original scheme was broken in [Bordage et al., &ldquo;On the privacy of a code-based single-server computational PIR scheme&rdquo;, Cryptogr. Comm., 2021] by an attack arising from highly probable rank differences in sub-matrices of the user&rsquo;s query. Here, this attack is now circumvented by ensuring that the sub-matrices have negligible rank difference. Furthermore, the rank difference cannot be attributed to the desired file index, thereby ensuring the privacy of the scheme. In the case of retrieving multiple files, the rate of the modified scheme is largely unaffected and at par with the original scheme.</p></p class="citation"></blockquote><h2 id=eessiv-10>eess.IV (10)</h2><h3 id=218285-beyond-strong-labels-weakly-supervised-learning-based-on-gaussian-pseudo-labels-for-the-segmentation-of-ellipse-like-vascular-structures-in-non-contrast-cts-qixiang-ma-et-al-2024>(218/285) Beyond Strong labels: Weakly-supervised Learning Based on Gaussian Pseudo Labels for The Segmentation of Ellipse-like Vascular Structures in Non-contrast CTs (Qixiang Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qixiang Ma, Antoine Łucas, Huazhong Shu, Adrien Kaladji, Pascal Haigron. (2024)<br><strong>Beyond Strong labels: Weakly-supervised Learning Based on Gaussian Pseudo Labels for The Segmentation of Ellipse-like Vascular Structures in Non-contrast CTs</strong><br><button class=copy-to-clipboard title="Beyond Strong labels: Weakly-supervised Learning Based on Gaussian Pseudo Labels for The Segmentation of Ellipse-like Vascular Structures in Non-contrast CTs" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Reconstruction Loss, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03492v1.pdf filename=2402.03492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep-learning-based automated segmentation of vascular structures in preoperative CT scans contributes to computer-assisted diagnosis and intervention procedure in vascular diseases. While CT angiography (CTA) is the common standard, non-contrast CT imaging is significant as a contrast-risk-free alternative, avoiding complications associated with contrast agents. However, the challenges of labor-intensive labeling and high labeling variability due to the ambiguity of vascular boundaries hinder conventional strong-label-based, fully-supervised learning in non-contrast CTs. This paper introduces a <b>weakly-supervised</b> <b>framework</b> <b>using</b> ellipses&rsquo; topology in slices, including 1) an efficient annotation process based on predefined standards, 2) ellipse-fitting processing, 3) the generation of 2D Gaussian heatmaps serving as pseudo labels, 4) a training process through a combination of voxel <b>reconstruction</b> <b>loss</b> and distribution loss with the pseudo labels. We assess the effectiveness of the proposed method on one local and two public datasets comprising non-contrast CT scans, particularly focusing on the abdominal aorta. On the local dataset, our <b>weakly-supervised</b> <b>learning</b> <b>approach</b> based on pseudo labels outperforms strong-label-based fully-supervised learning (1.54% of Dice score on average), reducing labeling time by around 82.0%. The efficiency in generating pseudo labels allows the inclusion of label-agnostic external data in the training set, leading to an additional improvement in performance (2.74% of Dice score on average) with a reduction of 66.3% labeling time, where the labeling time remains considerably less than that of strong labels. On the public dataset, the pseudo labels achieve an overall improvement of 1.95% in Dice score for 2D models while a reduction of 11.65 voxel spacing in Hausdorff distance for 3D model.</p></p class="citation"></blockquote><h3 id=219285-swin-umamba-mamba-based-unet-with-imagenet-based-pretraining-jiarun-liu-et-al-2024>(219/285) Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining (Jiarun Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, Shanshan Wang. (2024)<br><strong>Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining</strong><br><button class=copy-to-clipboard title="Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03302v1.pdf filename=2402.03302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> are constrained by their local receptive fields, and vision <b>transformers</b> (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed specifically for medical image segmentation tasks, leveraging the advantages of ImageNet-based pretraining. Our experimental results reveal the vital role of ImageNet-based training in enhancing the performance of Mamba-based models. Swin-UMamba demonstrates superior performance with a large margin compared to <b>CNNs,</b> ViTs, and latest Mamba-based models. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba outperforms its closest counterpart U-Mamba by an average score of 3.58%. The code and models of Swin-UMamba are publicly available at: <a href=https://github.com/JiarunLiu/Swin-UMamba>https://github.com/JiarunLiu/Swin-UMamba</a></p></p class="citation"></blockquote><h3 id=220285-improving-pediatric-low-grade-neuroepithelial-tumors-molecular-subtype-identification-using-a-novel-auroc-loss-function-for-convolutional-neural-networks-khashayar-namdar-et-al-2024>(220/285) Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype Identification Using a Novel AUROC Loss Function for Convolutional Neural Networks (Khashayar Namdar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khashayar Namdar, Matthias W. Wagner, Cynthia Hawkins, Uri Tabori, Birgit B. Ertl-Wagner, Farzad Khalvati. (2024)<br><strong>Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype Identification Using a Novel AUROC Loss Function for Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype Identification Using a Novel AUROC Loss Function for Convolutional Neural Networks" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, q-bio-QM<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03547v1.pdf filename=2402.03547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial for treatment planning. However, the gold standard to determine the PLGNT subtype is biopsy, which can be impractical or dangerous for patients. This research improves the performance of <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> in classifying PLGNT subtypes through MRI scans by introducing a loss function that specifically improves the model&rsquo;s Area Under the Receiver Operating Characteristic (ROC) Curve (AUROC), offering a non-invasive diagnostic alternative. In this study, a retrospective dataset of 339 children with PLGNT (143 BRAF fusion, 71 with BRAF V600E mutation, and 125 non-BRAF) was curated. We employed a <b>CNN</b> model with Monte Carlo random data splitting. The baseline model was trained using binary cross entropy (BCE), and achieved an AUROC of 86.11% for differentiating BRAF fusion and BRAF V600E mutations, which was improved to 87.71% using our proposed AUROC loss function (p-value 0.045). With multiclass classification, the AUROC improved from 74.42% to 76. 59% (p-value 0.0016).</p></p class="citation"></blockquote><h3 id=221285-ct-based-anatomical-segmentation-for-thoracic-surgical-planning-a-benchmark-study-for-3d-u-shaped-deep-learning-models-arash-harirpoush-et-al-2024>(221/285) CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models (Arash Harirpoush et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arash Harirpoush, Amirhossein Rasoulian, Marta Kersten-Oertel, Yiming Xiao. (2024)<br><strong>CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models</strong><br><button class=copy-to-clipboard title="CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03230v1.pdf filename=2402.03230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent rising interests in patient-specific thoracic surgical planning and <b>simulation</b> require efficient and robust creation of digital anatomical models from automatic medical image segmentation algorithms. Deep learning (DL) is now state-of-the-art in various radiological tasks, and U-shaped DL models have particularly excelled in medical image segmentation since the inception of the 2D UNet. To date, many variants of U-shaped models have been proposed by the integration of different attention mechanisms and network configurations. Leveraging the recent development of large multi-label databases, systematic benchmark studies for these models can provide valuable insights for clinical deployment and future model designs, but such studies are still rare. We conduct the first benchmark study for variants of 3D U-shaped models (3DUNet, STUNet, AttentionUNet, SwinUNETR, FocalSegNet, and a novel 3D SwinUnet with four variants) with a focus on CT-based anatomical segmentation for thoracic surgery. Our study systematically examines the impact of different attention mechanisms, number of resolution stages, and network configurations on segmentation accuracy and computational complexity. To allow cross-reference with other recent benchmarking studies, we also included a performance assessment of the BTCV abdominal structural segmentation. With the STUNet ranking at the top, our study demonstrated the value of <b>CNN-based</b> U-shaped models for the investigated tasks and the benefit of residual blocks in network configuration designs to boost segmentation performance.</p></p class="citation"></blockquote><h3 id=222285-panoramic-image-inpainting-with-gated-convolution-and-contextual-reconstruction-loss-li-yu-et-al-2024>(222/285) Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss (Li Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Yu, Yanjun Gao, Farhad Pakdaman, Moncef Gabbouj. (2024)<br><strong>Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss</strong><br><button class=copy-to-clipboard title="Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, cs-MM, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Graph Attention Networks, Convolution, Reconstruction Loss<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02936v1.pdf filename=2402.02936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based methods have demonstrated encouraging results in tackling the task of panoramic image inpainting. However, it is challenging for existing methods to distinguish valid pixels from invalid pixels and find suitable references for corrupted areas, thus leading to artifacts in the inpainted results. In response to these challenges, we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use the Cubemap Projection (CMP) format as network input. The generator employs <b>gated</b> <b>convolutions</b> to distinguish valid pixels from invalid ones, while a side branch is designed utilizing contextual <b>reconstruction</b> <b>(CR)</b> loss to guide the generators to find the most suitable reference patch for inpainting the missing region. The proposed method is compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in terms of PSNR and SSIM. Experimental results and ablation study demonstrate that the proposed method outperforms SOTA both quantitatively and qualitatively.</p></p class="citation"></blockquote><h3 id=223285-inva-integrative-variational-autoencoder-for-harmonization-of-multi-modal-neuroimaging-data-bowen-lei-et-al-2024>(223/285) InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data (Bowen Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Lei, Rajarshi Guhaniyogi, Krishnendu Chandra, Aaron Scheffler, Bani Mallick. (2024)<br><strong>InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data</strong><br><button class=copy-to-clipboard title="InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-NE, eess-IV, eess.IV, stat-AP, stat-ML<br>Keyword Score: 23<br>Keywords: Autoencoder, Multi-modal, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02734v1.pdf filename=2402.02734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a significant interest in exploring non-linear associations among multiple images derived from diverse imaging modalities. While there is a growing literature on image-on-image regression to delineate predictive inference of an image based on multiple images, existing approaches have limitations in efficiently borrowing information between multiple imaging modalities in the prediction of an image. Building on the literature of <b>Variational</b> <b>Auto</b> Encoders (VAEs), this article proposes a novel approach, referred to as Integrative <b>Variational</b> <b>Autoencoder</b> (\texttt{InVA}) method, which borrows information from multiple images obtained from different sources to draw predictive inference of an image. The proposed approach captures complex non-linear association between the outcome image and input images, while allowing rapid computation. Numerical results demonstrate substantial advantages of \texttt{InVA} over VAEs, which typically do not allow borrowing information between input images. The proposed framework offers highly accurate predictive inferences for costly positron emission topography (PET) from multiple measures of cortical structure in human brain scans readily available from magnetic resonance imaging (MRI).</p></p class="citation"></blockquote><h3 id=224285-an-end-to-end-deep-learning-pipeline-to-derive-blood-input-with-partial-volume-corrections-for-automated-parametric-brain-pet-mapping-rugved-chavan-et-al-2024>(224/285) An end-to-end deep learning pipeline to derive blood input with partial volume corrections for automated parametric brain PET mapping (Rugved Chavan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rugved Chavan, Gabriel Hyman, Zoraiz Qureshi, Nivetha Jayakumar, William Terrell, Stuart Berr, David Schiff, Megan Wardius, Nathan Fountain, Thomas Muttikkal, Mark Quigg, Miaomiao Zhang, Bijoy Kundu. (2024)<br><strong>An end-to-end deep learning pipeline to derive blood input with partial volume corrections for automated parametric brain PET mapping</strong><br><button class=copy-to-clipboard title="An end-to-end deep learning pipeline to derive blood input with partial volume corrections for automated parametric brain PET mapping" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03414v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03414v1.pdf filename=2402.03414v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic 2-[18F] fluoro-2-deoxy-D-glucose positron emission tomography (dFDG-PET) for human brain imaging has considerable clinical potential, yet its utilization remains limited. A key challenge in the quantitative analysis of dFDG-PET is characterizing a patient-specific blood input function, traditionally reliant on invasive arterial blood sampling. This research introduces a novel approach employing non-invasive deep learning model-based computations from the internal carotid arteries (ICA) with partial volume (PV) corrections, thereby eliminating the need for invasive arterial sampling. We present an end-to-end pipeline incorporating a 3D U-Net based ICA-net for ICA segmentation, alongside a <b>Recurrent</b> <b>Neural</b> <b>Network</b> <b>(RNN)</b> based MCIF-net for the derivation of a model-corrected blood input function (MCIF) with PV corrections. The developed 3D U-Net and <b>RNN</b> was trained and validated using a 5-fold cross-validation approach on 50 human brain FDG PET datasets. The ICA-net achieved an average Dice score of 82.18% and an Intersection over Union of 68.54% across all tested scans. Furthermore, the MCIF-net exhibited a minimal root mean squared error of 0.0052. The application of this pipeline to ground truth data for dFDG-PET brain scans resulted in the precise localization of seizure onset regions, which contributed to a successful clinical outcome, with the patient achieving a seizure-free state after treatment. These results underscore the efficacy of the ICA-net and MCIF-net deep learning pipeline in learning the ICA structure&rsquo;s distribution and automating MCIF computation with PV corrections. This advancement marks a significant leap in non-invasive neuroimaging.</p></p class="citation"></blockquote><h3 id=225285-rrwnet-recursive-refinement-network-for-effective-retinal-arteryvein-segmentation-and-classification-josé-morano-et-al-2024>(225/285) RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification (José Morano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>José Morano, Guilherme Aresta, Hrvoje Bogunović. (2024)<br><strong>RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification</strong><br><button class=copy-to-clipboard title="RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03166v1.pdf filename=2402.03166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The caliber and configuration of retinal blood vessels serve as important biomarkers for various diseases and medical conditions. A thorough analysis of the retinal vasculature requires the segmentation of blood vessels and their classification into arteries and veins, which is typically performed on color fundus images obtained by retinography, a widely used imaging technique. Nonetheless, manually performing these tasks is labor-intensive and prone to human error. Various automated methods have been proposed to address this problem. However, the current state of art in artery/vein segmentation and classification faces challenges due to manifest classification errors that affect the topological consistency of segmentation maps. This study presents an innovative end-to-end framework, RRWNet, designed to recursively refine semantic segmentation maps and correct manifest classification errors. The framework consists of a fully <b>convolutional</b> <b>neural</b> <b>network</b> with a Base subnetwork that generates base segmentation maps from input images, and a Recursive Refinement subnetwork that iteratively and recursively improves these maps. Evaluation on public datasets demonstrates the state-of-the-art performance of the proposed method, yielding more topologically consistent segmentation maps with fewer manifest classification errors than existing approaches. In addition, the Recursive Refinement module proves effective in post-processing segmentation maps from other methods, automatically correcting classification errors and improving topological consistency. The model code, weights, and predictions are publicly available at <a href=https://github.com/j-morano/rrwnet>https://github.com/j-morano/rrwnet</a>.</p></p class="citation"></blockquote><h3 id=226285-ct-material-decomposition-using-spectral-diffusion-posterior-sampling-xiao-jiang-et-al-2024>(226/285) CT Material Decomposition using Spectral Diffusion Posterior Sampling (Xiao Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Jiang, Grace J. Gang, J. Webster Stayman. (2024)<br><strong>CT Material Decomposition using Spectral Diffusion Posterior Sampling</strong><br><button class=copy-to-clipboard title="CT Material Decomposition using Spectral Diffusion Posterior Sampling" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV, physics-med-ph<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03476v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03476v1.pdf filename=2402.03476v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we introduce a new deep learning approach based on diffusion posterior sampling (DPS) to perform material decomposition from spectral CT measurements. This approach combines sophisticated prior knowledge from <b>unsupervised</b> training with a rigorous physical model of the measurements. A faster and more stable variant is proposed that uses a jumpstarted process to reduce the number of time steps required in the reverse process and a gradient approximation to reduce the computational cost. Performance is investigated for two spectral CT systems: dual-kVp and dual-layer detector CT. On both systems, DPS achieves high Structure Similarity Index Metric Measure(SSIM) with only 10% of iterations as used in the model-based material decomposition(MBMD). Jumpstarted DPS (JSDPS) further reduces computational time by over 85% and achieves the highest accuracy, the lowest uncertainty, and the lowest computational costs compared to classic DPS and MBMD. The results demonstrate the potential of JSDPS for providing relatively fast and accurate material decomposition based on spectral CT data.</p></p class="citation"></blockquote><h3 id=227285-deep-nonlinear-hyperspectral-unmixing-using-multi-task-learning-saeid-mehrdad-et-al-2024>(227/285) Deep Nonlinear Hyperspectral Unmixing Using Multi-task Learning (Saeid Mehrdad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saeid Mehrdad, Seyed AmirHossein Janani. (2024)<br><strong>Deep Nonlinear Hyperspectral Unmixing Using Multi-task Learning</strong><br><button class=copy-to-clipboard title="Deep Nonlinear Hyperspectral Unmixing Using Multi-task Learning" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, cs-NE, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03398v1.pdf filename=2402.03398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nonlinear hyperspectral unmixing has recently received considerable attention, as linear mixture models do not lead to an acceptable resolution in some problems. In fact, most nonlinear unmixing methods are designed by assuming specific assumptions on the nonlinearity model which subsequently limits the unmixing performance. In this paper, we propose an <b>unsupervised</b> nonlinear unmixing approach based on deep learning by incorporating a general nonlinear model with no special assumptions. This model consists of two branches. In the first branch, endmembers are learned by reconstructing the rows of hyperspectral images using some hidden layers, and in the second branch, abundance values are learned based on the columns of respective images. Then, using multi-task learning, we introduce an auxiliary task to enforce the two branches to work together. This technique can be considered as a regularizer mitigating overfitting, which improves the performance of the total network. Extensive experiments on synthetic and real data verify the effectiveness of the proposed method compared to some state-of-the-art hyperspectral unmixing methods.</p></p class="citation"></blockquote><h2 id=cssi-3>cs.SI (3)</h2><h3 id=228285-recommendation-fairness-in-social-networks-over-time-meng-cao-et-al-2024>(228/285) Recommendation Fairness in Social Networks Over Time (Meng Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Cao, Hussain Hussain, Sandipan Sikdar, Denis Helic, Markus Strohmaier, Roman Kern. (2024)<br><strong>Recommendation Fairness in Social Networks Over Time</strong><br><button class=copy-to-clipboard title="Recommendation Fairness in Social Networks Over Time" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CY, cs-IR, cs-SI, cs.SI<br>Keyword Score: 40<br>Keywords: Counter-factual, Fairness, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03450v1.pdf filename=2402.03450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In social <b>recommender</b> <b>systems,</b> it is crucial that the <b>recommendation</b> models provide equitable visibility for different demographic groups, such as gender or race. Most existing research has addressed this problem by only studying individual static snapshots of networks that typically change over time. To address this gap, we study the evolution of <b>recommendation</b> <b>fairness</b> over time and its relation to dynamic network properties. We examine three real-world dynamic networks by evaluating the <b>fairness</b> of six <b>recommendation</b> algorithms and analyzing the association between <b>fairness</b> and network properties over time. We further study how interventions on network properties influence <b>fairness</b> by examining <b>counterfactual</b> scenarios with alternative evolution outcomes and differing network properties. Our results on empirical datasets suggest that <b>recommendation</b> <b>fairness</b> improves over time, regardless of the <b>recommendation</b> method. We also find that two network properties, minority ratio, and homophily ratio, exhibit stable correlations with <b>fairness</b> over time. Our <b>counterfactual</b> study further suggests that an extreme homophily ratio potentially contributes to unfair <b>recommendations</b> even with a balanced minority ratio. Our work provides insights into the evolution of <b>fairness</b> within dynamic networks in social science. We believe that our findings will help system operators and policymakers to better comprehend the implications of temporal changes and interventions targeting <b>fairness</b> in social networks.</p></p class="citation"></blockquote><h3 id=229285-mquine-a-cure-for-z-paradox-in-knowledge-graph-embedding-models-yang-liu-et-al-2024>(229/285) MQuinE: a cure for &lsquo;Z-paradox&rsquo; in knowledge graph embedding models (Yang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Liu, Huang Fang, Yunfeng Cai, Mingming Sun. (2024)<br><strong>MQuinE: a cure for &lsquo;Z-paradox&rsquo; in knowledge graph embedding models</strong><br><button class=copy-to-clipboard title="MQuinE: a cure for 'Z-paradox' in knowledge graph embedding models" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-LG, cs-SI, cs.SI<br>Keyword Score: 20<br>Keywords: Graph Embedding, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03583v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03583v2.pdf filename=2402.03583v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge <b>graph</b> <b>embedding</b> (KGE) models achieved state-of-the-art results on many knowledge <b>graph</b> <b>tasks</b> including link prediction and <b>information</b> <b>retrieval.</b> Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction tasks.</p></p class="citation"></blockquote><h3 id=230285-security-advice-for-parents-and-children-about-content-filtering-and-circumvention-as-found-on-youtube-and-tiktok-ran-elgedawy-et-al-2024>(230/285) Security Advice for Parents and Children About Content Filtering and Circumvention as Found on YouTube and TikTok (Ran Elgedawy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ran Elgedawy, John Sadik, Anuj Gautam, Trinity Bissahoyo, Christopher Childress, Jacob Leonard, Clay Shubert, Scott Ruoti. (2024)<br><strong>Security Advice for Parents and Children About Content Filtering and Circumvention as Found on YouTube and TikTok</strong><br><button class=copy-to-clipboard title="Security Advice for Parents and Children About Content Filtering and Circumvention as Found on YouTube and TikTok" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CY, cs-HC, cs-SI, cs.SI<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03255v1.pdf filename=2402.03255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s digital age, concerns about online security and privacy have become paramount. However, addressing these issues can be difficult, especially within the context of family relationships, wherein parents and children may have conflicting interests. In this environment, parents and children may turn to online security advice to determine how to proceed. In this paper, we examine the advice available to parents and children regarding content filtering and circumvention as found on YouTube and TikTok. In an analysis of 839 videos returned from queries on these topics, we found that half (n=399) provide relevant advice. Our results show that of these videos, roughly three-quarters are accurate, with the remaining one-fourth containing factually incorrect advice. We find that videos targeting children are both more likely to be incorrect and actionable than videos targeting parents, leaving children at increased risk of taking harmful action. Moreover, we find that while advice videos targeting parents will occasionally discuss the ethics of content filtering and device monitoring (including <b>recommendations</b> to respect children&rsquo;s autonomy) no such discussion of the ethics or risks of circumventing content filtering is given to children, leaving them unaware of any risks that may be involved with doing so. Ultimately, our research indicates that video-based social media sites are already effective sources of security advice propagation and that the public would benefit from security researchers and practitioners engaging more with these platforms, both for the creation of content and of tools designed to help with more effective filtering.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=231285-quantized-approximately-orthogonal-recurrent-neural-networks-armand-foucault-et-al-2024>(231/285) Quantized Approximately Orthogonal Recurrent Neural Networks (Armand Foucault et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Armand Foucault, Franck Mamalet, François Malgouyres. (2024)<br><strong>Quantized Approximately Orthogonal Recurrent Neural Networks</strong><br><button class=copy-to-clipboard title="Quantized Approximately Orthogonal Recurrent Neural Networks" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-LG, cs-NE, cs.NE, eess-SP, math-ST, stat-TH<br>Keyword Score: 40<br>Keywords: Quantization, Quantization, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04012v1.pdf filename=2402.04012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Orthogonal <b>recurrent</b> <b>neural</b> <b>networks</b> (ORNNs) are an appealing option for learning tasks involving time series with long-term dependencies, thanks to their simplicity and computational stability. However, these networks often require a substantial number of parameters to perform well, which can be prohibitive in power-constrained environments, such as compact devices. One approach to address this issue is neural network <b>quantization.</b> The construction of such networks remains an open problem, acknowledged for its inherent instability.In this paper, we explore the <b>quantization</b> of the <b>recurrent</b> <b>and</b> <b>input</b> weight matrices in ORNNs, leading to <b>Quantized</b> approximately Orthogonal <b>RNNs</b> (QORNNs). We investigate one post-training <b>quantization</b> (PTQ) strategy and three <b>quantization-aware</b> training (QAT) algorithms that incorporate orthogonal constraints and <b>quantized</b> weights. Empirical results demonstrate the advantages of employing QAT over PTQ. The most efficient model achieves results similar to state-of-the-art full-precision ORNN and LSTM on a variety of standard benchmarks, even with 3-bits <b>quantization.</b></p></p class="citation"></blockquote><h2 id=quant-ph-3>quant-ph (3)</h2><h3 id=232285-curriculum-reinforcement-learning-for-quantum-architecture-search-under-hardware-errors-yash-j-patel-et-al-2024>(232/285) Curriculum reinforcement learning for quantum architecture search under hardware errors (Yash J. Patel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yash J. Patel, Akash Kundu, Mateusz Ostaszewski, Xavier Bonet-Monroig, Vedran Dunjko, Onur Danaci. (2024)<br><strong>Curriculum reinforcement learning for quantum architecture search under hardware errors</strong><br><button class=copy-to-clipboard title="Curriculum reinforcement learning for quantum architecture search under hardware errors" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, cs-LG, quant-ph, quant-ph<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03500v1.pdf filename=2402.03500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms <b>(VQAs)</b> offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search <b>(QAS)</b> algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based <b>reinforcement</b> <b>learning</b> <b>QAS</b> (CRLQAS) algorithm designed to tackle challenges in realistic <b>VQA</b> deployment. The algorithm incorporates (i) a 3D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation as an optimizer for faster convergence. To facilitate studies, we developed an optimized simulator for our algorithm, significantly improving computational efficiency in simulating noisy quantum circuits by employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing <b>QAS</b> algorithms across several metrics in both noiseless and noisy environments.</p></p class="citation"></blockquote><h3 id=233285-fast-classical-simulation-of-harvardquera-iqp-circuits-dmitri-maslov-et-al-2024>(233/285) Fast classical simulation of Harvard/QuEra IQP circuits (Dmitri Maslov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dmitri Maslov, Sergey Bravyi, Felix Tripier, Andrii Maksymov, Joe Latone. (2024)<br><strong>Fast classical simulation of Harvard/QuEra IQP circuits</strong><br><button class=copy-to-clipboard title="Fast classical simulation of Harvard/QuEra IQP circuits" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-ET, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03211v1.pdf filename=2402.03211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Establishing an advantage for (white-box) computations by a quantum computer against its classical counterpart is currently a key goal for the quantum computation community. A quantum advantage is achieved once a certain computational capability of a quantum computer is so complex that it can no longer be reproduced by classical means, and as such, the quantum advantage can be seen as a continued negotiation between classical <b>simulations</b> and quantum computational experiments. A recent publication (Bluvstein et al., Nature 626:58-65, 2024) introduces a type of Instantaneous Quantum Polynomial-Time (IQP) computation complemented by a $48$-qubit (logical) experimental demonstration using quantum hardware. The authors state that the ``simulation of such logical circuits is challenging&rsquo;&rsquo; and project the <b>simulation</b> time to grow rapidly with the number of CNOT layers added, see Figure 5d/bottom therein. However, we report a classical <b>simulation</b> algorithm that takes only $0.00257947$ seconds to compute an amplitude for the $48$-qubit computation, which is roughly $10^3$ times faster than that reported by the original authors. Our algorithm is furthermore not subject to a significant decline in performance due to the additional CNOT layers. We simulated these types of IQP computations for up to $96$ qubits, taking an average of $4.16629$ seconds to compute a single amplitude, and estimated that a $192$-qubit <b>simulation</b> should be tractable for computations relying on Tensor Processing Units.</p></p class="citation"></blockquote><h3 id=234285-unleashing-the-expressive-power-of-pulse-based-quantum-neural-networks-han-xiao-tao-et-al-2024>(234/285) Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks (Han-Xiao Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han-Xiao Tao, Jiaqi Hu, Re-Bing Wu. (2024)<br><strong>Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks</strong><br><button class=copy-to-clipboard title="Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-ET, cs-LG, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02880v1.pdf filename=2402.02880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum machine learning (QML) based on Noisy Intermediate-Scale Quantum (NISQ) devices requires the optimal utilization of limited quantum resources. The commonly used gate-based QML models are convenient for software engineers, but their expressivity is restricted by the permissible circuit depth within a finite coherence time. In contrast, pulse-based models enable the construction of &ldquo;infinitely&rdquo; deep quantum neural networks within the same coherence time, which may unleash greater expressive power for complex learning tasks. In this paper, we investigate this potential from the perspective of quantum control theory. We first indicate that the nonlinearity of pulse-based models comes from the encoding process that can be viewed as the continuous limit of data-reuploading in gate-based models. Subsequently, we prove that the pulse-based model can approximate arbitrary nonlinear functions when the underlying physical system is ensemble controllable. Under this condition, numerical <b>simulations</b> show that the expressivity can be enhanced by either increasing the pulse length or the number of qubits. As anticipated, we demonstrate through numerical examples that the pulse-based model can unleash more expressive power compared to the gate-based model. These findings establish a theoretical foundation for understanding and designing expressive QML models using NISQ devices.</p></p class="citation"></blockquote><h2 id=cssd-6>cs.SD (6)</h2><h3 id=235285-a-comprehensive-study-of-the-current-state-of-the-art-in-nepali-automatic-speech-recognition-systems-rupak-raj-ghimire-et-al-2024>(235/285) A Comprehensive Study of the Current State-of-the-Art in Nepali Automatic Speech Recognition Systems (Rupak Raj Ghimire et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rupak Raj Ghimire, Bal Krishna Bal, Prakash Poudyal. (2024)<br><strong>A Comprehensive Study of the Current State-of-the-Art in Nepali Automatic Speech Recognition Systems</strong><br><button class=copy-to-clipboard title="A Comprehensive Study of the Current State-of-the-Art in Nepali Automatic Speech Recognition Systems" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CL, cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03050v1.pdf filename=2402.03050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we examine the research conducted in the field of Nepali <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR).</b> The primary objective of this survey is to conduct a comprehensive review of the works on Nepali <b>Automatic</b> <b>Speech</b> <b>Recognition</b> Systems completed to date, explore the different datasets used, examine the technology utilized, and take account of the obstacles encountered in implementing the Nepali <b>ASR</b> system. In tandem with the global trends of ever-increasing research on <b>speech</b> <b>recognition</b> based research, the number of Nepalese <b>ASR-related</b> projects are also growing. Nevertheless, the investigation of language and acoustic models of the Nepali language has not received adequate attention compared to languages that possess ample resources. In this context, we provide a framework as well as directions for future investigations.</p></p class="citation"></blockquote><h3 id=236285-dual-knowledge-distillation-for-efficient-sound-event-detection-yang-xiao-et-al-2024>(236/285) Dual Knowledge Distillation for Efficient Sound Event Detection (Yang Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Xiao, Rohan Kumar Das. (2024)<br><strong>Dual Knowledge Distillation for Efficient Sound Event Detection</strong><br><button class=copy-to-clipboard title="Dual Knowledge Distillation for Efficient Sound Event Detection" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-CL, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02781v1.pdf filename=2402.02781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sound <b>event</b> <b>detection</b> (SED) is essential for recognizing specific sounds and their temporal locations within acoustic signals. This becomes challenging particularly for on-device applications, where computational resources are limited. To address this issue, we introduce a novel framework referred to as dual <b>knowledge</b> <b>distillation</b> for developing efficient SED systems in this work. Our proposed dual <b>knowledge</b> <b>distillation</b> commences with temporal-averaging <b>knowledge</b> <b>distillation</b> (TAKD), utilizing a mean student model derived from the temporal averaging of the student model&rsquo;s parameters. This allows the student model to indirectly learn from a pre-trained teacher model, ensuring a stable <b>knowledge</b> <b>distillation.</b> Subsequently, we introduce embedding-enhanced feature <b>distillation</b> (EEFD), which involves incorporating an embedding <b>distillation</b> layer within the student model to bolster contextual learning. On DCASE 2023 Task 4A public evaluation dataset, our proposed SED system with dual <b>knowledge</b> <b>distillation</b> having merely one-third of the baseline model&rsquo;s parameters, demonstrates superior performance in terms of PSDS1 and PSDS2. This highlights the importance of proposed dual <b>knowledge</b> <b>distillation</b> for compact SED systems, which can be ideal for edge devices.</p></p class="citation"></blockquote><h3 id=237285-exploring-federated-self-supervised-learning-for-general-purpose-audio-understanding-yasar-abbas-ur-rehman-et-al-2024>(237/285) Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding (Yasar Abbas Ur Rehman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasar Abbas Ur Rehman, Kin Wai Lau, Yuyang Xie, Lan Ma, Jiajun Shen. (2024)<br><strong>Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding</strong><br><button class=copy-to-clipboard title="Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CV, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02889v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02889v1.pdf filename=2402.02889v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of Federated Learning (FL) and <b>Self-supervised</b> <b>Learning</b> (SSL) offers a unique and synergetic combination to exploit the audio data for general-purpose audio understanding, without compromising user data privacy. However, rare efforts have been made to investigate the SSL models in the FL regime for general-purpose audio understanding, especially when the training data is generated by large-scale heterogeneous audio sources. In this paper, we evaluate the performance of feature-matching and predictive audio-SSL techniques when integrated into large-scale FL settings simulated with non-independently identically distributed (non-iid) data. We propose a novel Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning intermediate feature representations from large-scale decentralized heterogeneous clients, holding unlabelled audio data. Our study has found that audio F-SSL approaches perform on par with the centralized audio-SSL approaches on the audio-retrieval task. Extensive experiments demonstrate the effectiveness and significance of FASSL as it assists in obtaining the optimal global model for state-of-the-art FL aggregation methods.</p></p class="citation"></blockquote><h3 id=238285-focal-modulation-networks-for-interpretable-sound-classification-luca-della-libera-et-al-2024>(238/285) Focal Modulation Networks for Interpretable Sound Classification (Luca Della Libera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Della Libera, Cem Subakan, Mirco Ravanelli. (2024)<br><strong>Focal Modulation Networks for Interpretable Sound Classification</strong><br><button class=copy-to-clipboard title="Focal Modulation Networks for Interpretable Sound Classification" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02754v1.pdf filename=2402.02754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing success of deep neural networks has raised concerns about their inherent black-box nature, posing challenges related to interpretability and trust. While there has been extensive exploration of interpretation techniques in vision and language, interpretability in the audio domain has received limited attention, primarily focusing on post-hoc explanations. This paper addresses the problem of interpretability by-design in the audio domain by utilizing the recently proposed attention-free focal modulation networks (FocalNets). We apply FocalNets to the task of environmental sound classification for the first time and evaluate their interpretability properties on the popular ESC-50 dataset. Our method outperforms a similarly sized vision <b>transformer</b> both in terms of accuracy and interpretability. Furthermore, it is competitive against PIQ, a method specifically designed for post-hoc interpretation in the audio domain.</p></p class="citation"></blockquote><h3 id=239285-adversarial-data-augmentation-for-robust-speaker-verification-zhenyu-zhou-et-al-2024>(239/285) Adversarial Data Augmentation for Robust Speaker Verification (Zhenyu Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyu Zhou, Junhui Chen, Namin Wang, Lantian Li, Dong Wang. (2024)<br><strong>Adversarial Data Augmentation for Robust Speaker Verification</strong><br><button class=copy-to-clipboard title="Adversarial Data Augmentation for Robust Speaker Verification" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02699v1.pdf filename=2402.02699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>augmentation</b> (DA) has gained widespread popularity in deep speaker models due to its ease of implementation and significant effectiveness. It enriches training <b>data</b> <b>by</b> simulating real-life acoustic variations, enabling deep neural networks to learn speaker-related representations while disregarding irrelevant acoustic variations, thereby improving robustness and generalization. However, a potential issue with the vanilla DA is augmentation residual, i.e., unwanted distortion caused by different types of augmentation. To address this problem, this paper proposes a novel approach called <b>adversarial</b> <b>data</b> <b>augmentation</b> (A-DA) which combines DA with <b>adversarial</b> <b>learning.</b> Specifically, it involves an additional augmentation classifier to categorize various augmentation types used in <b>data</b> <b>augmentation.</b> This <b>adversarial</b> <b>learning</b> empowers the network to generate speaker embeddings that can deceive the augmentation classifier, making the learned speaker embeddings more robust in the face of augmentation variations. Experiments conducted on VoxCeleb and CN-Celeb datasets demonstrate that our proposed A-DA outperforms standard DA in both augmentation matched and mismatched test conditions, showcasing its superior robustness and generalization against acoustic variations.</p></p class="citation"></blockquote><h3 id=240285-how-phonemes-contribute-to-deep-speaker-models-pengqi-li-et-al-2024>(240/285) How phonemes contribute to deep speaker models? (Pengqi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengqi Li, Tianhao Wang, Lantian Li, Askar Hamdulla, Dong Wang. (2024)<br><strong>How phonemes contribute to deep speaker models?</strong><br><button class=copy-to-clipboard title="How phonemes contribute to deep speaker models?" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02730v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02730v1.pdf filename=2402.02730v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Which phonemes convey more speaker traits is a long-standing question, and various perception experiments were conducted with human subjects. For speaker recognition, studies were conducted with the conventional statistical models and the drawn conclusions are more or less consistent with the perception results. However, which phonemes are more important with modern deep neural models is still unexplored, due to the opaqueness of the decision process. This paper conducts a novel study for the attribution of phonemes with two types of deep speaker models that are based on TDNN and <b>CNN</b> respectively, from the perspective of model explanation. Specifically, we conducted the study by two post-explanation methods: LayerCAM and Time Align Occlusion (TAO). Experimental results showed that: (1) At the population level, vowels are more important than consonants, confirming the human perception studies. However, fricatives are among the most unimportant phonemes, which contrasts with previous studies. (2) At the speaker level, a large between-speaker variation is observed regarding phoneme importance, indicating that whether a phoneme is important or not is largely speaker-dependent.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=241285-cooperative-learning-with-gaussian-processes-for-euler-lagrange-systems-tracking-control-under-switching-topologies-zewen-yang-et-al-2024>(241/285) Cooperative Learning with Gaussian Processes for Euler-Lagrange Systems Tracking Control under Switching Topologies (Zewen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zewen Yang, Songbo Dong, Armin Lederer, Xiaobing Dai, Siyu Chen, Stefan Sosnowski, Georges Hattab, Sandra Hirche. (2024)<br><strong>Cooperative Learning with Gaussian Processes for Euler-Lagrange Systems Tracking Control under Switching Topologies</strong><br><button class=copy-to-clipboard title="Cooperative Learning with Gaussian Processes for Euler-Lagrange Systems Tracking Control under Switching Topologies" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-LG, cs-MA, cs-SY, cs.MA, eess-SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03048v1.pdf filename=2402.03048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents an innovative learning-based approach to tackle the tracking control problem of Euler-Lagrange multi-agent systems with partially unknown dynamics operating under switching communication topologies. The approach leverages a correlation-aware cooperative algorithm framework built upon <b>Gaussian</b> <b>process</b> regression, which adeptly captures inter-agent correlations for uncertainty predictions. A standout feature is its exceptional efficiency in deriving the aggregation weights achieved by circumventing the computationally intensive posterior variance calculations. Through Lyapunov stability analysis, the distributed control law ensures bounded tracking errors with high probability. <b>Simulation</b> experiments validate the protocol&rsquo;s efficacy in effectively managing complex scenarios, establishing it as a promising solution for robust tracking control in multi-agent systems characterized by uncertain dynamics and dynamic communication structures.</p></p class="citation"></blockquote><h3 id=242285-llm-multi-agent-systems-challenges-and-open-problems-shanshan-han-et-al-2024>(242/285) LLM Multi-Agent Systems: Challenges and Open Problems (Shanshan Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu, Chaoyang He. (2024)<br><strong>LLM Multi-Agent Systems: Challenges and Open Problems</strong><br><button class=copy-to-clipboard title="LLM Multi-Agent Systems: Challenges and Open Problems" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 20<br>Keywords: Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03578v1.pdf filename=2402.03578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores existing works of multi-agent systems and identifies challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents within a multi-agent system, these systems can tackle complex tasks through collaboration. We discuss optimizing task allocation, fostering robust <b>reasoning</b> through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore the potential application of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems.</p></p class="citation"></blockquote><h2 id=eesssy-4>eess.SY (4)</h2><h3 id=243285-multi-agent-reinforcement-learning-for-offloading-cellular-communications-with-cooperating-uavs-abhishek-mondal-et-al-2024>(243/285) Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs (Abhishek Mondal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Mondal, Deepak Mishra, Ganesh Prasad, George C. Alexandropoulos, Azzam Alnahari, Riku Jantti. (2024)<br><strong>Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs</strong><br><button class=copy-to-clipboard title="Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02957v1.pdf filename=2402.02957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective solutions for intelligent data collection in terrestrial cellular networks are crucial, especially in the context of Internet of Things applications. The limited spectrum and coverage area of terrestrial base stations pose challenges in meeting the escalating data rate demands of network users. Unmanned aerial vehicles, known for their high agility, mobility, and flexibility, present an alternative means to offload data traffic from terrestrial BSs, serving as additional access points. This paper introduces a novel approach to efficiently maximize the utilization of multiple UAVs for data traffic offloading from terrestrial BSs. Specifically, the focus is on maximizing user association with UAVs by jointly optimizing UAV trajectories and users association indicators under quality of service constraints. Since, the formulated UAVs control problem is nonconvex and combinatorial, this study leverages the multi agent <b>reinforcement</b> <b>learning</b> framework. In this framework, each UAV acts as an independent agent, aiming to maintain inter UAV cooperative behavior. The proposed approach utilizes the finite state Markov decision process to account for UAVs velocity constraints and the relationship between their trajectories and state space. A low complexity distributed state action reward state action algorithm is presented to determine UAVs optimal sequential decision making policies over training episodes. The extensive <b>simulation</b> results validate the proposed analysis and offer valuable insights into the optimal UAV trajectories. The derived trajectories demonstrate superior average UAV association performance compared to benchmark techniques such as Q learning and particle swarm optimization.</p></p class="citation"></blockquote><h3 id=244285-autopilot-system-for-depth-and-pitch-control-in-underwater-vehicles-navigating-near-surface-waves-and-disturbances-vladimir-petrov-et-al-2024>(244/285) Autopilot System for Depth and Pitch Control in Underwater Vehicles: Navigating Near-Surface Waves and Disturbances (Vladimir Petrov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vladimir Petrov, Gage MacLin, Venanzio Cichella. (2024)<br><strong>Autopilot System for Depth and Pitch Control in Underwater Vehicles: Navigating Near-Surface Waves and Disturbances</strong><br><button class=copy-to-clipboard title="Autopilot System for Depth and Pitch Control in Underwater Vehicles: Navigating Near-Surface Waves and Disturbances" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03510v1.pdf filename=2402.03510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a framework for depth and pitch control of underwater vehicles in near-surface wave conditions. By effectively managing tail, sail plane angles and hover tank operations utilizing a Linear Quadratic Regulator controller and L1 Adaptive Autopilot augmentation, the system ensures balanced control input distribution and significantly attenuates wave disturbances. This development in underwater vehicle control systems offers potential for improved functionality across a range of marine applications. The proposed framework is demonstrated to be robust in a variety of wave conditions, enabling more precise navigation and improved safety in operational scenarios. The effectiveness of this control strategy is validated through extensive <b>simulations</b> using the Joubert BB2 model.</p></p class="citation"></blockquote><h3 id=245285-decentralized-event-triggered-online-learning-for-safe-consensus-of-multi-agent-systems-with-gaussian-process-regression-xiaobing-dai-et-al-2024>(245/285) Decentralized Event-Triggered Online Learning for Safe Consensus of Multi-Agent Systems with Gaussian Process Regression (Xiaobing Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaobing Dai, Zewen Yang, Mengtian Xu, Fangzhou Liu, Georges Hattab, Sandra Hirche. (2024)<br><strong>Decentralized Event-Triggered Online Learning for Safe Consensus of Multi-Agent Systems with Gaussian Process Regression</strong><br><button class=copy-to-clipboard title="Decentralized Event-Triggered Online Learning for Safe Consensus of Multi-Agent Systems with Gaussian Process Regression" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03174v1.pdf filename=2402.03174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Consensus control in multi-agent systems has received significant attention and practical implementation across various domains. However, managing consensus control under unknown dynamics remains a significant challenge for control design due to system uncertainties and environmental disturbances. This paper presents a novel learning-based distributed control law, augmented by an auxiliary dynamics. <b>Gaussian</b> <b>processes</b> are harnessed to compensate for the unknown components of the multi-agent system. For continuous enhancement in predictive performance of <b>Gaussian</b> <b>process</b> model, a data-efficient online learning strategy with a decentralized event-triggered mechanism is proposed. Furthermore, the control performance of the proposed approach is ensured via the Lyapunov theory, based on a probabilistic guarantee for prediction error bounds. To demonstrate the efficacy of the proposed learning-based controller, a comparative analysis is conducted, contrasting it with both conventional distributed control laws and offline learning methodologies.</p></p class="citation"></blockquote><h3 id=246285-alive-a-low-cost-interactive-vaccine-storage-environment-module-ensuring-easy-portability-and-remote-tracking-of-operational-logistics-to-the-last-mile-arkadeep-datta-et-al-2024>(246/285) ALIVE: A Low-Cost Interactive Vaccine Storage Environment Module ensuring easy portability and remote tracking of operational logistics to the last mile (Arkadeep Datta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arkadeep Datta, Arani Mukhopadhyay, Amitava Datta, Ranjan Ganguly. (2024)<br><strong>ALIVE: A Low-Cost Interactive Vaccine Storage Environment Module ensuring easy portability and remote tracking of operational logistics to the last mile</strong><br><button class=copy-to-clipboard title="ALIVE: A Low-Cost Interactive Vaccine Storage Environment Module ensuring easy portability and remote tracking of operational logistics to the last mile" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02691v1.pdf filename=2402.02691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The COVID-19 pandemic has profoundly reshaped our lives, <b>prompting</b> a search for solutions to its far-reaching effects. Vaccines emerged as a beacon of hope, yet reaching remote areas faces last-mile hurdles and cost issues due to loss of vaccine potency due to poor temperature regulation of the storage units and unanticipated vaccine wastage en route, a common occurrence in conventional vaccine transportation methods. We introduce ALIVE, a low-cost Interactive Vaccine Storage Environment module. ALIVE provides an off-grid, self-sufficient solution for vaccine storage and transport, enabled by active cooling technology. ALIVE&rsquo;s innovation lies in its integration with the Internet of Things (IoT), allowing real-time monitoring and control. This IoT-enabled Application Programming Interface (API) features a data acquisition and environment parameter control system, managing oversight and decision-making. ALIVE&rsquo;s compact, lightweight design makes it adaptable to various logistical scenarios, while its versatility enables it to maintain both time-invariant and time-dependent thermophysical and spatial parameters. Operationalized through a PID algorithm, ALIVE ensures precise temperature control within the vaccine chamber. Its dynamic features, such as remote actuation and data sharing, demonstrate its adaptability and potential applications. Despite the frugal nature of development, the system promises significant benefits, including reduced vaccine loss and remote monitoring advantages. Collaborations with healthcare partners seek to further enhance ALIVE&rsquo;s readiness and expand its impact. ALIVE revolutionizes vaccine logistics, offering scalable, cost-effective solutions for bridging accessibility gaps in challenging distribution scenarios. Its adaptability positions it for widespread application, from last-mile vaccine delivery to environment-controlled supply chains and beyond.</p></p class="citation"></blockquote><h2 id=csro-2>cs.RO (2)</h2><h3 id=247285-replication-of-impedance-identification-experiments-on-a-reinforcement-learning-controlled-digital-twin-of-human-elbows-hao-yu-et-al-2024>(247/285) Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows (Hao Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Yu, Zebin Huang, Qingbo Liu, Ignacio Carlucho, Mustafa Suphi Erden. (2024)<br><strong>Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows</strong><br><button class=copy-to-clipboard title="Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02904v1.pdf filename=2402.02904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a pioneering effort to replicate human neuromechanical experiments within a virtual environment utilising a digital human model. By employing MyoSuite, a state-of-the-art human motion <b>simulation</b> platform enhanced by <b>Reinforcement</b> <b>Learning</b> (RL), multiple types of impedance identification experiments of human elbow were replicated on a musculoskeletal model. We compared the elbow movement controlled by an RL agent with the motion of an actual human elbow in terms of the impedance identified in torque-perturbation experiments. The findings reveal that the RL agent exhibits higher elbow impedance to stabilise the target elbow motion under perturbation than a human does, likely due to its shorter reaction time and superior sensory capabilities. This study serves as a preliminary exploration into the potential of virtual environment <b>simulations</b> for neuromechanical research, offering an initial yet promising alternative to conventional experimental approaches. An RL-controlled digital twin with complete musculoskeletal models of the human body is expected to be useful in designing experiments and validating rehabilitation theory before experiments on real human subjects.</p></p class="citation"></blockquote><h3 id=248285-dexdiffuser-generating-dexterous-grasps-with-diffusion-models-zehang-weng-et-al-2024>(248/285) DexDiffuser: Generating Dexterous Grasps with Diffusion Models (Zehang Weng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehang Weng, Haofei Lu, Danica Kragic, Jens Lundell. (2024)<br><strong>DexDiffuser: Generating Dexterous Grasps with Diffusion Models</strong><br><button class=copy-to-clipboard title="DexDiffuser: Generating Dexterous Grasps with Diffusion Models" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02989v1.pdf filename=2402.02989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce DexDiffuser, a novel dexterous grasping method that generates, evaluates, and refines grasps on partial object point clouds. DexDiffuser includes the conditional diffusion-based grasp sampler DexSampler and the dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality grasps conditioned on object point clouds by iterative denoising of randomly sampled grasps. We also introduce two grasp refinement strategies: Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR). Our <b>simulation</b> and real-world experiments on the Allegro Hand consistently demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger grasp generation method FFHNet with an, on average, 21.71&ndash;22.20% higher grasp success rate.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=249285-darts-diffusion-approximated-residual-time-sampling-for-low-variance-time-of-flight-rendering-in-homogeneous-scattering-medium-qianyue-he-et-al-2024>(249/285) DARTS: Diffusion Approximated Residual Time Sampling for Low Variance Time-of-flight Rendering in Homogeneous Scattering Medium (Qianyue He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianyue He, Xin Jin, Haitian Jiang, Dongyu Du. (2024)<br><strong>DARTS: Diffusion Approximated Residual Time Sampling for Low Variance Time-of-flight Rendering in Homogeneous Scattering Medium</strong><br><button class=copy-to-clipboard title="DARTS: Diffusion Approximated Residual Time Sampling for Low Variance Time-of-flight Rendering in Homogeneous Scattering Medium" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: I-3-7, cs-GR, cs.GR<br>Keyword Score: 23<br>Keywords: Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03106v1.pdf filename=2402.03106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time-of-flight (ToF) devices have greatly propelled the advancement of various <b>multi-modal</b> perception applications. However, due to complexity in both sampling path construction and vertex connection in time domain, it is extremely challenging to accurately render time-resolved information in ToF device <b>simulation,</b> particularly in scenes involving complex geometric structures, diverse materials and volumetric scattering media. Existing works either exhibit significant bias or variance in ToF rendering tasks or prove ineffective in scenes involving participating media and camera-warped settings. To address this challenge, in this paper, we integrate the transient diffusion theory into path construction to generate the unbiased full transport of time-resolved radiance. Additionally, we devise an elliptical sampling method to provide controllable vertex connection satisfying any required photon traversal time. To our knowledge, our work is the first to explore importance sampling according to transient radiance, enabling temporal path construction of higher quality in multiple scattering settings. Extensive experiments show that our sampling method can significantly improve both quality and efficiency of ToF rendering within both path tracing and photon-based frameworks, with at least a 5x MSE reduction versus SOTA methods in equal rendering time. Our method introduces no memory overhead and negligible extra computation compared to the boost in speed, providing a straightforward plug-in for various existing rendering frameworks.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=250285-minlp-based-hybrid-strategy-for-operating-mode-selection-of-tes-backed-up-refrigeration-systems-g-bejarano-et-al-2024>(250/285) MINLP-based hybrid strategy for operating mode selection of TES-backed-up refrigeration systems (G. Bejarano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>G. Bejarano, D. Rodríguez, J. M. Lemos, M. Vargas, M. G. Ortega. (2024)<br><strong>MINLP-based hybrid strategy for operating mode selection of TES-backed-up refrigeration systems</strong><br><button class=copy-to-clipboard title="MINLP-based hybrid strategy for operating mode selection of TES-backed-up refrigeration systems" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03580v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03580v2.pdf filename=2402.03580v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This brief deals with the satisfaction of the daily cooling demand by a hybrid system that consists of a vapour-compression refrigeration cycle and a thermal energy storage (TES) unit, based on phase change materials. The addition of the TES tank to the original refrigeration plant allows to schedule the cooling production regardless of the instantaneous demand, given that the TES tank can store cold energy and release it whenever deemed appropriate. The scheduling problem is posed as an optimization problem based on mixed-integer non-linear programming (MINLP), since it includes both discrete and continuous variables. The latter corresponds to the references on the main cooling powers involved in the problem (cooling production at the evaporator and TES charging/discharging), whereas the discrete variables define the operating mode scheduling. Therefore, in addition to the hybrid features of the physical plant, a hybrid optimal control strategy is also proposed. A receding horizon approach is applied, similar to model predictive control (MPC) strategies, while economic criteria are imposed in the objective function, as well as feasibility issues. The TES state estimation is also addressed, since its instantaneous charge ratio is not measurable. The proposed strategy is applied in <b>simulation</b> to a challenging cooling demand profile and the main advantages of the MINLP-based strategy over a non-linear MPC-based scheduling strategy previously developed are highlighted, regarding operating cost, ease of tuning, and ability to adapt to cooling demand variations.</p></p class="citation"></blockquote><h3 id=251285-dual-lagrangian-learning-for-conic-optimization-mathieu-tanneau-et-al-2024>(251/285) Dual Lagrangian Learning for Conic Optimization (Mathieu Tanneau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathieu Tanneau, Pascal Van Hentenryck. (2024)<br><strong>Dual Lagrangian Learning for Conic Optimization</strong><br><button class=copy-to-clipboard title="Dual Lagrangian Learning for Conic Optimization" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03086v1.pdf filename=2402.03086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents Dual Lagrangian Learning (DLL), a principled learning methodology that combines conic duality theory with the representation power of ML models. DLL leverages conic duality to provide dual-feasible solutions, and therefore valid Lagrangian dual bounds, for parametric linear and nonlinear conic optimization problems. The paper introduces differentiable conic projection layers, a systematic dual completion procedure, and a <b>self-supervised</b> <b>learning</b> framework. The effectiveness of DLL is demonstrated on linear and nonlinear parametric optimization problems for which DLL provides valid dual bounds within 0.5% of optimality.</p></p class="citation"></blockquote><h2 id=csce-3>cs.CE (3)</h2><h3 id=252285-dynamic-flux-surrogate-based-partitioned-methods-for-interface-problems-pavel-bochev-et-al-2024>(252/285) Dynamic flux surrogate-based partitioned methods for interface problems (Pavel Bochev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pavel Bochev, Justin Owen, Paul Kuberry. (2024)<br><strong>Dynamic flux surrogate-based partitioned methods for interface problems</strong><br><button class=copy-to-clipboard title="Dynamic flux surrogate-based partitioned methods for interface problems" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: G-0; G-1, Primary (65N30), Secondary (5Q35), cs-CE, cs.CE, math-DS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03560v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03560v1.pdf filename=2402.03560v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Partitioned methods for coupled problems rely on data transfers between subdomains to synchronize the subdomain equations and enable their independent solution. By treating each subproblem as a separate entity, these methods enable code reuse, increase concurrency and provide a convenient framework for plug-and-play multiphysics <b>simulations.</b> However, accuracy and stability of partitioned methods depends critically on the type of information exchanged between the subproblems. The exchange mechanisms can vary from minimally intrusive remap across interfaces to more accurate but also more intrusive and expensive estimates of the necessary information based on monolithic formulations of the coupled system. These transfer mechanisms are separated by accuracy, performance and intrusiveness gaps that tend to limit the scope of the resulting partitioned methods to specific <b>simulation</b> scenarios. Data-driven system identification techniques provide an opportunity to close these gaps by enabling the construction of accurate, computationally efficient and minimally intrusive data transfer surrogates. This approach shifts the principal computational burden to an offline phase, leaving the application of the surrogate as the sole additional cost during the online <b>simulation</b> phase. In this paper we formulate and demonstrate such a \emph{dynamic flux surrogate-based} partitioned method for a model advection-diffusion transmission problem by using Dynamic Mode Decomposition (DMD) to learn the dynamics of the interface flux from data. The accuracy of the resulting DMD flux surrogate is comparable to that of a dual Schur complement reconstruction, yet its application cost is significantly lower. Numerical results confirm the attractive properties of the new partitioned approach.</p></p class="citation"></blockquote><h3 id=253285-learning-solutions-of-parametric-navier-stokes-with-physics-informed-neural-networks-m-naderibeni-et-al-2024>(253/285) Learning solutions of parametric Navier-Stokes with physics-informed neural networks (M. Naderibeni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M. Naderibeni, M. J. T. Reinders, L. Wu, D. M. J. Tax. (2024)<br><strong>Learning solutions of parametric Navier-Stokes with physics-informed neural networks</strong><br><button class=copy-to-clipboard title="Learning solutions of parametric Navier-Stokes with physics-informed neural networks" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs-LG, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03153v1.pdf filename=2402.03153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We leverage Physics-Informed Neural Networks (PINNs) to learn solution functions of parametric Navier-Stokes Equations (NSE). Our proposed approach results in a feasible optimization problem setup that bypasses PINNs&rsquo; limitations in converging to solutions of highly nonlinear parametric-PDEs like NSE. We consider the parameter(s) of interest as inputs of PINNs along with spatio-temporal coordinates, and train PINNs on generated numerical solutions of parametric-PDES for instances of the parameters. We perform experiments on the classical 2D flow past cylinder problem aiming to learn velocities and pressure functions over a range of Reynolds numbers as parameter of interest. Provision of training data from generated numerical <b>simulations</b> allows for interpolation of the solution functions for a range of parameters. Therefore, we compare PINNs with unconstrained conventional Neural Networks (NN) on this problem setup to investigate the effectiveness of considering the PDEs regularization in the loss function. We show that our proposed approach results in optimizing PINN models that learn the solution functions while making sure that flow predictions are in line with conservational laws of mass and momentum. Our results show that PINN results in accurate prediction of gradients compared to NN model, this is clearly visible in predicted vorticity fields given that none of these models were trained on vorticity labels.</p></p class="citation"></blockquote><h3 id=254285-a-comprehensive-numerical-approach-to-coil-placement-in-cerebral-aneurysms-mathematical-modeling-and-in-silico-occlusion-classification-fabian-holzberger-et-al-2024>(254/285) A Comprehensive Numerical Approach to Coil Placement in Cerebral Aneurysms: Mathematical Modeling and In Silico Occlusion Classification (Fabian Holzberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabian Holzberger, Markus Muhr, Barbara Wohlmuth. (2024)<br><strong>A Comprehensive Numerical Approach to Coil Placement in Cerebral Aneurysms: Mathematical Modeling and In Silico Occlusion Classification</strong><br><button class=copy-to-clipboard title="A Comprehensive Numerical Approach to Coil Placement in Cerebral Aneurysms: Mathematical Modeling and In Silico Occlusion Classification" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02798v1.pdf filename=2402.02798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Endovascular coil embolization is one of the primary treatment techniques for cerebral aneurysms. Although it is a well established and minimally invasive method, it bears the risk of sub-optimal coil placement which can lead to incomplete occlusion of the aneurysm possibly causing recurrence. One of the key features of coils is that they have an imprinted natural shape supporting the fixation within the aneurysm. For the spatial discretization our mathematical coil model is based on the Discrete Elastic Rod model which results in a dimension-reduced 1D system of differential equations. We include bending and twisting responses to account for the coils natural curvature. Collisions between coil segments and the aneurysm-wall are handled by an efficient contact algorithm that relies on an octree based collision detection. The numerical solution of the model is obtained by a symplectic semi-implicit Euler time stepping method. Our model can be easily incorporated into blood flow <b>simulations</b> of embolized aneurysms. In order to differentiate optimal from sub-optimal placements, we employ a suitable in silico Raymond-Roy type occlusion classification and measure the local packing density in the aneurysm at its neck, wall-region and core. We investigate the impact of uncertainties in the coil parameters and embolization procedure. To this end, we vary the position and the angle of insertion of the microcatheter, and approximate the local packing density distributions by evaluating sample statistics.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=255285-video-super-resolution-for-optimized-bitrate-and-green-online-streaming-vignesh-v-menon-et-al-2024>(255/285) Video Super-Resolution for Optimized Bitrate and Green Online Streaming (Vignesh V Menon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vignesh V Menon, Prajit T Rajendran, Amritha Premkumar, Benjamin Bross, Detlev Marpe. (2024)<br><strong>Video Super-Resolution for Optimized Bitrate and Green Online Streaming</strong><br><button class=copy-to-clipboard title="Video Super-Resolution for Optimized Bitrate and Green Online Streaming" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs.MM<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03513v1.pdf filename=2402.03513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional per-title encoding schemes strive to optimize encoding resolutions to deliver the utmost perceptual quality for each bitrate ladder representation. Nevertheless, maintaining encoding time within an acceptable threshold is equally imperative in online streaming applications. Furthermore, modern client devices are equipped with the capability for fast deep-learning-based video super-resolution (VSR) techniques, enhancing the perceptual quality of the decoded bitstream. This suggests that opting for lower resolutions in representations during the encoding process can curtail the overall energy consumption without substantially compromising perceptual quality. In this context, this paper introduces a video super-resolution-based latency-aware optimized bitrate encoding scheme (ViSOR) designed for online adaptive streaming applications. ViSOR determines the encoding resolution for each target bitrate, ensuring the highest achievable perceptual quality after VSR within the bound of a maximum acceptable latency. Random forest-based prediction models are trained to predict the perceptual quality after VSR and the encoding time for each resolution using the spatiotemporal features extracted for each video segment. Experimental results show that ViSOR targeting fast super-resolution <b>convolutional</b> <b>neural</b> <b>network</b> (FSRCNN) achieves an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder encoding of 4 s segments using the x265 encoder, when the maximum acceptable latency for each representation is set as two seconds. Considering a just noticeable difference (JND) of six VMAF points, the average cumulative storage consumption and encoding energy for each segment is reduced by 79.32 % and 68.21 %, respectively, contributing towards greener streaming.</p></p class="citation"></blockquote><h2 id=astro-phsr-1>astro-ph.SR (1)</h2><h3 id=256285-active-region-based-flare-forecasting-with-sliding-window-multivariate-time-series-forest-classifiers-anli-ji-et-al-2024>(256/285) Active Region-based Flare Forecasting with Sliding Window Multivariate Time Series Forest Classifiers (Anli Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anli Ji, Berkay Aydin. (2024)<br><strong>Active Region-based Flare Forecasting with Sliding Window Multivariate Time Series Forest Classifiers</strong><br><button class=copy-to-clipboard title="Active Region-based Flare Forecasting with Sliding Window Multivariate Time Series Forest Classifiers" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.SR<br>Categories: astro-ph-SR, astro-ph.SR, cs-LG, stat-AP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03474v1.pdf filename=2402.03474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past few decades, many applications of physics-based <b>simulations</b> and data-driven techniques (including machine learning and deep learning) have emerged to analyze and predict solar flares. These approaches are pivotal in understanding the dynamics of solar flares, primarily aiming to forecast these events and minimize potential risks they may pose to Earth. Although current methods have made significant progress, there are still limitations to these data-driven approaches. One prominent drawback is the lack of consideration for the temporal evolution characteristics in the active regions from which these flares originate. This oversight hinders the ability of these methods to grasp the relationships between high-dimensional active region features, thereby limiting their usability in operations. This study centers on the development of interpretable classifiers for multivariate time series and the demonstration of a novel feature ranking method with sliding window-based sub-interval ranking. The primary contribution of our work is to bridge the gap between complex, less understandable black-box models used for high-dimensional data and the exploration of relevant sub-intervals from multivariate time series, specifically in the context of solar flare forecasting. Our findings demonstrate that our sliding-window time series forest classifier performs effectively in solar flare prediction (with a True Skill Statistic of over 85%) while also pinpointing the most crucial features and sub-intervals for a given learning task.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=257285-knowledge-acquisition-and-integration-with-expert-in-the-loop-sajjadur-rahman-et-al-2024>(257/285) Knowledge Acquisition and Integration with Expert-in-the-loop (Sajjadur Rahman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sajjadur Rahman, Frederick Choi, Hannah Kim, Dan Zhang, Estevam Hruschka. (2024)<br><strong>Knowledge Acquisition and Integration with Expert-in-the-loop</strong><br><button class=copy-to-clipboard title="Knowledge Acquisition and Integration with Expert-in-the-loop" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-DB, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Recommendation, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03291v1.pdf filename=2402.03291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Constructing and serving knowledge graphs (KGs) is an iterative and human-centered process involving on-demand programming and analysis. In this paper, we present Kyurem, a programmable and interactive widget library that facilitates <b>human-in-the-loop</b> knowledge acquisition and integration to enable continuous curation a knowledge graph (KG). Kyurem provides a seamless environment within computational notebooks where data scientists explore a KG to identify opportunities for acquiring new knowledge and verify <b>recommendations</b> provided by AI agents for integrating the acquired knowledge in the KG. We refined Kyurem through participatory design and conducted case studies in a real-world setting for evaluation. The case-studies show that introduction of Kyurem within an existing HR knowledge graph construction and serving platform improved the user experience of the experts and helped eradicate inefficiencies related to knowledge acquisition and integration tasks</p></p class="citation"></blockquote><h3 id=258285-matcha-an-ide-plugin-for-creating-accurate-privacy-nutrition-labels-tianshi-li-et-al-2024>(258/285) Matcha: An IDE Plugin for Creating Accurate Privacy Nutrition Labels (Tianshi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianshi Li, Lorrie Faith Cranor, Yuvraj Agarwal, Jason I. Hong. (2024)<br><strong>Matcha: An IDE Plugin for Creating Accurate Privacy Nutrition Labels</strong><br><button class=copy-to-clipboard title="Matcha: An IDE Plugin for Creating Accurate Privacy Nutrition Labels" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CR, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03582v1.pdf filename=2402.03582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Apple and Google introduced their versions of privacy nutrition labels to the mobile app stores to better inform users of the apps&rsquo; data practices. However, these labels are self-reported by developers and have been found to contain many inaccuracies due to misunderstandings of the label taxonomy. In this work, we present Matcha, an IDE plugin that uses automated code analysis to help developers create accurate Google Play data safety labels. Developers can benefit from Matcha&rsquo;s ability to detect user data accesses and transmissions while staying in control of the generated label by adding custom Java annotations and modifying an auto-generated XML specification. Our evaluation with 12 developers showed that Matcha helped our participants improved the accuracy of a label they created with Google&rsquo;s official tool for a real-world app they developed. We found that participants preferred Matcha for its accuracy benefits. Drawing on Matcha, we discuss general design <b>recommendations</b> for developer tools used to create accurate standardized privacy notices.</p></p class="citation"></blockquote><h3 id=259285-meeting-bridges-designing-information-artifacts-that-bridge-from-synchronous-meetings-to-asynchronous-collaboration-ruotong-wang-et-al-2024>(259/285) Meeting Bridges: Designing Information Artifacts that Bridge from Synchronous Meetings to Asynchronous Collaboration (Ruotong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruotong Wang, Lin Qiu, Justin Cranshaw, Amy X. Zhang. (2024)<br><strong>Meeting Bridges: Designing Information Artifacts that Bridge from Synchronous Meetings to Asynchronous Collaboration</strong><br><button class=copy-to-clipboard title="Meeting Bridges: Designing Information Artifacts that Bridge from Synchronous Meetings to Asynchronous Collaboration" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03259v1.pdf filename=2402.03259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A recent surge in remote meetings has led to complaints of <code>Zoom fatigue'' and </code>collaboration overload,&rsquo;&rsquo; negatively impacting worker productivity and well-being. One way to alleviate the burden of meetings is to de-emphasize their synchronous participation by shifting work to and enabling sensemaking during post-meeting asynchronous activities. Towards this goal, we propose the design concept of meeting bridges, or information artifacts that can encapsulate meeting information towards bridging to and facilitating post-meeting activities. Through 13 interviews and a survey of 198 information workers, we learn how people use online meeting information after meetings are over, finding five main uses: as an archive, as task reminders, to onboard or support inclusion, for group sensemaking, and as a launching point for follow-on collaboration. However, we also find that current common meeting artifacts, such as notes and recordings, present challenges in serving as meeting bridges. After conducting co-design sessions with 16 participants, we <b>distill</b> key principles for the design of meeting bridges to optimally support asynchronous collaboration goals. Overall, our findings point to the opportunity of designing information artifacts that not only support users to access but also continue to transform and engage in meeting information post-meeting.</p></p class="citation"></blockquote><h3 id=260285-teach-me-how-to-improvise-co-designing-an-augmented-piano-training-system-for-improvisation-jordan-aiko-deja-et-al-2024>(260/285) Teach Me How to ImproVISe: Co-Designing an Augmented Piano Training System for Improvisation (Jordan Aiko Deja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jordan Aiko Deja, Sandi Štor, Ilonka Pucihar, Klen Čopič Pucihar, Matjaž Kljun. (2024)<br><strong>Teach Me How to ImproVISe: Co-Designing an Augmented Piano Training System for Improvisation</strong><br><button class=copy-to-clipboard title="Teach Me How to ImproVISe: Co-Designing an Augmented Piano Training System for Improvisation" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-SD, cs.HC, eess-AS<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02999v1.pdf filename=2402.02999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Improvisation is a vital but often neglected aspect of traditional piano teaching. Challenges such as difficulty in assessment and subjectivity have hindered its effective instruction. Technological approaches, including augmentation, aim to enhance piano instruction, but the specific application of digital augmentation for piano improvisation is under-explored. This paper outlines a co-design process developing an Augmented Reality (AR) Piano Improvisation Training System, ImproVISe, involving improvisation teachers. The prototype, featuring basic improvisation concepts, was created and refined through expert interaction. Their insights guided the identification of objectives, tools, interaction metaphors, and software features. The findings offer design guidelines and <b>recommendations</b> to address challenges in assessing piano improvisation in a learning context.</p></p class="citation"></blockquote><h2 id=csse-4>cs.SE (4)</h2><h3 id=261285-user-centric-evaluation-of-chatgpt-capability-of-generating-r-program-code-tanha-miah-et-al-2024>(261/285) User-Centric Evaluation of ChatGPT Capability of Generating R Program Code (Tanha Miah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanha Miah, Hong Zhu. (2024)<br><strong>User-Centric Evaluation of ChatGPT Capability of Generating R Program Code</strong><br><button class=copy-to-clipboard title="User-Centric Evaluation of ChatGPT Capability of Generating R Program Code" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: ChatGPT, Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03130v1.pdf filename=2402.03130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper reports an evaluation of <b>ChatGPT&rsquo;s</b> capability of generating R programming language <b>code</b> <b>from</b> natural language input. A dataset specially designed for generating R program <b>code</b> <b>was</b> constructed with metadata to support scenario-based testing and evaluation of <b>code</b> <b>generation</b> capabilities in various usage scenarios of different levels of difficulty and different types of programs. The evaluation takes a multiple attempt process in which the tester tries to complete the <b>code</b> <b>generation</b> task through a number of attempts until a satisfactory solution is obtained or gives up after a fixed number of maximal attempts. In each attempt the tester formulates a natural language input to <b>ChatGPT</b> based on the previous results and the task to be completed. In addition to the metrics of average numbers of attempts and average amount of time taken to complete the tasks, the final generated solutions are then assessed on a number of quality attributes, including accuracy, completeness, conciseness, readability, well structuredness, logic clarity, depth of ex-planation, and coverage of parameters. Our experiments demonstrated that <b>ChatGPT</b> is in general highly capable of generating high quality R program <b>code</b> <b>as</b> well as textual explanations although it may fail on hard programming tasks. The experiment data also shows that human developers can hardly learn from experiences naturally to improve the skill of using <b>ChatGPT</b> to generate code.</p></p class="citation"></blockquote><h3 id=262285-a-survey-on-effective-invocation-methods-of-massive-llm-services-can-wang-et-al-2024>(262/285) A Survey on Effective Invocation Methods of Massive LLM Services (Can Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Can Wang, Bolin Zhang, Dianbo Sui, Zhiying Tum, Xiaoyu Liu, Jiabao Kang. (2024)<br><strong>A Survey on Effective Invocation Methods of Massive LLM Services</strong><br><button class=copy-to-clipboard title="A Survey on Effective Invocation Methods of Massive LLM Services" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-DC, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03408v1.pdf filename=2402.03408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models as a service (LMaaS) enable users to accomplish tasks without requiring specialized knowledge, simply by paying a service provider. However, numerous providers offer massive <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> services with variations in latency, performance, and pricing. Consequently, constructing the cost-saving <b>LLM</b> services invocation strategy with low-latency and high-performance responses that meet specific task demands becomes a pressing challenge. This paper provides a comprehensive overview of the <b>LLM</b> services invocation methods. Technically, we give a formal definition of the problem of constructing effective invocation strategy in LMaaS and present the <b>LLM</b> services invocation framework. The framework classifies existing methods into four different components, including input abstract, semantic cache, solution design, and output enhancement, which can be freely combined with each other. Finally, we emphasize the open challenges that have not yet been well addressed in this task and shed light on future research.</p></p class="citation"></blockquote><h3 id=263285-predicting-configuration-performance-in-multiple-environments-with-sequential-meta-learning-jingzhi-gong-et-al-2024>(263/285) Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning (Jingzhi Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingzhi Gong, Tao Chen. (2024)<br><strong>Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning</strong><br><button class=copy-to-clipboard title="Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-PF, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03183v1.pdf filename=2402.03183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning and predicting the performance of given software configurations are of high importance to many software engineering activities. While configurable software systems will almost certainly face diverse running environments (e.g., version, hardware, and workload), current work often either builds performance models under a single environment or fails to properly handle data from diverse settings, hence restricting their accuracy for new environments. In this paper, we target configuration performance learning under multiple environments. We do so by designing SeMPL - a <b>meta-learning</b> <b>framework</b> that learns the common understanding from configurations measured in distinct <b>(meta)</b> <b>environments</b> and generalizes them to the unforeseen, target environment. What makes it unique is that unlike common <b>meta-learning</b> <b>frameworks</b> (e.g., MAML and MetaSGD) that train the <b>meta</b> <b>environments</b> in parallel, we train them sequentially, one at a time. The order of training naturally allows discriminating the contributions among <b>meta</b> <b>environments</b> in the <b>meta-model</b> <b>built,</b> which fits better with the characteristic of configuration data that is known to dramatically differ between different environments. Through comparing with 15 state-of-the-art models under nine systems, our extensive experimental results demonstrate that SeMPL performs considerably better on 89% of the systems with up to 99% accuracy improvement, while being data-efficient, leading to a maximum of 3.86x speedup. All code and data can be found at our repository: <a href=https://github.com/ideas-labo/SeMPL>https://github.com/ideas-labo/SeMPL</a>.</p></p class="citation"></blockquote><h3 id=264285-how-do-software-practitioners-perceive-human-centric-defects-vedant-chauhan-et-al-2024>(264/285) How do software practitioners perceive human-centric defects? (Vedant Chauhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vedant Chauhan, Chetan Arora, Hourieh Khalajzadeh, John Grundy. (2024)<br><strong>How do software practitioners perceive human-centric defects?</strong><br><button class=copy-to-clipboard title="How do software practitioners perceive human-centric defects?" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02726v1.pdf filename=2402.02726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: Human-centric software design and development focuses on how users want to carry out their tasks rather than making users accommodate their software. Software users can have different genders, ages, cultures, languages, disabilities, socioeconomic statuses, and educational backgrounds, among many other differences. Due to the inherently varied nature of these differences and their impact on software usage, preferences and issues of users can vary, resulting in user-specific defects that we term as `human-centric defects&rsquo; (HCDs). Objective: This research aims to understand the perception and current management practices of such human-centric defects by software practitioners, identify key challenges in reporting, understanding and fixing them, and provide <b>recommendations</b> to improve HCDs management in software engineering. Method: We conducted a survey and interviews with software engineering practitioners to gauge their knowledge and experience on HCDs and the defect tracking process. Results: We analysed fifty (50) survey- and ten (10) interview- responses from SE practitioners and identified that there are multiple gaps in the current management of HCDs in software engineering practice. There is a lack of awareness regarding human-centric aspects, causing them to be lost or under-appreciated during software development. Our results revealed that handling HCDs could be improved by following a better feedback process with end-users, a more descriptive taxonomy, and suitable automation. Conclusion: HCDs present a major challenge to software practitioners, given their diverse end-user base. In the software engineering domain, research on HCDs has been limited and requires effort from the research and practice communities to create better awareness and support regarding human-centric aspects.</p></p class="citation"></blockquote><h2 id=mathna-3>math.NA (3)</h2><h3 id=265285-variational-discretizations-of-ideal-magnetohydrodynamics-in-smooth-regime-using-finite-element-exterior-calculus-valentin-carlier-et-al-2024>(265/285) Variational discretizations of ideal magnetohydrodynamics in smooth regime using finite element exterior calculus (Valentin Carlier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valentin Carlier, Martin Campos-Pinto. (2024)<br><strong>Variational discretizations of ideal magnetohydrodynamics in smooth regime using finite element exterior calculus</strong><br><button class=copy-to-clipboard title="Variational discretizations of ideal magnetohydrodynamics in smooth regime using finite element exterior calculus" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: G-1-8; J-2, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02905v1.pdf filename=2402.02905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a new class of finite element approximations to ideal compressible magnetohydrodynamic equations in smooth regime. Following variational approximations developed for fluid models in the last decade, our discretizations are built via a discrete variational principle mimicking the continuous Euler-Poincare principle, and to further exploit the geometrical structure of the problem, vector fields are represented by their action as Lie derivatives on differential forms of any degree. The resulting semi-discrete approximations are shown to conserve the total mass, entropy and energy of the solutions for a wide class of finite element approximations. In addition, the divergence-free nature of the magnetic field is preserved in a pointwise sense and a time discretization is proposed, preserving those invariants and giving a reversible scheme at the fully discrete level. Numerical <b>simulations</b> are conducted to verify the accuracy of our approach and its ability to preserve the invariants for several test problems.</p></p class="citation"></blockquote><h3 id=266285-do-we-need-decay-preserving-error-estimate-for-solving-parabolic-equations-with-initial-singularity-jiwei-zhang-et-al-2024>(266/285) Do we need decay-preserving error estimate for solving parabolic equations with initial singularity? (Jiwei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiwei Zhang, Zhimin Zhang, Chengchao Zhao. (2024)<br><strong>Do we need decay-preserving error estimate for solving parabolic equations with initial singularity?</strong><br><button class=copy-to-clipboard title="Do we need decay-preserving error estimate for solving parabolic equations with initial singularity?" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M06, 65M15, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02849v1.pdf filename=2402.02849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Solutions exhibiting weak initial singularities arise in various equations, including diffusion and subdiffusion equations. When employing the well-known L1 scheme to solve subdiffusion equations with weak singularities, numerical <b>simulations</b> reveal that this scheme exhibits varying convergence rates for different choices of model parameters (i.e., domain size, final time $T$, and reaction coefficient $\kappa$). This elusive phenomenon is not unique to the L1 scheme but is also observed in other numerical methods for reaction-diffusion equations such as the backward Euler (IE) scheme, Crank-Nicolson (C-N) scheme, and two-step backward differentiation formula (BDF2) scheme. The existing literature lacks an explanation for the existence of two different convergence regimes, which has puzzled us for a long while and motivated us to study this inconsistency between the standard convergence theory and numerical experiences. In this paper, we provide a general methodology to systematically obtain error estimates that incorporate the exponential decaying feature of the solution. We term this novel error estimate the `decay-preserving error estimate&rsquo; and apply it to the aforementioned IE, C-N, and BDF2 schemes. Our decay-preserving error estimate consists of a low-order term with an exponential coefficient and a high-order term with an algebraic coefficient, both of which depend on the model parameters. Our estimates reveal that the varying convergence rates are caused by a trade-off between these two components in different model parameter regimes. By considering the model parameters, we capture different states of the convergence rate that traditional error estimates fail to explain. This approach retains more properties of the continuous solution. We validate our analysis with numerical results.</p></p class="citation"></blockquote><h3 id=267285-uncertainty-quantification-of-phase-transition-problems-with-an-injection-boundary-zhenyi-zhang-et-al-2024>(267/285) Uncertainty Quantification of Phase Transition Problems with an Injection Boundary (Zhenyi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenyi Zhang, Shengbo Ma, Zhennan Zhou. (2024)<br><strong>Uncertainty Quantification of Phase Transition Problems with an Injection Boundary</strong><br><button class=copy-to-clipboard title="Uncertainty Quantification of Phase Transition Problems with an Injection Boundary" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02806v1.pdf filename=2402.02806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop an enthalpy-based modeling and computational framework to quantify uncertainty in Stefan problems with an injection boundary. Inspired by airfoil icing studies, we consider a system featuring an injection boundary inducing domain changes and a free boundary separating phases, resulting in two types of moving boundaries. Our proposed enthalpy-based formulation seamlessly integrates thermal diffusion across the domain with energy fluxes at the boundaries, addressing a modified injection condition for boundary movement. Uncertainty then stems from random variations in the injection boundary. The primary focus of our Uncertainty Quantification (UQ) centers on investigating the effects of uncertainty on free boundary propagation. Through mapping to a reference domain, we derive an enthalpy-based numerical scheme tailored to the transformed coordinate system, facilitating a simple and efficient <b>simulation.</b> Numerical and UQ studies in one and two dimensions validate the proposed model and the extended enthalpy method. They offer intriguing insights into ice accretion and other multiphysics processes involving phase transitions.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=268285-intent-profiling-and-translation-through-emergent-communication-salwa-mostafa-et-al-2024>(268/285) Intent Profiling and Translation Through Emergent Communication (Salwa Mostafa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salwa Mostafa, Mohammed S. Elbamby, Mohamed K. Abdel-Aziz, Mehdi Bennis. (2024)<br><strong>Intent Profiling and Translation Through Emergent Communication</strong><br><button class=copy-to-clipboard title="Intent Profiling and Translation Through Emergent Communication" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-LG, cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02768v1.pdf filename=2402.02768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To effectively express and satisfy network application requirements, intent-based network management has emerged as a promising solution. In intent-based methods, users and applications express their intent in a high-level abstract language to the network. Although this abstraction simplifies network operation, it induces many challenges to efficiently express applications&rsquo; intents and map them to different network capabilities. Therefore, in this work, we propose an AI-based framework for intent profiling and translation. We consider a scenario where applications interacting with the network express their needs for network services in their domain language. The machine-to-machine communication (i.e., between applications and the network) is complex since it requires networks to learn how to understand the domain languages of each application, which is neither practical nor scalable. Instead, a framework based on emergent communication is proposed for intent profiling, in which applications express their abstract quality-of-experience (QoE) intents to the network through emergent communication messages. Subsequently, the network learns how to interpret these communication messages and map them to network capabilities (i.e., slices) to guarantee the requested Quality-of-Service (QoS). <b>Simulation</b> results show that the proposed method outperforms self-learning slicing and other baselines, and achieves a performance close to the perfect knowledge baseline.</p></p class="citation"></blockquote><h3 id=269285-stitching-the-spectrum-semantic-spectrum-segmentation-with-wideband-signal-stitching-daniel-uvaydov-et-al-2024>(269/285) Stitching the Spectrum: Semantic Spectrum Segmentation with Wideband Signal Stitching (Daniel Uvaydov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Uvaydov, Milin Zhang, Clifton Paul Robinson, Salvatore D&rsquo;Oro, Tommaso Melodia, Francesco Restuccia. (2024)<br><strong>Stitching the Spectrum: Semantic Spectrum Segmentation with Wideband Signal Stitching</strong><br><button class=copy-to-clipboard title="Stitching the Spectrum: Semantic Spectrum Segmentation with Wideband Signal Stitching" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03465v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03465v2.pdf filename=2402.03465v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spectrum has become an extremely scarce and congested resource. As a consequence, spectrum sensing enables the coexistence of different wireless technologies in shared spectrum bands. Most existing work requires spectrograms to classify signals. Ultimately, this implies that images need to be continuously created from I/Q samples, thus creating unacceptable latency for real-time operations. In addition, spectrogram-based approaches do not achieve sufficient granularity level as they are based on <b>object</b> <b>detection</b> performed on pixels and are based on rectangular bounding boxes. For this reason, we propose a completely novel approach based on semantic spectrum segmentation, where multiple signals are simultaneously classified and localized in both time and frequency at the I/Q level. Conversely from the state-of-the-art computer vision algorithm, we add non-local blocks to combine the spatial features of signals, and thus achieve better performance. In addition, we propose a novel data generation approach where a limited set of easy-to-collect real-world wireless signals are ``stitched together&rsquo;&rsquo; to generate large-scale, wideband, and diverse datasets. Experimental results obtained on multiple testbeds (including the Arena testbed) using multiple antennas, multiple sampling frequencies, and multiple radios over the course of 3 days show that our approach classifies and localizes signals with a mean intersection over union (IOU) of 96.70% across 5 wireless protocols while performing in real-time with a latency of 2.6 ms. Moreover, we demonstrate that our approach based on non-local blocks achieves 7% more accuracy when segmenting the most challenging signals with respect to the state-of-the-art U-Net algorithm. We will release our 17 GB dataset and code.</p></p class="citation"></blockquote><h2 id=physicschem-ph-1>physics.chem-ph (1)</h2><h3 id=270285-gaussian-plane-wave-neural-operator-for-electron-density-estimation-seongsu-kim-et-al-2024>(270/285) Gaussian Plane-Wave Neural Operator for Electron Density Estimation (Seongsu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seongsu Kim, Sungsoo Ahn. (2024)<br><strong>Gaussian Plane-Wave Neural Operator for Electron Density Estimation</strong><br><button class=copy-to-clipboard title="Gaussian Plane-Wave Neural Operator for Electron Density Estimation" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.chem-ph<br>Categories: cs-LG, physics-chem-ph, physics.chem-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04278v1.pdf filename=2402.04278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work studies machine learning for electron density prediction, which is fundamental for understanding chemical systems and density functional theory (DFT) <b>simulations.</b> To this end, we introduce the Gaussian plane-wave neural operator (GPWNO), which operates in the infinite-dimensional functional space using the plane-wave and Gaussian-type orbital bases, widely recognized in the context of DFT. In particular, both high- and low-frequency components of the density can be effectively represented due to the complementary nature of the two bases. Extensive experiments on QM9, MD, and material project datasets demonstrate GPWNO&rsquo;s superior performance over ten baselines.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=271285-genetic-guided-gflownets-advancing-in-practical-molecular-optimization-benchmark-hyeonah-kim-et-al-2024>(271/285) Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark (Hyeonah Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyeonah Kim, Minsu Kim, Sanghyeok Choi, Jinkyoo Park. (2024)<br><strong>Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark</strong><br><button class=copy-to-clipboard title="Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, cs-NE, q-bio-BM, q-bio.BM<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05961v1.pdf filename=2402.05961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a novel variant of GFlowNet, genetic-guided GFlowNet (Genetic GFN), which integrates an iterative genetic search into GFlowNet. Genetic search effectively guides the GFlowNet to high-rewarded regions, addressing global over-exploration that results in training inefficiency and exploring limited regions. In addition, training strategies, such as rank-based replay training and <b>unsupervised</b> maximum likelihood pre-training, are further introduced to improve the sample efficiency of Genetic GFN. The proposed method shows a state-of-the-art score of 16.213, significantly outperforming the reported best score in the benchmark of 15.185, in practical molecular optimization (PMO), which is an official benchmark for sample-efficient molecular optimization. Remarkably, ours exceeds all baselines, including <b>reinforcement</b> <b>learning,</b> Bayesian optimization, generative models, GFlowNets, and genetic algorithms, in 14 out of 23 tasks.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=272285-the-gigs-up-how-chatgpt-stacks-up-against-quora-on-gig-economy-insights-thomas-lancaster-2024>(272/285) The Gig&rsquo;s Up: How ChatGPT Stacks Up Against Quora on Gig Economy Insights (Thomas Lancaster, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Lancaster. (2024)<br><strong>The Gig&rsquo;s Up: How ChatGPT Stacks Up Against Quora on Gig Economy Insights</strong><br><button class=copy-to-clipboard title="The Gig's Up: How ChatGPT Stacks Up Against Quora on Gig Economy Insights" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Generative AI, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02676v1.pdf filename=2402.02676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> is changing the way in which humans seek to find answers to questions in different fields including on the gig economy and labour markets, but there is limited information available about closely <b>ChatGPT</b> simulated output matches that obtainable from existing question and answer platforms. This paper uses <b>ChatGPT</b> as a research assistant to explore how far <b>ChatGPT</b> can replicate Quora question and answers, using data from the gig economy as an indicative case study. The results from content analysis suggest that Quora is likely to be asked questions from users looking to make money and answers are likely to include personal experiences and examples. <b>ChatGPT</b> simulated versions are less personal and more concept-based, including considerations on employment implications and labour rights. It appears therefore that <b>generative</b> <b>AI</b> simulates only part of what a human would want in their answers relating to the gig economy. The paper proposes that a similar comparative methodology would also be useful across other research fields to help in establishing the best real world uses of <b>generative</b> <b>AI.</b></p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=273285-preliminary-report-on-mantis-shrimp-a-multi-survey-computer-vision-photometric-redshift-model-andrew-engel-et-al-2024>(273/285) Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision Photometric Redshift Model (Andrew Engel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Engel, Gautham Narayan, Nell Byler. (2024)<br><strong>Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision Photometric Redshift Model</strong><br><button class=copy-to-clipboard title="Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision Photometric Redshift Model" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph.IM, cs-AI<br>Keyword Score: 13<br>Keywords: Convolutional Neural Network, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03535v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03535v1.pdf filename=2402.03535v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The availability of large, public, <b>multi-modal</b> astronomical datasets presents an opportunity to execute novel research that straddles the line between science of AI and science of astronomy. Photometric redshift estimation is a well-established subfield of astronomy. Prior works show that computer vision models typically outperform catalog-based models, but these models face additional complexities when incorporating images from more than one instrument or sensor. In this report, we detail our progress creating Mantis Shrimp, a multi-survey computer vision model for photometric redshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and infrared (UnWISE) imagery. We use deep learning interpretability diagnostics to measure how the model leverages information from the different inputs. We reason about the behavior of the <b>CNNs</b> from the interpretability metrics, specifically framing the result in terms of physically-grounded knowledge of galaxy properties.</p></p class="citation"></blockquote><h2 id=cslo-2>cs.LO (2)</h2><h3 id=274285-polynomial-lawvere-logic-giorgio-bacci-et-al-2024>(274/285) Polynomial Lawvere Logic (Giorgio Bacci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giorgio Bacci, Radu Mardare, Prakash Panangaden, Gordon Plotkin. (2024)<br><strong>Polynomial Lawvere Logic</strong><br><button class=copy-to-clipboard title="Polynomial Lawvere Logic" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03543v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03543v2.pdf filename=2402.03543v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study Polynomial Lawvere logic (PL), a logic on the quantale of the extended positive reals, developed for <b>reasoning</b> about metric spaces. PL is appropriate for encoding quantitative <b>reasoning</b> principles, such as quantitative equational logic. PL formulas include the polynomial functions on the extended positive reals, and its judgements include inequalities between polynomials. We present an inference system for PL and prove a series of completeness and incompleteness results relying and the Krivine-Stengle Positivstellensatz (a variant of Hilbert&rsquo;s Nullstellensatz) including completeness for finitely axiomatisable PL theories. We also study complexity results both for both PL and its affine fragment (AL). We demonstrate that the satisfiability of a finite set of judgements is NP-complete in AL and in PSPACE for PL; and that deciding the semantical consequence from a finite set of judgements is co-NP complete in AL and in PSPACE in PL.</p></p class="citation"></blockquote><h3 id=275285-proof-theory-and-decision-procedures-for-deontic-stit-logics-tim-s-lyon-et-al-2024>(275/285) Proof Theory and Decision Procedures for Deontic STIT Logics (Tim S. Lyon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim S. Lyon, Kees van Berkel. (2024)<br><strong>Proof Theory and Decision Procedures for Deontic STIT Logics</strong><br><button class=copy-to-clipboard title="Proof Theory and Decision Procedures for Deontic STIT Logics" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO, math-LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03148v1.pdf filename=2402.03148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the automation of <b>reasoning</b> with deontic STIT logics by means of proof theory. Our methodology consists of leveraging sound and cut-free complete sequent-style calculi to write a proof-search algorithm deciding deontic, multi-agent STIT logics with (un)limited choice. In order to ensure the termination of our proof-search algorithm, we introduce a special loop-checking mechanism. Despite the acknowledged potential for deontic <b>reasoning</b> in the context of autonomous vehicles and other areas of AI, this work is the first to provide a syntactic decision procedure for deontic STIT logics. Our proof-search procedures are designed to provide verifiable witnesses/certificates of the (in)validity of formulae, which permit an analysis of the (non)theoremhood of formulae and act as explanations thereof. We utilize our proof-search algorithm to address agent-based normative <b>reasoning</b> tasks such as compliance checking.</p></p class="citation"></blockquote><h2 id=mathat-1>math.AT (1)</h2><h3 id=276285-symmetric-monoidal-smash-products-in-homotopy-type-theory-axel-ljungström-2024>(276/285) Symmetric Monoidal Smash Products in Homotopy Type Theory (Axel Ljungström, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Axel Ljungström. (2024)<br><strong>Symmetric Monoidal Smash Products in Homotopy Type Theory</strong><br><button class=copy-to-clipboard title="Symmetric Monoidal Smash Products in Homotopy Type Theory" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AT<br>Categories: cs-LO, math-AT, math.AT<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03523v1.pdf filename=2402.03523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Homotopy Type Theory, few constructions have proved as troublesome as the smash product. While its definition is just as direct as in classical mathematics, one quickly realises that in order to define and reason about functions over iterations of it, one has to verify an exponentially growing number of coherences. This has led to crucial results concerning smash products remaining open. One particularly important such result is the fact that smash products form a (1-coherent) symmetric monoidal product on the universe of pointed types. This fact was used, without a complete proof, by e.g. Brunerie in his PhD thesis to construct the cup product on integral cohomology and is, more generally, a fundamental result in traditional algebraic topology. In this paper, we salvage the situation by introducing a simple informal heuristic for <b>reasoning</b> about functions defined over iterated smash products. We then use the heuristic to verify e.g. the hexagon and pentagon identities, thereby obtaining a proof of symmetric monoidality. We also provide a formal statement of the heuristic in terms of an induction principle concerning the construction of homotopies of functions defined over iterated smash products. The key results presented here have been formalised in the proof assistant Cubical Agda.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=277285-algorithms-and-complexity-of-difference-logic-konrad-k-dabrowski-et-al-2024>(277/285) Algorithms and Complexity of Difference Logic (Konrad K. Dabrowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konrad K. Dabrowski, Peter Jonsson, Sebastian Ordyniak, George Osipov. (2024)<br><strong>Algorithms and Complexity of Difference Logic</strong><br><button class=copy-to-clipboard title="Algorithms and Complexity of Difference Logic" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs-LO, cs.DS<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03273v1.pdf filename=2402.03273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Difference Logic (DL) is a fragment of linear arithmetics where atoms are constraints x+k &lt;= y for variables x,y (ranging over Q or Z) and integer k. We study the complexity of deciding the truth of existential DL sentences. This problem appears in many contexts: examples include verification, bioinformatics, telecommunications, and spatio-temporal <b>reasoning</b> in AI. We begin by considering sentences in CNF with rational-valued variables. We restrict the allowed clauses via two natural parameters: arity and coefficient bounds. The problem is NP-hard for most choices of these parameters. As a response to this, we refine our understanding by analyzing the time complexity and the parameterized complexity (with respect to well-studied parameters such as primal and incidence treewidth). We obtain a comprehensive picture of the complexity landscape in both cases. Finally, we generalize our results to integer domains and sentences that are not in CNF.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=278285-heana-a-hybrid-time-amplitude-analog-optical-accelerator-with-flexible-dataflows-for-energy-efficient-cnn-inference-sairam-sri-vatsavai-et-al-2024>(278/285) HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference (Sairam Sri Vatsavai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sairam Sri Vatsavai, Venkata Sai Praneeth Karempudi, Ishan Thakkar. (2024)<br><strong>HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference</strong><br><button class=copy-to-clipboard title="HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs-ET, cs.AR<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03247v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03247v2.pdf filename=2402.03247v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several photonic microring resonators (MRRs) based analog accelerators have been proposed to accelerate the inference of integer-quantized <b>CNNs</b> with remarkably higher throughput and energy efficiency compared to their electronic counterparts. However, the existing analog photonic accelerators suffer from three shortcomings: (i) severe hampering of wavelength parallelism due to various crosstalk effects, (ii) inflexibility of supporting various dataflows other than the weight-stationary dataflow, and (iii) failure in fully leveraging the ability of photodetectors to perform in-situ accumulations. These shortcomings collectively hamper the performance and energy efficiency of prior accelerators. To tackle these shortcomings, we present a novel Hybrid timE Amplitude aNalog optical Accelerator, called HEANA. HEANA employs hybrid time-amplitude analog optical multipliers (TAOMs) that increase the flexibility of HEANA to support multiple dataflows. A spectrally hitless arrangement of TAOMs significantly reduces the crosstalk effects, thereby increasing the wavelength parallelism in HEANA. Moreover, HEANA employs our invented balanced photo-charge accumulators (BPCAs) that enable buffer-less, in-situ, temporal accumulations to eliminate the need to use reduction networks in HEANA, relieving it from related latency and energy overheads. Our evaluation for the inference of four modern <b>CNNs</b> indicates that HEANA provides improvements of atleast 66x and 84x in frames-per-second (FPS) and FPS/W (energy-efficiency), respectively, for equal-area comparisons, on gmean over two MRR-based analog <b>CNN</b> accelerators from prior work.</p></p class="citation"></blockquote><h3 id=279285-a-comparative-analysis-of-microrings-based-incoherent-photonic-gemm-accelerators-sairam-sri-vatsavai-et-al-2024>(279/285) A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators (Sairam Sri Vatsavai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sairam Sri Vatsavai, Venkata Sai Praneeth Karempudi, Oluwaseun Adewunmi Alo, Ishan Thakkar. (2024)<br><strong>A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators</strong><br><button class=copy-to-clipboard title="A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-ET, cs-LG, cs-NE, cs.AR<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03149v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03149v2.pdf filename=2402.03149v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several microring resonator (MRR) based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs) in deep neural networks with exceptional throughput and energy efficiency. To implement GEMM functions, these MRR-based architectures, in general, manipulate optical signals in five different ways: (i) Splitting (copying) of multiple optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing) of multiple optical signals to achieve a certain fan-in, (iii) Modulation of optical signals to imprint input values onto analog signal amplitude, (iv) Weighting of modulated optical signals to achieve analog input-weight multiplication, (v) Summation of optical signals. The MRR-based GEMM accelerators undertake the first four ways of signal manipulation in an arbitrary order ignoring the possible impact of the order of these manipulations on their performance. In this paper, we conduct a detailed analysis of accelerator organizations with three different orders of these manipulations: (1) Modulation-Aggregation-Splitting-Weighting (MASW), (2) Aggregation-Splitting-Modulation-Weighting (ASMW), and (3) Splitting-Modulation-Weighting-Aggregation (SMWA). We show that these organizations affect the crosstalk noise and optical signal losses in different magnitudes, which renders these organizations with different levels of processing parallelism at the circuit level, and different magnitudes of throughput and energy-area efficiency at the system level. Our evaluation results for four <b>CNN</b> models show that SMWA organization achieves up to 4.4$\times$, 5$\times$, and 5.2$\times$ better throughput, energy efficiency, and area-energy efficiency, respectively, compared to ASMW and MASW organizations on average.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=280285-scoped-effects-as-parameterized-algebraic-theories-sam-lindley-et-al-2024>(280/285) Scoped Effects as Parameterized Algebraic Theories (Sam Lindley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sam Lindley, Cristina Matache, Sean Moss, Sam Staton, Nicolas Wu, Zhixuan Yang. (2024)<br><strong>Scoped Effects as Parameterized Algebraic Theories</strong><br><button class=copy-to-clipboard title="Scoped Effects as Parameterized Algebraic Theories" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-LO, cs-PL, cs.PL, math-CT<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.03103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.03103v1.pdf filename=2402.03103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Notions of computation can be modelled by monads. Algebraic effects offer a characterization of monads in terms of algebraic operations and equational axioms, where operations are basic programming features, such as reading or updating the state, and axioms specify observably equivalent expressions. However, many useful programming features depend on additional mechanisms such as delimited scopes or dynamically allocated resources. Such mechanisms can be supported via extensions to algebraic effects including scoped effects and parameterized algebraic theories. We present a fresh perspective on scoped effects by translation into a variation of parameterized algebraic theories. The translation enables a new approach to equational <b>reasoning</b> for scoped effects and gives rise to an alternative characterization of monads in terms of generators and equations involving both scoped and algebraic operations. We demonstrate the power of our fresh perspective by way of equational characterizations of several known models of scoped effects.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=281285-design-and-implementation-of-an-automated-disaster-recovery-system-for-a-kubernetes-cluster-using-lstm-ji-beom-kim-et-al-2024>(281/285) Design and Implementation of an Automated Disaster-recovery System for a Kubernetes Cluster Using LSTM (Ji-Beom Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ji-Beom Kim, Je-Bum Choi, Eun-Sung Jung. (2024)<br><strong>Design and Implementation of an Automated Disaster-recovery System for a Kubernetes Cluster Using LSTM</strong><br><button class=copy-to-clipboard title="Design and Implementation of an Automated Disaster-recovery System for a Kubernetes Cluster Using LSTM" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02938v1.pdf filename=2402.02938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing importance of data in the modern business environment, effective data man-agement and protection strategies are gaining increasing research attention. Data protection in a cloud environment is crucial for safeguarding information assets and maintaining sustainable services. This study introduces a system structure that integrates Kubernetes management plat-forms with backup and restoration tools. This system is designed to immediately detect disasters and automatically recover applications from another kubernetes cluster. The experimental results show that this system executes the restoration process within 15 s without <b>human</b> <b>intervention,</b> enabling rapid recovery. This, in turn, significantly reduces the potential for delays and errors compared with manual recovery processes, thereby enhancing data management and recovery ef-ficiency in cloud environments. Moreover, our research model predicts the CPU utilization of the cluster using Long Short-Term Memory (LSTM). The necessity of scheduling through this predict is made clearer through comparison with experiments without scheduling, demonstrating its ability to prevent performance degradation. This research highlights the efficiency and necessity of automatic recovery systems in cloud environments, setting a new direction for future research.</p></p class="citation"></blockquote><h3 id=282285-practical-rateless-set-reconciliation-lei-yang-et-al-2024>(282/285) Practical Rateless Set Reconciliation (Lei Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Yang, Yossi Gilad, Mohammad Alizadeh. (2024)<br><strong>Practical Rateless Set Reconciliation</strong><br><button class=copy-to-clipboard title="Practical Rateless Set Reconciliation" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-NI, cs.DC<br>Keyword Score: 10<br>Keywords: BLOOM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02668v1.pdf filename=2402.02668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Set reconciliation, where two parties hold fixed-length bit strings and run a protocol to learn the strings they are missing from each other, is a fundamental task in many distributed systems. We present Rateless Invertible <b>Bloom</b> Lookup Tables (Rateless IBLT), the first set reconciliation protocol, to the best of our knowledge, that achieves low computation cost and near-optimal communication cost across a wide range of scenarios: set differences of one to millions, bit strings of a few bytes to megabytes, and workloads injected by potential adversaries. Rateless IBLT is based on a novel encoder that incrementally encodes the set difference into an infinite stream of coded symbols, resembling rateless error-correcting codes. We compare Rateless IBLT with state-of-the-art set reconciliation schemes and demonstrate significant improvements. Rateless IBLT achieves 3&ndash;4x lower communication cost than non-rateless schemes with similar computation cost, and 2&ndash;2000x lower computation cost than schemes with similar communication cost. We show the real-world benefits of Rateless IBLT by applying it to synchronize the state of the Ethereum blockchain, and demonstrate 5.6x lower end-to-end completion time and 4.4x lower communication cost compared to the system used in production.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=283285-mining-a-minimal-set-of-behavioral-patterns-using-incremental-evaluation-mehdi-acheli-et-al-2024>(283/285) Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation (Mehdi Acheli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mehdi Acheli, Daniela Grigori, Matthias Weidlich. (2024)<br><strong>Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation</strong><br><button class=copy-to-clipboard title="Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs-LG, cs.DB<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02921v1.pdf filename=2402.02921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Process mining provides methods to analyse event logs generated by information systems during the execution of processes. It thereby supports the design, validation, and execution of processes in domains ranging from healthcare, through manufacturing, to e-commerce. To explore the regularities of flexible processes that show a large behavioral variability, it was suggested to mine recurrent behavioral patterns that jointly describe the underlying process. Existing approaches to behavioral pattern mining, however, suffer from two limitations. First, they show limited scalability as incremental computation is incorporated only in the generation of pattern candidates, but not in the evaluation of their quality. Second, process analysis based on mined patterns shows limited effectiveness due to an overwhelmingly large number of patterns obtained in practical application scenarios, many of which are redundant. In this paper, we address these limitations to facilitate the analysis of complex, flexible processes based on behavioral patterns. Specifically, we improve COBPAM, our initial behavioral pattern mining algorithm, by an incremental procedure to evaluate the quality of pattern candidates, optimizing thereby its efficiency. Targeting a more effective use of the resulting patterns, we further propose <b>pruning</b> strategies for redundant patterns and show how relations between the remaining patterns are extracted and visualized to provide process insights. Our experiments with diverse real-world datasets indicate a considerable reduction of the runtime needed for pattern mining, while a qualitative assessment highlights how relations between patterns guide the analysis of the underlying process.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=284285-multi-region-markovian-gaussian-process-an-efficient-method-to-discover-directional-communications-across-multiple-brain-regions-weihan-li-et-al-2024>(284/285) Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions (Weihan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weihan Li, Chengrui Li, Yule Wang, Anqi Wu. (2024)<br><strong>Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions</strong><br><button class=copy-to-clipboard title="Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-LG, q-bio-NC, q-bio.NC<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02686v1.pdf filename=2402.02686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Studying the complex interactions between different brain regions is crucial in neuroscience. Various statistical methods have explored the latent communication across multiple brain regions. Two main categories are the <b>Gaussian</b> <b>Process</b> (GP) and Linear Dynamical System (LDS), each with unique strengths. The GP-based approach effectively discovers latent variables such as frequency bands and communication directions. Conversely, the LDS-based approach is computationally efficient but lacks powerful expressiveness in latent representation. In this study, we merge both methodologies by creating an LDS mirroring a multi-output GP, termed Multi-Region Markovian <b>Gaussian</b> <b>Process</b> (MRM-GP). Our work is the first to establish a connection between an LDS and a multi-output GP that explicitly models frequencies and phase delays within the latent space of neural recordings. Consequently, the model achieves a linear inference cost over time points and provides an interpretable low-dimensional representation, revealing communication directions across brain regions and separating oscillatory communications into different frequency bands.</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=285285-estimation-of-conditional-average-treatment-effects-on-distributed-data-a-privacy-preserving-approach-yuji-kawamata-et-al-2024>(285/285) Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach (Yuji Kawamata et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuji Kawamata, Ryoki Motai, Yukihiko Okada, Akira Imakura, Tetsuya Sakurai. (2024)<br><strong>Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach</strong><br><button class=copy-to-clipboard title="Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-CR, cs-LG, stat-ME, stat.ME<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.02672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.02672v1.pdf filename=2402.02672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimation of conditional average treatment effects (CATEs) is an important topic in various fields such as medical and social sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data if they contain privacy information. To address this issue, we proposed data collaboration double machine learning (DC-DML), a method that can estimate CATE models with privacy preservation of distributed data, and evaluated the method through numerical experiments. Our contributions are <b>summarized</b> in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Semi-parametric or non-parametric CATE models enable estimation and testing that is more robust to model mis-specification than parametric models. However, to our knowledge, no communication-efficient method has been proposed for estimating and testing semi-parametric or non-parametric CATE models on distributed data. Second, our method enables collaborative estimation between different parties as well as multiple time points because the dimensionality-reduced intermediate representations can be accumulated. Third, our method performed as well or better than other methods in evaluation experiments using synthetic, semi-synthetic and real-world datasets.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.06</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.08</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-48>cs.CL (48)</a><ul><li><a href=#1285-illuminate-a-novel-approach-for-depression-detection-with-explainable-analysis-and-proactive-therapy-using-prompt-engineering-aryan-agrawal-2024>(1/285) Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering (Aryan Agrawal, 2024)</a></li><li><a href=#2285-enhancing-textbook-question-answering-task-with-large-language-models-and-retrieval-augmented-generation-hessa-abdulrahman-alawwad-et-al-2024>(2/285) Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation (Hessa Abdulrahman Alawwad et al., 2024)</a></li><li><a href=#3285-lb-kbqa-large-language-model-and-bert-based-knowledge-based-question-and-answering-system-yan-zhao-et-al-2024>(3/285) LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System (Yan Zhao et al., 2024)</a></li><li><a href=#4285-constrained-decoding-for-cross-lingual-label-projection-duong-minh-le-et-al-2024>(4/285) Constrained Decoding for Cross-lingual Label Projection (Duong Minh Le et al., 2024)</a></li><li><a href=#5285-arabic-synonym-bert-based-adversarial-examples-for-text-classification-norah-alshahrani-et-al-2024>(5/285) Arabic Synonym BERT-based Adversarial Examples for Text Classification (Norah Alshahrani et al., 2024)</a></li><li><a href=#6285-multi-multimodal-understanding-leaderboard-with-text-and-images-zichen-zhu-et-al-2024>(6/285) Multi: Multimodal Understanding Leaderboard with Text and Images (Zichen Zhu et al., 2024)</a></li><li><a href=#7285-swag-storytelling-with-action-guidance-zeeshan-patel-et-al-2024>(7/285) SWAG: Storytelling With Action Guidance (Zeeshan Patel et al., 2024)</a></li><li><a href=#8285-llm-agents-in-interaction-measuring-personality-consistency-and-linguistic-alignment-in-interacting-populations-of-large-language-models-ivar-frisch-et-al-2024>(8/285) LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models (Ivar Frisch et al., 2024)</a></li><li><a href=#9285-kivi-a-tuning-free-asymmetric-2bit-quantization-for-kv-cache-zirui-liu-et-al-2024>(9/285) KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache (Zirui Liu et al., 2024)</a></li><li><a href=#10285-graph-neural-network-and-ner-based-text-summarization-imaad-zaffar-khan-et-al-2024>(10/285) Graph Neural Network and NER-Based Text Summarization (Imaad Zaffar Khan et al., 2024)</a></li><li><a href=#11285-resolving-transcription-ambiguity-in-spanish-a-hybrid-acoustic-lexical-system-for-punctuation-restoration-xiliang-zhu-et-al-2024>(11/285) Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration (Xiliang Zhu et al., 2024)</a></li><li><a href=#12285-nevermind-instruction-override-and-moderation-in-large-language-models-edward-kim-2024>(12/285) Nevermind: Instruction Override and Moderation in Large Language Models (Edward Kim, 2024)</a></li><li><a href=#13285-uncertainty-of-thoughts-uncertainty-aware-planning-enhances-information-seeking-in-large-language-models-zhiyuan-hu-et-al-2024>(13/285) Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models (Zhiyuan Hu et al., 2024)</a></li><li><a href=#14285-english-prompts-are-better-for-nli-based-zero-shot-emotion-classification-than-target-language-prompts-patrick-barreiß-et-al-2024>(14/285) English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts (Patrick Barreiß et al., 2024)</a></li><li><a href=#15285-how-do-large-language-models-learn-in-context-query-and-key-matrices-of-in-context-heads-are-two-towers-for-metric-learning-zeping-yu-et-al-2024>(15/285) How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning (Zeping Yu et al., 2024)</a></li><li><a href=#16285-large-language-models-are-geographically-biased-rohin-manvi-et-al-2024>(16/285) Large Language Models are Geographically Biased (Rohin Manvi et al., 2024)</a></li><li><a href=#17285-chain-of-feedback-mitigating-the-effects-of-inconsistency-in-responses-jinwoo-ahn-2024>(17/285) Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses (Jinwoo Ahn, 2024)</a></li><li><a href=#18285-evaluating-the-factuality-of-zero-shot-summarizers-across-varied-domains-sanjana-ramprasad-et-al-2024>(18/285) Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains (Sanjana Ramprasad et al., 2024)</a></li><li><a href=#19285-psychological-assessments-with-large-language-models-a-privacy-focused-and-cost-effective-approach-sergi-blanco-cuaresma-2024>(19/285) Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach (Sergi Blanco-Cuaresma, 2024)</a></li><li><a href=#20285-deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models-zhihong-shao-et-al-2024>(20/285) DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Zhihong Shao et al., 2024)</a></li><li><a href=#21285-cidar-culturally-relevant-instruction-dataset-for-arabic-zaid-alyafeai-et-al-2024>(21/285) CIDAR: Culturally Relevant Instruction Dataset For Arabic (Zaid Alyafeai et al., 2024)</a></li><li><a href=#22285-multilingual-transformer-and-bertopic-for-short-text-topic-modeling-the-case-of-serbian-darija-medvecki-et-al-2024>(22/285) Multilingual transformer and BERTopic for short text topic modeling: The case of Serbian (Darija Medvecki et al., 2024)</a></li><li><a href=#23285-easyinstruct-an-easy-to-use-instruction-processing-framework-for-large-language-models-yixin-ou-et-al-2024>(23/285) EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models (Yixin Ou et al., 2024)</a></li><li><a href=#24285-unimem-towards-a-unified-view-of-long-context-large-language-models-junjie-fang-et-al-2024>(24/285) UniMem: Towards a Unified View of Long-Context Large Language Models (Junjie Fang et al., 2024)</a></li><li><a href=#25285-putting-context-in-context-the-impact-of-discussion-structure-on-text-classification-nicolò-penzo-et-al-2024>(25/285) Putting Context in Context: the Impact of Discussion Structure on Text Classification (Nicolò Penzo et al., 2024)</a></li><li><a href=#26285-zero-shot-clinical-trial-patient-matching-with-llms-michael-wornow-et-al-2024>(26/285) Zero-Shot Clinical Trial Patient Matching with LLMs (Michael Wornow et al., 2024)</a></li><li><a href=#27285-texshape-information-theoretic-sentence-embedding-for-language-models-h-kaan-kale-et-al-2024>(27/285) TexShape: Information Theoretic Sentence Embedding for Language Models (H. Kaan Kale et al., 2024)</a></li><li><a href=#28285-deal-or-no-deal-or-who-knows-forecasting-uncertainty-in-conversations-using-large-language-models-anthony-sicilia-et-al-2024>(28/285) Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models (Anthony Sicilia et al., 2024)</a></li><li><a href=#29285-jobskape-a-framework-for-generating-synthetic-job-postings-to-enhance-skill-matching-antoine-magron-et-al-2024>(29/285) JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance Skill Matching (Antoine Magron et al., 2024)</a></li><li><a href=#30285-bge-m3-embedding-multi-lingual-multi-functionality-multi-granularity-text-embeddings-through-self-knowledge-distillation-jianlv-chen-et-al-2024>(30/285) BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation (Jianlv Chen et al., 2024)</a></li><li><a href=#31285-homograph-attacks-on-maghreb-sentiment-analyzers-fatima-zahra-qachfar-et-al-2024>(31/285) Homograph Attacks on Maghreb Sentiment Analyzers (Fatima Zahra Qachfar et al., 2024)</a></li><li><a href=#32285-linguistic-features-for-sentence-difficulty-prediction-in-absa-adrian-gabriel-chifu-et-al-2024>(32/285) Linguistic features for sentence difficulty prediction in ABSA (Adrian-Gabriel Chifu et al., 2024)</a></li><li><a href=#33285-best-practices-for-text-annotation-with-large-language-models-petter-törnberg-2024>(33/285) Best Practices for Text Annotation with Large Language Models (Petter Törnberg, 2024)</a></li><li><a href=#34285-intent-based-prompt-calibration-enhancing-prompt-optimization-with-synthetic-boundary-cases-elad-levi-et-al-2024>(34/285) Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases (Elad Levi et al., 2024)</a></li><li><a href=#35285-multi-lingual-malaysian-embedding-leveraging-large-language-models-for-semantic-representations-husein-zolkepli-et-al-2024>(35/285) Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for Semantic Representations (Husein Zolkepli et al., 2024)</a></li><li><a href=#36285-ks-lottery-finding-certified-lottery-tickets-for-multilingual-language-models-fei-yuan-et-al-2024>(36/285) KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models (Fei Yuan et al., 2024)</a></li><li><a href=#37285-unified-hallucination-detection-for-multimodal-large-language-models-xiang-chen-et-al-2024>(37/285) Unified Hallucination Detection for Multimodal Large Language Models (Xiang Chen et al., 2024)</a></li><li><a href=#38285-sociolinguistically-informed-interpretability-a-case-study-on-hinglish-emotion-classification-kushal-tatariya-et-al-2024>(38/285) Sociolinguistically Informed Interpretability: A Case Study on Hinglish Emotion Classification (Kushal Tatariya et al., 2024)</a></li><li><a href=#39285-sidu-txt-an-xai-algorithm-for-nlp-with-a-holistic-assessment-approach-mohammad-n-s-jahromi-et-al-2024>(39/285) SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach (Mohammad N. S. Jahromi et al., 2024)</a></li><li><a href=#40285-automated-cognate-detection-as-a-supervised-link-prediction-task-with-cognate-transformer-v-s-d-s-mahesh-akavarapu-et-al-2024>(40/285) Automated Cognate Detection as a Supervised Link Prediction Task with Cognate Transformer (V. S. D. S. Mahesh Akavarapu et al., 2024)</a></li><li><a href=#41285-approximate-attributions-for-off-the-shelf-siamese-transformers-lucas-möller-et-al-2024>(41/285) Approximate Attributions for Off-the-Shelf Siamese Transformers (Lucas Möller et al., 2024)</a></li><li><a href=#42285-comparing-knowledge-sources-for-open-domain-scientific-claim-verification-juraj-vladika-et-al-2024>(42/285) Comparing Knowledge Sources for Open-Domain Scientific Claim Verification (Juraj Vladika et al., 2024)</a></li><li><a href=#43285-rethinking-optimization-and-architecture-for-tiny-language-models-yehui-tang-et-al-2024>(43/285) Rethinking Optimization and Architecture for Tiny Language Models (Yehui Tang et al., 2024)</a></li><li><a href=#44285-racer-an-llm-powered-methodology-for-scalable-analysis-of-semi-structured-mental-health-interviews-satpreet-harcharan-singh-et-al-2024>(44/285) RACER: An LLM-powered Methodology for Scalable Analysis of Semi-structured Mental Health Interviews (Satpreet Harcharan Singh et al., 2024)</a></li><li><a href=#45285-financial-report-chunking-for-effective-retrieval-augmented-generation-antonio-jimeno-yepes-et-al-2024>(45/285) Financial Report Chunking for Effective Retrieval Augmented Generation (Antonio Jimeno Yepes et al., 2024)</a></li><li><a href=#46285-accurate-and-well-calibrated-icd-code-assignment-through-attention-over-diverse-label-embeddings-gonçalo-gomes-et-al-2024>(46/285) Accurate and Well-Calibrated ICD Code Assignment Through Attention Over Diverse Label Embeddings (Gonçalo Gomes et al., 2024)</a></li><li><a href=#47285-eevee-an-easy-annotation-tool-for-natural-language-processing-axel-sorensen-et-al-2024>(47/285) EEVEE: An Easy Annotation Tool for Natural Language Processing (Axel Sorensen et al., 2024)</a></li><li><a href=#48285-with-a-little-help-from-my-linguistic-friends-topic-segmentation-of-multi-party-casual-conversations-amandine-decker-et-al-2024>(48/285) With a Little Help from my (Linguistic) Friends: Topic Segmentation of Multi-party Casual Conversations (Amandine Decker et al., 2024)</a></li></ul></li><li><a href=#csir-12>cs.IR (12)</a><ul><li><a href=#49285-large-language-model-distilling-medication-recommendation-model-qidong-liu-et-al-2024>(49/285) Large Language Model Distilling Medication Recommendation Model (Qidong Liu et al., 2024)</a></li><li><a href=#50285-harnessing-pubmed-user-query-logs-for-post-hoc-explanations-of-recommended-similar-articles-ashley-shin-et-al-2024>(50/285) Harnessing PubMed User Query Logs for Post Hoc Explanations of Recommended Similar Articles (Ashley Shin et al., 2024)</a></li><li><a href=#51285-list-aware-reranking-truncation-joint-model-for-search-and-retrieval-augmented-generation-shicheng-xu-et-al-2024>(51/285) List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation (Shicheng Xu et al., 2024)</a></li><li><a href=#52285-finest-stabilizing-recommendations-by-rank-preserving-fine-tuning-sejoon-oh-et-al-2024>(52/285) FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning (Sejoon Oh et al., 2024)</a></li><li><a href=#53285-domain-adaptation-of-multilingual-semantic-search----literature-review-anna-bringmann-et-al-2024>(53/285) Domain Adaptation of Multilingual Semantic Search &ndash; Literature Review (Anna Bringmann et al., 2024)</a></li><li><a href=#54285-intersectional-two-sided-fairness-in-recommendation-yifan-wang-et-al-2024>(54/285) Intersectional Two-sided Fairness in Recommendation (Yifan Wang et al., 2024)</a></li><li><a href=#55285-event-based-product-carousel-recommendation-with-query-click-graph-luyi-ma-et-al-2024>(55/285) Event-based Product Carousel Recommendation with Query-Click Graph (Luyi Ma et al., 2024)</a></li><li><a href=#56285-comparison-of-topic-modelling-approaches-in-the-banking-context-bayode-ogunleye-et-al-2024>(56/285) Comparison of Topic Modelling Approaches in the Banking Context (Bayode Ogunleye et al., 2024)</a></li><li><a href=#57285-understanding-and-guiding-weakly-supervised-entity-alignment-with-potential-isomorphism-propagation-yuanyi-wang-et-al-2024>(57/285) Understanding and Guiding Weakly Supervised Entity Alignment with Potential Isomorphism Propagation (Yuanyi Wang et al., 2024)</a></li><li><a href=#58285-dynamic-sparse-learning-a-novel-paradigm-for-efficient-recommendation-shuyao-wang-et-al-2024>(58/285) Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation (Shuyao Wang et al., 2024)</a></li><li><a href=#59285-denoising-time-cycle-modeling-for-recommendation-sicong-xie-et-al-2024>(59/285) Denoising Time Cycle Modeling for Recommendation (Sicong Xie et al., 2024)</a></li><li><a href=#60285-trinity-syncretizing-multi-long-taillong-term-interests-all-in-one-jing-yan-et-al-2024>(60/285) Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One (Jing Yan et al., 2024)</a></li></ul></li><li><a href=#cslg-79>cs.LG (79)</a><ul><li><a href=#61285-iced-zero-shot-transfer-in-reinforcement-learning-via-in-context-environment-design-samuel-garcin-et-al-2024>(61/285) ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design (Samuel Garcin et al., 2024)</a></li><li><a href=#62285-automatic-combination-of-sample-selection-strategies-for-few-shot-learning-branislav-pecher-et-al-2024>(62/285) Automatic Combination of Sample Selection Strategies for Few-Shot Learning (Branislav Pecher et al., 2024)</a></li><li><a href=#63285-make-every-move-count-llm-based-high-quality-rtl-code-generation-using-mcts-matthew-delorenzo-et-al-2024>(63/285) Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS (Matthew DeLorenzo et al., 2024)</a></li><li><a href=#64285-empowering-time-series-analysis-with-large-language-models-a-survey-yushan-jiang-et-al-2024>(64/285) Empowering Time Series Analysis with Large Language Models: A Survey (Yushan Jiang et al., 2024)</a></li><li><a href=#65285-the-matrix-a-bayesian-learning-model-for-llms-siddhartha-dalal-et-al-2024>(65/285) The Matrix: A Bayesian learning model for LLMs (Siddhartha Dalal et al., 2024)</a></li><li><a href=#66285-a-survey-on-transformer-compression-yehui-tang-et-al-2024>(66/285) A Survey on Transformer Compression (Yehui Tang et al., 2024)</a></li><li><a href=#67285-guard-role-playing-to-generate-natural-language-jailbreakings-to-test-guideline-adherence-of-large-language-models-haibo-jin-et-al-2024>(67/285) GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models (Haibo Jin et al., 2024)</a></li><li><a href=#68285-shortened-llama-a-simple-depth-pruning-for-large-language-models-bo-kyeong-kim-et-al-2024>(68/285) Shortened LLaMA: A Simple Depth Pruning for Large Language Models (Bo-Kyeong Kim et al., 2024)</a></li><li><a href=#69285-diffusion-world-model-zihan-ding-et-al-2024>(69/285) Diffusion World Model (Zihan Ding et al., 2024)</a></li><li><a href=#70285-path-signatures-and-graph-neural-networks-for-slow-earthquake-analysis-better-together-hans-riess-et-al-2024>(70/285) Path Signatures and Graph Neural Networks for Slow Earthquake Analysis: Better Together? (Hans Riess et al., 2024)</a></li><li><a href=#71285-fairness-and-privacy-guarantees-in-federated-contextual-bandits-sambhav-solanki-et-al-2024>(71/285) Fairness and Privacy Guarantees in Federated Contextual Bandits (Sambhav Solanki et al., 2024)</a></li><li><a href=#72285-skill-set-optimization-reinforcing-language-model-behavior-via-transferable-skills-kolby-nottingham-et-al-2024>(72/285) Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills (Kolby Nottingham et al., 2024)</a></li><li><a href=#73285-is-mamba-capable-of-in-context-learning-riccardo-grazzi-et-al-2024>(73/285) Is Mamba Capable of In-Context Learning? (Riccardo Grazzi et al., 2024)</a></li><li><a href=#74285-text-guided-image-clustering-andreas-stephan-et-al-2024>(74/285) Text-Guided Image Clustering (Andreas Stephan et al., 2024)</a></li><li><a href=#75285-powergraph-a-power-grid-benchmark-dataset-for-graph-neural-networks-anna-varbella-et-al-2024>(75/285) PowerGraph: A power grid benchmark dataset for graph neural networks (Anna Varbella et al., 2024)</a></li><li><a href=#76285-vision-language-models-provide-promptable-representations-for-reinforcement-learning-william-chen-et-al-2024>(76/285) Vision-Language Models Provide Promptable Representations for Reinforcement Learning (William Chen et al., 2024)</a></li><li><a href=#77285-assessing-the-impact-of-distribution-shift-on-reinforcement-learning-performance-ted-fujimoto-et-al-2024>(77/285) Assessing the Impact of Distribution Shift on Reinforcement Learning Performance (Ted Fujimoto et al., 2024)</a></li><li><a href=#78285-effective-acquisition-functions-for-active-correlation-clustering-linus-aronsson-et-al-2024>(78/285) Effective Acquisition Functions for Active Correlation Clustering (Linus Aronsson et al., 2024)</a></li><li><a href=#79285-distinguishing-the-knowable-from-the-unknowable-with-language-models-gustaf-ahdritz-et-al-2024>(79/285) Distinguishing the Knowable from the Unknowable with Language Models (Gustaf Ahdritz et al., 2024)</a></li><li><a href=#80285-single-gpu-gnn-systems-traps-and-pitfalls-yidong-gong-et-al-2024>(80/285) Single-GPU GNN Systems: Traps and Pitfalls (Yidong Gong et al., 2024)</a></li><li><a href=#81285-hamlet-graph-transformer-neural-operator-for-partial-differential-equations-andrey-bryutkin-et-al-2024>(81/285) HAMLET: Graph Transformer Neural Operator for Partial Differential Equations (Andrey Bryutkin et al., 2024)</a></li><li><a href=#82285-can-we-remove-the-square-root-in-adaptive-gradient-methods-a-second-order-perspective-wu-lin-et-al-2024>(82/285) Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective (Wu Lin et al., 2024)</a></li><li><a href=#83285-understanding-the-reasoning-ability-of-language-models-from-the-perspective-of-reasoning-paths-aggregation-xinyi-wang-et-al-2024>(83/285) Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation (Xinyi Wang et al., 2024)</a></li><li><a href=#84285-mobilitygpt-enhanced-human-mobility-modeling-with-a-gpt-model-ammar-haydari-et-al-2024>(84/285) MobilityGPT: Enhanced Human Mobility Modeling with a GPT model (Ammar Haydari et al., 2024)</a></li><li><a href=#85285-less-is-ken-a-universal-and-simple-non-parametric-pruning-algorithm-for-large-language-models-michele-mastromattei-et-al-2024>(85/285) Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models (Michele Mastromattei et al., 2024)</a></li><li><a href=#86285-fine-tuning-reinforcement-learning-models-is-secretly-a-forgetting-mitigation-problem-maciej-wołczyk-et-al-2024>(86/285) Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem (Maciej Wołczyk et al., 2024)</a></li><li><a href=#87285-revisiting-vae-for-unsupervised-time-series-anomaly-detection-a-frequency-perspective-zexin-wang-et-al-2024>(87/285) Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective (Zexin Wang et al., 2024)</a></li><li><a href=#88285-position-paper-what-can-large-language-models-tell-us-about-time-series-analysis-ming-jin-et-al-2024>(88/285) Position Paper: What Can Large Language Models Tell Us about Time Series Analysis (Ming Jin et al., 2024)</a></li><li><a href=#89285-beyond-expectations-learning-with-stochastic-dominance-made-practical-shicong-cen-et-al-2024>(89/285) Beyond Expectations: Learning with Stochastic Dominance Made Practical (Shicong Cen et al., 2024)</a></li><li><a href=#90285-statistical-guarantees-for-link-prediction-using-graph-neural-networks-alan-chung-et-al-2024>(90/285) Statistical Guarantees for Link Prediction using Graph Neural Networks (Alan Chung et al., 2024)</a></li><li><a href=#91285-fusemoe-mixture-of-experts-transformers-for-fleximodal-fusion-xing-han-et-al-2024>(91/285) FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion (Xing Han et al., 2024)</a></li><li><a href=#92285-continual-domain-adversarial-adaptation-via-double-head-discriminators-yan-shen-et-al-2024>(92/285) Continual Domain Adversarial Adaptation via Double-Head Discriminators (Yan Shen et al., 2024)</a></li><li><a href=#93285-generalization-properties-of-adversarial-training-for-ell_0-bounded-adversarial-attacks-payam-delgosha-et-al-2024>(93/285) Generalization Properties of Adversarial Training for $\ell_0$-Bounded Adversarial Attacks (Payam Delgosha et al., 2024)</a></li><li><a href=#94285-online-feature-updates-improve-online-generalized-label-shift-adaptation-ruihan-wu-et-al-2024>(94/285) Online Feature Updates Improve Online (Generalized) Label Shift Adaptation (Ruihan Wu et al., 2024)</a></li><li><a href=#95285-early-prediction-of-onset-of-sepsis-in-clinical-setting-fahim-mohammad-et-al-2024>(95/285) Early prediction of onset of sepsis in Clinical Setting (Fahim Mohammad et al., 2024)</a></li><li><a href=#96285-zero-shot-object-level-ood-detection-with-context-aware-inpainting-quang-huy-nguyen-et-al-2024>(96/285) Zero-shot Object-Level OOD Detection with Context-Aware Inpainting (Quang-Huy Nguyen et al., 2024)</a></li><li><a href=#97285-a-framework-for-partially-observed-reward-states-in-rlhf-chinmaya-kausik-et-al-2024>(97/285) A Framework for Partially Observed Reward-States in RLHF (Chinmaya Kausik et al., 2024)</a></li><li><a href=#98285-rethink-model-re-basin-and-the-linear-mode-connectivity-xingyu-qu-et-al-2024>(98/285) Rethink Model Re-Basin and the Linear Mode Connectivity (Xingyu Qu et al., 2024)</a></li><li><a href=#99285-toward-green-and-human-like-artificial-intelligence-a-complete-survey-on-contemporary-few-shot-learning-approaches-georgios-tsoumplekas-et-al-2024>(99/285) Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches (Georgios Tsoumplekas et al., 2024)</a></li><li><a href=#100285-on-the-development-of-a-practical-bayesian-optimisation-algorithm-for-expensive-experiments-and-simulations-with-changing-environmental-conditions-mike-diessner-et-al-2024>(100/285) On the development of a practical Bayesian optimisation algorithm for expensive experiments and simulations with changing environmental conditions (Mike Diessner et al., 2024)</a></li><li><a href=#101285-ds-ms-tcn-otago-exercises-recognition-with-a-dual-scale-multi-stage-temporal-convolutional-network-meng-shang-et-al-2024>(101/285) DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network (Meng Shang et al., 2024)</a></li><li><a href=#102285-deep-autoregressive-density-nets-vs-neural-ensembles-for-model-based-offline-reinforcement-learning-abdelhakim-benechehab-et-al-2024>(102/285) Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning (Abdelhakim Benechehab et al., 2024)</a></li><li><a href=#103285-contrastive-diffuser-planning-towards-high-return-states-via-contrastive-learning-yixiang-shan-et-al-2024>(103/285) Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning (Yixiang Shan et al., 2024)</a></li><li><a href=#104285-counterfactual-fairness-is-not-demographic-parity-and-other-observations-ricardo-silva-2024>(104/285) Counterfactual Fairness Is Not Demographic Parity, and Other Observations (Ricardo Silva, 2024)</a></li><li><a href=#105285-learning-with-mixture-of-prototypes-for-out-of-distribution-detection-haodong-lu-et-al-2024>(105/285) Learning with Mixture of Prototypes for Out-of-Distribution Detection (Haodong Lu et al., 2024)</a></li><li><a href=#106285-how-good-is-a-single-basin-kai-lion-et-al-2024>(106/285) How Good is a Single Basin? (Kai Lion et al., 2024)</a></li><li><a href=#107285-a-reinforcement-learning-approach-for-dynamic-rebalancing-in-bike-sharing-system-jiaqi-liang-et-al-2024>(107/285) A Reinforcement Learning Approach for Dynamic Rebalancing in Bike-Sharing System (Jiaqi Liang et al., 2024)</a></li><li><a href=#108285-deconstructing-the-goldilocks-zone-of-neural-network-initialization-artem-vysogorets-et-al-2024>(108/285) Deconstructing the Goldilocks Zone of Neural Network Initialization (Artem Vysogorets et al., 2024)</a></li><li><a href=#109285-revisiting-the-dataset-bias-problem-from-a-statistical-perspective-kien-do-et-al-2024>(109/285) Revisiting the Dataset Bias Problem from a Statistical Perspective (Kien Do et al., 2024)</a></li><li><a href=#110285-regulation-games-for-trustworthy-machine-learning-mohammad-yaghini-et-al-2024>(110/285) Regulation Games for Trustworthy Machine Learning (Mohammad Yaghini et al., 2024)</a></li><li><a href=#111285-deep-reinforcement-learning-for-picker-routing-problem-in-warehousing-george-dunn-et-al-2024>(111/285) Deep Reinforcement Learning for Picker Routing Problem in Warehousing (George Dunn et al., 2024)</a></li><li><a href=#112285-how-does-unlabeled-data-provably-help-out-of-distribution-detection-xuefeng-du-et-al-2024>(112/285) How Does Unlabeled Data Provably Help Out-of-Distribution Detection? (Xuefeng Du et al., 2024)</a></li><li><a href=#113285-fair-active-ranking-from-pairwise-preferences-sruthi-gorantla-et-al-2024>(113/285) Fair Active Ranking from Pairwise Preferences (Sruthi Gorantla et al., 2024)</a></li><li><a href=#114285-the-last-dance--robust-backdoor-attack-via-diffusion-models-and-bayesian-approach-orson-mengara-2024>(114/285) The last Dance : Robust backdoor attack via diffusion models and bayesian approach (Orson Mengara, 2024)</a></li><li><a href=#115285-optimal-and-near-optimal-adaptive-vector-quantization-ran-ben-basat-et-al-2024>(115/285) Optimal and Near-Optimal Adaptive Vector Quantization (Ran Ben-Basat et al., 2024)</a></li><li><a href=#116285-a-multi-step-loss-function-for-robust-learning-of-the-dynamics-in-model-based-reinforcement-learning-abdelhakim-benechehab-et-al-2024>(116/285) A Multi-step Loss Function for Robust Learning of the Dynamics in Model-based Reinforcement Learning (Abdelhakim Benechehab et al., 2024)</a></li><li><a href=#117285-boosting-long-delayed-reinforcement-learning-with-auxiliary-short-delayed-task-qingyuan-wu-et-al-2024>(117/285) Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task (Qingyuan Wu et al., 2024)</a></li><li><a href=#118285-just-cluster-it-an-approach-for-exploration-in-high-dimensions-using-clustering-and-pre-trained-representations-stefan-sylvius-wagner-et-al-2024>(118/285) Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations (Stefan Sylvius Wagner et al., 2024)</a></li><li><a href=#119285-non-stationary-latent-auto-regressive-bandits-anna-l-trella-et-al-2024>(119/285) Non-Stationary Latent Auto-Regressive Bandits (Anna L. Trella et al., 2024)</a></li><li><a href=#120285-probabilistic-actor-critic-learning-to-explore-with-pac-bayes-uncertainty-bahareh-tasdighi-et-al-2024>(120/285) Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty (Bahareh Tasdighi et al., 2024)</a></li><li><a href=#121285-open-rl-benchmark-comprehensive-tracked-experiments-for-reinforcement-learning-shengyi-huang-et-al-2024>(121/285) Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning (Shengyi Huang et al., 2024)</a></li><li><a href=#122285-whom-to-trust-elective-learning-for-distributed-gaussian-process-regression-zewen-yang-et-al-2024>(122/285) Whom to Trust? Elective Learning for Distributed Gaussian Process Regression (Zewen Yang et al., 2024)</a></li><li><a href=#123285-on-the-impact-of-output-perturbation-on-fairness-in-binary-linear-classification-vitalii-emelianov-et-al-2024>(123/285) On the Impact of Output Perturbation on Fairness in Binary Linear Classification (Vitalii Emelianov et al., 2024)</a></li><li><a href=#124285-decoding-time-realignment-of-language-models-tianlin-liu-et-al-2024>(124/285) Decoding-time Realignment of Language Models (Tianlin Liu et al., 2024)</a></li><li><a href=#125285-kernel-pca-for-out-of-distribution-detection-kun-fang-et-al-2024>(125/285) Kernel PCA for Out-of-Distribution Detection (Kun Fang et al., 2024)</a></li><li><a href=#126285-frugal-actor-critic-sample-efficient-off-policy-deep-reinforcement-learning-using-unique-experiences-nikhil-kumar-singh-et-al-2024>(126/285) Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning Using Unique Experiences (Nikhil Kumar Singh et al., 2024)</a></li><li><a href=#127285-evading-data-contamination-detection-for-language-models-is-too-easy-jasper-dekoninck-et-al-2024>(127/285) Evading Data Contamination Detection for Language Models is (too) Easy (Jasper Dekoninck et al., 2024)</a></li><li><a href=#128285-learning-from-teaching-regularization-generalizable-correlations-should-be-easy-to-imitate-can-jin-et-al-2024>(128/285) Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate (Can Jin et al., 2024)</a></li><li><a href=#129285-exgc-bridging-efficiency-and-explainability-in-graph-condensation-junfeng-fang-et-al-2024>(129/285) EXGC: Bridging Efficiency and Explainability in Graph Condensation (Junfeng Fang et al., 2024)</a></li><li><a href=#130285-standard-gaussian-process-is-all-you-need-for-high-dimensional-bayesian-optimization-zhitong-xu-et-al-2024>(130/285) Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization (Zhitong Xu et al., 2024)</a></li><li><a href=#131285-representation-surgery-for-multi-task-model-merging-enneng-yang-et-al-2024>(131/285) Representation Surgery for Multi-Task Model Merging (Enneng Yang et al., 2024)</a></li><li><a href=#132285-understanding-what-affects-generalization-gap-in-visual-reinforcement-learning-theory-and-empirical-evidence-jiafei-lyu-et-al-2024>(132/285) Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence (Jiafei Lyu et al., 2024)</a></li><li><a href=#133285-sample-complexity-characterization-for-linear-contextual-mdps-junze-deng-et-al-2024>(133/285) Sample Complexity Characterization for Linear Contextual MDPs (Junze Deng et al., 2024)</a></li><li><a href=#134285-causal-feature-selection-for-responsible-machine-learning-raha-moraffah-et-al-2024>(134/285) Causal Feature Selection for Responsible Machine Learning (Raha Moraffah et al., 2024)</a></li><li><a href=#135285-poisson-process-for-bayesian-optimization-xiaoxing-wang-et-al-2024>(135/285) Poisson Process for Bayesian Optimization (Xiaoxing Wang et al., 2024)</a></li><li><a href=#136285-counterfactual-explanations-of-black-box-machine-learning-models-using-causal-discovery-with-applications-to-credit-rating-daisuke-takahashi-et-al-2024>(136/285) Counterfactual Explanations of Black-box Machine Learning Models using Causal Discovery with Applications to Credit Rating (Daisuke Takahashi et al., 2024)</a></li><li><a href=#137285-verifiable-evaluations-of-machine-learning-models-using-zksnarks-tobin-south-et-al-2024>(137/285) Verifiable evaluations of machine learning models using zkSNARKs (Tobin South et al., 2024)</a></li><li><a href=#138285-utility-based-reinforcement-learning-unifying-single-objective-and-multi-objective-reinforcement-learning-peter-vamplew-et-al-2024>(138/285) Utility-Based Reinforcement Learning: Unifying Single-objective and Multi-objective Reinforcement Learning (Peter Vamplew et al., 2024)</a></li><li><a href=#139285-boosting-voting-classifiers-and-randomized-sample-compression-schemes-arthur-da-cunha-et-al-2024>(139/285) Boosting, Voting Classifiers and Randomized Sample Compression Schemes (Arthur da Cunha et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#140285-enhancing-the-stability-of-llm-based-speech-generation-systems-through-self-supervised-representations-álvaro-martín-cortinas-et-al-2024>(140/285) Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations (Álvaro Martín-Cortinas et al., 2024)</a></li><li><a href=#141285-description-on-ieee-icme-2024-grand-challenge-semi-supervised-acoustic-scene-classification-under-domain-shift-jisheng-bai-et-al-2024>(141/285) Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic Scene Classification under Domain Shift (Jisheng Bai et al., 2024)</a></li></ul></li><li><a href=#csai-11>cs.AI (11)</a><ul><li><a href=#142285-graph-enhanced-large-language-models-in-asynchronous-plan-reasoning-fangru-lin-et-al-2024>(142/285) Graph-enhanced Large Language Models in Asynchronous Plan Reasoning (Fangru Lin et al., 2024)</a></li><li><a href=#143285-c-rag-certified-generation-risks-for-retrieval-augmented-language-models-mintong-kang-et-al-2024>(143/285) C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models (Mintong Kang et al., 2024)</a></li><li><a href=#144285-deal-decoding-time-alignment-for-large-language-models-james-y-huang-et-al-2024>(144/285) DeAL: Decoding-time Alignment for Large Language Models (James Y. Huang et al., 2024)</a></li><li><a href=#145285-neural-networks-for-abstraction-and-reasoning-towards-broad-generalization-in-machines-mikel-bober-irizar-et-al-2024>(145/285) Neural networks for abstraction and reasoning: Towards broad generalization in machines (Mikel Bober-Irizar et al., 2024)</a></li><li><a href=#146285-beyond-text-improving-llms-decision-making-for-robot-navigation-via-vocal-cues-xingpeng-sun-et-al-2024>(146/285) Beyond Text: Improving LLM&rsquo;s Decision Making for Robot Navigation via Vocal Cues (Xingpeng Sun et al., 2024)</a></li><li><a href=#147285-toward-human-ai-alignment-in-large-scale-multi-player-games-sugandha-sharma-et-al-2024>(147/285) Toward Human-AI Alignment in Large-Scale Multi-Player Games (Sugandha Sharma et al., 2024)</a></li><li><a href=#148285-understanding-the-planning-of-llm-agents-a-survey-xu-huang-et-al-2024>(148/285) Understanding the planning of LLM agents: A survey (Xu Huang et al., 2024)</a></li><li><a href=#149285-multi-step-problem-solving-through-a-verifier-an-empirical-analysis-on-model-induced-process-supervision-zihan-wang-et-al-2024>(149/285) Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision (Zihan Wang et al., 2024)</a></li><li><a href=#150285-v-irl-grounding-virtual-intelligence-in-real-life-jihan-yang-et-al-2024>(150/285) V-IRL: Grounding Virtual Intelligence in Real Life (Jihan Yang et al., 2024)</a></li><li><a href=#151285-decidable-reasoning-about-time-in-finite-domain-situation-calculus-theories-till-hofmann-et-al-2024>(151/285) Decidable Reasoning About Time in Finite-Domain Situation Calculus Theories (Till Hofmann et al., 2024)</a></li><li><a href=#152285-mastering-zero-shot-interactions-in-cooperative-and-competitive-simultaneous-games-yannik-mahlau-et-al-2024>(152/285) Mastering Zero-Shot Interactions in Cooperative and Competitive Simultaneous Games (Yannik Mahlau et al., 2024)</a></li></ul></li><li><a href=#cscr-6>cs.CR (6)</a><ul><li><a href=#153285-detecting-scams-using-large-language-models-liming-jiang-2024>(153/285) Detecting Scams Using Large Language Models (Liming Jiang, 2024)</a></li><li><a href=#154285-conversation-reconstruction-attack-against-gpt-models-junjie-chu-et-al-2024>(154/285) Conversation Reconstruction Attack Against GPT Models (Junjie Chu et al., 2024)</a></li><li><a href=#155285-unihenn-designing-more-versatile-homomorphic-encryption-based-cnns-without-im2col-hyunmin-choi-et-al-2024>(155/285) UniHENN: Designing More Versatile Homomorphic Encryption-based CNNs without im2col (Hyunmin Choi et al., 2024)</a></li><li><a href=#156285-unraveling-the-key-of-machine-learning-solutions-for-android-malware-detection-jiahao-liu-et-al-2024>(156/285) Unraveling the Key of Machine Learning Solutions for Android Malware Detection (Jiahao Liu et al., 2024)</a></li><li><a href=#157285-towards-eliminating-hard-label-constraints-in-gradient-inversion-attacks-yanbo-wang-et-al-2024>(157/285) Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks (Yanbo Wang et al., 2024)</a></li><li><a href=#158285-disdet-exploring-detectability-of-backdoor-attack-on-diffusion-models-yang-sui-et-al-2024>(158/285) DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models (Yang Sui et al., 2024)</a></li></ul></li><li><a href=#statml-9>stat.ML (9)</a><ul><li><a href=#159285-minimum-description-length-and-generalization-guarantees-for-representation-learning-milad-sefidgaran-et-al-2024>(159/285) Minimum Description Length and Generalization Guarantees for Representation Learning (Milad Sefidgaran et al., 2024)</a></li><li><a href=#160285-towards-understanding-the-word-sensitivity-of-attention-layers-a-study-via-random-features-simone-bombari-et-al-2024>(160/285) Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features (Simone Bombari et al., 2024)</a></li><li><a href=#161285-graph-neural-machine-a-new-model-for-learning-with-tabular-data-giannis-nikolentzos-et-al-2024>(161/285) Graph Neural Machine: A New Model for Learning with Tabular Data (Giannis Nikolentzos et al., 2024)</a></li><li><a href=#162285-non-asymptotic-analysis-of-biased-adaptive-stochastic-approximation-sobihan-surendran-et-al-2024>(162/285) Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation (Sobihan Surendran et al., 2024)</a></li><li><a href=#163285-challenges-in-variable-importance-ranking-under-correlation-annie-liang-et-al-2024>(163/285) Challenges in Variable Importance Ranking Under Correlation (Annie Liang et al., 2024)</a></li><li><a href=#164285-diffusive-gibbs-sampling-wenlin-chen-et-al-2024>(164/285) Diffusive Gibbs Sampling (Wenlin Chen et al., 2024)</a></li><li><a href=#165285-attention-meets-post-hoc-interpretability-a-mathematical-perspective-gianluigi-lopardo-et-al-2024>(165/285) Attention Meets Post-hoc Interpretability: A Mathematical Perspective (Gianluigi Lopardo et al., 2024)</a></li><li><a href=#166285-on-least-squares-estimation-in-softmax-gating-mixture-of-experts-huy-nguyen-et-al-2024>(166/285) On Least Squares Estimation in Softmax Gating Mixture of Experts (Huy Nguyen et al., 2024)</a></li><li><a href=#167285-bayes-optimal-fair-classification-with-linear-disparity-constraints-via-pre--in--and-post-processing-xianli-zeng-et-al-2024>(167/285) Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing (Xianli Zeng et al., 2024)</a></li></ul></li><li><a href=#cscv-41>cs.CV (41)</a><ul><li><a href=#168285-cross-domain-few-shot-object-detection-via-enhanced-open-set-object-detector-yuqian-fu-et-al-2024>(168/285) Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector (Yuqian Fu et al., 2024)</a></li><li><a href=#169285-unsupervised-semantic-segmentation-of-high-resolution-uav-imagery-for-road-scene-parsing-zihan-ma-et-al-2024>(169/285) Unsupervised semantic segmentation of high-resolution UAV imagery for road scene parsing (Zihan Ma et al., 2024)</a></li><li><a href=#170285-exploring-the-synergies-of-hybrid-cnns-and-vits-architectures-for-computer-vision-a-survey-haruna-yunusa-et-al-2024>(170/285) Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey (Haruna Yunusa et al., 2024)</a></li><li><a href=#171285-video-lavit-unified-video-language-pre-training-with-decoupled-visual-motional-tokenization-yang-jin-et-al-2024>(171/285) Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization (Yang Jin et al., 2024)</a></li><li><a href=#172285-nnmamba-3d-biomedical-image-segmentation-classification-and-landmark-detection-with-state-space-model-haifan-gong-et-al-2024>(172/285) nnMamba: 3D Biomedical Image Segmentation, Classification and Landmark Detection with State Space Model (Haifan Gong et al., 2024)</a></li><li><a href=#173285-physics-encoded-graph-neural-networks-for-deformation-prediction-under-contact-mahdi-saleh-et-al-2024>(173/285) Physics-Encoded Graph Neural Networks for Deformation Prediction under Contact (Mahdi Saleh et al., 2024)</a></li><li><a href=#174285-hassod-hierarchical-adaptive-self-supervised-object-detection-shengcao-cao-et-al-2024>(174/285) HASSOD: Hierarchical Adaptive Self-Supervised Object Detection (Shengcao Cao et al., 2024)</a></li><li><a href=#175285-clip-can-understand-depth-dunam-kim-et-al-2024>(175/285) CLIP Can Understand Depth (Dunam Kim et al., 2024)</a></li><li><a href=#176285-froster-frozen-clip-is-a-strong-teacher-for-open-vocabulary-action-recognition-xiaohu-huang-et-al-2024>(176/285) FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition (Xiaohu Huang et al., 2024)</a></li><li><a href=#177285-adatreeformer-few-shot-domain-adaptation-for-tree-counting-from-a-single-high-resolution-image-hamed-amini-amirkolaee-et-al-2024>(177/285) AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image (Hamed Amini Amirkolaee et al., 2024)</a></li><li><a href=#178285-delving-into-multi-modal-multi-task-foundation-models-for-road-scene-understanding-from-learning-paradigm-perspectives-sheng-luo-et-al-2024>(178/285) Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives (Sheng Luo et al., 2024)</a></li><li><a href=#179285-vln-video-utilizing-driving-videos-for-outdoor-vision-and-language-navigation-jialu-li-et-al-2024>(179/285) VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation (Jialu Li et al., 2024)</a></li><li><a href=#180285-constrained-multiview-representation-for-self-supervised-contrastive-learning-siyuan-dai-et-al-2024>(180/285) Constrained Multiview Representation for Self-supervised Contrastive Learning (Siyuan Dai et al., 2024)</a></li><li><a href=#181285-training-free-consistent-text-to-image-generation-yoad-tewel-et-al-2024>(181/285) Training-Free Consistent Text-to-Image Generation (Yoad Tewel et al., 2024)</a></li><li><a href=#182285-organic-or-diffused-can-we-distinguish-human-art-from-ai-generated-images-anna-yoo-jeong-ha-et-al-2024>(182/285) Organic or Diffused: Can We Distinguish Human Art from AI-generated Images? (Anna Yoo Jeong Ha et al., 2024)</a></li><li><a href=#183285-good-teachers-explain-explanation-enhanced-knowledge-distillation-amin-parchami-araghi-et-al-2024>(183/285) Good Teachers Explain: Explanation-Enhanced Knowledge Distillation (Amin Parchami-Araghi et al., 2024)</a></li><li><a href=#184285-time--memory--and-parameter-efficient-visual-adaptation-otniel-bogdan-mercea-et-al-2024>(184/285) Time-, Memory- and Parameter-Efficient Visual Adaptation (Otniel-Bogdan Mercea et al., 2024)</a></li><li><a href=#185285-enhancing-compositional-generalization-via-compositional-feature-alignment-haoxiang-wang-et-al-2024>(185/285) Enhancing Compositional Generalization via Compositional Feature Alignment (Haoxiang Wang et al., 2024)</a></li><li><a href=#186285-joint-attention-guided-feature-fusion-network-for-saliency-detection-of-surface-defects-xiaoheng-jiang-et-al-2024>(186/285) Joint Attention-Guided Feature Fusion Network for Saliency Detection of Surface Defects (Xiaoheng Jiang et al., 2024)</a></li><li><a href=#187285-image-caption-encoding-for-improving-zero-shot-generalization-eric-yang-yu-et-al-2024>(187/285) Image-Caption Encoding for Improving Zero-Shot Generalization (Eric Yang Yu et al., 2024)</a></li><li><a href=#188285-aoneus-a-neural-rendering-framework-for-acoustic-optical-sensor-fusion-mohamad-qadri-et-al-2024>(188/285) AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion (Mohamad Qadri et al., 2024)</a></li><li><a href=#189285-activeanno3d----an-active-learning-framework-for-multi-modal-3d-object-detection-ahmed-ghita-et-al-2024>(189/285) ActiveAnno3D &ndash; An Active Learning Framework for Multi-Modal 3D Object Detection (Ahmed Ghita et al., 2024)</a></li><li><a href=#190285-iguane-a-3d-generalizable-cyclegan-for-multicenter-harmonization-of-brain-mr-images-vincent-roca-et-al-2024>(190/285) IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images (Vincent Roca et al., 2024)</a></li><li><a href=#191285-one-shot-neural-face-reenactment-via-finding-directions-in-gans-latent-space-stella-bounareli-et-al-2024>(191/285) One-shot Neural Face Reenactment via Finding Directions in GAN&rsquo;s Latent Space (Stella Bounareli et al., 2024)</a></li><li><a href=#192285-a-computer-vision-based-approach-for-stalking-detection-using-a-cnn-lstm-mlp-hybrid-fusion-model-murad-hasan-et-al-2024>(192/285) A Computer Vision Based Approach for Stalking Detection Using a CNN-LSTM-MLP Hybrid Fusion Model (Murad Hasan et al., 2024)</a></li><li><a href=#193285-transcending-adversarial-perturbations-manifold-aided-adversarial-examples-with-legitimate-semantics-shuai-li-et-al-2024>(193/285) Transcending Adversarial Perturbations: Manifold-Aided Adversarial Examples with Legitimate Semantics (Shuai Li et al., 2024)</a></li><li><a href=#194285-taylor-videos-for-action-recognition-lei-wang-et-al-2024>(194/285) Taylor Videos for Action Recognition (Lei Wang et al., 2024)</a></li><li><a href=#195285-retrieval-augmented-score-distillation-for-text-to-3d-generation-junyoung-seo-et-al-2024>(195/285) Retrieval-Augmented Score Distillation for Text-to-3D Generation (Junyoung Seo et al., 2024)</a></li><li><a href=#196285-toonaging-face-re-aging-upon-artistic-portrait-style-transfer-bumsoo-kim-et-al-2024>(196/285) ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer (Bumsoo Kim et al., 2024)</a></li><li><a href=#197285-interactivevideo-user-centric-controllable-video-generation-with-synergistic-multimodal-instructions-yiyuan-zhang-et-al-2024>(197/285) InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions (Yiyuan Zhang et al., 2024)</a></li><li><a href=#198285-decoder-only-image-registration-xi-jia-et-al-2024>(198/285) Decoder-Only Image Registration (Xi Jia et al., 2024)</a></li><li><a href=#199285-an-inpainting-infused-pipeline-for-attire-and-background-replacement-felipe-rodrigues-perche-mahlow-et-al-2024>(199/285) An Inpainting-Infused Pipeline for Attire and Background Replacement (Felipe Rodrigues Perche-Mahlow et al., 2024)</a></li><li><a href=#200285-test-time-adaptation-for-depth-completion-hyoungseob-park-et-al-2024>(200/285) Test-Time Adaptation for Depth Completion (Hyoungseob Park et al., 2024)</a></li><li><a href=#201285-instancediffusion-instance-level-control-for-image-generation-xudong-wang-et-al-2024>(201/285) InstanceDiffusion: Instance-level Control for Image Generation (Xudong Wang et al., 2024)</a></li><li><a href=#202285-direct-a-video-customized-video-generation-with-user-directed-camera-movement-and-object-motion-shiyuan-yang-et-al-2024>(202/285) Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion (Shiyuan Yang et al., 2024)</a></li><li><a href=#203285-visual-text-meets-low-level-vision-a-comprehensive-survey-on-visual-text-processing-yan-shu-et-al-2024>(203/285) Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing (Yan Shu et al., 2024)</a></li><li><a href=#204285-pfdm-parser-free-virtual-try-on-via-diffusion-model-yunfang-niu-et-al-2024>(204/285) PFDM: Parser-Free Virtual Try-on via Diffusion Model (Yunfang Niu et al., 2024)</a></li><li><a href=#205285-houghtoradon-transform-new-neural-network-layer-for-features-improvement-in-projection-space-alexandra-zhabitskaya-et-al-2024>(205/285) HoughToRadon Transform: New Neural Network Layer for Features Improvement in Projection Space (Alexandra Zhabitskaya et al., 2024)</a></li><li><a href=#206285-pixel-wise-color-constancy-via-smoothness-techniques-in-multi-illuminant-scenes-umut-cem-entok-et-al-2024>(206/285) Pixel-Wise Color Constancy via Smoothness Techniques in Multi-Illuminant Scenes (Umut Cem Entok et al., 2024)</a></li><li><a href=#207285-improving-robustness-of-lidar-camera-fusion-model-against-weather-corruption-from-fusion-strategy-perspective-yihao-huang-et-al-2024>(207/285) Improving Robustness of LiDAR-Camera Fusion Model against Weather Corruption from Fusion Strategy Perspective (Yihao Huang et al., 2024)</a></li><li><a href=#208285-using-motion-cues-to-supervise-single-frame-body-pose-and-shape-estimation-in-low-data-regimes-andrey-davydov-et-al-2024>(208/285) Using Motion Cues to Supervise Single-Frame Body Pose and Shape Estimation in Low Data Regimes (Andrey Davydov et al., 2024)</a></li></ul></li><li><a href=#csit-9>cs.IT (9)</a><ul><li><a href=#209285-fast-and-accurate-cooperative-radio-map-estimation-enabled-by-gan-zezhong-zhang-et-al-2024>(209/285) Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN (Zezhong Zhang et al., 2024)</a></li><li><a href=#210285-multi-agent-reinforcement-learning-for-energy-saving-in-multi-cell-massive-mimo-systems-tianzhang-cai-et-al-2024>(210/285) Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems (Tianzhang Cai et al., 2024)</a></li><li><a href=#211285-rejection-sampled-universal-quantization-for-smaller-quantization-errors-chih-wei-ling-et-al-2024>(211/285) Rejection-Sampled Universal Quantization for Smaller Quantization Errors (Chih Wei Ling et al., 2024)</a></li><li><a href=#212285-on-the-performance-of-ris-aided-spatial-modulation-for-downlink-transmission-xusheng-zhu-et-al-2024>(212/285) On the Performance of RIS-Aided Spatial Modulation for Downlink Transmission (Xusheng Zhu et al., 2024)</a></li><li><a href=#213285-joint-beamforming-design-for-the-star-ris-enabled-isac-systems-with-multiple-targets-and-multiple-users-shuang-zhang-et-al-2024>(213/285) Joint Beamforming Design for the STAR-RIS-Enabled ISAC Systems with Multiple Targets and Multiple Users (Shuang Zhang et al., 2024)</a></li><li><a href=#214285-successive-bayesian-reconstructor-for-fas-channel-estimation-zijian-zhang-et-al-2024>(214/285) Successive Bayesian Reconstructor for FAS Channel Estimation (Zijian Zhang et al., 2024)</a></li><li><a href=#215285-explicit-formula-for-partial-information-decomposition-aobo-lyu-et-al-2024>(215/285) Explicit Formula for Partial Information Decomposition (Aobo Lyu et al., 2024)</a></li><li><a href=#216285-algorithms-for-computing-the-free-distance-of-convolutional-codes-zita-abreu-et-al-2024>(216/285) Algorithms for Computing the Free Distance of Convolutional Codes (Zita Abreu et al., 2024)</a></li><li><a href=#217285-code-based-single-server-private-information-retrieval-circumventing-the-sub-query-attack-neehar-verma-et-al-2024>(217/285) Code-Based Single-Server Private Information Retrieval: Circumventing the Sub-Query Attack (Neehar Verma et al., 2024)</a></li></ul></li><li><a href=#eessiv-10>eess.IV (10)</a><ul><li><a href=#218285-beyond-strong-labels-weakly-supervised-learning-based-on-gaussian-pseudo-labels-for-the-segmentation-of-ellipse-like-vascular-structures-in-non-contrast-cts-qixiang-ma-et-al-2024>(218/285) Beyond Strong labels: Weakly-supervised Learning Based on Gaussian Pseudo Labels for The Segmentation of Ellipse-like Vascular Structures in Non-contrast CTs (Qixiang Ma et al., 2024)</a></li><li><a href=#219285-swin-umamba-mamba-based-unet-with-imagenet-based-pretraining-jiarun-liu-et-al-2024>(219/285) Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining (Jiarun Liu et al., 2024)</a></li><li><a href=#220285-improving-pediatric-low-grade-neuroepithelial-tumors-molecular-subtype-identification-using-a-novel-auroc-loss-function-for-convolutional-neural-networks-khashayar-namdar-et-al-2024>(220/285) Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype Identification Using a Novel AUROC Loss Function for Convolutional Neural Networks (Khashayar Namdar et al., 2024)</a></li><li><a href=#221285-ct-based-anatomical-segmentation-for-thoracic-surgical-planning-a-benchmark-study-for-3d-u-shaped-deep-learning-models-arash-harirpoush-et-al-2024>(221/285) CT-based Anatomical Segmentation for Thoracic Surgical Planning: A Benchmark Study for 3D U-shaped Deep Learning Models (Arash Harirpoush et al., 2024)</a></li><li><a href=#222285-panoramic-image-inpainting-with-gated-convolution-and-contextual-reconstruction-loss-li-yu-et-al-2024>(222/285) Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss (Li Yu et al., 2024)</a></li><li><a href=#223285-inva-integrative-variational-autoencoder-for-harmonization-of-multi-modal-neuroimaging-data-bowen-lei-et-al-2024>(223/285) InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data (Bowen Lei et al., 2024)</a></li><li><a href=#224285-an-end-to-end-deep-learning-pipeline-to-derive-blood-input-with-partial-volume-corrections-for-automated-parametric-brain-pet-mapping-rugved-chavan-et-al-2024>(224/285) An end-to-end deep learning pipeline to derive blood input with partial volume corrections for automated parametric brain PET mapping (Rugved Chavan et al., 2024)</a></li><li><a href=#225285-rrwnet-recursive-refinement-network-for-effective-retinal-arteryvein-segmentation-and-classification-josé-morano-et-al-2024>(225/285) RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification (José Morano et al., 2024)</a></li><li><a href=#226285-ct-material-decomposition-using-spectral-diffusion-posterior-sampling-xiao-jiang-et-al-2024>(226/285) CT Material Decomposition using Spectral Diffusion Posterior Sampling (Xiao Jiang et al., 2024)</a></li><li><a href=#227285-deep-nonlinear-hyperspectral-unmixing-using-multi-task-learning-saeid-mehrdad-et-al-2024>(227/285) Deep Nonlinear Hyperspectral Unmixing Using Multi-task Learning (Saeid Mehrdad et al., 2024)</a></li></ul></li><li><a href=#cssi-3>cs.SI (3)</a><ul><li><a href=#228285-recommendation-fairness-in-social-networks-over-time-meng-cao-et-al-2024>(228/285) Recommendation Fairness in Social Networks Over Time (Meng Cao et al., 2024)</a></li><li><a href=#229285-mquine-a-cure-for-z-paradox-in-knowledge-graph-embedding-models-yang-liu-et-al-2024>(229/285) MQuinE: a cure for &lsquo;Z-paradox&rsquo; in knowledge graph embedding models (Yang Liu et al., 2024)</a></li><li><a href=#230285-security-advice-for-parents-and-children-about-content-filtering-and-circumvention-as-found-on-youtube-and-tiktok-ran-elgedawy-et-al-2024>(230/285) Security Advice for Parents and Children About Content Filtering and Circumvention as Found on YouTube and TikTok (Ran Elgedawy et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#231285-quantized-approximately-orthogonal-recurrent-neural-networks-armand-foucault-et-al-2024>(231/285) Quantized Approximately Orthogonal Recurrent Neural Networks (Armand Foucault et al., 2024)</a></li></ul></li><li><a href=#quant-ph-3>quant-ph (3)</a><ul><li><a href=#232285-curriculum-reinforcement-learning-for-quantum-architecture-search-under-hardware-errors-yash-j-patel-et-al-2024>(232/285) Curriculum reinforcement learning for quantum architecture search under hardware errors (Yash J. Patel et al., 2024)</a></li><li><a href=#233285-fast-classical-simulation-of-harvardquera-iqp-circuits-dmitri-maslov-et-al-2024>(233/285) Fast classical simulation of Harvard/QuEra IQP circuits (Dmitri Maslov et al., 2024)</a></li><li><a href=#234285-unleashing-the-expressive-power-of-pulse-based-quantum-neural-networks-han-xiao-tao-et-al-2024>(234/285) Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks (Han-Xiao Tao et al., 2024)</a></li></ul></li><li><a href=#cssd-6>cs.SD (6)</a><ul><li><a href=#235285-a-comprehensive-study-of-the-current-state-of-the-art-in-nepali-automatic-speech-recognition-systems-rupak-raj-ghimire-et-al-2024>(235/285) A Comprehensive Study of the Current State-of-the-Art in Nepali Automatic Speech Recognition Systems (Rupak Raj Ghimire et al., 2024)</a></li><li><a href=#236285-dual-knowledge-distillation-for-efficient-sound-event-detection-yang-xiao-et-al-2024>(236/285) Dual Knowledge Distillation for Efficient Sound Event Detection (Yang Xiao et al., 2024)</a></li><li><a href=#237285-exploring-federated-self-supervised-learning-for-general-purpose-audio-understanding-yasar-abbas-ur-rehman-et-al-2024>(237/285) Exploring Federated Self-Supervised Learning for General Purpose Audio Understanding (Yasar Abbas Ur Rehman et al., 2024)</a></li><li><a href=#238285-focal-modulation-networks-for-interpretable-sound-classification-luca-della-libera-et-al-2024>(238/285) Focal Modulation Networks for Interpretable Sound Classification (Luca Della Libera et al., 2024)</a></li><li><a href=#239285-adversarial-data-augmentation-for-robust-speaker-verification-zhenyu-zhou-et-al-2024>(239/285) Adversarial Data Augmentation for Robust Speaker Verification (Zhenyu Zhou et al., 2024)</a></li><li><a href=#240285-how-phonemes-contribute-to-deep-speaker-models-pengqi-li-et-al-2024>(240/285) How phonemes contribute to deep speaker models? (Pengqi Li et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#241285-cooperative-learning-with-gaussian-processes-for-euler-lagrange-systems-tracking-control-under-switching-topologies-zewen-yang-et-al-2024>(241/285) Cooperative Learning with Gaussian Processes for Euler-Lagrange Systems Tracking Control under Switching Topologies (Zewen Yang et al., 2024)</a></li><li><a href=#242285-llm-multi-agent-systems-challenges-and-open-problems-shanshan-han-et-al-2024>(242/285) LLM Multi-Agent Systems: Challenges and Open Problems (Shanshan Han et al., 2024)</a></li></ul></li><li><a href=#eesssy-4>eess.SY (4)</a><ul><li><a href=#243285-multi-agent-reinforcement-learning-for-offloading-cellular-communications-with-cooperating-uavs-abhishek-mondal-et-al-2024>(243/285) Multi-Agent Reinforcement Learning for Offloading Cellular Communications with Cooperating UAVs (Abhishek Mondal et al., 2024)</a></li><li><a href=#244285-autopilot-system-for-depth-and-pitch-control-in-underwater-vehicles-navigating-near-surface-waves-and-disturbances-vladimir-petrov-et-al-2024>(244/285) Autopilot System for Depth and Pitch Control in Underwater Vehicles: Navigating Near-Surface Waves and Disturbances (Vladimir Petrov et al., 2024)</a></li><li><a href=#245285-decentralized-event-triggered-online-learning-for-safe-consensus-of-multi-agent-systems-with-gaussian-process-regression-xiaobing-dai-et-al-2024>(245/285) Decentralized Event-Triggered Online Learning for Safe Consensus of Multi-Agent Systems with Gaussian Process Regression (Xiaobing Dai et al., 2024)</a></li><li><a href=#246285-alive-a-low-cost-interactive-vaccine-storage-environment-module-ensuring-easy-portability-and-remote-tracking-of-operational-logistics-to-the-last-mile-arkadeep-datta-et-al-2024>(246/285) ALIVE: A Low-Cost Interactive Vaccine Storage Environment Module ensuring easy portability and remote tracking of operational logistics to the last mile (Arkadeep Datta et al., 2024)</a></li></ul></li><li><a href=#csro-2>cs.RO (2)</a><ul><li><a href=#247285-replication-of-impedance-identification-experiments-on-a-reinforcement-learning-controlled-digital-twin-of-human-elbows-hao-yu-et-al-2024>(247/285) Replication of Impedance Identification Experiments on a Reinforcement-Learning-Controlled Digital Twin of Human Elbows (Hao Yu et al., 2024)</a></li><li><a href=#248285-dexdiffuser-generating-dexterous-grasps-with-diffusion-models-zehang-weng-et-al-2024>(248/285) DexDiffuser: Generating Dexterous Grasps with Diffusion Models (Zehang Weng et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#249285-darts-diffusion-approximated-residual-time-sampling-for-low-variance-time-of-flight-rendering-in-homogeneous-scattering-medium-qianyue-he-et-al-2024>(249/285) DARTS: Diffusion Approximated Residual Time Sampling for Low Variance Time-of-flight Rendering in Homogeneous Scattering Medium (Qianyue He et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#250285-minlp-based-hybrid-strategy-for-operating-mode-selection-of-tes-backed-up-refrigeration-systems-g-bejarano-et-al-2024>(250/285) MINLP-based hybrid strategy for operating mode selection of TES-backed-up refrigeration systems (G. Bejarano et al., 2024)</a></li><li><a href=#251285-dual-lagrangian-learning-for-conic-optimization-mathieu-tanneau-et-al-2024>(251/285) Dual Lagrangian Learning for Conic Optimization (Mathieu Tanneau et al., 2024)</a></li></ul></li><li><a href=#csce-3>cs.CE (3)</a><ul><li><a href=#252285-dynamic-flux-surrogate-based-partitioned-methods-for-interface-problems-pavel-bochev-et-al-2024>(252/285) Dynamic flux surrogate-based partitioned methods for interface problems (Pavel Bochev et al., 2024)</a></li><li><a href=#253285-learning-solutions-of-parametric-navier-stokes-with-physics-informed-neural-networks-m-naderibeni-et-al-2024>(253/285) Learning solutions of parametric Navier-Stokes with physics-informed neural networks (M. Naderibeni et al., 2024)</a></li><li><a href=#254285-a-comprehensive-numerical-approach-to-coil-placement-in-cerebral-aneurysms-mathematical-modeling-and-in-silico-occlusion-classification-fabian-holzberger-et-al-2024>(254/285) A Comprehensive Numerical Approach to Coil Placement in Cerebral Aneurysms: Mathematical Modeling and In Silico Occlusion Classification (Fabian Holzberger et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#255285-video-super-resolution-for-optimized-bitrate-and-green-online-streaming-vignesh-v-menon-et-al-2024>(255/285) Video Super-Resolution for Optimized Bitrate and Green Online Streaming (Vignesh V Menon et al., 2024)</a></li></ul></li><li><a href=#astro-phsr-1>astro-ph.SR (1)</a><ul><li><a href=#256285-active-region-based-flare-forecasting-with-sliding-window-multivariate-time-series-forest-classifiers-anli-ji-et-al-2024>(256/285) Active Region-based Flare Forecasting with Sliding Window Multivariate Time Series Forest Classifiers (Anli Ji et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#257285-knowledge-acquisition-and-integration-with-expert-in-the-loop-sajjadur-rahman-et-al-2024>(257/285) Knowledge Acquisition and Integration with Expert-in-the-loop (Sajjadur Rahman et al., 2024)</a></li><li><a href=#258285-matcha-an-ide-plugin-for-creating-accurate-privacy-nutrition-labels-tianshi-li-et-al-2024>(258/285) Matcha: An IDE Plugin for Creating Accurate Privacy Nutrition Labels (Tianshi Li et al., 2024)</a></li><li><a href=#259285-meeting-bridges-designing-information-artifacts-that-bridge-from-synchronous-meetings-to-asynchronous-collaboration-ruotong-wang-et-al-2024>(259/285) Meeting Bridges: Designing Information Artifacts that Bridge from Synchronous Meetings to Asynchronous Collaboration (Ruotong Wang et al., 2024)</a></li><li><a href=#260285-teach-me-how-to-improvise-co-designing-an-augmented-piano-training-system-for-improvisation-jordan-aiko-deja-et-al-2024>(260/285) Teach Me How to ImproVISe: Co-Designing an Augmented Piano Training System for Improvisation (Jordan Aiko Deja et al., 2024)</a></li></ul></li><li><a href=#csse-4>cs.SE (4)</a><ul><li><a href=#261285-user-centric-evaluation-of-chatgpt-capability-of-generating-r-program-code-tanha-miah-et-al-2024>(261/285) User-Centric Evaluation of ChatGPT Capability of Generating R Program Code (Tanha Miah et al., 2024)</a></li><li><a href=#262285-a-survey-on-effective-invocation-methods-of-massive-llm-services-can-wang-et-al-2024>(262/285) A Survey on Effective Invocation Methods of Massive LLM Services (Can Wang et al., 2024)</a></li><li><a href=#263285-predicting-configuration-performance-in-multiple-environments-with-sequential-meta-learning-jingzhi-gong-et-al-2024>(263/285) Predicting Configuration Performance in Multiple Environments with Sequential Meta-learning (Jingzhi Gong et al., 2024)</a></li><li><a href=#264285-how-do-software-practitioners-perceive-human-centric-defects-vedant-chauhan-et-al-2024>(264/285) How do software practitioners perceive human-centric defects? (Vedant Chauhan et al., 2024)</a></li></ul></li><li><a href=#mathna-3>math.NA (3)</a><ul><li><a href=#265285-variational-discretizations-of-ideal-magnetohydrodynamics-in-smooth-regime-using-finite-element-exterior-calculus-valentin-carlier-et-al-2024>(265/285) Variational discretizations of ideal magnetohydrodynamics in smooth regime using finite element exterior calculus (Valentin Carlier et al., 2024)</a></li><li><a href=#266285-do-we-need-decay-preserving-error-estimate-for-solving-parabolic-equations-with-initial-singularity-jiwei-zhang-et-al-2024>(266/285) Do we need decay-preserving error estimate for solving parabolic equations with initial singularity? (Jiwei Zhang et al., 2024)</a></li><li><a href=#267285-uncertainty-quantification-of-phase-transition-problems-with-an-injection-boundary-zhenyi-zhang-et-al-2024>(267/285) Uncertainty Quantification of Phase Transition Problems with an Injection Boundary (Zhenyi Zhang et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#268285-intent-profiling-and-translation-through-emergent-communication-salwa-mostafa-et-al-2024>(268/285) Intent Profiling and Translation Through Emergent Communication (Salwa Mostafa et al., 2024)</a></li><li><a href=#269285-stitching-the-spectrum-semantic-spectrum-segmentation-with-wideband-signal-stitching-daniel-uvaydov-et-al-2024>(269/285) Stitching the Spectrum: Semantic Spectrum Segmentation with Wideband Signal Stitching (Daniel Uvaydov et al., 2024)</a></li></ul></li><li><a href=#physicschem-ph-1>physics.chem-ph (1)</a><ul><li><a href=#270285-gaussian-plane-wave-neural-operator-for-electron-density-estimation-seongsu-kim-et-al-2024>(270/285) Gaussian Plane-Wave Neural Operator for Electron Density Estimation (Seongsu Kim et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#271285-genetic-guided-gflownets-advancing-in-practical-molecular-optimization-benchmark-hyeonah-kim-et-al-2024>(271/285) Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark (Hyeonah Kim et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#272285-the-gigs-up-how-chatgpt-stacks-up-against-quora-on-gig-economy-insights-thomas-lancaster-2024>(272/285) The Gig&rsquo;s Up: How ChatGPT Stacks Up Against Quora on Gig Economy Insights (Thomas Lancaster, 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#273285-preliminary-report-on-mantis-shrimp-a-multi-survey-computer-vision-photometric-redshift-model-andrew-engel-et-al-2024>(273/285) Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision Photometric Redshift Model (Andrew Engel et al., 2024)</a></li></ul></li><li><a href=#cslo-2>cs.LO (2)</a><ul><li><a href=#274285-polynomial-lawvere-logic-giorgio-bacci-et-al-2024>(274/285) Polynomial Lawvere Logic (Giorgio Bacci et al., 2024)</a></li><li><a href=#275285-proof-theory-and-decision-procedures-for-deontic-stit-logics-tim-s-lyon-et-al-2024>(275/285) Proof Theory and Decision Procedures for Deontic STIT Logics (Tim S. Lyon et al., 2024)</a></li></ul></li><li><a href=#mathat-1>math.AT (1)</a><ul><li><a href=#276285-symmetric-monoidal-smash-products-in-homotopy-type-theory-axel-ljungström-2024>(276/285) Symmetric Monoidal Smash Products in Homotopy Type Theory (Axel Ljungström, 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#277285-algorithms-and-complexity-of-difference-logic-konrad-k-dabrowski-et-al-2024>(277/285) Algorithms and Complexity of Difference Logic (Konrad K. Dabrowski et al., 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#278285-heana-a-hybrid-time-amplitude-analog-optical-accelerator-with-flexible-dataflows-for-energy-efficient-cnn-inference-sairam-sri-vatsavai-et-al-2024>(278/285) HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference (Sairam Sri Vatsavai et al., 2024)</a></li><li><a href=#279285-a-comparative-analysis-of-microrings-based-incoherent-photonic-gemm-accelerators-sairam-sri-vatsavai-et-al-2024>(279/285) A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators (Sairam Sri Vatsavai et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#280285-scoped-effects-as-parameterized-algebraic-theories-sam-lindley-et-al-2024>(280/285) Scoped Effects as Parameterized Algebraic Theories (Sam Lindley et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#281285-design-and-implementation-of-an-automated-disaster-recovery-system-for-a-kubernetes-cluster-using-lstm-ji-beom-kim-et-al-2024>(281/285) Design and Implementation of an Automated Disaster-recovery System for a Kubernetes Cluster Using LSTM (Ji-Beom Kim et al., 2024)</a></li><li><a href=#282285-practical-rateless-set-reconciliation-lei-yang-et-al-2024>(282/285) Practical Rateless Set Reconciliation (Lei Yang et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#283285-mining-a-minimal-set-of-behavioral-patterns-using-incremental-evaluation-mehdi-acheli-et-al-2024>(283/285) Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation (Mehdi Acheli et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#284285-multi-region-markovian-gaussian-process-an-efficient-method-to-discover-directional-communications-across-multiple-brain-regions-weihan-li-et-al-2024>(284/285) Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions (Weihan Li et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#285285-estimation-of-conditional-average-treatment-effects-on-distributed-data-a-privacy-preserving-approach-yuji-kawamata-et-al-2024>(285/285) Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach (Yuji Kawamata et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>