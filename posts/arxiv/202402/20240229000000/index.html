<!doctype html><html><head><title>arXiv @ 2024.02.29</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.29"><meta property="og:description" content="Primary Categories astro-ph.CO (1) astro-ph.IM (1) cs.AI (11) cs.AR (1) cs.CE (1) cs.CG (1) cs.CL (68) cs.CR (8) cs.CV (80) cs.CY (2) cs.DB (2) cs.DC (3) cs.DS (3) cs.GR (1) cs.GT (4) cs.HC (5) cs.IR (8) cs.IT (1) cs.LG (63) cs.LO (2) cs.MA (1) cs.NE (2) cs.NI (5) cs.PL (1) cs.RO (13) cs.SD (4) cs.SE (4) cs.SI (3) cs.SY (1) econ.TH (1) eess.IV (6) eess.SY (7) hep-ph (1) math.CO (1) math."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240229000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-29T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-29T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.29"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240229000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Thursday, Feb 29, 2024</p></div><div class=title><h1>arXiv @ 2024.02.29</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#astro-phco-1>astro-ph.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csai-11>cs.AI (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cscl-68>cs.CL (68)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cscr-8>cs.CR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cscv-80>cs.CV (80)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csdb-2>cs.DB (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csdc-3>cs.DC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csds-3>cs.DS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csgt-4>cs.GT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csir-8>cs.IR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csit-1>cs.IT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cslg-63>cs.LG (63)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cslo-2>cs.LO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csni-5>cs.NI (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csro-13>cs.RO (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cssd-4>cs.SD (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#csse-4>cs.SE (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cssi-3>cs.SI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#cssy-1>cs.SY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#econth-1>econ.TH (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#eessiv-6>eess.IV (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#eesssy-7>eess.SY (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#hep-ph-1>hep-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#mathna-5>math.NA (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#mathst-1>math.ST (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#nlinao-1>nlin.AO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#physicschem-ph-1>physics.chem-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#physicscomp-ph-2>physics.comp-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#physicsmed-ph-1>physics.med-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#physicsoptics-1>physics.optics (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#q-biogn-1>q-bio.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#q-bioqm-2>q-bio.QM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#quant-ph-3>quant-ph (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>AI-generated Text Detection</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Active Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td>2</td><td>3</td><td>1</td><td></td></tr><tr><td>Alpaca</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Autoencoder</td><td></td><td>1</td><td>1</td><td>4</td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><td>BART</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>BLOOM</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Benchmarking</td><td>5</td><td>20</td><td>23</td><td>13</td><td></td></tr><tr><td>Black Box</td><td></td><td>1</td><td>2</td><td>4</td><td></td></tr><tr><td>ChatGPT</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td></td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Clustering</td><td></td><td>3</td><td>1</td><td>1</td><td></td></tr><tr><td>Code Generation</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Continual Learning</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>2</td><td>2</td><td>1</td><td></td></tr><tr><td>Convolution</td><td></td><td>2</td><td>5</td><td>1</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>4</td><td>10</td><td>3</td><td></td></tr><tr><td>Counter-factual</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Counterfactual Reasoning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>12</td><td>3</td><td>1</td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Document Classification</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Document Embedding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Few-shot</td><td></td><td>4</td><td>3</td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>3</td><td>20</td><td>16</td><td>6</td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td>3</td><td>3</td><td></td></tr><tr><td>GLUE</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>GPT</td><td>3</td><td>13</td><td>1</td><td>3</td><td></td></tr><tr><td>GPT-2</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>GPT-3</td><td></td><td>6</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>5</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>1</td><td>7</td><td></td><td>1</td><td></td></tr><tr><td>GPT-4 turbo</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Generative AI</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>6</td><td></td><td></td></tr><tr><td>Grammatical Error Correction</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Graph</td><td></td><td>4</td><td>2</td><td>9</td><td>2</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td></td><td>8</td><td></td></tr><tr><td>Grounding</td><td>1</td><td>2</td><td>1</td><td>1</td><td></td></tr><tr><td>Human Intervention</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>In-context Learning</td><td></td><td>1</td><td>2</td><td>5</td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Intent Detection</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>1</td><td>10</td><td>3</td><td></td></tr><tr><td>Knowledge Graph</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>LLaMA</td><td></td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td></td><td>7</td><td></td></tr><tr><td>Language Generation</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>14</td><td>78</td><td>4</td><td>10</td><td>2</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Low-Resource</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Markov Game</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Metaphor Detection</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Model Quantization</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>4</td><td>11</td><td>12</td><td>9</td><td>1</td></tr><tr><td>Multiple Instance Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Mutual Information</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>N-gram</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Node Anomaly Detection</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Object Detection</td><td></td><td>1</td><td>3</td><td></td><td></td></tr><tr><td>Online Reinforcement Learning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Open-Domain Dialogue</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Open-Domain Question Answering</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>4</td><td>4</td><td>1</td></tr><tr><td>Out-of-domain</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Perplexity</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Prompt</td><td>3</td><td>11</td><td>7</td><td>3</td><td>1</td></tr><tr><td>Pruning</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Quantization</td><td></td><td>1</td><td>2</td><td>1</td><td>1</td></tr><tr><td>Question Answering</td><td></td><td>17</td><td>2</td><td></td><td></td></tr><tr><td>Reasoning</td><td>2</td><td>5</td><td>2</td><td>3</td><td></td></tr><tr><td>Recommendation</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Recommender System</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reconstruction Loss</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td></td><td>2</td><td>7</td><td></td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>3</td><td>2</td><td></td></tr><tr><td>Retrieval Augmentation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td></td><td>9</td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Scaling Law</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>3</td><td>6</td><td>3</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td>1</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Semi-Supervised Training</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td>2</td><td>1</td><td>5</td><td>4</td></tr><tr><td>Simulator</td><td>1</td><td>2</td><td>1</td><td>5</td><td>4</td></tr><tr><td>Sora</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Summarization</td><td></td><td>5</td><td></td><td></td><td>1</td></tr><tr><td>SuperGLUE</td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Supervised Learning</td><td></td><td>6</td><td>8</td><td>4</td><td></td></tr><tr><td>T5</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Segmentation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Transformer</td><td>1</td><td>6</td><td>9</td><td>13</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td>1</td><td>3</td><td>1</td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td>6</td><td>4</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>4</td><td>2</td><td></td></tr><tr><td>Word Embedding</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td></td><td>7</td><td>3</td><td>1</td><td>1</td></tr><tr><td>Zero-shot Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td>1</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-68>cs.CL (68)</h2><h3 id=168--1341-a-language-model-based-framework-for-new-concept-placement-in-ontologies-hang-dong-et-al-2024>(1/68 | 1/341) A Language Model based Framework for New Concept Placement in Ontologies (Hang Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Dong, Jiaoyan Chen, Yuan He, Yongsheng Gao, Ian Horrocks. (2024)<br><strong>A Language Model based Framework for New Concept Placement in Ontologies</strong><br><button class=copy-to-clipboard title="A Language Model based Framework for New Concept Placement in Ontologies" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; I-2-4, cs-CL, cs-IR, cs.CL<br>Keyword Score: 133<br>Keywords: Benchmarking, Contrastive Learning, Fine-tuning, Fine-tuning, Zero-shot, BERT, GPT, LLaMA, Instruction Tuning, Large Language Model, Large Language Model, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17897v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17897v2.pdf filename=2402.17897v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and <b>contrastive</b> <b>learning</b> with <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> such as <b>BERT</b> for edge search, and adapt a <b>BERT</b> <b>fine-tuning-based</b> multi-label Edge-Cross-encoder, and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> such as <b>GPT</b> series, FLAN-T5, and <b>Llama</b> 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking <b>benchmark.</b> The best settings in our framework use <b>fine-tuned</b> <b>PLM</b> for search and a multi-label Cross-encoder for selection. <b>Zero-shot</b> <b>prompting</b> of <b>LLMs</b> is still not adequate for the task, and we propose explainable <b>instruction</b> <b>tuning</b> of <b>LLMs</b> for improved performance. Our study shows the advantages of <b>PLMs</b> and highlights the encouraging performance of <b>LLMs</b> that motivates future studies.</p></p class="citation"></blockquote><h3 id=268--2341-reasoning-in-conversation-solving-subjective-tasks-through-dialogue-simulation-for-large-language-models-xiaolong-wang-et-al-2024>(2/68 | 2/341) Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models (Xiaolong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaolong Wang, Yile Wang, Yuanchi Zhang, Fuwen Luo, Peng Li, Maosong Sun, Yang Liu. (2024)<br><strong>Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models</strong><br><button class=copy-to-clipboard title="Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 120<br>Keywords: Simulation, Simulator, ChatGPT, GPT, GPT-4, Mathematical Reasoning, Metaphor Detection, Open-Domain Question Answering, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17226v1.pdf filename=2402.17226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have achieved remarkable performance in objective tasks such as <b>open-domain</b> <b>question</b> <b>answering</b> and <b>mathematical</b> <b>reasoning,</b> which can often be solved through recalling learned factual knowledge or chain-of-thought style <b>reasoning.</b> However, we find that the performance of <b>LLMs</b> in subjective tasks is still unsatisfactory, such as <b>metaphor</b> <b>recognition,</b> dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted <b>reasoning</b> pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of <b>LLMs,</b> we propose RiC <b>(Reasoning</b> in Conversation), a method that focuses on solving subjective tasks through dialogue <b>simulation.</b> The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source <b>LLMs</b> including <b>GPT-4,</b> <b>ChatGPT,</b> and OpenChat across twelve tasks. Experimental results show that RiC can yield significant improvement compared with various baselines.</p></p class="citation"></blockquote><h3 id=368--3341-beyond-the-known-investigating-llms-performance-on-out-of-domain-intent-detection-pei-wang-et-al-2024>(3/68 | 3/341) Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection (Pei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pei Wang, Keqing He, Yejie Wang, Xiaoshuai Song, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu. (2024)<br><strong>Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection</strong><br><button class=copy-to-clipboard title="Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Few-shot, Fine-tuning, Fine-tuning, Knowledge Transfer, Out-of-domain, Zero-shot, ChatGPT, Intent Detection, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17256v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17256v2.pdf filename=2402.17256v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-domain</b> (OOD) <b>intent</b> <b>detection</b> aims to examine whether the user&rsquo;s query falls outside the predefined domain of the system, which is crucial for the proper functioning of task-oriented dialogue (TOD) systems. Previous methods address it by <b>fine-tuning</b> discriminative models. Recently, some studies have been exploring the application of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> represented by <b>ChatGPT</b> to various downstream tasks, but it is still unclear for their ability on OOD detection task.This paper conducts a comprehensive evaluation of <b>LLMs</b> under various experimental settings, and then outline the strengths and weaknesses of <b>LLMs.</b> We find that <b>LLMs</b> exhibit strong <b>zero-shot</b> and <b>few-shot</b> capabilities, but is still at a disadvantage compared to models <b>fine-tuned</b> with full resource. More deeply, through a series of additional analysis experiments, we discuss and <b>summarize</b> the challenges faced by <b>LLMs</b> and provide guidance for future work including injecting domain <b>knowledge,</b> <b>strengthening</b> <b>knowledge</b> <b>transfer</b> from IND(In-domain) to OOD, and understanding long instructions.</p></p class="citation"></blockquote><h3 id=468--4341-deep-learning-detection-method-for-large-language-models-generated-scientific-content-bushra-alhijawi-et-al-2024>(4/68 | 4/341) Deep Learning Detection Method for Large Language Models-Generated Scientific Content (Bushra Alhijawi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bushra Alhijawi, Rawan Jarrar, Aseel AbuAlRub, Arwa Bader. (2024)<br><strong>Deep Learning Detection Method for Large Language Models-Generated Scientific Content</strong><br><button class=copy-to-clipboard title="Deep Learning Detection Method for Large Language Models-Generated Scientific Content" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 106<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Multi-modal, Multi-modal, BERT, ChatGPT, GPT, GPT-3, AI-generated Text Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00828v1.pdf filename=2403.00828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> such as <b>GPT-3</b> and <b>BERT,</b> reshape how textual content is written and communicated. These models have the potential to generate scientific content that is indistinguishable from that written by humans. Hence, <b>LLMs</b> carry severe consequences for the scientific community, which relies on the integrity and reliability of publications. This research paper presents a novel <b>ChatGPT-generated</b> scientific text detection method, AI-Catcher. AI-Catcher integrates two deep learning models, multilayer perceptron (MLP) and <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNN).</b> The MLP learns the feature representations of the linguistic and statistical features. The <b>CNN</b> extracts high-level representations of the sequential patterns from the textual content. AI-Catcher is a <b>multimodal</b> model that fuses hidden patterns derived from MLP and <b>CNN.</b> In addition, a new <b>ChatGPT-Generated</b> scientific text dataset is collected to enhance <b>AI-generated</b> <b>text</b> <b>detection</b> tools, AIGTxt. AIGTxt contains 3000 records collected from published academic articles across ten domains and divided into three classes: Human-written, <b>ChatGPT-generated,</b> and Mixed text. Several experiments are conducted to evaluate the performance of AI-Catcher. The comparative results demonstrate the capability of AI-Catcher to distinguish between human-written and <b>ChatGPT-generated</b> scientific text more accurately than alternative methods. On average, AI-Catcher improved accuracy by 37.4%.</p></p class="citation"></blockquote><h3 id=568--5341-evaluating-very-long-term-conversational-memory-of-llm-agents-adyasha-maharana-et-al-2024>(5/68 | 5/341) Evaluating Very Long-Term Conversational Memory of LLM Agents (Adyasha Maharana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang. (2024)<br><strong>Evaluating Very Long-Term Conversational Memory of LLM Agents</strong><br><button class=copy-to-clipboard title="Evaluating Very Long-Term Conversational Memory of LLM Agents" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 99<br>Keywords: Graph, Benchmarking, Multi-modal, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Grounding, Open-Domain Dialogue, Question Answering, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17753v1.pdf filename=2402.17753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing works on long-term <b>open-domain</b> <b>dialogues</b> focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging <b>LLM-based</b> agent architectures and <b>grounding</b> their dialogues on personas and temporal event <b>graphs.</b> Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and <b>grounding</b> to the event <b>graphs.</b> Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation <b>benchmark</b> to measure long-term memory in models, encompassing <b>question</b> <b>answering,</b> event <b>summarization,</b> and <b>multi-modal</b> dialogue generation tasks. Our experimental results indicate that <b>LLMs</b> exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context <b>LLMs</b> or <b>RAG</b> can offer improvements but these models still substantially lag behind human performance.</p></p class="citation"></blockquote><h3 id=668--6341-deep-learning-based-named-entity-recognition-models-for-recipes-mansi-goel-et-al-2024>(6/68 | 6/341) Deep Learning Based Named Entity Recognition Models for Recipes (Mansi Goel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mansi Goel, Ayush Agarwal, Shubham Agrawal, Janak Kapuriya, Akhil Vamshi Konam, Rishabh Gupta, Shrey Rastogi, Niharika, Ganesh Bagler. (2024)<br><strong>Deep Learning Based Named Entity Recognition Models for Recipes</strong><br><button class=copy-to-clipboard title="Deep Learning Based Named Entity Recognition Models for Recipes" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 93<br>Keywords: Clustering, Few-shot, Fine-tuning, Fine-tuning, Information Retrieval, Named Entity Recognition, Named Entity Recognition, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17447v1.pdf filename=2402.17447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing <b>named</b> <b>entities,</b> <b>the</b> building blocks of recipe text, are of immense value for various applications ranging from <b>information</b> <b>extraction</b> to novel recipe generation. <b>Named</b> <b>entity</b> <b>recognition</b> is a technique for extracting <b>information</b> <b>from</b> unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford <b>NER.</b> Based on the analysis, we sampled a subset of 88,526 phrases using a <b>clustering-based</b> approach while preserving the diversity to create the machine-annotated dataset. A thorough investigation of <b>NER</b> approaches on these three datasets involving statistical, <b>fine-tuning</b> of deep learning-based language models and <b>few-shot</b> <b>prompting</b> on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> provides deep insights. We conclude that <b>few-shot</b> <b>prompting</b> on <b>LLMs</b> has abysmal performance, whereas the <b>fine-tuned</b> spaCy-transformer emerges as the best model with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated, augmented, and machine-annotated datasets, respectively.</p></p class="citation"></blockquote><h3 id=768--7341-mathsensei-a-tool-augmented-large-language-model-for-mathematical-reasoning-debrup-das-et-al-2024>(7/68 | 7/341) MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning (Debrup Das et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Debrup Das, Debopriyo Banerjee, Somak Aditya, Ashish Kulkarni. (2024)<br><strong>MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning</strong><br><button class=copy-to-clipboard title="MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, Massive Multitask Language Understanding (MMLU), Mathematical Reasoning, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17231v1.pdf filename=2402.17231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tool-augmented <b>Large</b> <b>Language</b> <b>Models</b> (TALM) are known to enhance the skillset of <b>large</b> <b>language</b> <b>models</b> <b>(LLM),</b> thereby, leading to their improved <b>reasoning</b> abilities across many tasks. While, TALMs have been successfully employed in different <b>question-answering</b> <b>benchmarks,</b> their efficacy on complex <b>mathematical</b> <b>reasoning</b> <b>benchmarks,</b> and the potential complimentary benefits offered by tools for knowledge retrieval and <b>mathematical</b> <b>equation</b> solving, are open research <b>questions.</b> <b>In</b> this work, we present MATHSENSEI, a tool-augmented <b>large</b> <b>language</b> <b>model</b> for <b>mathematical</b> <b>reasoning.</b> Augmented with tools for knowledge retrieval (Bing Web Search), program execution (Python), and symbolic equation solving (Wolfram-Alpha), we study the complimentary benefits of these tools through evaluations on <b>mathematical</b> <b>reasoning</b> datasets. We perform exhaustive ablations on MATH,a popular dataset for evaluating <b>mathematical</b> <b>reasoning</b> on diverse <b>mathematical</b> <b>disciplines.</b> We also conduct experiments involving well-known tool planners to study the impact of tool sequencing on the model performance. MATHSENSEI achieves 13.5% better accuracy over <b>gpt-3.5-turbo</b> with chain-of-thought on the MATH dataset. We further observe that TALMs are not as effective for simpler math word problems (in GSM-8k), and the benefit increases as the complexity and required knowledge increases (progressively over AQuA, <b>MMLU-Math,</b> and higher level complex <b>questions</b> <b>in</b> MATH). The code and data are available at <a href=https://github.com/Debrup-61/MathSensei>https://github.com/Debrup-61/MathSensei</a>.</p></p class="citation"></blockquote><h3 id=868--8341-benchmarking-gpt-4-on-algorithmic-problems-a-systematic-evaluation-of-prompting-strategies-flavio-petruzzellis-et-al-2024>(8/68 | 8/341) Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies (Flavio Petruzzellis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti. (2024)<br><strong>Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies</strong><br><button class=copy-to-clipboard title="Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-NE, cs.CL<br>Keyword Score: 86<br>Keywords: Benchmarking, Benchmarking, GPT, GPT-3, GPT-3.5, GPT-4, Transformer, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17396v1.pdf filename=2402.17396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that <b>LLMs</b> lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic <b>benchmarking</b> of <b>GPT-4,</b> one of the most advanced <b>LLMs</b> available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of <b>GPT-4</b> with that of its predecessor <b>(GPT-3.5)</b> and with a variant of the <b>Transformer-Encoder</b> architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced <b>prompting</b> techniques allows <b>GPT-4</b> to reach superior accuracy on all tasks, demonstrating that state-of-the-art <b>LLMs</b> constitute a very strong baseline also in challenging tasks that require systematic generalization.</p></p class="citation"></blockquote><h3 id=968--9341-can-llm-generate-culturally-relevant-commonsense-qa-data-case-study-in-indonesian-and-sundanese-rifki-afina-putri-et-al-2024>(9/68 | 9/341) Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese (Rifki Afina Putri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rifki Afina Putri, Faiz Ghifari Haznitrama, Dea Adhista, Alice Oh. (2024)<br><strong>Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese</strong><br><button class=copy-to-clipboard title="Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Low-Resource, GPT, GPT-4, GPT-4 turbo, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17302v1.pdf filename=2402.17302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of <b>question</b> <b>answering</b> <b>(QA)</b> dataset that incorporates knowledge and cultural nuance embedded in a language, especially for <b>low-resource</b> languages. In this study, we investigate the effectiveness of using <b>LLMs</b> in generating culturally relevant commonsense <b>QA</b> datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both <b>LLMs</b> and human annotators. Our experiments show that the current best-performing <b>LLM,</b> <b>GPT-4</b> <b>Turbo,</b> is capable of generating <b>questions</b> <b>with</b> adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages. We also <b>benchmark</b> various <b>LLMs</b> on our generated datasets and find that they perform better on the <b>LLM-generated</b> datasets compared to those created by humans.</p></p class="citation"></blockquote><h3 id=1068--10341-self-refinement-of-language-models-from-external-proxy-metrics-feedback-keshav-ramji-et-al-2024>(10/68 | 10/341) Self-Refinement of Language Models from External Proxy Metrics Feedback (Keshav Ramji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keshav Ramji, Young-Suk Lee, Ramón Fernandez Astudillo, Md Arafat Sultan, Tahira Naseem, Asim Munawar, Radu Florian, Salim Roukos. (2024)<br><strong>Self-Refinement of Language Models from External Proxy Metrics Feedback</strong><br><button class=copy-to-clipboard title="Self-Refinement of Language Models from External Proxy Metrics Feedback" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Supervised Learning, Zero-shot, LLaMA, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00827v1.pdf filename=2403.00827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is often desirable for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to capture multiple objectives when providing a response. In document-grounded response generation, for example, agent responses are expected to be relevant to a user&rsquo;s query while also being grounded in a given document. In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an <b>LLM</b> to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response. ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan-T5-XXL and <b>Llama-2-13B-Chat,</b> to evaluate its performance on document-grounded <b>question</b> <b>answering</b> datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality. We further show that <b>fine-tuning</b> <b>Llama-2-13B-Chat</b> on the synthetic dialogue data generated by ProMiSe yields significant performance improvements over the <b>zero-shot</b> baseline as well as a <b>supervised</b> <b>fine-tuned</b> model on human annotated data.</p></p class="citation"></blockquote><h3 id=1168--11341-rear-a-relevance-aware-retrieval-augmented-framework-for-open-domain-question-answering-yuhao-wang-et-al-2024>(11/68 | 11/341) REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering (Yuhao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, Ji-Rong Wen. (2024)<br><strong>REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering</strong><br><button class=copy-to-clipboard title="REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 80<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Open-Domain Question Answering, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17497v1.pdf filename=2402.17497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Considering the limited internal parametric knowledge, <b>retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> has been widely used to extend the knowledge scope of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Despite the extensive efforts on <b>RAG</b> research, in existing methods, <b>LLMs</b> cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware <b>Retrieval-augmented</b> <b>approach</b> <b>for</b> <b>open-domain</b> <b>question</b> <b>answering</b> <b>(QA).</b> As the key motivation, we aim to enhance the self-awareness of source relevance for <b>LLMs,</b> so as to adaptively utilize external knowledge in <b>RAG</b> systems. Specially, we develop a new architecture for <b>LLM</b> based <b>RAG</b> system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four <b>open-domain</b> <b>QA</b> <b>tasks</b> show that REAR significantly outperforms previous a number of competitive <b>RAG</b> approaches. Our code and data can be accessed at <a href=https://github.com/RUCAIBox/REAR>https://github.com/RUCAIBox/REAR</a>.</p></p class="citation"></blockquote><h3 id=1268--12341-enhancing-eeg-to-text-decoding-through-transferable-representations-from-pre-trained-contrastive-eeg-text-masked-autoencoder-jiaqi-wang-et-al-2024>(12/68 | 12/341) Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder (Jiaqi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Wang, Zhenxi Song, Zhengyu Ma, Xipeng Qiu, Min Zhang, Zhiguo Zhang. (2024)<br><strong>Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder</strong><br><button class=copy-to-clipboard title="Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Autoencoder, Self-supervised Learning, Self-supervised Learning, BART, BLEU, Large Language Model, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17433v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17433v2.pdf filename=2402.17433v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked <b>Autoencoder</b> (CET-MAE), a novel model that orchestrates compound <b>self-supervised</b> <b>learning</b> across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modules alongside the EEG stream from CET-MAE and further enables an <b>LLM</b> (specifically <b>BART)</b> to decode text from EEG sequences. Comprehensive experiments conducted on the popular text-evoked EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms the state-of-the-art in <b>ROUGE-1</b> F1 and <b>BLEU-4</b> scores by 8.34% and 32.21%, respectively. These results indicate significant advancements in the field and underscores the proposed framework&rsquo;s potential to enable more powerful and widespread BCI applications.</p></p class="citation"></blockquote><h3 id=1368--13341-when-scaling-meets-llm-finetuning-the-effect-of-data-model-and-finetuning-method-biao-zhang-et-al-2024>(13/68 | 13/341) When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method (Biao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biao Zhang, Zhongtao Liu, Colin Cherry, Orhan Firat. (2024)<br><strong>When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method</strong><br><button class=copy-to-clipboard title="When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Neural Machine Translation, Large Language Model, Large Language Model, Prompt, Scaling Law, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17193v1.pdf filename=2402.17193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> often adopt <b>finetuning</b> to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the <b>scaling</b> <b>properties)</b> of different <b>finetuning</b> methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different <b>scaling</b> <b>factors,</b> including <b>LLM</b> model size, pretraining data size, new <b>finetuning</b> parameter size and <b>finetuning</b> data size, affect the <b>finetuning</b> performance. We consider two types of <b>finetuning</b> &ndash; full-model tuning (FMT) and parameter efficient tuning (PET, including <b>prompt</b> tuning and LoRA), and explore their <b>scaling</b> <b>behaviors</b> in the data-limited regime where the <b>LLM</b> model size substantially outweighs the <b>finetuning</b> data size. Based on two sets of pretrained bilingual <b>LLMs</b> from 1B to 16B and experiments on bilingual <b>machine</b> <b>translation</b> and multilingual <b>summarization</b> <b>benchmarks,</b> we find that 1) <b>LLM</b> <b>finetuning</b> follows a powerbased multiplicative joint <b>scaling</b> <b>law</b> between <b>finetuning</b> data size and each other <b>scaling</b> <b>factor;</b> 2) <b>LLM</b> <b>finetuning</b> benefits more from <b>LLM</b> model <b>scaling</b> <b>than</b> pretraining data <b>scaling,</b> <b>and</b> PET parameter <b>scaling</b> <b>is</b> generally ineffective; and 3) the optimal <b>finetuning</b> method is highly task- and <b>finetuning</b> data-dependent. We hope our findings could shed light on understanding, selecting and developing <b>LLM</b> <b>finetuning</b> methods.</p></p class="citation"></blockquote><h3 id=1468--14341-blendsql-a-scalable-dialect-for-unifying-hybrid-question-answering-in-relational-algebra-parker-glenn-et-al-2024>(14/68 | 14/341) BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra (Parker Glenn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parker Glenn, Parag Pravin Dakle, Liang Wang, Preethi Raghavan. (2024)<br><strong>BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra</strong><br><button class=copy-to-clipboard title="BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Zero-shot, Transformer, Question Answering, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17882v1.pdf filename=2402.17882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many existing end-to-end systems for hybrid <b>question</b> <b>answering</b> tasks can often be boiled down to a <b>&ldquo;prompt-and-pray&rdquo;</b> paradigm, where the user has limited control and insight into the intermediate <b>reasoning</b> steps used to achieve the final result. Additionally, due to the context size limitation of many <b>transformer-based</b> <b>LLMs,</b> it is often not reasonable to expect that the full structured and unstructured context will fit into a given <b>prompt</b> in a <b>zero-shot</b> setting, let alone a <b>few-shot</b> setting. We introduce BlendSQL, a superset of SQLite to act as a unified dialect for orchestrating <b>reasoning</b> across both unstructured and structured data. For hybrid <b>question</b> <b>answering</b> tasks involving multi-hop <b>reasoning,</b> we encode the full decomposed <b>reasoning</b> roadmap into a single interpretable BlendSQL query. Notably, we show that BlendSQL can scale to massive datasets and improve the performance of end-to-end systems while using 35% fewer tokens. Our code is available and installable as a package at <a href=https://github.com/parkervg/blendsql>https://github.com/parkervg/blendsql</a>.</p></p class="citation"></blockquote><h3 id=1568--15341-training-free-long-context-scaling-of-large-language-models-chenxin-an-et-al-2024>(15/68 | 15/341) Training-Free Long-Context Scaling of Large Language Models (Chenxin An et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, Lingpeng Kong. (2024)<br><strong>Training-Free Long-Context Scaling of Large Language Models</strong><br><button class=copy-to-clipboard title="Training-Free Long-Context Scaling of Large Language Models" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17463v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17463v1.pdf filename=2402.17463v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of <b>finetuning</b> <b>large-scale</b> <b>models</b> <b>with</b> longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of <b>finetuned</b> models. When compared with proprietary models, our training-free 70B model attains 94% of the performance of <b>gpt-3.5-16k,</b> indicating it is a viable open-source alternative. All code and data used in this work are released at \url{https://github.com/HKUNLP/ChunkLlama}.</p></p class="citation"></blockquote><h3 id=1668--16341-sofa-shielded-on-the-fly-alignment-via-priority-rule-following-xinyu-lu-et-al-2024>(16/68 | 16/341) SoFA: Shielded On-the-fly Alignment via Priority Rule Following (Xinyu Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Lu, Bowen Yu, Yaojie Lu, Hongyu Lin, Haiyang Yu, Le Sun, Xianpei Han, Yongbin Li. (2024)<br><strong>SoFA: Shielded On-the-fly Alignment via Priority Rule Following</strong><br><button class=copy-to-clipboard title="SoFA: Shielded On-the-fly Alignment via Priority Rule Following" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Knowledge Distillation, Simulation, Simulator, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17358v1.pdf filename=2402.17358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The alignment problem in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced <b>LLMs,</b> such as <b>GPT-4,</b> exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for <b>distilling</b> priority following signals from <b>LLM</b> <b>simulations</b> to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately.</p></p class="citation"></blockquote><h3 id=1768--17341-comparing-effectiveness-of-regularization-methods-on-text-classification-simple-and-complex-model-in-data-shortage-situation-jongga-lee-et-al-2024>(17/68 | 17/341) Comparing effectiveness of regularization methods on text classification: Simple and complex model in data shortage situation (Jongga Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jongga Lee, Jaeseung Yim, Seohee Park, Changwon Lim. (2024)<br><strong>Comparing effectiveness of regularization methods on text classification: Simple and complex model in data shortage situation</strong><br><button class=copy-to-clipboard title="Comparing effectiveness of regularization methods on text classification: Simple and complex model in data shortage situation" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Adversarial Learning, Convolutional Neural Network, Semi-Supervised Learning, Supervised Learning, Supervised Learning, Text Classification, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00825v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00825v1.pdf filename=2403.00825v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text</b> <b>classification</b> is the task of assigning a document to a predefined class. However, it is expensive to acquire enough labeled documents or to label them. In this paper, we study the regularization methods&rsquo; effects on various classification models when only a few labeled data are available. We compare a simple <b>word</b> <b>embedding-based</b> model, which is simple but effective, with complex models <b>(CNN</b> and BiLSTM). In <b>supervised</b> <b>learning,</b> <b>adversarial</b> <b>training</b> can further regularize the model. When an unlabeled dataset is available, we can regularize the model using <b>semi-supervised</b> <b>learning</b> methods such as the Pi model and virtual <b>adversarial</b> <b>training.</b> We evaluate the regularization effects on four <b>text</b> <b>classification</b> datasets (AG news, DBpedia, Yahoo! Answers, Yelp Polarity), using only 0.1% to 0.5% of the original labeled training documents. The simple model performs relatively well in fully <b>supervised</b> <b>learning,</b> but with the help of <b>adversarial</b> <b>training</b> and <b>semi-supervised</b> <b>learning,</b> both simple and complex models can be regularized, showing better results for complex models. Although the simple model is robust to overfitting, a complex model with well-designed prior beliefs can be also robust to overfitting.</p></p class="citation"></blockquote><h3 id=1868--18341-can-gpt-4-identify-propaganda-annotation-and-detection-of-propaganda-spans-in-news-articles-maram-hasanain-et-al-2024>(18/68 | 18/341) Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles (Maram Hasanain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maram Hasanain, Fatema Ahmed, Firoj Alam. (2024)<br><strong>Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles</strong><br><button class=copy-to-clipboard title="Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 66<br>Keywords: Fine-tuning, Low-Resource, Multi-modal, Multi-modal, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17478v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17478v1.pdf filename=2402.17478v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of propaganda has spiked on mainstream and social media, aiming to manipulate or mislead users. While efforts to automatically detect propaganda techniques in textual, visual, or <b>multimodal</b> content have increased, most of them primarily focus on English content. The majority of the recent initiatives targeting medium to <b>low-resource</b> languages produced relatively small annotated datasets, with a skewed distribution, posing challenges for the development of sophisticated propaganda detection models. To address this challenge, we carefully develop the largest propaganda dataset to date, ArPro, comprised of 8K paragraphs from newspaper articles, labeled at the text span level following a taxonomy of 23 propagandistic techniques. Furthermore, our work offers the first attempt to understand the performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> using <b>GPT-4,</b> for fine-grained propaganda detection from text. Results showed that <b>GPT-4&rsquo;s</b> performance degrades as the task moves from simply classifying a paragraph as propagandistic or not, to the fine-grained task of detecting propaganda techniques and their manifestation in text. Compared to models <b>fine-tuned</b> on the dataset for propaganda detection at different classification granularities, <b>GPT-4</b> is still far behind. Finally, we evaluate <b>GPT-4</b> on a dataset consisting of six other languages for span detection, and results suggest that the model struggles with the task across languages. Our dataset and resources will be released to the community.</p></p class="citation"></blockquote><h3 id=1968--19341-consistency-matters-explore-llms-consistency-from-a-black-box-perspective-fufangchen-zhao-et-al-2024>(19/68 | 19/341) Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective (Fufangchen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fufangchen Zhao, Guoqiang Jin, Jiaheng Huang, Rui Zhao, Fei Tan. (2024)<br><strong>Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective</strong><br><button class=copy-to-clipboard title="Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 65<br>Keywords: Black Box, GPT-3, GPT-3.5, Natural Language Generation, BLEU, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17411v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17411v2.pdf filename=2402.17411v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays both commercial and open-source academic <b>LLM</b> have become the mainstream models of NLP. However, there is still a lack of research on <b>LLM</b> consistency, meaning that throughout the various stages of <b>LLM</b> research and deployment, its internal parameters and capabilities should remain unchanged. This issue exists in both the industrial and academic sectors. The solution to this problem is often time-consuming and labor-intensive, and there is also an additional cost of secondary deployment, resulting in economic and time losses. To fill this gap, we build an <b>LLM</b> consistency task dataset and design several baselines. Additionally, we choose models of diverse scales for the main experiments. Specifically, in the LightGBM experiment, we used traditional <b>NLG</b> metrics (i.e., <b>ROUGE,</b> <b>BLEU,</b> METEOR) as the features needed for model training. The final result exceeds the manual evaluation and <b>GPT3.5</b> as well as other models in the main experiment, achieving the best performance. In the end, we use the best performing LightGBM model as the base model to build the evaluation tool, which can effectively assist in the deployment of business models. Our code and tool demo are available at <a href=https://github.com/heavenhellchen/Consistency.git>https://github.com/heavenhellchen/Consistency.git</a></p></p class="citation"></blockquote><h3 id=2068--20341-researchy-questions-a-dataset-of-multi-perspective-decompositional-questions-for-llm-web-agents-corby-rosset-et-al-2024>(20/68 | 20/341) Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents (Corby Rosset et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Corby Rosset, Ho-Lam Chung, Guanghui Qin, Ethan C. Chau, Zhuo Feng, Ahmed Awadallah, Jennifer Neville, Nikhil Rao. (2024)<br><strong>Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents</strong><br><button class=copy-to-clipboard title="Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-4, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17896v1.pdf filename=2402.17896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>question</b> <b>answering</b> <b>(QA)</b> datasets are no longer challenging to most powerful <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Traditional <b>QA</b> <b>benchmarks</b> like TriviaQA, NaturalQuestions, ELI5 and HotpotQA mainly study <code>known unknowns'' with clear indications of both what information is missing, and how to find it to answer the &lt;b>question.&lt;/b> &lt;b>Hence,&lt;/b> good performance on these &lt;b>benchmarks&lt;/b> provides a false sense of security. A yet unmet need of the NLP community is a bank of non-factoid, multi-perspective &lt;b>questions&lt;/b> &lt;b>involving&lt;/b> a great deal of unclear information needs, i.e. </code>unknown uknowns&rsquo;&rsquo;. We claim we can find such <b>questions</b> <b>in</b> search engine logs, which is surprising because most <b>question-intent</b> <b>queries</b> are indeed factoid. We present Researchy <b>Questions,</b> <b>a</b> dataset of search engine queries tediously filtered to be non-factoid, <code>decompositional'' and multi-perspective. We show that users spend a lot of </code>effort&rsquo;&rsquo; on these <b>questions</b> <b>in</b> terms of signals like clicks and session length, and that they are also challenging for <b>GPT-4.</b> We also show that ``slow thinking&rsquo;&rsquo; answering techniques, like decomposition into sub-questions shows benefit over answering directly. We release $\sim$ 100k Researchy <b>Questions,</b> <b>along</b> with the Clueweb22 URLs that were clicked.</p></p class="citation"></blockquote><h3 id=2168--21341-multitask-multilingual-model-adaptation-with-featurized-low-rank-mixtures-chu-cheng-lin-et-al-2024>(21/68 | 21/341) Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures (Chu-Cheng Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chu-Cheng Lin, Xinyi Wang, Jonathan H. Clark, Han Lu, Yun Zhu, Chenxi Whitehouse, Hongkun Yu. (2024)<br><strong>Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures</strong><br><button class=copy-to-clipboard title="Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Supervised Learning, Supervised Learning, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17934v1.pdf filename=2402.17934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adapting pretrained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient <b>fine-tuning</b> (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning. FLix associates each unique dataset feature, such as the dataset&rsquo;s language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads to significant improvements over a variety of tasks for both <b>supervised</b> <b>learning</b> and <b>zero-shot</b> settings using different training data mixtures.</p></p class="citation"></blockquote><h3 id=2268--22341-jmlr-joint-medical-llm-and-retrieval-training-for-enhancing-reasoning-and-professional-question-answering-capability-junda-wang-et-al-2024>(22/68 | 22/341) JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability (Junda Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu. (2024)<br><strong>JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability</strong><br><button class=copy-to-clipboard title="JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Information Retrieval, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17887v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17887v2.pdf filename=2402.17887v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> play an increasingly vital role in medical knowledge acquisition and <b>question-answering</b> <b>systems.</b> To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an <b>Information</b> <b>Retrieval</b> (IR) system and an <b>LLM</b> during the <b>fine-tuning</b> phase. This approach, which we call Joint Medical <b>LLM</b> and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical <b>question-answering</b> <b>tasks.</b> By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model&rsquo;s ability to leverage medical knowledge for <b>reasoning</b> and answering <b>questions.</b> <b>Our</b> experimental results demonstrate that JMLR-13B (81.2% on Amboos, 61.3% on MedQA) outperforms models using conventional pre-training and <b>fine-tuning</b> Meditron-70B (76.4% on AMBOSS, 60.3% on MedQA). For models of the same 7B scale, JMLR-7B(68.7% on Amboos, 51.7% on MedQA) significantly outperforms other public models (Meditron-7B: 50.1%, 47.9%), proving its superiority in terms of cost (our training time: 37 hours, traditional method: 144 hours), efficiency, and effectiveness in medical <b>question-answering</b> <b>tasks.</b> Through this work, we provide a new and efficient knowledge enhancement tool for healthcare, demonstrating the great potential of integrating IR and <b>LLM</b> training in precision medical <b>information</b> <b>retrieval</b> and <b>question-answering</b> <b>systems.</b></p></p class="citation"></blockquote><h3 id=2368--23341-follow-my-instruction-and-spill-the-beans-scalable-data-extraction-from-retrieval-augmented-generation-systems-zhenting-qi-et-al-2024>(23/68 | 23/341) Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems (Zhenting Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenting Qi, Hanlin Zhang, Eric Xing, Sham Kakade, Himabindu Lakkaraju. (2024)<br><strong>Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems</strong><br><button class=copy-to-clipboard title="Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, GPT, Instruction Following, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17840v1.pdf filename=2402.17840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in <b>Retrieval-In-Context</b> <b>RAG</b> <b>Language</b> Models (LMs). We show that an adversary can exploit LMs&rsquo; <b>instruction-following</b> <b>capabilities</b> to easily extract text data verbatim from the datastore of <b>RAG</b> systems built with <b>instruction-tuned</b> <b>LMs</b> via <b>prompt</b> injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production <b>RAG</b> models <b>GPTs,</b> we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized <b>GPTs</b> with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by <b>prompting</b> the <b>GPTs</b> with only 100 queries generated by themselves.</p></p class="citation"></blockquote><h3 id=2468--24341-massive-activations-in-large-language-models-mingjie-sun-et-al-2024>(24/68 | 24/341) Massive Activations in Large Language Models (Mingjie Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingjie Sun, Xinlei Chen, J. Zico Kolter, Zhuang Liu. (2024)<br><strong>Massive Activations in Large Language Models</strong><br><button class=copy-to-clipboard title="Massive Activations in Large Language Models" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Vision Transformer, Transformer, Large Language Model, Large Language Model, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17762v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17762v1.pdf filename=2402.17762v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We observe an empirical phenomenon in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> &ndash; very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various <b>LLMs</b> and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in <b>LLMs.</b> Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the <b>self-attention</b> output. Last, we also study massive activations in <b>Vision</b> <b>Transformers.</b></p></p class="citation"></blockquote><h3 id=2568--25341-ambignlg-addressing-task-ambiguity-in-instruction-for-nlg-ayana-niwa-et-al-2024>(25/68 | 25/341) AmbigNLG: Addressing Task Ambiguity in Instruction for NLG (Ayana Niwa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayana Niwa, Hayate Iso. (2024)<br><strong>AmbigNLG: Addressing Task Ambiguity in Instruction for NLG</strong><br><button class=copy-to-clipboard title="AmbigNLG: Addressing Task Ambiguity in Instruction for NLG" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Language Generation, Natural Language Generation, Natural Language Generation, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17717v1.pdf filename=2402.17717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce AmbigNLG, a new task designed to tackle the challenge of task ambiguity in instructions for <b>Natural</b> <b>Language</b> <b>Generation</b> <b>(NLG)</b> tasks. Despite the impressive capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in understanding and executing a wide range of tasks through <b>natural</b> <b>language</b> <b>interaction,</b> their performance is significantly hindered by the ambiguity present in real-world instructions. To address this, AmbigNLG seeks to identify and mitigate such ambiguities, aiming to refine instructions to match user expectations better. We introduce a dataset, AmbigSNI-NLG, consisting of 2,500 instances, and develop an ambiguity taxonomy for categorizing and annotating instruction ambiguities. Our approach demonstrates substantial improvements in <b>text</b> <b>generation</b> quality, highlighting the critical role of clear and specific instructions in enhancing <b>LLM</b> performance in <b>NLG</b> tasks.</p></p class="citation"></blockquote><h3 id=2668--26341-nextlevelbert-investigating-masked-language-modeling-with-higher-level-representations-for-long-documents-tamara-czinczoll-et-al-2024>(26/68 | 26/341) NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents (Tamara Czinczoll et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tamara Czinczoll, Christoph Hönes, Maximilian Schall, Gerard de Melo. (2024)<br><strong>NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents</strong><br><button class=copy-to-clipboard title="NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Zero-shot, Document Classification, Question Answering, Document Embedding, Masked Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17682v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17682v1.pdf filename=2402.17682v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a <b>Masked</b> <b>Language</b> <b>Model</b> operating not on tokens, but on higher-level semantic representations in the form of <b>text</b> <b>embeddings.</b> We pretrain NextLevelBERT to predict the vector representation of entire <b>masked</b> <b>text</b> <b>chunks</b> and evaluate the effectiveness of the resulting <b>document</b> <b>vectors</b> on three task types: 1) Semantic Textual Similarity via <b>zero-shot</b> <b>document</b> <b>embeddings,</b> 2) Long <b>document</b> <b>classification,</b> 3) Multiple-choice <b>question</b> <b>answering.</b> We find that next level <b>Masked</b> <b>Language</b> <b>Modeling</b> is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high. We make model and code available.</p></p class="citation"></blockquote><h3 id=2768--27341-prescribing-large-language-models-for-perioperative-care-whats-the-right-dose-for-pre-trained-models-bing-xue-et-al-2024>(27/68 | 27/341) Prescribing Large Language Models for Perioperative Care: What&rsquo;s The Right Dose for Pre-trained Models? (Bing Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bing Xue, Charles Alba, Joanna Abraham, Thomas Kannampallil, Chenyang Lu. (2024)<br><strong>Prescribing Large Language Models for Perioperative Care: What&rsquo;s The Right Dose for Pre-trained Models?</strong><br><button class=copy-to-clipboard title="Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: J-3; I-2-7, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Self-supervised Learning, Domain Adaptation, Large Language Model, Large Language Model, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17493v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17493v2.pdf filename=2402.17493v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess&rsquo;s MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three <b>domain</b> <b>adaptation</b> and <b>finetuning</b> strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: <b>self-supervised</b> objectives; incorporating labels with semi-supervised <b>fine-tuning;</b> and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision recall curve (AUPRC) for classification tasks, and mean squared error (MSE) and R2 for regression tasks. Pre-trained <b>LLMs</b> outperformed traditional <b>word</b> <b>embeddings,</b> with absolute maximal gains of 38.3% for AUROC and 14% for AUPRC. Adapting models further improved performance: (1) <b>self-supervised</b> <b>finetuning</b> by 3.2% for AUROC and 1.5% for AUPRC; (2) semi-supervised <b>finetuning</b> by 1.8% for AUROC and 2% for AUPRC, compared to <b>self-supervised</b> <b>finetuning;</b> (3) foundational modelling by 3.6% for AUROC and 2.6% for AUPRC, compared to <b>self-supervised</b> <b>finetuning.</b> Pre-trained clinical <b>LLMs</b> offer opportunities for postoperative risk predictions in unforeseen data, with peaks in foundational models indicating the potential of task-agnostic learning towards the generalizability of <b>LLMs</b> in perioperative care.</p></p class="citation"></blockquote><h3 id=2868--28341-fact-and-reflection-far-improves-confidence-calibration-of-large-language-models-xinran-zhao-et-al-2024>(28/68 | 28/341) Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models (Xinran Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, Tongshuang Wu, Jianshu Chen. (2024)<br><strong>Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models</strong><br><button class=copy-to-clipboard title="Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Question Answering, Question Answering, Large Language Model, Large Language Model, Prompt, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17124v1.pdf filename=2402.17124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For a <b>LLM</b> to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that <b>LLM</b> performances are greatly impacted by <b>prompts,</b> the confidence calibration in <b>prompting</b> <b>LLMs</b> has yet to be thoroughly explored. In this paper, we explore how different <b>prompting</b> strategies influence <b>LLM</b> confidence calibration and how it could be improved. We conduct extensive experiments on six <b>prompting</b> methods in the <b>question-answering</b> <b>context</b> and we observe that, while these methods help improve the expected <b>LLM</b> calibration, they also trigger <b>LLMs</b> to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) <b>prompting,</b> which improves the <b>LLM</b> calibration in two steps. First, FaR elicits the known &ldquo;facts&rdquo; that are relevant to the input <b>prompt</b> from the <b>LLM.</b> And then it asks the model to &ldquo;reflect&rdquo; over them to generate the final answer. Experiments show that FaR <b>prompting</b> achieves significantly better calibration; it lowers the Expected Calibration Error by 23.5% on our multi-purpose <b>QA</b> tasks. Notably, FaR <b>prompting</b> even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger <b>retrieval</b> <b>augmentation</b> for solving these harder instances.</p></p class="citation"></blockquote><h3 id=2968--29341-measuring-vision-language-stem-skills-of-neural-models-jianhao-shen-et-al-2024>(29/68 | 29/341) Measuring Vision-Language STEM Skills of Neural Models (Jianhao Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhao Shen, Ye Yuan, Srbuhi Mirzoyan, Ming Zhang, Chenguang Wang. (2024)<br><strong>Measuring Vision-Language STEM Skills of Neural Models</strong><br><button class=copy-to-clipboard title="Measuring Vision-Language STEM Skills of Neural Models" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 59<br>Keywords: Benchmarking, Foundation Model, Multi-modal, Multi-modal, GPT, GPT-3, GPT-3.5, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17205v1.pdf filename=2402.17205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of <b>multimodal</b> <b>vision-language</b> information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art <b>foundation</b> <b>models</b> such as CLIP and <b>GPT-3.5-Turbo</b> to our <b>benchmark.</b> Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well below (averaging 54.7%) the performance of elementary students, not to mention near expert-level performance. To understand and increase the performance on our dataset, we teach the models on a training split of our dataset. Even though we observe improved performance, the model performance remains relatively low compared to average elementary students. To solve STEM problems, we will need novel algorithmic innovations from the community.</p></p class="citation"></blockquote><h3 id=3068--30341-are-llms-capable-of-data-based-statistical-and-causal-reasoning-benchmarking-advanced-quantitative-reasoning-with-data-xiao-liu-et-al-2024>(30/68 | 30/341) Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data (Xiao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng. (2024)<br><strong>Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data</strong><br><button class=copy-to-clipboard title="Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Benchmarking, Benchmarking, GPT, GPT-4, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17644v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17644v1.pdf filename=2402.17644v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantitative <b>reasoning</b> is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative <b>Reasoning</b> with Data (QRData) <b>benchmark,</b> aiming to evaluate <b>Large</b> <b>Language</b> <b>Models&rsquo;</b> capability in statistical and causal <b>reasoning</b> with real-world data. The <b>benchmark</b> comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models&rsquo; quantitative <b>reasoning</b> abilities on data and text, we enrich the <b>benchmark</b> with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language <b>reasoning,</b> program-based <b>reasoning,</b> and agent <b>reasoning</b> methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model <b>GPT-4</b> achieves an accuracy of 58%, which has a <b>large</b> <b>room</b> <b>for</b> improvement. Among open-source models, Deepseek-coder-instruct, a code <b>LLM</b> pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals that models encounter difficulties in data analysis and causal <b>reasoning,</b> and struggle in using causal knowledge and provided data simultaneously. Code and data are in <a href=https://github.com/xxxiaol/QRData>https://github.com/xxxiaol/QRData</a>.</p></p class="citation"></blockquote><h3 id=3168--31341-investigating-continual-pretraining-in-large-language-models-insights-and-implications-çağatay-yıldız-et-al-2024>(31/68 | 31/341) Investigating Continual Pretraining in Large Language Models: Insights and Implications (Çağatay Yıldız et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Çağatay Yıldız, Nishaanth Kanna Ravichandran, Prishruit Punia, Matthias Bethge, Beyza Ermis. (2024)<br><strong>Investigating Continual Pretraining in Large Language Models: Insights and Implications</strong><br><button class=copy-to-clipboard title="Investigating Continual Pretraining in Large Language Models: Insights and Implications" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Continual Learning, Fine-tuning, Knowledge Transfer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17400v1.pdf filename=2402.17400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the evolving domain of <b>Continual</b> <b>Learning</b> (CL) in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> with a focus on developing strategies for efficient and sustainable training. Our primary emphasis is on <b>continual</b> <b>domain-adaptive</b> pretraining, a process designed to equip <b>LLMs</b> with the ability to integrate new information from various domains while retaining previously learned <b>knowledge</b> <b>and</b> enhancing cross-domain <b>knowledge</b> <b>transfer</b> without relying on domain-specific identification. Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of <b>LLMs</b> to changing data landscapes in practical scenarios. To this end, we introduce a new <b>benchmark</b> designed to measure the adaptability of <b>LLMs</b> to these evolving data environments, offering a comprehensive framework for evaluation. We examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the <b>knowledge</b> <b>transfer</b> within these models. Our findings uncover several key insights: (i) when the sequence of domains shows semantic similarity, <b>continual</b> <b>pretraining</b> enables <b>LLMs</b> to better specialize in the current domain compared to stand-alone <b>fine-tuning,</b> (ii) training across a diverse range of domains enhances both backward and forward <b>knowledge</b> <b>transfer,</b> and (iii) smaller models are particularly sensitive to <b>continual</b> <b>pretraining,</b> showing the most significant rates of both forgetting and learning. We posit that our research marks a shift towards establishing a more realistic <b>benchmark</b> for investigating CL in <b>LLMs,</b> and has the potential to play a key role in guiding the direction of future research in the field.</p></p class="citation"></blockquote><h3 id=3268--32341-the-era-of-1-bit-llms-all-large-language-models-are-in-158-bits-shuming-ma-et-al-2024>(32/68 | 32/341) The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits (Shuming Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei. (2024)<br><strong>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</strong><br><button class=copy-to-clipboard title="The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Transformer, Large Language Model, Large Language Model, Perplexity, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17764v1.pdf filename=2402.17764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research, such as BitNet, is paving the way for a new era of 1-bit <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> In this work, we introduce a 1-bit <b>LLM</b> variant, namely BitNet b1.58, in which every single parameter (or weight) of the <b>LLM</b> is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) <b>Transformer</b> <b>LLM</b> with the same model size and training tokens in terms of both <b>perplexity</b> and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit <b>LLM</b> defines a new <b>scaling</b> <b>law</b> and recipe for training new generations of <b>LLMs</b> that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=3368--33341-recost-external-knowledge-guided-data-efficient-instruction-tuning-qi-zhang-et-al-2024>(33/68 | 33/341) RECOST: External Knowledge Guided Data-efficient Instruction Tuning (Qi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Zhang, Yiming Zhang, Haobo Wang, Junbo Zhao. (2024)<br><strong>RECOST: External Knowledge Guided Data-efficient Instruction Tuning</strong><br><button class=copy-to-clipboard title="RECOST: External Knowledge Guided Data-efficient Instruction Tuning" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Alpaca, In-context Learning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17355v1.pdf filename=2402.17355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the current landscape of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> the process of <b>instruction</b> <b>tuning</b> serves as an essential step. Considering the high computing power overhead, data-efficient <b>instruction</b> <b>tuning</b> was proposed to reduce the training data size in this process, aiming at selecting high-quality <b>instructional</b> <b>data.</b> Nevertheless, we argue that most current data-efficient <b>instruction-tuning</b> <b>methods</b> are highly dependent on the quality of the original <b>instruction-tuning</b> <b>dataset.</b> When it comes to datasets synthesized by <b>LLMs,</b> a common scenario in this field, dirty samples will even be selected with a higher probability than other samples. To address these challenges, we utilized external knowledge (relevant examples or paragraphs) to evaluate those samples synthesized by <b>LLMs</b> with an <b>in-context-based</b> relative predictive entropy. Based on the new metric, we proposed a framework, dubbed as \textbf{RECOST}, which integrates external-knowledge-base re-ranking and diversity-consistent sampling into a single pipeline. Through extensive experiments on several synthetic datasets <b>(Alpaca</b> and <b>Alpaca-gpt4),</b> we demonstrate the effectiveness of our method and achieve even better results with only \textbf{1%} of the full dataset.</p></p class="citation"></blockquote><h3 id=3468--34341-mini-ensemble-low-rank-adapters-for-parameter-efficient-fine-tuning-pengjie-ren-et-al-2024>(34/68 | 34/341) Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning (Pengjie Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengjie Ren, Chengshun Shi, Shiguang Wu, Mengqi Zhang, Zhaochun Ren, Maarten de Rijke, Zhumin Chen, Jiahuan Pei. (2024)<br><strong>Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning</strong><br><button class=copy-to-clipboard title="Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Instruction Following, Natural Language Understanding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17263v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17263v1.pdf filename=2402.17263v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>fine-tuning</b> (PEFT) is a popular method for tailoring pre-trained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> especially as the models&rsquo; scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter <b>fine-tuning.</b> We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential. The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theoretical analysis and empirical studies on various NLP tasks. Our experimental results show that, compared to LoRA, MELoRA achieves better performance with 8 times fewer trainable parameters on <b>natural</b> <b>language</b> <b>understanding</b> tasks and 36 times fewer trainable parameters on <b>instruction</b> <b>following</b> tasks, which demonstrates the effectiveness of MELoRA.</p></p class="citation"></blockquote><h3 id=3568--35341-creating-suspenseful-stories-iterative-planning-with-large-language-models-kaige-xie-et-al-2024>(35/68 | 35/341) Creating Suspenseful Stories: Iterative Planning with Large Language Models (Kaige Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaige Xie, Mark Riedl. (2024)<br><strong>Creating Suspenseful Stories: Iterative Planning with Large Language Models</strong><br><button class=copy-to-clipboard title="Creating Suspenseful Stories: Iterative Planning with Large Language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Supervised Learning, Zero-shot, Language Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17119v1.pdf filename=2402.17119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, suspense is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have greatly promoted <b>language</b> <b>generation</b> in general, state-of-the-art <b>LLMs</b> are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully <b>zero-shot</b> manner and does not rely on any <b>supervised</b> story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with <b>LLMs.</b> Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=3668--36341-large-language-modelsllms-on-tabular-data-prediction-generation-and-understanding----a-survey-xi-fang-et-al-2024>(36/68 | 36/341) Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding &ndash; A Survey (Xi Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, Christos Faloutsos. (2024)<br><strong>Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding &ndash; A Survey</strong><br><button class=copy-to-clipboard title="Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding -- A Survey" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17944v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17944v2.pdf filename=2402.17944v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent breakthroughs in <b>large</b> <b>language</b> <b>modeling</b> have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, <b>question</b> <b>answering,</b> and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that <b>summarizes</b> and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.</p></p class="citation"></blockquote><h3 id=3768--37341-unleashing-the-potential-of-large-language-models-as-prompt-optimizers-an-analogical-analysis-with-gradient-based-model-optimizers-xinyu-tang-et-al-2024>(37/68 | 37/341) Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers (Xinyu Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Siyuan Lu, Yaliang Li, Ji-Rong Wen. (2024)<br><strong>Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers</strong><br><button class=copy-to-clipboard title="Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Massive Multitask Language Understanding (MMLU), Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17564v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17564v1.pdf filename=2402.17564v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic <b>prompt</b> optimization is an important approach to improving the performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Recent research demonstrates the potential of using <b>LLMs</b> as <b>prompt</b> optimizers, which can generate improved task <b>prompts</b> via iterative refinement. In this paper, we propose a novel perspective to investigate the design of <b>LLM-based</b> <b>prompt</b> optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. Focused on the two aspects, we borrow the theoretical framework and learning methods from gradient-based optimization to design improved strategies for <b>LLM-based</b> <b>prompt</b> optimizers. By systematically analyzing a rich set of improvement strategies, we further develop a capable Gradient-inspired <b>LLM-based</b> <b>Prompt</b> Optimizer called GPO. At each step, it first retrieves relevant <b>prompts</b> from the optimization trajectory as the update direction. Then, it utilizes the generation-based refinement strategy to perform the update, while controlling the edit distance through a cosine-based decay strategy. Extensive experiments demonstrate the effectiveness and efficiency of GPO. In particular, GPO brings an additional improvement of up to 56.8% on Big-Bench Hard and 55.3% on <b>MMLU</b> compared to baseline methods.</p></p class="citation"></blockquote><h3 id=3868--38341-predict-the-next-word-humans-exhibit-uncertainty-in-this-task-and-language-models-_____-evgenia-ilia-et-al-2024>(38/68 | 38/341) Predict the Next Word: <humans exhibit uncertainty in this task and language models _____>(Evgenia Ilia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evgenia Ilia, Wilker Aziz. (2024)<br><strong>Predict the Next Word: <humans exhibit uncertainty in this task and language models _____></strong><br><button class=copy-to-clipboard title="Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: BLOOM, ChatGPT, GPT-2, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17527v1.pdf filename=2402.17527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models (LMs) are statistical models trained to assign probability to human-generated <b>text.</b> <b>As</b> such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM&rsquo;s ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the &rsquo;next word prediction&rsquo; task. This can be seen as assessing a form of calibration, which, in the context of <b>text</b> <b>classification,</b> Baan et al. (2022) termed calibration to human uncertainty. We assess <b>GPT2,</b> <b>BLOOM</b> and <b>ChatGPT</b> and find that they exhibit fairly low calibration to human uncertainty. We also verify the failure of expected calibration error (ECE) to reflect this, and as such, advise the community against relying on it in this setting.</p></p class="citation"></blockquote><h3 id=3968--39341-acquiring-linguistic-knowledge-from-multimodal-input-theodor-amariucai-et-al-2024>(39/68 | 39/341) Acquiring Linguistic Knowledge from Multimodal Input (Theodor Amariucai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Theodor Amariucai, Alex Warstadt. (2024)<br><strong>Acquiring Linguistic Knowledge from Multimodal Input</strong><br><button class=copy-to-clipboard title="Acquiring Linguistic Knowledge from Multimodal Input" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Grounding, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17936v1.pdf filename=2402.17936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In contrast to children, language models (LMs) exhibit considerably inferior data efficiency when acquiring language. In this submission to the BabyLM Challenge (Warstadt et al., 2023), we test the hypothesis that this data efficiency gap is partly caused by a lack of <b>multimodal</b> input and <b>grounding</b> in the learning environment of typical language models. Although previous work looking into this question found that <b>multimodal</b> training can even harm language-only performance, we speculate that these findings can be attributed to catastrophic forgetting of complex language due to <b>fine-tuning</b> on captions data. To test our hypothesis, we perform an ablation study on FLAVA (Singh et al., 2022), a <b>multimodal</b> <b>vision-and-language</b> model, independently varying the volume of text and vision input to quantify how much text data (if any) can be offset by vision at different data scales. We aim to limit catastrophic forgetting through a multitask pretraining regime that includes unimodal text-only tasks and data sampled from WiT, the relatively diverse Wikipedia-based dataset (Srinivasan et al., 2021). Our results are largely negative: <b>Multimodal</b> pretraining does not harm our models&rsquo; language performance but does not consistently help either. That said, our conclusions are limited by our having been able to conduct only a small number of runs. While we must leave open the possibility that <b>multimodal</b> input explains some of the gap in data efficiency between LMs and humans, positive evidence for this hypothesis will require better architectures and techniques for <b>multimodal</b> training.</p></p class="citation"></blockquote><h3 id=4068--40341-probing-multimodal-large-language-models-for-global-and-local-semantic-representation-mingxu-tao-et-al-2024>(40/68 | 40/341) Probing Multimodal Large Language Models for Global and Local Semantic Representation (Mingxu Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao. (2024)<br><strong>Probing Multimodal Large Language Models for Global and Local Semantic Representation</strong><br><button class=copy-to-clipboard title="Probing Multimodal Large Language Models for Global and Local Semantic Representation" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Object Detection, Multi-modal, Multi-modal, Image2text, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17304v1.pdf filename=2402.17304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The success of <b>large</b> <b>language</b> <b>models</b> has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs), which achieve state-of-the-art performance on <b>image-to-text</b> tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local <b>object</b> <b>information.</b> In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through <b>object</b> <b>detection</b> tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to encode global information.</p></p class="citation"></blockquote><h3 id=4168--41341-stable-lm-2-16b-technical-report-marco-bellagente-et-al-2024>(41/68 | 41/341) Stable LM 2 1.6B Technical Report (Marco Bellagente et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, Meng Lee, Emad Mostaque, Michael Pieler, Nikhil Pinnaparju, Paulo Rocha, Harry Saini, Hannah Teufel, Niccolo Zanichelli, Carlos Riquelme. (2024)<br><strong>Stable LM 2 1.6B Technical Report</strong><br><button class=copy-to-clipboard title="Stable LM 2 1.6B Technical Report" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, stat-ML<br>Keyword Score: 33<br>Keywords: Benchmarking, Few-shot, Quantization, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17834v1.pdf filename=2402.17834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce StableLM 2 1.6B, the first in a new generation of our language model series. In this technical report, we present in detail the data and training procedure leading to the base and instruction-tuned versions of StableLM 2 1.6B. The weights for both models are available via Hugging Face for anyone to download and use. The report contains thorough evaluations of these models, including zero- and <b>few-shot</b> <b>benchmarks,</b> multilingual <b>benchmarks,</b> and the <b>MT</b> <b>benchmark</b> focusing on multi-turn dialogues. At the time of publishing this report, StableLM 2 1.6B was the state-of-the-art open model under 2B parameters by a significant margin. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several <b>quantized</b> checkpoints and provide their performance metrics compared to the original model.</p></p class="citation"></blockquote><h3 id=4268--42341-tower-an-open-multilingual-large-language-model-for-translation-related-tasks-duarte-m-alves-et-al-2024>(42/68 | 42/341) Tower: An Open Multilingual Large Language Model for Translation-Related Tasks (Duarte M. Alves et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duarte M. Alves, José Pombal, Nuno M. Guerreiro, Pedro H. Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, André F. T. Martins. (2024)<br><strong>Tower: An Open Multilingual Large Language Model for Translation-Related Tasks</strong><br><button class=copy-to-clipboard title="Tower: An Open Multilingual Large Language Model for Translation-Related Tasks" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17733v1.pdf filename=2402.17733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While general-purpose <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open <b>LLMs</b> are competitive only when specializing on a single task. In this paper, we propose a recipe for tailoring <b>LLMs</b> to multiple tasks present in translation workflows. We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by <b>finetuning</b> on instructions relevant for translation processes, creating TowerInstruct. Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed <b>LLMs.</b> To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for <b>LLMs</b> focusing on the translation ecosystem, and a collection of model generations, including ours, on our <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=4368--43341-truthx-alleviating-hallucinations-by-editing-large-language-models-in-truthful-space-shaolei-zhang-et-al-2024>(43/68 | 43/341) TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space (Shaolei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaolei Zhang, Tian Yu, Yang Feng. (2024)<br><strong>TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space</strong><br><button class=copy-to-clipboard title="TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Contrastive Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17811v1.pdf filename=2402.17811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable capabilities across various tasks. However, they sometimes suffer from producing hallucinations, particularly in cases where they may generate untruthful responses despite possessing the correct knowledge. In this paper, we propose TruthX, an inference-time method to elicit the truthfulness of <b>LLMs</b> by editing their internal representations in truthful space. TruthX employs an auto-encoder to map <b>LLM&rsquo;s</b> representations into semantic and truthful latent spaces respectively, and applies <b>contrastive</b> <b>learning</b> to identify a truthful editing direction within the truthful space. During inference, by editing <b>LLM&rsquo;s</b> internal representations in truthful space, TruthX effectively enhances the truthfulness of <b>LLMs.</b> Experiments show that TruthX effectively improves the truthfulness of 13 advanced <b>LLMs</b> by an average of 20% on TruthfulQA <b>benchmark.</b> Further analyses suggest that the truthful space acquired by TruthX plays a pivotal role in controlling <b>LLM</b> to produce truthful or hallucinatory responses.</p></p class="citation"></blockquote><h3 id=4468--44341-exploiting-emotion-semantic-correlations-for-empathetic-response-generation-zhou-yang-et-al-2024>(44/68 | 44/341) Exploiting Emotion-Semantic Correlations for Empathetic Response Generation (Zhou Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhou Yang, Zhaochun Ren, Yufeng Wang, Xiaofei Zhu, Zhihao Chen, Tiecheng Cai, Yunbing Wu, Yisong Su, Sibo Ju, Xiangwen Liao. (2024)<br><strong>Exploiting Emotion-Semantic Correlations for Empathetic Response Generation</strong><br><button class=copy-to-clipboard title="Exploiting Emotion-Semantic Correlations for Empathetic Response Generation" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17437v1.pdf filename=2402.17437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Empathetic response generation aims to generate empathetic responses by understanding the speaker&rsquo;s emotional feelings from the language of dialogue. Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions. However, linguistic research has shown that emotional words in language are dynamic and have correlations with other grammar semantic roles, i.e., words with semantic meanings, in grammar. Previous methods overlook these two characteristics, which easily lead to misunderstandings of emotions and neglect of key semantics. To address this issue, we propose a dynamical Emotion-Semantic Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM constructs dynamic emotion-semantic vectors through the interaction of context and emotions. We introduce dependency trees to reflect the correlations between emotions and semantics. Based on dynamic emotion-semantic vectors and dependency trees, we propose a dynamic correlation <b>graph</b> <b>convolutional</b> <b>network</b> to guide the model in learning context meanings in dialogue and generating empathetic responses. Experimental results on the EMPATHETIC-DIALOGUES dataset show that ESCM understands semantics and emotions more accurately and expresses fluent and informative empathetic responses. Our analysis results also indicate that the correlations between emotions and semantics are frequently used in dialogues, which is of great significance for empathetic perception and expression.</p></p class="citation"></blockquote><h3 id=4568--45341-skt5scisumm----a-hybrid-generative-approach-for-multi-document-scientific-summarization-huy-quoc-to-et-al-2024>(45/68 | 45/341) SKT5SciSumm &ndash; A Hybrid Generative Approach for Multi-Document Scientific Summarization (Huy Quoc To et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huy Quoc To, Hung-Nghiep Tran, Andr&rsquo;e Greiner-Petter, Felix Beierle, Akiko Aizawa. (2024)<br><strong>SKT5SciSumm &ndash; A Hybrid Generative Approach for Multi-Document Scientific Summarization</strong><br><button class=copy-to-clipboard title="SKT5SciSumm -- A Hybrid Generative Approach for Multi-Document Scientific Summarization" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Clustering, T5, Transformer, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17311v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17311v1.pdf filename=2402.17311v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Summarization</b> for scientific text has shown significant benefits both for the research community and human society. Given the fact that the nature of scientific text is distinctive and the input of the multi-document <b>summarization</b> task is substantially long, the task requires sufficient embedding generation and text truncation without losing important information. To tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid framework for multi-document scientific <b>summarization</b> (MDSS). We leverage the Sentence-Transformer version of Scientific Paper Embeddings using Citation-Informed <b>Transformers</b> (SPECTER) to encode and represent textual sentences, allowing for efficient extractive <b>summarization</b> using k-means <b>clustering.</b> We employ the <b>T5</b> family of models to generate abstractive summaries using extracted sentences. SKT5SciSumm achieves state-of-the-art performance on the Multi-XScience dataset. Through extensive experiments and evaluation, we showcase the benefits of our model by using less complicated models to achieve remarkable results, thereby highlighting its potential in advancing the field of multi-document <b>summarization</b> for scientific text.</p></p class="citation"></blockquote><h3 id=4668--46341-llm-resistant-math-word-problem-generation-via-adversarial-attacks-roy-xie-et-al-2024>(46/68 | 46/341) LLM-Resistant Math Word Problem Generation via Adversarial Attacks (Roy Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra. (2024)<br><strong>LLM-Resistant Math Word Problem Generation via Adversarial Attacks</strong><br><button class=copy-to-clipboard title="LLM-Resistant Math Word Problem Generation via Adversarial Attacks" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17916v1.pdf filename=2402.17916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with <b>LLMs&rsquo;</b> rapid advancements, the educational community faces the challenge of assessing students&rsquo; true problem-solving abilities in the presence of <b>LLMs.</b> In this work, we explore a new paradigm for ensuring fair evaluation &ndash; generating <b>adversarial</b> <b>examples</b> which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by <b>LLMs.</b> Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate <b>adversarial</b> <b>examples</b> that cause <b>LLMs</b> to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source <b>LLMs,</b> quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify shared vulnerabilities among <b>LLMs</b> and propose a cost-effective approach to attack high-cost models. Additionally, we conduct automatic analysis on math problems and investigate the cause of failure to guide future research on <b>LLM&rsquo;s</b> mathematical capability.</p></p class="citation"></blockquote><h3 id=4768--47341-beyond-prompt-brittleness-evaluating-the-reliability-and-consistency-of-political-worldviews-in-llms-tanise-ceron-et-al-2024>(47/68 | 47/341) Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs (Tanise Ceron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanise Ceron, Neele Falk, Ana Barić, Dmitry Nikolaev, Sebastian Padó. (2024)<br><strong>Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs</strong><br><button class=copy-to-clipboard title="Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17649v1.pdf filename=2402.17649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the widespread use of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect. Recent studies report that, <b>prompted</b> with political questionnaires, <b>LLMs</b> show left-liberal leanings. However, it is as yet unclear whether these leanings are reliable (robust to <b>prompt</b> variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of <b>LLMs&rsquo;</b> stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains. We study <b>LLMs</b> ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance towards environment protection, social welfare but also (right-wing) law and order, with no consistent preferences in foreign policy, migration, and economy.</p></p class="citation"></blockquote><h3 id=4868--48341-an-effective-mixture-of-experts-approach-for-code-switching-speech-recognition-leveraging-encoder-disentanglement-tzu-ting-yang-et-al-2024>(48/68 | 48/341) An Effective Mixture-Of-Experts Approach For Code-Switching Speech Recognition Leveraging Encoder Disentanglement (Tzu-Ting Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tzu-Ting Yang, Hsin-Wei Wang, Yi-Cheng Wang, Chi-Han Lin, Berlin Chen. (2024)<br><strong>An Effective Mixture-Of-Experts Approach For Code-Switching Speech Recognition Leveraging Encoder Disentanglement</strong><br><button class=copy-to-clipboard title="An Effective Mixture-Of-Experts Approach For Code-Switching Speech Recognition Leveraging Encoder Disentanglement" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 30<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17189v1.pdf filename=2402.17189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the massive developments of end-to-end (E2E) neural networks, recent years have witnessed unprecedented breakthroughs in <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR).</b> However, the codeswitching phenomenon remains a major obstacle that hinders <b>ASR</b> from perfection, as the lack of labeled data and the variations between languages often lead to degradation of <b>ASR</b> performance. In this paper, we focus exclusively on improving the acoustic encoder of E2E <b>ASR</b> to tackle the challenge caused by the codeswitching phenomenon. Our main contributions are threefold: First, we introduce a novel disentanglement loss to enable the lower-layer of the encoder to capture inter-lingual acoustic information while mitigating linguistic confusion at the higher-layer of the encoder. Second, through comprehensive experiments, we verify that our proposed method outperforms the prior-art methods using pretrained dual-encoders, meanwhile having access only to the codeswitching corpus and consuming half of the parameterization. Third, the apparent differentiation of the encoders&rsquo; output features also corroborates the complementarity between the disentanglement loss and the mixture-of-experts (MoE) architecture.</p></p class="citation"></blockquote><h3 id=4968--49341-extreme-encoder-output-frame-rate-reduction-improving-computational-latencies-of-large-end-to-end-models-rohit-prabhavalkar-et-al-2024>(49/68 | 49/341) Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models (Rohit Prabhavalkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohit Prabhavalkar, Zhong Meng, Weiran Wang, Adam Stooke, Xingyu Cai, Yanzhang He, Arun Narayanan, Dongseong Hwang, Tara N. Sainath, Pedro J. Moreno. (2024)<br><strong>Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models</strong><br><button class=copy-to-clipboard title="Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 30<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17184v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17184v1.pdf filename=2402.17184v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The accuracy of end-to-end (E2E) <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> models continues to improve as they are scaled to larger sizes, with some now reaching billions of parameters. Widespread deployment and adoption of these models, however, requires computationally efficient strategies for decoding. In the present work, we study one such strategy: applying multiple frame reduction layers in the encoder to compress encoder outputs into a small number of output frames. While similar techniques have been investigated in previous work, we achieve dramatically more reduction than has previously been demonstrated through the use of multiple funnel reduction layers. Through ablations, we study the impact of various architectural choices in the encoder to identify the most effective strategies. We demonstrate that we can generate one encoder output frame for every 2.56 sec of input <b>speech,</b> <b>without</b> significantly affecting word error rate on a large-scale voice search task, while improving encoder and decoder latencies by 48% and 92% respectively, relative to a strong but computationally expensive baseline.</p></p class="citation"></blockquote><h3 id=5068--50341-unsupervised-multiple-choices-question-answering-via-universal-corpus-qin-zhang-et-al-2024>(50/68 | 50/341) Unsupervised multiple choices question answering via universal corpus (Qin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qin Zhang, Hao Ge, Xiaojun Chen, Meng Fang. (2024)<br><strong>Unsupervised multiple choices question answering via universal corpus</strong><br><button class=copy-to-clipboard title="Unsupervised multiple choices question answering via universal corpus" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 28<br>Keywords: Graph, Knowledge Graph, Unsupervised Learning, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17333v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17333v1.pdf filename=2402.17333v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>question</b> <b>answering</b> is a promising yet challenging task, which alleviates the burden of building large-scale annotated data in a new domain. It motivates us to study the <b>unsupervised</b> multiple-choice <b>question</b> <b>answering</b> (MCQA) problem. In this paper, we propose a novel framework designed to generate synthetic MCQA data barely based on contexts from the universal domain without relying on any form of manual annotation. Possible answers are extracted and used to produce related <b>questions,</b> <b>then</b> we leverage both named entities (NE) and <b>knowledge</b> <b>graphs</b> to discover plausible distractors to form complete synthetic samples. Experiments on multiple MCQA datasets demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=5168--51341-fine-grained-natural-language-inference-based-faithfulness-evaluation-for-diverse-summarisation-tasks-huajian-zhang-et-al-2024>(51/68 | 51/341) Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks (Huajian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huajian Zhang, Yumo Xu, Laura Perez-Beltrachini. (2024)<br><strong>Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks</strong><br><button class=copy-to-clipboard title="Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Natural Language Inference, Natural Language Inference<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17630v1.pdf filename=2402.17630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study existing approaches to leverage off-the-shelf <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses. That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences. We propose a novel approach, namely InFusE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses. Departing from previous studies which focus on single short document summarisation, we analyse <b>NLI</b> based faithfulness evaluation for diverse summarisation tasks. We introduce DiverSumm, a new <b>benchmark</b> comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation). In experiments, InFusE obtains superior performance across the different summarisation tasks. Our code and data are available at <a href=https://github.com/HJZnlp/infuse>https://github.com/HJZnlp/infuse</a>.</p></p class="citation"></blockquote><h3 id=5268--52341-kodialogbench-evaluating-conversational-understanding-of-language-models-with-korean-dialogue-benchmark-seongbo-jang-et-al-2024>(52/68 | 52/341) KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark (Seongbo Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seongbo Jang, Seonghyeon Lee, Hwanjo Yu. (2024)<br><strong>KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark</strong><br><button class=copy-to-clipboard title="KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Low-Resource, Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17377v1.pdf filename=2402.17377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As language models are often deployed as <b>chatbot</b> assistants, it becomes a virtue for models to engage in conversations in a user&rsquo;s first language. While these models are trained on a wide range of languages, a comprehensive evaluation of their proficiency in <b>low-resource</b> languages such as Korean has been lacking. In this work, we introduce KoDialogBench, a <b>benchmark</b> designed to assess language models&rsquo; conversational capabilities in Korean. To this end, we collect native Korean dialogues on daily topics from public sources, or translate dialogues from other languages. We then structure these conversations into diverse test datasets, spanning from dialogue comprehension to response selection tasks. Leveraging the proposed <b>benchmark,</b> we conduct extensive evaluations and analyses of various language models to measure a foundational understanding of Korean dialogues. Experimental results indicate that there exists significant room for improvement in models&rsquo; conversation skills. Furthermore, our in-depth comparisons across different language models highlight the effectiveness of recent training techniques in enhancing conversational proficiency. We anticipate that KoDialogBench will promote the progress towards conversation-aware Korean language models.</p></p class="citation"></blockquote><h3 id=5368--53341-re-ex-revising-after-explanation-reduces-the-factual-errors-in-llm-responses-juyeon-kim-et-al-2024>(53/68 | 53/341) Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses (Juyeon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juyeon Kim, Jeongeun Lee, Yoonho Chang, Chanyeol Choi, Junseong Kim, Jy-yong Sohn. (2024)<br><strong>Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses</strong><br><button class=copy-to-clipboard title="Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17097v1.pdf filename=2402.17097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mitigating hallucination issues is one of the main challenges of <b>LLMs</b> we need to overcome, in order to reliably use them in real-world scenarios. Recently, various methods are proposed to check the factual errors in the <b>LLM-generated</b> texts and revise them accordingly, to reduce the hallucination issue. In this paper, we propose Re-Ex, a method of revising <b>LLM-generated</b> texts, which introduces a novel step dubbed as the factual error explanation step. Re-Ex revises the initial response of <b>LLMs</b> using 3-steps: first, external tools are used to get the evidences on the factual errors in the response; second, <b>LLMs</b> are instructed to explain the problematic parts of the response based on the evidences gathered in the first step; finally, <b>LLMs</b> revise the response using the explanation obtained in the second step. In addition to the explanation step, we propose new <b>prompting</b> techniques to reduce the amount of tokens and wall-clock time required for the response revision process. Compared with existing methods including Factool, CoVE, and RARR, Re-Ex provides better revision performance with less time and fewer tokens in multiple <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=5468--54341-neural-automated-writing-evaluation-with-corrective-feedback-izia-xiaoxiao-wang-et-al-2024>(54/68 | 54/341) Neural Automated Writing Evaluation with Corrective Feedback (Izia Xiaoxiao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Izia Xiaoxiao Wang, Xihan Wu, Edith Coates, Min Zeng, Jiexin Kuang, Siliang Liu, Mengyang Qiu, Jungyeul Park. (2024)<br><strong>Neural Automated Writing Evaluation with Corrective Feedback</strong><br><button class=copy-to-clipboard title="Neural Automated Writing Evaluation with Corrective Feedback" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Grammatical Error Correction, Grammatical Error Correction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17613v1.pdf filename=2402.17613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The utilization of technology in second language learning and teaching has become ubiquitous. For the assessment of writing specifically, automated writing evaluation (AWE) and <b>grammatical</b> <b>error</b> <b>correction</b> <b>(GEC)</b> have become immensely popular and effective methods for enhancing writing proficiency and delivering instant and individualized feedback to learners. By leveraging the power of natural language processing (NLP) and machine learning algorithms, AWE and <b>GEC</b> systems have been developed separately to provide language learners with automated corrective feedback and more accurate and unbiased scoring that would otherwise be subject to examiners. In this paper, we propose an integrated system for automated writing evaluation with corrective feedback as a means of bridging the gap between AWE and <b>GEC</b> results for second language learners. This system enables language learners to simulate the essay writing tests: a student writes and submits an essay, and the system returns the assessment of the writing along with suggested <b>grammatical</b> <b>error</b> <b>corrections.</b> Given that automated scoring and <b>grammatical</b> <b>correction</b> <b>are</b> more efficient and cost-effective than human grading, this integrated system would also alleviate the burden of manually correcting innumerable essays.</p></p class="citation"></blockquote><h3 id=5568--55341-linguistic-knowledge-can-enhance-encoder-decoder-models-if-you-let-it-alessio-miaschi-et-al-2024>(55/68 | 55/341) Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It) (Alessio Miaschi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessio Miaschi, Felice Dell&rsquo;Orletta, Giulia Venturi. (2024)<br><strong>Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)</strong><br><button class=copy-to-clipboard title="Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, T5<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17608v1.pdf filename=2402.17608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically <b>T5,</b> with linguistic knowledge for the prediction of a target task. In particular, we investigate whether <b>fine-tuning</b> a <b>T5</b> model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual <b>T5</b> models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate <b>fine-tuning</b> has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.</p></p class="citation"></blockquote><h3 id=5668--56341-latent-attention-for-linear-time-transformers-rares-dolga-et-al-2024>(56/68 | 56/341) Latent Attention for Linear Time Transformers (Rares Dolga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rares Dolga, Marius Cobzarenco, David Barber. (2024)<br><strong>Latent Attention for Linear Time Transformers</strong><br><button class=copy-to-clipboard title="Latent Attention for Linear Time Transformers" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, stat-ML<br>Keyword Score: 20<br>Keywords: Transformer, Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17512v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17512v2.pdf filename=2402.17512v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The time complexity of the standard attention mechanism in a <b>transformer</b> scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our &ldquo;Latte <b>Transformer&rdquo;</b> model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of <b>language</b> <b>generation</b> tasks. Whilst next token prediction scales linearly with the sequence length for a standard <b>transformer,</b> a Latte <b>Transformer</b> requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.</p></p class="citation"></blockquote><h3 id=5768--57341-extreme-miscalibration-and-the-illusion-of-adversarial-robustness-vyas-raina-et-al-2024>(57/68 | 57/341) Extreme Miscalibration and the Illusion of Adversarial Robustness (Vyas Raina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vyas Raina, Samson Tan, Volkan Cevher, Aditya Rawal, Sheng Zha, George Karypis. (2024)<br><strong>Extreme Miscalibration and the Illusion of Adversarial Robustness</strong><br><button class=copy-to-clipboard title="Extreme Miscalibration and the Illusion of Adversarial Robustness" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17509v1.pdf filename=2402.17509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based Natural Language Processing (NLP) models are vulnerable to <b>adversarial</b> <b>attacks,</b> where small perturbations can cause a model to misclassify. <b>Adversarial</b> <b>Training</b> (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with <b>adversarial</b> <b>attack</b> search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the <b>adversarial</b> <b>attack</b> to find <b>adversarial</b> <b>examples.</b> Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can be scaled during \textit{training} to improve genuine robustness.</p></p class="citation"></blockquote><h3 id=5868--58341-fairbelief----assessing-harmful-beliefs-in-language-models-mattia-setzu-et-al-2024>(58/68 | 58/341) FairBelief &ndash; Assessing Harmful Beliefs in Language Models (Mattia Setzu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mattia Setzu, Marta Marchiori Manerba, Pasquale Minervini, Debora Nozza. (2024)<br><strong>FairBelief &ndash; Assessing Harmful Beliefs in Language Models</strong><br><button class=copy-to-clipboard title="FairBelief -- Assessing Harmful Beliefs in Language Models" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fairness, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17389v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17389v1.pdf filename=2402.17389v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful <b>fairness</b> auditing. This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage <b>prompting</b> to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a <b>fairness</b> dataset specifically designed to quantify LMs&rsquo; outputs&rsquo; hurtfulness. Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models. We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing tasks, they show hurtful beliefs about specific genders. Interestingly, training procedure and dataset, model scale, and architecture induce beliefs of different degrees of hurtfulness.</p></p class="citation"></blockquote><h3 id=5968--59341-llmguard-guarding-against-unsafe-llm-behavior-shubh-goyal-et-al-2024>(59/68 | 59/341) LLMGuard: Guarding Against Unsafe LLM Behavior (Shubh Goyal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubh Goyal, Medha Hira, Shubham Mishra, Sukriti Goyal, Arnav Goel, Niharika Dadu, Kirushikesh DB, Sameep Mehta, Nishtha Madaan. (2024)<br><strong>LLMGuard: Guarding Against Unsafe LLM Behavior</strong><br><button class=copy-to-clipboard title="LLMGuard: Guarding Against Unsafe LLM Behavior" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00826v1.pdf filename=2403.00826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although the rise of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in enterprise settings brings new opportunities and capabilities, it also brings challenges, such as the risk of generating inappropriate, biased, or misleading content that violates regulations and can have legal concerns. To alleviate this, we present &ldquo;LLMGuard&rdquo;, a tool that monitors user interactions with an <b>LLM</b> application and flags content against specific behaviours or conversation topics. To do this robustly, LLMGuard employs an ensemble of detectors.</p></p class="citation"></blockquote><h3 id=6068--60341-speak-out-of-turn-safety-vulnerability-of-large-language-models-in-multi-turn-dialogue-zhenhong-zhou-et-al-2024>(60/68 | 60/341) Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue (Zhenhong Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenhong Zhou, Jiuyang Xiang, Haopeng Chen, Quan Liu, Zherui Li, Sen Su. (2024)<br><strong>Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue</strong><br><button class=copy-to-clipboard title="Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17262v1.pdf filename=2402.17262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have been demonstrated to generate illegal or unethical responses, particularly when subjected to &ldquo;jailbreak.&rdquo; Research on jailbreak has highlighted the safety issues of <b>LLMs.</b> However, prior studies have predominantly focused on single-turn dialogue, ignoring the potential complexities and risks presented by multi-turn dialogue, a crucial mode through which humans derive information from <b>LLMs.</b> In this paper, we argue that humans could exploit multi-turn dialogue to induce <b>LLMs</b> into generating harmful information. <b>LLMs</b> may not intend to reject cautionary or borderline unsafe queries, even if each turn is closely served for one malicious purpose in a multi-turn dialogue. Therefore, by decomposing an unsafe query into several sub-queries for multi-turn dialogue, we induced <b>LLMs</b> to answer harmful sub-questions incrementally, culminating in an overall harmful response. Our experiments, conducted across a wide range of <b>LLMs,</b> indicate current inadequacies in the safety mechanisms of <b>LLMs</b> in multi-turn dialogue. Our findings expose vulnerabilities of <b>LLMs</b> in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=6168--61341-from-text-segmentation-to-smart-chaptering-a-novel-benchmark-for-structuring-video-transcriptions-fabian-retkowski-et-al-2024>(61/68 | 61/341) From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions (Fabian Retkowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabian Retkowski, Alexander Waibel. (2024)<br><strong>From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions</strong><br><button class=copy-to-clipboard title="From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Text Segmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17633v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17633v1.pdf filename=2402.17633v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text</b> <b>segmentation</b> is a fundamental task in natural language processing, where documents are split into contiguous sections. However, prior research in this area has been constrained by limited datasets, which are either small in scale, synthesized, or only contain well-structured documents. In this paper, we address these limitations by introducing a novel <b>benchmark</b> YTSeg focusing on spoken content that is inherently more unstructured and both topically and structurally diverse. As part of this work, we introduce an efficient hierarchical segmentation model MiniSeg, that outperforms state-of-the-art baselines. Lastly, we expand the notion of <b>text</b> <b>segmentation</b> to a more practical &ldquo;smart chaptering&rdquo; task that involves the segmentation of unstructured content, the generation of meaningful segment titles, and a potential real-time application of the models.</p></p class="citation"></blockquote><h3 id=6268--62341-clustering-document-parts-detecting-and-characterizing-influence-campaigns-from-documents-zhengxiang-wang-et-al-2024>(62/68 | 62/341) Clustering Document Parts: Detecting and Characterizing Influence Campaigns From Documents (Zhengxiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengxiang Wang, Owen Rambow. (2024)<br><strong>Clustering Document Parts: Detecting and Characterizing Influence Campaigns From Documents</strong><br><button class=copy-to-clipboard title="Clustering Document Parts: Detecting and Characterizing Influence Campaigns From Documents" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Clustering, Document Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17151v1.pdf filename=2402.17151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel <b>clustering</b> pipeline to detect and characterize influence campaigns from <b>documents.</b> <b>This</b> approach clusters parts of <b>document,</b> <b>detects</b> clusters that likely reflect an influence campaign, and then identifies <b>documents</b> <b>linked</b> to an influence campaign via their association with the high-influence clusters. Our approach outperforms both the direct <b>document-level</b> <b>classification</b> and the direct <b>document-level</b> <b>clustering</b> approach in predicting if a <b>document</b> <b>is</b> part of an influence campaign. We propose various novel techniques to enhance our pipeline, including using an existing event factuality prediction system to obtain <b>document</b> <b>parts,</b> and aggregating multiple <b>clustering</b> experiments to improve the performance of both cluster and <b>document</b> <b>classification.</b> Classifying <b>documents</b> <b>on</b> the top of <b>clustering</b> not only accurately extracts the parts of the <b>documents</b> <b>that</b> are relevant to influence campaigns, but also capture influence campaigns as a coordinated and holistic phenomenon. Our approach makes possible more fine-grained and interpretable characterizations of influence campaigns from documents.</p></p class="citation"></blockquote><h3 id=6368--63341-information-flow-routes-automatically-interpreting-language-models-at-scale-javier-ferrando-et-al-2024>(63/68 | 63/341) Information Flow Routes: Automatically Interpreting Language Models at Scale (Javier Ferrando et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Javier Ferrando, Elena Voita. (2024)<br><strong>Information Flow Routes: Automatically Interpreting Language Models at Scale</strong><br><button class=copy-to-clipboard title="Information Flow Routes: Automatically Interpreting Language Models at Scale" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Graph, LLaMA<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00824v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00824v1.pdf filename=2403.00824v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as <b>graphs</b> where nodes correspond to token representations and edges to operations inside the network. We automatically build these <b>graphs</b> in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Additionally, the applicability of our method is far beyond patching: we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can talk about model behavior in general, for specific types of predictions, or different domains. We experiment with <b>Llama</b> 2 and show that the role of some attention heads is overall important, e.g. previous token heads and subword merging heads. Next, we find similarities in <b>Llama</b> 2 behavior when handling tokens of the same part of speech. Finally, we show that some model components can be specialized on domains such as coding or multilingual texts.</p></p class="citation"></blockquote><h3 id=6468--64341-towards-optimal-learning-of-language-models-yuxian-gu-et-al-2024>(64/68 | 64/341) Towards Optimal Learning of Language Models (Yuxian Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxian Gu, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, Furu Wei. (2024)<br><strong>Towards Optimal Learning of Language Models</strong><br><button class=copy-to-clipboard title="Towards Optimal Learning of Language Models" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17759v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17759v2.pdf filename=2402.17759v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an &ldquo;LM-training-as-lossless-compression&rdquo; view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the <b>scaling</b> <b>law</b> of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at <a href=https://aka.ms/LearningLaw>https://aka.ms/LearningLaw</a>.</p></p class="citation"></blockquote><h3 id=6568--65341-retrieval-is-accurate-generation-bowen-cao-et-al-2024>(65/68 | 65/341) Retrieval is Accurate Generation (Bowen Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Cao, Deng Cai, Leyang Cui, Xuxin Cheng, Wei Bi, Yuexian Zou, Shuming Shi. (2024)<br><strong>Retrieval is Accurate Generation</strong><br><button class=copy-to-clipboard title="Retrieval is Accurate Generation" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17532v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17532v2.pdf filename=2402.17532v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Standard language models generate <b>text</b> <b>by</b> selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of <b>text</b> <b>can</b> be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended <b>text</b> <b>generation.</b> For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended <b>text</b> <b>generation.</b> Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines. In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift.</p></p class="citation"></blockquote><h3 id=6668--66341-spot-the-bot-coarse-grained-partition-of-semantic-paths-for-bots-and-humans-vasilii-a-gromov-et-al-2024>(66/68 | 66/341) Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and Humans (Vasilii A. Gromov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vasilii A. Gromov, Alexandra S. Kogan. (2024)<br><strong>Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and Humans</strong><br><button class=copy-to-clipboard title="Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and Humans" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: N-gram<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17392v1.pdf filename=2402.17392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, technology is rapidly advancing: bots are writing comments, articles, and reviews. Due to this fact, it is crucial to know if the text was written by a human or by a bot. This paper focuses on comparing structures of the coarse-grained partitions of semantic paths for human-written and bot-generated texts. We compare the clusterizations of datasets of <b>n-grams</b> from literary texts and texts generated by several bots. The hypothesis is that the structures and clusterizations are different. Our research supports the hypothesis. As the semantic structure may be different for different languages, we investigate Russian, English, German, and Vietnamese languages.</p></p class="citation"></blockquote><h3 id=6768--67341-a-dataset-for-metaphor-detection-in-early-medieval-hebrew-poetry-michael-toker-et-al-2024>(67/68 | 67/341) A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry (Michael Toker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Toker, Oren Mishali, Ophir Münz-Manor, Benny Kimelfeld, Yonatan Belinkov. (2024)<br><strong>A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry</strong><br><button class=copy-to-clipboard title="A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Metaphor Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17371v1.pdf filename=2402.17371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a large volume of late antique and medieval Hebrew texts. They represent a crucial linguistic and cultural bridge between Biblical and modern Hebrew. Poetry is prominent in these texts and one of its main haracteristics is the frequent use of <b>metaphor.</b> <b>Distinguishing</b> figurative and literal language use is a major task for scholars of the Humanities, especially in the fields of literature, linguistics, and hermeneutics. This paper presents a new, challenging dataset of late antique and medieval Hebrew poetry with expert annotations of <b>metaphor,</b> <b>as</b> well as some baseline results, which we hope will facilitate further research in this area.</p></p class="citation"></blockquote><h3 id=6868--68341-ravel-evaluating-interpretability-methods-on-disentangling-language-model-representations-jing-huang-et-al-2024>(68/68 | 68/341) RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations (Jing Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, Atticus Geiger. (2024)<br><strong>RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations</strong><br><button class=copy-to-clipboard title="RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17700v1.pdf filename=2402.17700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our <b>benchmark</b> at <a href=https://github.com/explanare/ravel>https://github.com/explanare/ravel</a>.</p></p class="citation"></blockquote><h2 id=csir-8>cs.IR (8)</h2><h3 id=18--69341-promptmm-multi-modal-knowledge-distillation-for-recommendation-with-prompt-tuning-wei-wei-et-al-2024>(1/8 | 69/341) PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning (Wei Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Wei, Jiabin Tang, Yangqin Jiang, Lianghao Xia, Chao Huang. (2024)<br><strong>PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning</strong><br><button class=copy-to-clipboard title="PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 83<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Model Compression, Multi-modal, Recommendation, Recommender System, BERT, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17188v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17188v2.pdf filename=2402.17188v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multimedia online platforms (e.g., Amazon, TikTok) have greatly benefited from the incorporation of multimedia (e.g., visual, textual, and acoustic) content into their personal <b>recommender</b> <b>systems.</b> These modalities provide intuitive semantics that facilitate modality-aware user preference <b>modeling.</b> <b>However,</b> two key challenges in <b>multi-modal</b> <b>recommenders</b> <b>remain</b> unresolved: i) The introduction of <b>multi-modal</b> encoders with a large number of additional parameters causes overfitting, given high-dimensional <b>multi-modal</b> features provided by extractors (e.g., ViT, <b>BERT).</b> ii) Side information inevitably introduces inaccuracies and redundancies, which skew the modality-interaction dependency from reflecting true user preference. To tackle these problems, we propose to simplify and empower <b>recommenders</b> <b>through</b> <b>Multi-modal</b> <b>Knowledge</b> <b>Distillation</b> (PromptMM) with the <b>prompt-tuning</b> that enables adaptive quality <b>distillation.</b> Specifically, PromptMM conducts <b>model</b> <b>compression</b> through <b>distilling</b> u-i edge relationship and <b>multi-modal</b> node content from cumbersome teachers to relieve students from the additional feature reduction parameters. To bridge the semantic gap between <b>multi-modal</b> context and collaborative signals for empowering the overfitting teacher, soft <b>prompt-tuning</b> is introduced to perform student task-adaptive. Additionally, to adjust the impact of inaccuracies in multimedia data, a disentangled <b>multi-modal</b> list-wise <b>distillation</b> is developed with modality-aware re-weighting mechanism. Experiments on real-world data demonstrate PromptMM&rsquo;s superiority over existing techniques. Ablation tests confirm the effectiveness of key components. Additional tests show the efficiency and effectiveness.</p></p class="citation"></blockquote><h3 id=28--70341-bases-large-scale-web-search-user-simulation-with-large-language-model-based-agents-ruiyang-ren-et-al-2024>(2/8 | 70/341) BASES: Large-scale Web Search User Simulation with Large Language Model based Agents (Ruiyang Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Hua Wu, Ji-Rong Wen, Haifeng Wang. (2024)<br><strong>BASES: Large-scale Web Search User Simulation with Large Language Model based Agents</strong><br><button class=copy-to-clipboard title="BASES: Large-scale Web Search User Simulation with Large Language Model based Agents" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 53<br>Keywords: Benchmarking, Simulation, Simulator, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17505v1.pdf filename=2402.17505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the excellent capacities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> it becomes feasible to develop <b>LLM-based</b> agents for reliable user <b>simulation.</b> Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct <b>large-scale</b> <b>user</b> <b>simulation</b> for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user <b>simulation</b> framework with <b>LLM-based</b> agents, designed to facilitate comprehensive <b>simulations</b> of web search user behaviors. Our <b>simulation</b> framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human <b>benchmarks</b> in both Chinese and English, demonstrating that BASES can effectively simulate <b>large-scale</b> <b>human-like</b> <b>search</b> behaviors. To further accommodate the research on web search, we develop WARRIORS, a new <b>large-scale</b> <b>dataset</b> <b>encompassing</b> web search user behaviors, including both Chinese and English versions, which can greatly bolster research in the field of <b>information</b> <b>retrieval.</b> Our code and data will be publicly released soon.</p></p class="citation"></blockquote><h3 id=38--71341-bivrec-bidirectional-view-based-multimodal-sequential-recommendation-jiaxi-hu-et-al-2024>(3/8 | 71/341) BiVRec: Bidirectional View-based Multimodal Sequential Recommendation (Jiaxi Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxi Hu, Jingtong Gao, Xiangyu Zhao, Yuehong Hu, Yuxuan Liang, Yiqi Wang, Ming He, Zitao Liu, Hongzhi Yin. (2024)<br><strong>BiVRec: Bidirectional View-based Multimodal Sequential Recommendation</strong><br><button class=copy-to-clipboard title="BiVRec: Bidirectional View-based Multimodal Sequential Recommendation" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17334v1.pdf filename=2402.17334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of <b>multimodal</b> information into sequential <b>recommender</b> <b>systems</b> has attracted significant attention in recent research. In the initial stages of <b>multimodal</b> sequential <b>recommendation</b> models, the mainstream paradigm was ID-dominant <b>recommendations,</b> wherein <b>multimodal</b> information was fused as side information. However, due to their limitations in terms of transferability and information intrusion, another paradigm emerged, wherein <b>multimodal</b> features were employed directly for <b>recommendation,</b> enabling <b>recommendation</b> across datasets. Nonetheless, it overlooked user ID information, resulting in low information utilization and high training costs. To this end, we propose an innovative framework, BivRec, that jointly trains the <b>recommendation</b> tasks in both ID and <b>multimodal</b> views, leveraging their synergistic relationship to enhance <b>recommendation</b> performance bidirectionally. To tackle the information heterogeneity issue, we first construct structured user interest representations and then learn the synergistic relationship between them. Specifically, BivRec comprises three modules: Multi-scale Interest Embedding, comprehensively modeling user interests by expanding user interaction sequences with multi-scale patching; Intra-View Interest Decomposition, constructing highly structured interest representations using carefully designed Gaussian attention and Cluster attention; and Cross-View Interest Learning, learning the synergistic relationship between the two <b>recommendation</b> views through coarse-grained overall semantic similarity and fine-grained interest allocation similarity BiVRec achieves state-of-the-art performance on five datasets and showcases various practical advantages.</p></p class="citation"></blockquote><h3 id=48--72341-re-modeling-personalized-item-frequency-information-for-next-basket-recommendation-sławomir-garcarz-et-al-2024>(4/8 | 72/341) [RE] Modeling Personalized Item Frequency Information for Next-basket Recommendation (Sławomir Garcarz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sławomir Garcarz, Avik Pal, Pim Praat. (2024)<br><strong>[RE] Modeling Personalized Item Frequency Information for Next-basket Recommendation</strong><br><button class=copy-to-clipboard title="[RE] Modeling Personalized Item Frequency Information for Next-basket Recommendation" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Fairness, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17925v1.pdf filename=2402.17925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on reproducing and extending the results of the paper: &ldquo;Modeling Personalized Item Frequency Information for Next-basket <b>Recommendation&rdquo;</b> which introduced the TIFU-KNN model and proposed to utilize Personalized Item Frequency (PIF) for Next Basket <b>Recommendation</b> (NBR). We utilized publicly available grocery shopping datasets used in the original paper and incorporated additional datasets to assess the generalizability of the findings. We evaluated the performance of the models using metrics such as Recall@K, NDCG@K, personalized-hit ratio (PHR), and Mean Reciprocal Rank (MRR). Furthermore, we conducted a thorough examination of <b>fairness</b> by considering user characteristics such as average basket size, item popularity, and novelty. Lastly, we introduced novel $\beta$-VAE architecture to model NBR. The experimental results confirmed that the reproduced model, TIFU-KNN, outperforms the baseline model, Personal Top Frequency, on various datasets and metrics. The findings also highlight the challenges posed by smaller basket sizes in some datasets and suggest avenues for future research to improve NBR performance.</p></p class="citation"></blockquote><h3 id=58--73341-natural-language-processing-methods-for-symbolic-music-generation-and-information-retrieval-a-survey-dinh-viet-toan-le-et-al-2024>(5/8 | 73/341) Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey (Dinh-Viet-Toan Le et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dinh-Viet-Toan Le, Louis Bigo, Mikaela Keller, Dorien Herremans. (2024)<br><strong>Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey</strong><br><button class=copy-to-clipboard title="Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs-SD, cs.IR, eess-AS<br>Keyword Score: 20<br>Keywords: Transformer, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17467v1.pdf filename=2402.17467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several adaptations of <b>Transformers</b> models have been developed in various domains since its breakthrough in Natural Language Processing (NLP). This trend has spread into the field of Music <b>Information</b> <b>Retrieval</b> (MIR), including studies processing music data. However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR. Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music. These analogies are also reflected through similar tasks in MIR and NLP. This survey reviews NLP methods applied to symbolic music generation and <b>information</b> <b>retrieval</b> studies following two axes. We first propose an overview of representations of symbolic music adapted from natural language sequential representations. Such representations are designed by considering the specificities of symbolic music. These representations are then processed by models. Such models, possibly originally developed for text and adapted for symbolic music, are trained on various tasks. We describe these models, in particular deep learning models, through different prisms, highlighting music-specialized mechanisms. We finally present a discussion surrounding the effective use of NLP tools for symbolic music data. This includes technical issues regarding NLP methods and fundamental differences between text and music, which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR.</p></p class="citation"></blockquote><h3 id=68--74341-difashion-towards-personalized-outfit-generation-and-recommendation-yiyan-xu-et-al-2024>(6/8 | 74/341) DiFashion: Towards Personalized Outfit Generation and Recommendation (Yiyan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyan Xu, Wenjie Wang, Fuli Feng, Yunshan Ma, Jizhi Zhang, Xiangnan He. (2024)<br><strong>DiFashion: Towards Personalized Outfit Generation and Recommendation</strong><br><button class=copy-to-clipboard title="DiFashion: Towards Personalized Outfit Generation and Recommendation" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Diffusion Model, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17279v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17279v2.pdf filename=2402.17279v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution of Outfit <b>Recommendation</b> (OR) in the realm of fashion has progressed through two distinct phases: Pre-defined Outfit <b>Recommendation</b> and Personalized Outfit Composition. Despite these advancements, both phases face limitations imposed by existing fashion products, hindering their effectiveness in meeting users&rsquo; diverse fashion needs. The emergence of AI-generated content has paved the way for OR to overcome these constraints, demonstrating the potential for personalized outfit generation. In pursuit of this, we introduce an innovative task named Generative Outfit <b>Recommendation</b> (GOR), with the goal of synthesizing a set of fashion images and assembling them to form visually harmonious outfits customized to individual users. The primary objectives of GOR revolve around achieving high fidelity, compatibility, and personalization of the generated outfits. To accomplish these, we propose DiFashion, a generative outfit recommender model that harnesses exceptional <b>diffusion</b> <b>models</b> for the simultaneous generation of multiple fashion images. To ensure the fulfillment of these objectives, three types of conditions are designed to guide the parallel generation process and Classifier-Free-Guidance are employed to enhance the alignment between generated images and conditions. DiFashion is applied to both personalized Fill-In-The-Blank and GOR tasks, and extensive experiments are conducted on the iFashion and Polyvore-U datasets. The results of quantitative and human-involved qualitative evaluations highlight the superiority of DiFashion over competitive baselines.</p></p class="citation"></blockquote><h3 id=78--75341-multimodal-learned-sparse-retrieval-with-probabilistic-expansion-control-thong-nguyen-et-al-2024>(7/8 | 75/341) Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control (Thong Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thong Nguyen, Mariya Hendriksen, Andrew Yates, Maarten de Rijke. (2024)<br><strong>Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control</strong><br><button class=copy-to-clipboard title="Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CV, cs-IR, cs.IR<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17535v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17535v1.pdf filename=2402.17535v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learned sparse retrieval (LSR) is a family of neural methods that encode queries and documents into sparse lexical vectors that can be indexed and retrieved efficiently with an inverted index. We explore the application of LSR to the <b>multi-modal</b> domain, with a focus on <b>text-image</b> retrieval. While LSR has seen success in text retrieval, its application in <b>multimodal</b> retrieval remains underexplored. Current approaches like LexLIP and STAIR require complex multi-step training on massive datasets. Our proposed approach efficiently transforms dense vectors from a frozen dense model into sparse lexical vectors. We address issues of high dimension co-activation and semantic deviation through a new training algorithm, using Bernoulli random variables to control query expansion. Experiments with two dense models (BLIP, ALBEF) and two datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively reduces co-activation and semantic deviation. Our best-performing sparsified model outperforms state-of-the-art <b>text-image</b> LSR models with a shorter training time and lower GPU memory requirements. Our approach offers an effective solution for training LSR retrieval models in <b>multimodal</b> settings. Our code and model checkpoints are available at github.com/thongnt99/lsr-multimodal</p></p class="citation"></blockquote><h3 id=88--76341-side-information-driven-session-based-recommendation-a-survey-xiaokun-zhang-et-al-2024>(8/8 | 76/341) Side Information-Driven Session-based Recommendation: A Survey (Xiaokun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaokun Zhang, Bo Xu, Chenliang Li, Yao Zhou, Liangyue Li, Hongfei Lin. (2024)<br><strong>Side Information-Driven Session-based Recommendation: A Survey</strong><br><button class=copy-to-clipboard title="Side Information-Driven Session-based Recommendation: A Survey" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 13<br>Keywords: Benchmarking, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17129v1.pdf filename=2402.17129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The session-based <b>recommendation</b> (SBR) garners increasing attention due to its ability to predict anonymous user intents within limited interactions. Emerging efforts incorporate various kinds of side information into their methods for enhancing task performance. In this survey, we thoroughly review the side information-driven session-based <b>recommendation</b> from a data-centric perspective. Our survey commences with an illustration of the motivation and necessity behind this research topic. This is followed by a detailed exploration of various <b>benchmarks</b> rich in side information, pivotal for advancing research in this field. Moreover, we delve into how these diverse types of side information enhance SBR, underscoring their characteristics and utility. A systematic review of research progress is then presented, offering an analysis of the most recent and representative developments within this topic. Finally, we present the future prospects of this vibrant topic.</p></p class="citation"></blockquote><h2 id=cscv-80>cs.CV (80)</h2><h3 id=180--77341-vision-transformers-with-natural-language-semantics-young-kyung-kim-et-al-2024>(1/80 | 77/341) Vision Transformers with Natural Language Semantics (Young Kyung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Young Kyung Kim, J. Matías Di Martino, Guillermo Sapiro. (2024)<br><strong>Vision Transformers with Natural Language Semantics</strong><br><button class=copy-to-clipboard title="Vision Transformers with Natural Language Semantics" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Distribution Shift, Distribution Shift, Out-of-distribution, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17863v1.pdf filename=2402.17863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tokens or patches within <b>Vision</b> <b>Transformers</b> (ViT) lack essential semantic information, unlike their counterparts in natural language processing (NLP). Typically, ViT tokens are associated with rectangular image patches that lack specific semantic context, making interpretation difficult and failing to effectively encapsulate information. We introduce a novel <b>transformer</b> model, Semantic <b>Vision</b> <b>Transformers</b> (sViT), which leverages recent progress on segmentation models to design novel tokenizer strategies. sViT effectively harnesses semantic information, creating an inductive bias reminiscent of <b>convolutional</b> <b>neural</b> <b>networks</b> while capturing global dependencies and contextual information within images that are characteristic of <b>transformers.</b> Through validation using real datasets, sViT demonstrates superiority over ViT, requiring less training data while maintaining similar or superior performance. Furthermore, sViT demonstrates significant superiority in <b>out-of-distribution</b> generalization and robustness to natural <b>distribution</b> <b>shifts,</b> attributed to its scale invariance semantic characteristic. Notably, the use of semantic tokens significantly enhances the model&rsquo;s interpretability. Lastly, the proposed paradigm facilitates the introduction of new and powerful augmentation techniques at the token (or segment) level, increasing training data diversity and generalization capabilities. Just as sentences are made of words, images are formed by semantic objects; our proposed methodology leverages recent progress in object segmentation and takes an important and natural step toward interpretable and robust <b>vision</b> <b>transformers.</b></p></p class="citation"></blockquote><h3 id=280--78341-fedlppa-learning-personalized-prompt-and-aggregation-for-federated-weakly-supervised-medical-image-segmentation-li-lin-et-al-2024>(2/80 | 78/341) FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation (Li Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Lin, Yixiang Liu, Jiewei Wu, Pujin Cheng, Zhiyuan Cai, Kenneth K. Y. Wong, Xiaoying Tang. (2024)<br><strong>FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 70<br>Keywords: Federated Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Prompt, Weakly Supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17502v1.pdf filename=2402.17502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) effectively mitigates the data silo challenge brought about by policies and privacy concerns, implicitly harnessing more data for deep model training. However, traditional centralized FL models grapple with diverse multi-center data, especially in the face of significant data heterogeneity, notably in medical contexts. In the realm of medical image segmentation, the growing imperative to curtail annotation costs has amplified the importance of <b>weakly-supervised</b> <b>techniques</b> <b>which</b> utilize sparse annotations such as points, scribbles, etc. A pragmatic FL paradigm shall accommodate diverse annotation formats across different sites, which research topic remains under-investigated. In such context, we propose a novel personalized FL framework with learnable <b>prompt</b> and aggregation (FedLPPA) to uniformly leverage heterogeneous <b>weak</b> <b>supervision</b> for medical image segmentation. In FedLPPA, a learnable universal knowledge <b>prompt</b> is maintained, complemented by multiple learnable personalized data distribution <b>prompts</b> and <b>prompts</b> representing the supervision sparsity. Integrated with sample features through a dual-attention mechanism, those <b>prompts</b> empower each local task decoder to adeptly adjust to both the local distribution and the supervision form. Concurrently, a dual-decoder strategy, predicated on <b>prompt</b> similarity, is introduced for enhancing the generation of pseudo-labels in <b>weakly-supervised</b> <b>learning,</b> <b>alleviating</b> overfitting and noise accumulation inherent to local data, while an adaptable aggregation method is employed to customize the task decoder on a parameter-wise basis. Extensive experiments on three distinct medical image segmentation tasks involving different modalities underscore the superiority of FedLPPA, with its efficacy closely parallels that of fully <b>supervised</b> centralized training. Our code and data will be available.</p></p class="citation"></blockquote><h3 id=380--79341-lspt-long-term-spatial-prompt-tuning-for-visual-representation-learning-shentong-mo-et-al-2024>(3/80 | 79/341) LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning (Shentong Mo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shentong Mo, Yansen Wang, Xufang Luo, Dongsheng Li. (2024)<br><strong>LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning</strong><br><button class=copy-to-clipboard title="LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 68<br>Keywords: Vision Transformer, Graph Attention Networks, Benchmarking, Representation Learning, Self-supervised Learning, Transformer, Prompt, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17406v1.pdf filename=2402.17406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual <b>Prompt</b> Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained <b>Vision</b> <b>Transformers</b> (ViTs) to downstream visual tasks using specialized learnable tokens termed as <b>prompts.</b> Contemporary VPT methodologies, especially when employed with <b>self-supervised</b> <b>vision</b> <b>transformers,</b> often default to the introduction of new learnable <b>prompts</b> or <b>gated</b> <b>prompt</b> tokens predominantly sourced from the model&rsquo;s previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of <b>prompts</b> within each <b>self-supervised</b> ViT. To bridge this crucial gap, we introduce Long-term Spatial <b>Prompt</b> Tuning (LSPT) - a revolutionary approach to visual <b>representation</b> <b>learning.</b> Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term <b>gated</b> <b>prompts.</b> This feature serves as temporal coding, curbing the risk of forgetting parameters acquired from earlier blocks. Further enhancing its prowess, LSPT brings into play patch tokens, serving as spatial coding. This is strategically designed to perpetually amass class-conscious features, thereby fortifying the model&rsquo;s prowess in distinguishing and identifying visual categories. To validate the efficacy of our proposed method, we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K <b>benchmarks.</b> Our empirical findings underscore the superiority of LSPT, showcasing its ability to set new <b>benchmarks</b> in visual <b>prompt</b> tuning performance.</p></p class="citation"></blockquote><h3 id=480--80341-a-large-scale-evaluation-of-pretraining-paradigms-for-the-detection-of-defects-in-electroluminescence-solar-cell-images-david-torpey-et-al-2024>(4/80 | 80/341) A Large-scale Evaluation of Pretraining Paradigms for the Detection of Defects in Electroluminescence Solar Cell Images (David Torpey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Torpey, Lawrence Pratt, Richard Klein. (2024)<br><strong>A Large-scale Evaluation of Pretraining Paradigms for the Detection of Defects in Electroluminescence Solar Cell Images</strong><br><button class=copy-to-clipboard title="A Large-scale Evaluation of Pretraining Paradigms for the Detection of Defects in Electroluminescence Solar Cell Images" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Benchmarking, Benchmarking, Out-of-distribution, Self-supervised Learning, Self-supervised Pre-training, Semi-Supervised Learning, Semi-Supervised Training, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17611v1.pdf filename=2402.17611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretraining has been shown to improve performance in many domains, including semantic segmentation, especially in domains with limited labelled data. In this work, we perform a large-scale evaluation and <b>benchmarking</b> of various pretraining methods for Solar Cell Defect Detection (SCDD) in electroluminescence images, a field with limited labelled datasets. We cover <b>supervised</b> training with semantic segmentation, <b>semi-supervised</b> <b>learning,</b> and two <b>self-supervised</b> <b>techniques.</b> We also experiment with both in-distribution and <b>out-of-distribution</b> (OOD) pretraining and observe how this affects downstream performance. The results suggest that <b>supervised</b> training on a large OOD dataset (COCO), <b>self-supervised</b> <b>pretraining</b> on a large OOD dataset (ImageNet), and <b>semi-supervised</b> <b>pretraining</b> (CCT) all yield statistically equivalent performance for mean Intersection over Union (mIoU). We achieve a new state-of-the-art for SCDD and demonstrate that certain pretraining schemes result in superior performance on underrepresented classes. Additionally, we provide a large-scale unlabelled EL image dataset of $22000$ images, and a $642$-image labelled semantic segmentation EL dataset, for further research in developing self- and <b>semi-supervised</b> <b>training</b> techniques in this domain.</p></p class="citation"></blockquote><h3 id=580--81341-shapellm-universal-3d-object-understanding-for-embodied-interaction-zekun-qi-et-al-2024>(5/80 | 81/341) ShapeLLM: Universal 3D Object Understanding for Embodied Interaction (Zekun Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, Kaisheng Ma. (2024)<br><strong>ShapeLLM: Universal 3D Object Understanding for Embodied Interaction</strong><br><button class=copy-to-clipboard title="ShapeLLM: Universal 3D Object Understanding for Embodied Interaction" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 64<br>Keywords: Benchmarking, Geometry, Knowledge Distillation, Multi-modal, Multi-modal, Grounding, Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17766v1.pdf filename=2402.17766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents ShapeLLM, the first 3D <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages. ShapeLLM is built upon an improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view image <b>distillation</b> for enhanced <b>geometry</b> understanding. By utilizing ReCon++ as the 3D point cloud input encoder for <b>LLMs,</b> ShapeLLM is trained on constructed <b>instruction-following</b> <b>data</b> and tested on our newly human-curated evaluation <b>benchmark,</b> 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3D <b>geometry</b> understanding and language-unified 3D interaction tasks, such as embodied visual <b>grounding.</b></p></p class="citation"></blockquote><h3 id=680--82341-vital-an-advanced-framework-for-automated-plant-disease-identification-in-leaf-images-using-vision-transformers-and-linear-projection-for-feature-reduction-abhishek-sebastian-et-al-2024>(6/80 | 82/341) ViTaL: An Advanced Framework for Automated Plant Disease Identification in Leaf Images Using Vision Transformers and Linear Projection For Feature Reduction (Abhishek Sebastian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Sebastian, Annis Fathima A, Pragna R, Madhan Kumar S, Yaswanth Kannan G, Vinay Murali. (2024)<br><strong>ViTaL: An Advanced Framework for Automated Plant Disease Identification in Leaf Images Using Vision Transformers and Linear Projection For Feature Reduction</strong><br><button class=copy-to-clipboard title="ViTaL: An Advanced Framework for Automated Plant Disease Identification in Leaf Images Using Vision Transformers and Linear Projection For Feature Reduction" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17424v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17424v2.pdf filename=2402.17424v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our paper introduces a robust framework for the automated identification of diseases in plant leaf images. The framework incorporates several key stages to enhance disease recognition accuracy. In the pre-processing phase, a thumbnail resizing technique is employed to resize images, minimizing the loss of critical image details while ensuring computational efficiency. Normalization procedures are applied to standardize image data before feature extraction. Feature extraction is facilitated through a novel framework built upon <b>Vision</b> <b>Transformers,</b> a state-of-the-art approach in image analysis. Additionally, alternative versions of the framework with an added layer of linear projection and blockwise linear projections are explored. This comparative analysis allows for the evaluation of the impact of linear projection on feature extraction and overall model performance. To assess the effectiveness of the proposed framework, various <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> architectures are utilized, enabling a comprehensive evaluation of linear projection&rsquo;s influence on key evaluation metrics. The findings demonstrate the efficacy of the proposed framework, with the top-performing model achieving a Hamming loss of 0.054. Furthermore, we propose a novel hardware design specifically tailored for scanning diseased leaves in an omnidirectional fashion. The hardware implementation utilizes a Raspberry Pi Compute Module to address low-memory configurations, ensuring practicality and affordability. This innovative hardware solution enhances the overall feasibility and accessibility of the proposed automated disease identification system. This research contributes to the field of agriculture by offering valuable insights and tools for the early detection and management of plant diseases, potentially leading to improved crop yields and enhanced food security.</p></p class="citation"></blockquote><h3 id=780--83341-vcd-knowledge-base-guided-visual-commonsense-discovery-in-images-xiangqing-shen-et-al-2024>(7/80 | 83/341) VCD: Knowledge Base Guided Visual Commonsense Discovery in Images (Xiangqing Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangqing Shen, Yurun Song, Siwei Wu, Rui Xia. (2024)<br><strong>VCD: Knowledge Base Guided Visual Commonsense Discovery in Images</strong><br><button class=copy-to-clipboard title="VCD: Knowledge Base Guided Visual Commonsense Discovery in Images" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: GPT, Question Answering, Reasoning, Visual Question Answering, Instruction Tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17213v1.pdf filename=2402.17213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Visual</b> <b>commonsense</b> <b>contains</b> knowledge about object properties, relationships, and behaviors in <b>visual</b> <b>data.</b> <b>Discovering</b> <b>visual</b> <b>commonsense</b> <b>can</b> provide a more comprehensive and richer understanding of images, and enhance the <b>reasoning</b> and decision-making capabilities of computer vision systems. However, the <b>visual</b> <b>commonsense</b> <b>defined</b> in existing <b>visual</b> <b>commonsense</b> <b>discovery</b> studies is coarse-grained and incomplete. In this work, we draw inspiration from a commonsense knowledge base ConceptNet in natural language processing, and systematically define the types of <b>visual</b> <b>commonsense.</b> <b>Based</b> on this, we introduce a new task, <b>Visual</b> <b>Commonsense</b> <b>Discovery</b> (VCD), aiming to extract fine-grained commonsense of different types contained within different objects in the image. We accordingly construct a dataset (VCDD) from <b>Visual</b> <b>Genome</b> <b>and</b> ConceptNet for VCD, featuring over 100,000 images and 14 million object-commonsense pairs. We furthermore propose a generative model (VCDM) that integrates a <b>vision-language</b> model with <b>instruction</b> <b>tuning</b> to tackle VCD. Automatic and human evaluations demonstrate VCDM&rsquo;s proficiency in VCD, particularly outperforming <b>GPT-4V</b> in implicit commonsense discovery. The value of VCD is further demonstrated by its application to two downstream tasks, including <b>visual</b> <b>commonsense</b> <b>evaluation</b> and <b>visual</b> <b>question</b> <b>answering.</b> The data and code will be made available on GitHub.</p></p class="citation"></blockquote><h3 id=880--84341-a-novel-image-space-formalism-of-fourier-domain-interpolation-neural-networks-for-noise-propagation-analysis-peter-dawood-et-al-2024>(8/80 | 84/341) A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis (Peter Dawood et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Dawood, Felix Breuer, Istvan Homolya, Jannik Stebani, Maximilian Gram, Peter M. Jakob, Moritz Zaiss, Martin Blaimer. (2024)<br><strong>A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis</strong><br><button class=copy-to-clipboard title="A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, physics-med-ph<br>Keyword Score: 55<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17410v1.pdf filename=2402.17410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: To develop an image space formalism of multi-layer <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during <b>CNN</b> inference. Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks. This operation is transformed into a <b>convolution</b> in the image space. After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space. This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics. Monte-Carlo <b>simulations</b> and numerical approaches based on auto-differentiation were used for validation. The framework was tested on retrospectively undersampled invivo brain images. Results: Inferences conducted in the image domain are quasi-identical to inferences in the k-space, underlined by corresponding quantitative metrics. Noise variance maps obtained from the analytical expression correspond with those obtained via Monte-Carlo <b>simulations,</b> as well as via an auto-differentiation approach. The noise resilience is well characterized, as in the case of classical Parallel Imaging. Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes in variance maps obtained via Monte-Carlo <b>simulations.</b> Conclusion: The quasi-equivalent image space formalism for neural networks for k-space interpolation enables fast and accurate description of the noise characteristics during <b>CNN</b> inference, analogous to <b>geometry-factor</b> maps in traditional parallel imaging methods.</p></p class="citation"></blockquote><h3 id=980--85341-carzero-cross-attention-alignment-for-radiology-zero-shot-classification-haoran-lai-et-al-2024>(9/80 | 85/341) CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification (Haoran Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoran Lai, Qingsong Yao, Zihang Jiang, Rongsheng Wang, Zhiyang He, Xiaodong Tao, S. Kevin Zhou. (2024)<br><strong>CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification</strong><br><button class=copy-to-clipboard title="CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Zero-shot, Image2text, Large Language Model, Prompt, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17417v1.pdf filename=2402.17417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of <b>Zero-Shot</b> <b>Learning</b> in the medical domain has been driven forward by using pre-trained models on <b>large-scale</b> <b>image-text</b> <b>pairs,</b> focusing on <b>image-text</b> alignment. However, existing methods primarily rely on cosine similarity for alignment, which may not fully capture the complex relationship between medical images and reports. To address this gap, we introduce a novel approach called Cross-Attention Alignment for Radiology <b>Zero-Shot</b> <b>Classification</b> (CARZero). Our approach innovatively leverages cross-attention mechanisms to process image and report features, creating a Similarity Representation that more accurately reflects the intricate relationships in medical semantics. This representation is then linearly projected to form an <b>image-text</b> similarity matrix for cross-modality alignment. Additionally, recognizing the pivotal role of <b>prompt</b> selection in <b>zero-shot</b> <b>learning,</b> CARZero incorporates a <b>Large</b> <b>Language</b> <b>Model-based</b> <b>prompt</b> alignment strategy. This strategy standardizes diverse diagnostic expressions into a unified format for both training and inference phases, overcoming the challenges of manual <b>prompt</b> design. Our approach is simple yet effective, demonstrating state-of-the-art performance in <b>zero-shot</b> <b>classification</b> on five official chest radiograph diagnostic test sets, including remarkable results on datasets with long-tail distributions of rare diseases. This achievement is attributed to our new <b>image-text</b> alignment strategy, which effectively addresses the complex relationship between medical images and reports.</p></p class="citation"></blockquote><h3 id=1080--86341-video-as-the-new-language-for-real-world-decision-making-sherry-yang-et-al-2024>(10/80 | 86/341) Video as the New Language for Real-World Decision Making (Sherry Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, Dale Schuurmans. (2024)<br><strong>Video as the New Language for Real-World Decision Making</strong><br><button class=copy-to-clipboard title="Video as the New Language for Real-World Decision Making" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Self-supervised Learning, Self-supervised Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17139v1.pdf filename=2402.17139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Both text and video data are abundant on the internet and support large-scale <b>self-supervised</b> <b>learning</b> through next token or frame prediction. However, they have not been equally leveraged: language models have had significant real-world impact, whereas video generation has remained largely limited to media entertainment. Yet video data captures important information about the physical world that is difficult to express in language. To address this gap, we discuss an under-appreciated opportunity to extend video generation to solve tasks in the real world. We observe how, akin to language, video can serve as a unified interface that can absorb internet knowledge and represent diverse tasks. Moreover, we demonstrate how, like language models, video generation can serve as planners, agents, compute engines, and environment simulators through techniques such as <b>in-context</b> <b>learning,</b> planning and <b>reinforcement</b> <b>learning.</b> We identify major impact opportunities in domains such as robotics, self-driving, and science, supported by recent work that demonstrates how such advanced capabilities in video generation are plausibly within reach. Lastly, we identify key challenges in video generation that mitigate progress. Addressing these challenges will enable video generation models to demonstrate unique value alongside language models in a wider array of AI applications.</p></p class="citation"></blockquote><h3 id=1180--87341-an-empirical-study-of-the-generalization-ability-of-lidar-3d-object-detectors-to-unseen-domains-george-eskandar-et-al-2024>(11/80 | 87/341) An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains (George Eskandar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George Eskandar, Chongzhe Zhang, Abhishek Kaushik, Karim Guirguis, Mohamed Sayed, Bin Yang. (2024)<br><strong>An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains</strong><br><button class=copy-to-clipboard title="An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 48<br>Keywords: Benchmarking, Black Box, Convolutional Neural Network, Data Augmentation, Transformer, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17562v1.pdf filename=2402.17562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D Object Detectors (3D-OD) are crucial for understanding the environment in many robotic tasks, especially autonomous driving. Including 3D information via Lidar sensors improves accuracy greatly. However, such detectors perform poorly on <b>domains</b> <b>they</b> were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications. There exist methods to adapt 3D-ODs to these <b>domains;</b> <b>however,</b> these methods treat 3D-ODs as a <b>black</b> <b>box,</b> neglecting underlying architectural decisions and source-domain training strategies. Instead, we dive deep into the details of 3D-ODs, focusing our efforts on fundamental factors that influence robustness prior to <b>domain</b> <b>adaptation.</b> We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-OD robustness and <b>domain</b> <b>adaptation:</b> architecture, voxel encoding, <b>data</b> <b>augmentations,</b> and anchor strategies. We assess their impact on the robustness of nine state-of-the-art 3D-ODs across six <b>benchmarks</b> encompassing three types of <b>domain</b> <b>gaps</b> - sensor type, weather, and location. Our main findings are: (1) <b>transformer</b> backbones with local point features are more robust than 3D <b>CNNs,</b> (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather <b>data</b> <b>than</b> on training with bad weather <b>data.</b> <b>We</b> outline our main conclusions and findings to provide practical guidance on developing more robust 3D-ODs.</p></p class="citation"></blockquote><h3 id=1280--88341-demonstrating-and-reducing-shortcuts-in-vision-language-representation-learning-maurits-bleeker-et-al-2024>(12/80 | 88/341) Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning (Maurits Bleeker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maurits Bleeker, Mariya Hendriksen, Andrew Yates, Maarten de Rijke. (2024)<br><strong>Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning</strong><br><button class=copy-to-clipboard title="Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Contrastive Learning, Fine-tuning, Representation Learning, Image2text, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17510v1.pdf filename=2402.17510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> models (VLMs) mainly rely on <b>contrastive</b> <b>training</b> to learn general-purpose <b>representations</b> <b>of</b> images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether <b>contrastive</b> <b>losses</b> are sufficient for learning task-optimal <b>representations</b> <b>that</b> contain all the information provided by the captions or whether the <b>contrastive</b> <b>learning</b> setup encourages the learning of a simple shortcut that minimizes <b>contrastive</b> <b>loss.</b> We introduce synthetic shortcuts for <b>vision-language:</b> a training and evaluation framework where we inject synthetic shortcuts into <b>image-text</b> data. We show that <b>contrastive</b> <b>VLMs</b> trained from scratch or <b>fine-tuned</b> with data containing these synthetic shortcuts mainly learn features that represent the shortcut. Hence, <b>contrastive</b> <b>losses</b> are not sufficient to learn task-optimal <b>representations,</b> <b>i.e.,</b> <b>representations</b> <b>that</b> contain all task-relevant information shared between the image and associated captions. We examine two methods to reduce shortcut learning in our training and evaluation framework: (i) latent target decoding and (ii) implicit feature modification. We show empirically that both methods improve performance on the evaluation task, but only partly reduce shortcut learning when training and evaluating with our shortcut learning framework. Hence, we show the difficulty and challenge of our shortcut learning framework for <b>contrastive</b> <b>vision-language</b> <b>representation</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=1380--89341-divavatar-diverse-3d-avatar-generation-with-a-single-prompt-weijing-tao-et-al-2024>(13/80 | 89/341) DivAvatar: Diverse 3D Avatar Generation with a Single Prompt (Weijing Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weijing Tao, Biwen Lei, Kunhao Liu, Shijian Lu, Miaomiao Cui, Xuansong Xie, Chunyan Miao. (2024)<br><strong>DivAvatar: Diverse 3D Avatar Generation with a Single Prompt</strong><br><button class=copy-to-clipboard title="DivAvatar: Diverse 3D Avatar Generation with a Single Prompt" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Diffusion Model, Fine-tuning, Fine-tuning, Geometry, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17292v1.pdf filename=2402.17292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-Avatar generation has recently made significant strides due to advancements in <b>diffusion</b> <b>models.</b> However, most existing work remains constrained by limited diversity, producing avatars with subtle differences in appearance for a given text <b>prompt.</b> We design DivAvatar, a novel framework that generates diverse avatars, empowering 3D creatives with a multitude of distinct and richly varied 3D avatars from a single text <b>prompt.</b> Different from most existing work that exploits scene-specific 3D representations such as NeRF, DivAvatar <b>finetunes</b> a 3D generative model (i.e., EVA3D), allowing diverse avatar generation from simply noise sampling in inference time. DivAvatar has two key designs that help achieve generation diversity and visual quality. The first is a noise sampling technique during training phase which is critical in generating diverse appearances. The second is a semantic-aware zoom mechanism and a novel depth loss, the former producing appearances of high textual fidelity by separate <b>fine-tuning</b> of specific body parts and the latter improving <b>geometry</b> quality greatly by smoothing the generated mesh in the features space. Extensive experiments show that DivAvatar is highly versatile in generating avatars of diverse appearances.</p></p class="citation"></blockquote><h3 id=1480--90341-reprune-channel-pruning-via-kernel-representative-selection-mincheol-park-et-al-2024>(14/80 | 90/341) REPrune: Channel Pruning via Kernel Representative Selection (Mincheol Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mincheol Park, Dongjin Kim, Cheonjun Park, Yuna Park, Gyeong Eun Gong, Won Woo Ro, Suhyun Kim. (2024)<br><strong>REPrune: Channel Pruning via Kernel Representative Selection</strong><br><button class=copy-to-clipboard title="REPrune: Channel Pruning via Kernel Representative Selection" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Clustering, Convolution, Convolutional Neural Network, Convolutional Neural Network, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17862v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17862v2.pdf filename=2402.17862v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Channel <b>pruning</b> is widely accepted to accelerate modern <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs).</b> The resulting pruned model benefits from its immediate deployment on general-purpose software and hardware resources. However, its large <b>pruning</b> granularity, specifically at the unit of a <b>convolution</b> filter, often leads to undesirable accuracy drops due to the inflexibility of deciding how and where to introduce sparsity to the <b>CNNs.</b> In this paper, we propose REPrune, a novel channel <b>pruning</b> technique that emulates kernel <b>pruning,</b> fully exploiting the finer but structured granularity. REPrune identifies similar kernels within each channel using agglomerative <b>clustering.</b> Then, it selects filters that maximize the incorporation of kernel representatives while optimizing the maximum cluster coverage problem. By integrating with a simultaneous training-pruning paradigm, REPrune promotes efficient, progressive <b>pruning</b> throughout training <b>CNNs,</b> avoiding the conventional train-prune-finetune sequence. Experimental results highlight that REPrune performs better in computer vision tasks than existing methods, effectively achieving a balance between acceleration ratio and performance retention.</p></p class="citation"></blockquote><h3 id=1580--91341-arcsin-adaptive-ranged-cosine-similarity-injected-noise-for-language-driven-visual-tasks-yang-liu-et-al-2024>(15/80 | 91/341) ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks (Yang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Liu, Xiaomin Yu, Gongyu Zhang, Christos Bergeles, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin. (2024)<br><strong>ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks</strong><br><button class=copy-to-clipboard title="ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Zero-shot, Question Answering, Visual Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17298v1.pdf filename=2402.17298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we address the challenging task of bridging the modality gap between learning from language and inference for <b>visual</b> <b>tasks,</b> <b>including</b> <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA),</b> Image Captioning (IC) and <b>Visual</b> <b>Entailment</b> <b>(VE).</b> We train models for these tasks in a <b>zero-shot</b> cross-modal transfer setting, a domain where the previous state-of-the-art method relied on the fixed scale noise injection, often compromising the semantic content of the original modality embedding. To combat it, we propose a novel method called Adaptive ranged cosine Similarity injected noise (ArcSin). First, we introduce an innovative adaptive noise scale that effectively generates the textual elements with more variability while preserving the original text feature&rsquo;s integrity. Second, a similarity pool strategy is employed, expanding the domain generalization potential by broadening the overall noise scale. This dual strategy effectively widens the scope of the original domain while safeguarding content integrity. Our empirical results demonstrate that these models closely rival those trained on images in terms of performance. Specifically, our method exhibits substantial improvements over the previous state-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively. Additionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and 1.4 pp in accuracy for <b>VQA,</b> <b>VQA-E,</b> and VE, respectively, pushing the boundaries of what is achievable within the constraints of image-trained model <b>benchmarks.</b> The code will be released.</p></p class="citation"></blockquote><h3 id=1680--92341-towards-fairness-aware-adversarial-learning-yanghao-zhang-et-al-2024>(16/80 | 92/341) Towards Fairness-Aware Adversarial Learning (Yanghao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanghao Zhang, Tianle Zhang, Ronghui Mu, Xiaowei Huang, Wenjie Ruan. (2024)<br><strong>Towards Fairness-Aware Adversarial Learning</strong><br><button class=copy-to-clipboard title="Towards Fairness-Aware Adversarial Learning" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Adversarial Learning, Fairness, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17729v1.pdf filename=2402.17729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>adversarial</b> <b>training</b> (AT) has proven effective in enhancing the model&rsquo;s robustness, the recently revealed issue of <b>fairness</b> in robustness has not been well addressed, i.e. the robust accuracy varies significantly among different categories. In this paper, instead of uniformly evaluating the model&rsquo;s average class performance, we delve into the issue of robust <b>fairness,</b> by considering the worst-case distribution across various classes. We propose a novel learning paradigm, named <b>Fairness-Aware</b> <b>Adversarial</b> <b>Learning</b> (FAAL). As a generalization of conventional AT, we re-define the problem of <b>adversarial</b> <b>training</b> as a min-max-max framework, to ensure both robustness and <b>fairness</b> of the trained model. Specifically, by taking advantage of distributional robust optimization, our method aims to find the worst distribution among different categories, and the solution is guaranteed to obtain the upper bound performance with high probability. In particular, FAAL can <b>fine-tune</b> an unfair robust model to be fair within only two epochs, without compromising the overall clean and robust accuracies. Extensive experiments on various image datasets validate the superior performance and efficiency of the proposed FAAL compared to other state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=1780--93341-structure-guided-adversarial-training-of-diffusion-models-ling-yang-et-al-2024>(17/80 | 93/341) Structure-Guided Adversarial Training of Diffusion Models (Ling Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui. (2024)<br><strong>Structure-Guided Adversarial Training of Diffusion Models</strong><br><button class=copy-to-clipboard title="Structure-Guided Adversarial Training of Diffusion Models" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Adversarial Learning, Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17563v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17563v2.pdf filename=2402.17563v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided <b>Adversarial</b> <b>training</b> of <b>Diffusion</b> <b>Models</b> (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate <b>adversarial</b> <b>training</b> of the <b>diffusion</b> <b>generator</b> against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing <b>diffusion</b> <b>transformers</b> (DiT) and outperforms existing methods in image generation and cross-domain <b>fine-tuning</b> tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively.</p></p class="citation"></blockquote><h3 id=1880--94341-diffusekrona-a-parameter-efficient-fine-tuning-method-for-personalized-diffusion-models-shyam-marjit-et-al-2024>(18/80 | 94/341) DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Models (Shyam Marjit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shyam Marjit, Harshit Singh, Nityanand Mathur, Sayak Paul, Chia-Mu Yu, Pin-Yu Chen. (2024)<br><strong>DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Models</strong><br><button class=copy-to-clipboard title="DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Models" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17412v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17412v2.pdf filename=2402.17412v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of subject-driven <b>text-to-image</b> (T2I) generative models, recent developments like DreamBooth and BLIP-Diffusion have led to impressive results yet encounter limitations due to their intensive <b>fine-tuning</b> demands and substantial parameter requirements. While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis. Addressing these constraints, we introduce \textbf{\textit{DiffuseKronA}}, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by 35% and 99.947% compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis. Crucially, \textit{DiffuseKronA} mitigates the issue of hyperparameter sensitivity, delivering consistent high-quality generations across a wide range of hyperparameters, thereby diminishing the necessity for extensive <b>fine-tuning.</b> Furthermore, a more controllable decomposition makes \textit{DiffuseKronA} more interpretable and even can achieve up to a 50% reduction with results comparable to LoRA-Dreambooth. Evaluated against diverse and complex input images and text <b>prompts,</b> \textit{DiffuseKronA} consistently outperforms existing models, producing diverse images of higher quality with improved fidelity and a more accurate color distribution of objects, all the while upholding exceptional parameter efficiency, thus presenting a substantial advancement in the field of T2I generative modeling. Our project page, consisting of links to the code, and pre-trained checkpoints, is available at <a href=https://diffusekrona.github.io/>https://diffusekrona.github.io/</a>.</p></p class="citation"></blockquote><h3 id=1980--95341-towards-robust-and-efficient-cloud-edge-elastic-model-adaptation-via-selective-entropy-distillation-yaofo-chen-et-al-2024>(19/80 | 95/341) Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation (Yaofo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaofo Chen, Shuaicheng Niu, Shoukai Xu, Hengjie Song, Yaowei Wang, Mingkui Tan. (2024)<br><strong>Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation</strong><br><button class=copy-to-clipboard title="Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Foundation Model, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17316v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17316v2.pdf filename=2402.17316v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its <b>distilled</b> ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as <b>distribution</b> <b>shifts),</b> which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by <b>distilling</b> from the stronger <b>foundation</b> <b>model</b> to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.</p></p class="citation"></blockquote><h3 id=2080--96341-feature-re-embedding-towards-foundation-model-level-performance-in-computational-pathology-wenhao-tang-et-al-2024>(20/80 | 96/341) Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology (Wenhao Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhao Tang, Fengtao Zhou, Sheng Huang, Xiang Zhu, Yi Zhang, Bo Liu. (2024)<br><strong>Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology</strong><br><button class=copy-to-clipboard title="Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Foundation Model, Multiple Instance Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17228v1.pdf filename=2402.17228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multiple</b> <b>instance</b> <b>learning</b> (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more. However, the existing MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a <b>foundation</b> <b>model.</b> This approach lacks the capability for feature <b>fine-tuning</b> within the specific downstream tasks, limiting its adaptability and performance. To address this issue, we propose a Re-embedded Regional <b>Transformer</b> (R$^2$T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions. Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R$^2$T is tailored to re-embed instance features online. It serves as a portable module that can seamlessly integrate into mainstream MIL models. Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of <b>foundation</b> <b>model</b> features, and further enhances the performance of <b>foundation</b> <b>model</b> features; 2) the R$^2$T can introduce more significant performance improvements to various MIL models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest methods by a large margin. The code is available at:~\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}.</p></p class="citation"></blockquote><h3 id=2180--97341-box-it-to-bind-it-unified-layout-control-and-attribute-binding-in-t2i-diffusion-models-ashkan-taghipour-et-al-2024>(21/80 | 97/341) Box It to Bind It: Unified Layout Control and Attribute Binding in T2I Diffusion Models (Ashkan Taghipour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Aref Miri Rekavandi, Hamid Laga, Farid Boussaid. (2024)<br><strong>Box It to Bind It: Unified Layout Control and Attribute Binding in T2I Diffusion Models</strong><br><button class=copy-to-clipboard title="Box It to Bind It: Unified Layout Control and Attribute Binding in T2I Diffusion Models" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17910v1.pdf filename=2402.17910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While latent <b>diffusion</b> <b>models</b> (LDMs) excel at creating imaginative images, they often lack precision in semantic fidelity and spatial control over where objects are generated. To address these deficiencies, we introduce the Box-it-to-Bind-it (B2B) module - a novel, training-free approach for improving spatial control and semantic accuracy in <b>text-to-image</b> (T2I) <b>diffusion</b> <b>models.</b> B2B targets three key challenges in T2I: catastrophic neglect, attribute binding, and layout guidance. The process encompasses two main steps: i) Object generation, which adjusts the latent encoding to guarantee object generation and directs it within specified bounding boxes, and ii) attribute binding, guaranteeing that generated objects adhere to their specified attributes in the <b>prompt.</b> B2B is designed as a compatible plug-and-play module for existing T2I models, markedly enhancing model performance in addressing the key challenges. We evaluate our technique using the established CompBench and TIFA score <b>benchmarks,</b> demonstrating significant performance improvements compared to existing methods. The source code will be made publicly available at <a href=https://github.com/nextaistudio/BoxIt2BindIt>https://github.com/nextaistudio/BoxIt2BindIt</a>.</p></p class="citation"></blockquote><h3 id=2280--98341-lane2seq-towards-unified-lane-detection-via-sequence-generation-kunyang-zhou-2024>(22/80 | 98/341) Lane2Seq: Towards Unified Lane Detection via Sequence Generation (Kunyang Zhou, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kunyang Zhou. (2024)<br><strong>Lane2Seq: Towards Unified Lane Detection via Sequence Generation</strong><br><button class=copy-to-clipboard title="Lane2Seq: Towards Unified Lane Detection via Sequence Generation" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, LLaMA, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17172v1.pdf filename=2402.17172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a novel sequence generation-based framework for lane detection, called Lane2Seq. It unifies various lane detection formats by casting lane detection as a sequence generation task. This is different from previous lane detection methods, which depend on well-designed task-specific head networks and corresponding loss functions. Lane2Seq only adopts a plain <b>transformer-based</b> encoder-decoder architecture with a simple cross-entropy loss. Additionally, we propose a new multi-format model tuning based on <b>reinforcement</b> <b>learning</b> to incorporate the task-specific knowledge into Lane2Seq. Experimental results demonstrate that such a simple sequence generation paradigm not only unifies lane detection but also achieves competitive performance on <b>benchmarks.</b> For example, Lane2Seq gets 97.95% and 97.42% F1 score on Tusimple and <b>LLAMAS</b> datasets, establishing a new state-of-the-art result for two <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=2380--99341-weakly-supervised-co-training-with-swapping-assignments-for-semantic-segmentation-xinyu-yang-et-al-2024>(23/80 | 99/341) Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation (Xinyu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Yang, Hossein Rahmani, Sue Black, Bryan M. Williams. (2024)<br><strong>Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Supervised Learning, Weakly-supervised Learning, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17891v1.pdf filename=2402.17891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Class activation maps (CAMs) are commonly employed in weakly <b>supervised</b> semantic segmentation (WSSS) to produce pseudo-labels. Due to incomplete or excessive class activation, existing studies often resort to offline CAM refinement, introducing additional stages or proposing offline modules. This can cause optimization difficulties for single-stage methods and limit generalizability. In this study, we aim to reduce the observed CAM inconsistency and error to mitigate reliance on refinement processes. We propose an end-to-end WSSS model incorporating guided CAMs, wherein our segmentation model is trained while concurrently optimizing CAMs online. Our method, Co-training with Swapping Assignments (CoSA), leverages a dual-stream framework, where one sub-network learns from the swapped assignments generated by the other. We introduce three techniques: i) soft <b>perplexity-based</b> regularization to penalize uncertain regions; ii) a threshold-searching approach to dynamically revise the confidence threshold; and iii) contrastive separation to address the coexistence problem. CoSA demonstrates exceptional performance, achieving mIoU of 76.2% and 51.0% on VOC and COCO validation datasets, respectively, surpassing existing baselines by a substantial margin. Notably, CoSA is the first single-stage approach to outperform all existing multi-stage methods including those with additional supervision. Code is avilable at \url{https://github.com/youshyee/CoSA}.</p></p class="citation"></blockquote><h3 id=2480--100341-adaptive-quantization-with-mixed-precision-based-on-low-cost-proxy-junzhe-chen-et-al-2024>(24/80 | 100/341) Adaptive quantization with mixed-precision based on low-cost proxy (Junzhe Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junzhe Chen, Qiao Yang, Senmao Tian, Shunli Zhang. (2024)<br><strong>Adaptive quantization with mixed-precision based on low-cost proxy</strong><br><button class=copy-to-clipboard title="Adaptive quantization with mixed-precision based on low-cost proxy" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Model Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17706v1.pdf filename=2402.17706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is critical to deploy complicated neural network <b>models</b> <b>on</b> hardware with limited resources. This paper proposes a novel <b>model</b> <b>quantization</b> method, named the Low-Cost Proxy-Based Adaptive Mixed-Precision <b>Model</b> <b>Quantization</b> (LCPAQ), which contains three key modules. The hardware-aware module is designed by considering the hardware limitations, while an adaptive mixed-precision <b>quantization</b> module is developed to evaluate the <b>quantization</b> sensitivity by using the Hessian matrix and Pareto frontier techniques. Integer linear programming is used to <b>fine-tune</b> the <b>quantization</b> across different layers. Then the low-cost proxy neural architecture search module efficiently explores the ideal <b>quantization</b> hyperparameters. Experiments on the ImageNet demonstrate that the proposed LCPAQ achieves comparable or superior <b>quantization</b> accuracy to existing mixed-precision <b>models.</b> <b>Notably,</b> LCPAQ achieves 1/200 of the search time compared with existing methods, which provides a shortcut in practical <b>quantization</b> use for resource-limited devices.</p></p class="citation"></blockquote><h3 id=2580--101341-sdf2net-shallow-to-deep-feature-fusion-network-for-polsar-image-classification-mohammed-q-alkhatib-et-al-2024>(25/80 | 101/341) SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image Classification (Mohammed Q. Alkhatib et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammed Q. Alkhatib, M. Sami Zitouni, Mina Al-Saad, Nour Aburaed, Hussain Al-Ahmad. (2024)<br><strong>SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image Classification</strong><br><button class=copy-to-clipboard title="SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image Classification" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17672v1.pdf filename=2402.17672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Polarimetric synthetic aperture radar (PolSAR) images encompass valuable information that can facilitate extensive land cover interpretation and generate diverse output products. Extracting meaningful features from PolSAR data poses challenges distinct from those encountered in optical imagery. Deep learning (DL) methods offer effective solutions for overcoming these challenges in PolSAR feature extraction. <b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> play a crucial role in capturing PolSAR image characteristics by leveraging kernel capabilities to consider local information and the complex-valued nature of PolSAR data. In this study, a novel three-branch fusion of complex-valued <b>CNN,</b> named the Shallow to Deep Feature Fusion Network (SDF2Net), is proposed for PolSAR image classification. To validate the performance of the proposed method, classification results are compared against multiple state-of-the-art approaches using the airborne synthetic aperture radar (AIRSAR) datasets of Flevoland and San Francisco, as well as the ESAR Oberpfaffenhofen dataset. The results indicate that the proposed approach demonstrates improvements in overallaccuracy, with a 1.3% and 0.8% enhancement for the AIRSAR datasets and a 0.5% improvement for the ESAR dataset. Analyses conducted on the Flevoland data underscore the effectiveness of the SDF2Net model, revealing a promising overall accuracy of 96.01% even with only a 1% sampling ratio.</p></p class="citation"></blockquote><h3 id=2680--102341-adapt-before-comparison-a-new-perspective-on-cross-domain-few-shot-segmentation-jonas-herzog-2024>(26/80 | 102/341) Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation (Jonas Herzog, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Herzog. (2024)<br><strong>Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation</strong><br><button class=copy-to-clipboard title="Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Few-shot, Fine-tuning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17614v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17614v1.pdf filename=2402.17614v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases. To alleviate this, recently cross-domain <b>few-shot</b> segmentation (CD-FSS) has emerged. Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains. Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network. We show test-time task-adaption is the key for successful CD-FSS instead. Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone. To avoid overfitting to the few labeled samples in <b>supervised</b> <b>fine-tuning,</b> consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers. Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task.</p></p class="citation"></blockquote><h3 id=2780--103341-sddgr-stable-diffusion-based-deep-generative-replay-for-class-incremental-object-detection-junsu-kim-et-al-2024>(27/80 | 103/341) SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection (Junsu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junsu Kim, Hoseong Cho, Jihyeon Kim, Yihalem Yimolal Tiruneh, Seungryul Baek. (2024)<br><strong>SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection</strong><br><button class=copy-to-clipboard title="SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17323v1.pdf filename=2402.17323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of class incremental learning (CIL), generative replay has become increasingly prominent as a method to mitigate the catastrophic forgetting, alongside the continuous improvements in generative models. However, its application in class incremental <b>object</b> <b>detection</b> (CIOD) has been significantly limited, primarily due to the complexities of scenes involving multiple labels. In this paper, we propose a novel approach called stable diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a diffusion-based generative model with pre-trained text-to-diffusion networks to generate realistic and diverse synthetic images. SDDGR incorporates an iterative refinement strategy to produce high-quality images encompassing old classes. Additionally, we adopt an L2 <b>knowledge</b> <b>distillation</b> technique to improve the retention of prior <b>knowledge</b> <b>in</b> synthetic images. Furthermore, our approach includes pseudo-labeling for old <b>objects</b> <b>within</b> new task images, preventing misclassification as background elements. Extensive experiments on the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing algorithms, achieving a new state-of-the-art in various CIOD scenarios. The source code will be made available to the public.</p></p class="citation"></blockquote><h3 id=2880--104341-one-shot-structure-aware-stylized-image-synthesis-hansam-cho-et-al-2024>(28/80 | 104/341) One-Shot Structure-Aware Stylized Image Synthesis (Hansam Cho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hansam Cho, Jonghyun Lee, Seunggyu Chang, Yonghyun Jeong. (2024)<br><strong>One-Shot Structure-Aware Stylized Image Synthesis</strong><br><button class=copy-to-clipboard title="One-Shot Structure-Aware Stylized Image Synthesis" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Generative Adversarial Network, Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17275v1.pdf filename=2402.17275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>GAN-based</b> models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, <b>diffusion</b> <b>models</b> have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with <b>out-of-domain</b> reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via <b>diffusion</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=2980--105341-transparent-image-layer-diffusion-using-latent-transparency-lvmin-zhang-et-al-2024>(29/80 | 105/341) Transparent Image Layer Diffusion using Latent Transparency (Lvmin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lvmin Zhang, Maneesh Agrawala. (2024)<br><strong>Transparent Image Layer Diffusion using Latent Transparency</strong><br><button class=copy-to-clipboard title="Transparent Image Layer Diffusion using Latent Transparency" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17113v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17113v3.pdf filename=2402.17113v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present LayerDiffuse, an approach enabling large-scale pretrained latent <b>diffusion</b> <b>models</b> to generate transparent images. The method allows generation of single transparent images or of multiple transparent layers. The method learns a &ldquo;latent transparency&rdquo; that encodes alpha channel transparency into the latent manifold of a pretrained latent <b>diffusion</b> <b>model.</b> It preserves the production-ready quality of the large <b>diffusion</b> <b>model</b> by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model. In this way, any latent <b>diffusion</b> <b>model</b> can be converted into a transparent image generator by <b>finetuning</b> it with the adjusted latent space. We train the model with 1M transparent image layer pairs collected using a <b>human-in-the-loop</b> collection scheme. We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc. A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting. Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock.</p></p class="citation"></blockquote><h3 id=3080--106341-structural-teacher-student-normality-learning-for-multi-class-anomaly-detection-and-localization-hanqiu-deng-et-al-2024>(30/80 | 106/341) Structural Teacher-Student Normality Learning for Multi-Class Anomaly Detection and Localization (Hanqiu Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanqiu Deng, Xingyu Li. (2024)<br><strong>Structural Teacher-Student Normality Learning for Multi-Class Anomaly Detection and Localization</strong><br><button class=copy-to-clipboard title="Structural Teacher-Student Normality Learning for Multi-Class Anomaly Detection and Localization" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Anomaly Detection, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17091v1.pdf filename=2402.17091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual <b>anomaly</b> <b>detection</b> is a challenging open-set task aimed at identifying unknown anomalous patterns while modeling normal data. The <b>knowledge</b> <b>distillation</b> paradigm has shown remarkable performance in one-class <b>anomaly</b> <b>detection</b> by leveraging teacher-student network feature comparisons. However, extending this paradigm to multi-class <b>anomaly</b> <b>detection</b> introduces novel scalability challenges. In this study, we address the significant performance degradation observed in previous teacher-student models when applied to multi-class <b>anomaly</b> <b>detection,</b> which we identify as resulting from cross-class interference. To tackle this issue, we introduce a novel approach known as Structural Teacher-Student Normality Learning (SNL): (1) We propose spatial-channel <b>distillation</b> and intra-&amp;inter-affinity <b>distillation</b> techniques to measure structural distance between the teacher and student networks. (2) We introduce a central residual aggregation module (CRAM) to encapsulate the normal representation space of the student network. We evaluate our proposed approach on two <b>anomaly</b> <b>detection</b> datasets, MVTecAD and VisA. Our method surpasses the state-of-the-art <b>distillation-based</b> algorithms by a significant margin of 3.9% and 1.5% on MVTecAD and 1.2% and 2.5% on VisA in the multi-class <b>anomaly</b> <b>detection</b> and localization tasks, respectively. Furthermore, our algorithm outperforms the current state-of-the-art unified models on both MVTecAD and VisA.</p></p class="citation"></blockquote><h3 id=3180--107341-oscar-object-state-captioning-and-state-change-representation-nguyen-nguyen-et-al-2024>(31/80 | 107/341) OSCaR: Object State Captioning and State Change Representation (Nguyen Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu. (2024)<br><strong>OSCaR: Object State Captioning and State Change Representation</strong><br><button class=copy-to-clipboard title="OSCaR: Object State Captioning and State Change Representation" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17128v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17128v2.pdf filename=2402.17128v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and <b>benchmark.</b> OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs). Our experiments demonstrate that while MLLMs show some skill, they lack a full understanding of object state changes. The <b>benchmark</b> includes a <b>fine-tuned</b> model that, despite initial capabilities, requires significant improvements in accuracy and generalization ability for effective understanding of these changes. Our code and dataset are available at <a href=https://github.com/nguyennm1024/OSCaR>https://github.com/nguyennm1024/OSCaR</a>.</p></p class="citation"></blockquote><h3 id=3280--108341-mcf-vc-mitigate-catastrophic-forgetting-in-class-incremental-learning-for-multimodal-video-captioning-huiyu-xiong-et-al-2024>(32/80 | 108/341) MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning (Huiyu Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huiyu Xiong, Lanxiao Wang, Heqian Qiu, Taijin Zhao, Benliu Qiu, Hongliang Li. (2024)<br><strong>MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning</strong><br><button class=copy-to-clipboard title="MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Knowledge Distillation, Knowledge Distillation, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17680v1.pdf filename=2402.17680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To address the problem of catastrophic forgetting due to the invisibility of old categories in sequential input, existing work based on relatively simple categorization tasks has made some progress. In contrast, video captioning is a more complex task in <b>multimodal</b> scenario, which has not been explored in the field of incremental learning. After identifying this stability-plasticity problem when analyzing video with sequential input, we originally propose a method to Mitigate Catastrophic Forgetting in class-incremental learning for <b>multimodal</b> Video Captioning (MCF-VC). As for effectively maintaining good performance on old tasks at the macro level, we design Fine-grained Sensitivity Selection (FgSS) based on the Mask of Linear&rsquo;s Parameters and Fisher Sensitivity to pick useful <b>knowledge</b> <b>from</b> old tasks. Further, in order to better constrain the <b>knowledge</b> <b>characteristics</b> of old and new tasks at the specific feature level, we have created the Two-stage <b>Knowledge</b> <b>Distillation</b> (TsKD), which is able to learn the new task well while weighing the old task. Specifically, we design two <b>distillation</b> losses, which constrain the cross modal semantic information of semantic attention feature map and the textual information of the final outputs respectively, so that the inter-model and intra-model stylized <b>knowledge</b> <b>of</b> the old class is retained while learning the new class. In order to illustrate the ability of our model to resist forgetting, we designed a metric CIDER_t to detect the stage forgetting rate. Our experiments on the public dataset MSR-VTT show that the proposed method significantly resists the forgetting of previous tasks without replaying old samples, and performs well on the new task.</p></p class="citation"></blockquote><h3 id=3380--109341-explicit-interaction-for-fusion-based-place-recognition-jingyi-xu-et-al-2024>(33/80 | 109/341) Explicit Interaction for Fusion-Based Place Recognition (Jingyi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyi Xu, Junyi Ma, Qi Wu, Zijie Zhou, Yue Wang, Xieyuanli Chen, Ling Pei. (2024)<br><strong>Explicit Interaction for Fusion-Based Place Recognition</strong><br><button class=copy-to-clipboard title="Explicit Interaction for Fusion-Based Place Recognition" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17264v1.pdf filename=2402.17264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fusion-based place recognition is an emerging technique jointly utilizing <b>multi-modal</b> perception data, to recognize previously visited places in GPS-denied scenarios for robots and autonomous vehicles. Recent fusion-based place recognition methods combine <b>multi-modal</b> features in implicit manners. While achieving remarkable results, they do not explicitly consider what the individual modality affords in the fusion system. Therefore, the benefit of <b>multi-modal</b> feature fusion may not be fully explored. In this paper, we propose a novel fusion-based network, dubbed EINet, to achieve explicit interaction of the two modalities. EINet uses LiDAR ranges to supervise more robust vision features for long time spans, and simultaneously uses camera RGB data to improve the discrimination of LiDAR point clouds. In addition, we develop a new <b>benchmark</b> for the place recognition task based on the nuScenes dataset. To establish this <b>benchmark</b> for future research with comprehensive comparisons, we introduce both <b>supervised</b> and <b>self-supervised</b> training schemes alongside evaluation protocols. We conduct extensive experiments on the proposed <b>benchmark,</b> and the experimental results show that our EINet exhibits better recognition performance as well as solid generalization ability compared to the state-of-the-art fusion-based place recognition approaches. Our open-source code and <b>benchmark</b> are released at: <a href=https://github.com/BIT-XJY/EINet>https://github.com/BIT-XJY/EINet</a>.</p></p class="citation"></blockquote><h3 id=3480--110341-generative-3d-part-assembly-via-part-whole-hierarchy-message-passing-bian-du-et-al-2024>(34/80 | 110/341) Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing (Bi&rsquo;an Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bi&rsquo;an Du, Xiang Gao, Wei Hu, Renjie Liao. (2024)<br><strong>Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing</strong><br><button class=copy-to-clipboard title="Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Message-Passing, Geometry, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17464v1.pdf filename=2402.17464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative 3D part assembly involves understanding part relationships and predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work often focus on the <b>geometry</b> of individual parts, neglecting part-whole hierarchies of objects. Leveraging two key observations: 1) super-part poses provide strong hints about part poses, and 2) predicting super-part poses is easier due to fewer superparts, we propose a part-whole-hierarchy message passing network for efficient 3D part assembly. We first introduce super-parts by grouping geometrically similar parts without any semantic labels. Then we employ a part-whole hierarchical encoder, wherein a super-part encoder predicts latent super-part poses based on input parts. Subsequently, we transform the point cloud using the latent poses, feeding it to the part encoder for aggregating super-part information and <b>reasoning</b> about part relationships to predict all part poses. In training, only ground-truth part poses are required. During inference, the predicted latent poses of super-parts enhance interpretability. Experimental results on the PartNet dataset show that our method achieves state-of-the-art performance in part and connectivity accuracy and enables an interpretable hierarchical part assembly.</p></p class="citation"></blockquote><h3 id=3580--111341-mitigating-distributional-shift-in-semantic-segmentation-via-uncertainty-estimation-from-unlabelled-data-david-s-w-williams-et-al-2024>(35/80 | 111/341) Mitigating Distributional Shift in Semantic Segmentation via Uncertainty Estimation from Unlabelled Data (David S. W. Williams et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David S. W. Williams, Daniele De Martini, Matthew Gadd, Paul Newman. (2024)<br><strong>Mitigating Distributional Shift in Semantic Segmentation via Uncertainty Estimation from Unlabelled Data</strong><br><button class=copy-to-clipboard title="Mitigating Distributional Shift in Semantic Segmentation via Uncertainty Estimation from Unlabelled Data" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Data Augmentation, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17653v1.pdf filename=2402.17653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowing when a trained segmentation model is encountering <b>data</b> <b>that</b> is different to its training <b>data</b> <b>is</b> important. Understanding and mitigating the effects of this play an important part in their application from a performance and assurance perspective - this being a safety concern in applications such as autonomous vehicles (AVs). This work presents a segmentation network that can detect errors caused by challenging test domains without any additional annotation in a single forward pass. As annotation costs limit the diversity of labelled datasets, we use easy-to-obtain, uncurated and unlabelled <b>data</b> <b>to</b> learn to perform uncertainty estimation by selectively enforcing consistency over <b>data</b> <b>augmentation.</b> To this end, a novel segmentation <b>benchmark</b> based on the SAX Dataset is used, which includes labelled test <b>data</b> <b>spanning</b> three autonomous-driving domains, ranging in appearance from dense urban to off-road. The proposed method, named Gamma-SSL, consistently outperforms uncertainty estimation and <b>Out-of-Distribution</b> (OoD) techniques on this difficult <b>benchmark</b> - by up to 10.7% in area under the receiver operating characteristic (ROC) curve and 19.2% in area under the precision-recall (PR) curve in the most challenging of the three scenarios.</p></p class="citation"></blockquote><h3 id=3680--112341-masked-gamma-ssl-learning-uncertainty-estimation-via-masked-image-modeling-david-s-w-williams-et-al-2024>(36/80 | 112/341) Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image Modeling (David S. W. Williams et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David S. W. Williams, Matthew Gadd, Paul Newman, Daniele De Martini. (2024)<br><strong>Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image Modeling</strong><br><button class=copy-to-clipboard title="Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image Modeling" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Foundation Model, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17622v1.pdf filename=2402.17622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass. We exploit general representations from <b>foundation</b> <b>models</b> and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques. For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a network&rsquo;s limitations at run time and act accordingly. To this end, we test our proposed method on a number of test domains including the SAX Segmentation <b>benchmark,</b> which includes labelled test data from dense urban, rural and off-road driving domains. The proposed method consistently outperforms uncertainty estimation and <b>Out-of-Distribution</b> (OoD) techniques on this difficult <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=3780--113341-socialcvae-predicting-pedestrian-trajectory-via-interaction-conditioned-latents-wei-xiang-et-al-2024>(37/80 | 113/341) SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents (Wei Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Xiang, Haoteng Yin, He Wang, Xiaogang Jin. (2024)<br><strong>SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents</strong><br><button class=copy-to-clipboard title="SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Autoencoder, Benchmarking, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17339v1.pdf filename=2402.17339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions. Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with a deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability. However, the deterministic nature of the learned steering behaviors from the empirical models limits the models&rsquo; practical performance. To address this issue, this work proposes the social conditional <b>variational</b> <b>autoencoder</b> (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the CVAE&rsquo;s condition, which illustrates the future occupancy of each pedestrian&rsquo;s local neighborhood area. The energy map is generated using an energy-based interaction model, which anticipates the energy cost (i.e., repulsion intensity) of pedestrians&rsquo; interactions with neighbors. Experimental results on two public <b>benchmarks</b> including 25 scenes demonstrate that SocialCVAE significantly improves prediction accuracy compared with the state-of-the-art methods, with up to 16.85% improvement in Average Displacement Error (ADE) and 69.18% improvement in Final Displacement Error (FDE).</p></p class="citation"></blockquote><h3 id=3880--114341-analyzing-regional-organization-of-the-human-hippocampus-in-3d-pli-using-contrastive-learning-and-geometric-unfolding-alexander-oberstrass-et-al-2024>(38/80 | 114/341) Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using Contrastive Learning and Geometric Unfolding (Alexander Oberstrass et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Oberstrass, Jordan DeKraker, Nicola Palomero-Gallagher, Sascha E. A. Muenzing, Alan C. Evans, Markus Axer, Katrin Amunts, Timo Dickscheid. (2024)<br><strong>Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using Contrastive Learning and Geometric Unfolding</strong><br><button class=copy-to-clipboard title="Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using Contrastive Learning and Geometric Unfolding" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17744v1.pdf filename=2402.17744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the cortical organization of the human brain requires interpretable descriptors for distinct structural and functional imaging data. 3D polarized light imaging (3D-PLI) is an imaging modality for visualizing fiber architecture in postmortem brains with high resolution that also captures the presence of cell bodies, for example, to identify hippocampal subfields. The rich texture in 3D-PLI images, however, makes this modality particularly difficult to analyze and best practices for characterizing architectonic patterns still need to be established. In this work, we demonstrate a novel method to analyze the regional organization of the human hippocampus in 3D-PLI by combining recent advances in unfolding methods with deep texture features obtained using a <b>self-supervised</b> <b>contrastive</b> <b>learning</b> approach. We identify clusters in the representations that correspond well with classical descriptions of hippocampal subfields, lending validity to the developed methodology.</p></p class="citation"></blockquote><h3 id=3980--115341-vrp-sam-sam-with-visual-reference-prompt-yanpeng-sun-et-al-2024>(39/80 | 115/341) VRP-SAM: SAM with Visual Reference Prompt (Yanpeng Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Qiang Chen, Gang Zhang, Errui Ding, Jingdong Wang, Zechao Li. (2024)<br><strong>VRP-SAM: SAM with Visual Reference Prompt</strong><br><button class=copy-to-clipboard title="VRP-SAM: SAM with Visual Reference Prompt" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Meta Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17726v1.pdf filename=2402.17726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a novel Visual Reference <b>Prompt</b> (VRP) encoder that empowers the Segment Anything Model (SAM) to utilize annotated reference images as <b>prompts</b> for segmentation, creating the VRP-SAM model. In essence, VRP-SAM can utilize annotated reference images to comprehend specific objects and perform segmentation of specific objects in target image. It is note that the VRP encoder can support a variety of annotation formats for reference images, including \textbf{point}, \textbf{box}, \textbf{scribble}, and \textbf{mask}. VRP-SAM achieves a breakthrough within the SAM framework by extending its versatility and applicability while preserving SAM&rsquo;s inherent strengths, thus enhancing user-friendliness. To enhance the generalization ability of VRP-SAM, the VRP encoder adopts a <b>meta-learning</b> <b>strategy.</b> To validate the effectiveness of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM demonstrates strong generalization capabilities, allowing it to perform segmentation of unseen objects and enabling cross-domain segmentation.</p></p class="citation"></blockquote><h3 id=4080--116341-robust-unsupervised-crowd-counting-and-localization-with-adaptive-resolution-sam-jia-wan-et-al-2024>(40/80 | 116/341) Robust Unsupervised Crowd Counting and Localization with Adaptive Resolution SAM (Jia Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jia Wan, Qiangqiang Wu, Wei Lin, Antoni B. Chan. (2024)<br><strong>Robust Unsupervised Crowd Counting and Localization with Adaptive Resolution SAM</strong><br><button class=copy-to-clipboard title="Robust Unsupervised Crowd Counting and Localization with Adaptive Resolution SAM" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17514v1.pdf filename=2402.17514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The existing crowd counting models require extensive training data, which is time-consuming to annotate. To tackle this issue, we propose a simple yet effective crowd counting method by utilizing the Segment-Everything-Everywhere Model (SEEM), an adaptation of the Segmentation Anything Model (SAM), to generate pseudo-labels for training crowd counting models. However, our initial investigation reveals that SEEM&rsquo;s performance in dense crowd scenes is limited, primarily due to the omission of many persons in high-density areas. To overcome this limitation, we propose an adaptive resolution SEEM to handle the scale variations, occlusions, and overlapping of people within crowd scenes. Alongside this, we introduce a robust localization method, based on Gaussian Mixture Models, for predicting the head positions in the predicted people masks. Given the mask and point pseudo-labels, we propose a robust loss function, which is designed to exclude uncertain regions based on SEEM&rsquo;s predictions, thereby enhancing the training process of the counting networks. Finally, we propose an iterative method for generating pseudo-labels. This method aims at improving the quality of the segmentation masks by identifying more tiny persons in high-density regions, which are often missed in the first pseudo-labeling stage. Overall, our proposed method achieves the best <b>unsupervised</b> performance in crowd counting, while also being comparable results to some <b>supervised</b> methods. This makes it a highly effective and versatile tool for crowd counting, especially in situations where labeled data is not available.</p></p class="citation"></blockquote><h3 id=4180--117341-a-vanilla-multi-task-framework-for-dense-visual-prediction-solution-to-1st-vcl-challenge----multi-task-robustness-track-zehui-chen-et-al-2024>(41/80 | 117/341) A Vanilla Multi-Task Framework for Dense Visual Prediction Solution to 1st VCL Challenge &ndash; Multi-Task Robustness Track (Zehui Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehui Chen, Qiuchen Wang, Zhenyu Li, Jiaming Liu, Shanghang Zhang, Feng Zhao. (2024)<br><strong>A Vanilla Multi-Task Framework for Dense Visual Prediction Solution to 1st VCL Challenge &ndash; Multi-Task Robustness Track</strong><br><button class=copy-to-clipboard title="A Vanilla Multi-Task Framework for Dense Visual Prediction Solution to 1st VCL Challenge -- Multi-Task Robustness Track" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17319v1.pdf filename=2402.17319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this report, we present our solution to the multi-task robustness track of the 1st Visual <b>Continual</b> <b>Learning</b> (VCL) Challenge at ICCV 2023 Workshop. We propose a vanilla framework named UniNet that seamlessly combines various visual perception algorithms into a multi-task model. Specifically, we choose DETR3D, Mask2Former, and BinsFormer for 3D <b>object</b> <b>detection,</b> instance segmentation, and depth estimation tasks, respectively. The final submission is a single model with InternImage-L backbone, and achieves a 49.6 overall score (29.5 Det mAP, 80.3 mTPS, 46.4 Seg mAP, and 7.93 silog) on SHIFT validation set. Besides, we provide some interesting observations in our experiments which may facilitate the development of multi-task learning in dense visual prediction.</p></p class="citation"></blockquote><h3 id=4280--118341-context-based-and-diversity-driven-specificity-in-compositional-zero-shot-learning-yun-li-et-al-2024>(42/80 | 118/341) Context-based and Diversity-driven Specificity in Compositional Zero-Shot Learning (Yun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yun Li, Zhe Liu, Hang Chen, Lina Yao. (2024)<br><strong>Context-based and Diversity-driven Specificity in Compositional Zero-Shot Learning</strong><br><button class=copy-to-clipboard title="Context-based and Diversity-driven Specificity in Compositional Zero-Shot Learning" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17251v1.pdf filename=2402.17251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compositional <b>Zero-Shot</b> <b>Learning</b> (CZSL) aims to recognize unseen attribute-object pairs based on a limited set of observed examples. Current CZSL methodologies, despite their advancements, tend to neglect the distinct specificity levels present in attributes. For instance, given images of sliced strawberries, they may fail to prioritize <code>Sliced-Strawberry' over a generic </code>Red-Strawberry&rsquo;, despite the former being more informative. They also suffer from ballooning search space when shifting from Close-World (CW) to Open-World (OW) CZSL. To address the issues, we introduce the Context-based and Diversity-driven Specificity learning framework for CZSL (CDS-CZSL). Our framework evaluates the specificity of attributes by considering the diversity of objects they apply to and their related context. This novel approach allows for more accurate predictions by emphasizing specific attribute-object pairs and improves composition filtering in OW-CZSL. We conduct experiments in both CW and OW scenarios, and our model achieves state-of-the-art results across three datasets.</p></p class="citation"></blockquote><h3 id=4380--119341-playground-v25-three-insights-towards-enhancing-aesthetic-quality-in-text-to-image-generation-daiqing-li-et-al-2024>(43/80 | 119/341) Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation (Daiqing Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, Suhail Doshi. (2024)<br><strong>Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17245v1.pdf filename=2402.17245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we share three insights for achieving state-of-the-art aesthetic quality in <b>text-to-image</b> generative models. We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details. First, we delve into the significance of the noise schedule in training a <b>diffusion</b> <b>model,</b> demonstrating its profound impact on realism and visual fidelity. Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset. Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations. Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions and aspect ratios, outperforming both widely-used open-source models like SDXL and Playground v2, and closed-source commercial systems such as DALLE 3 and Midjourney v5.2. Our model is open-source, and we hope the development of Playground v2.5 provides valuable guidelines for researchers aiming to elevate the aesthetic quality of <b>diffusion-based</b> <b>image</b> generation models.</p></p class="citation"></blockquote><h3 id=4480--120341-charactergen-efficient-3d-character-generation-from-single-images-with-multi-view-pose-canonicalization-hao-yang-peng-et-al-2024>(44/80 | 120/341) CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization (Hao-Yang Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu. (2024)<br><strong>CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization</strong><br><button class=copy-to-clipboard title="CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17214v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17214v2.pdf filename=2402.17214v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of digital content creation, generating high-quality 3D characters from single images is challenging, especially given the complexities of various body poses and the issues of self-occlusion and pose ambiguity. In this paper, we present CharacterGen, a framework developed to efficiently generate 3D characters. CharacterGen introduces a streamlined generation pipeline along with an image-conditioned multi-view <b>diffusion</b> <b>model.</b> This model effectively calibrates input poses to a canonical form while retaining key attributes of the input image, thereby addressing the challenges posed by diverse poses. A <b>transformer-based,</b> generalizable sparse-view reconstruction model is the other core component of our approach, facilitating the creation of detailed 3D models from multi-view images. We also adopt a texture-back-projection strategy to produce high-quality texture maps. Additionally, we have curated a dataset of anime characters, rendered in multiple poses and views, to train and evaluate our model. Our approach has been thoroughly evaluated through quantitative and qualitative experiments, showing its proficiency in generating 3D characters with high-quality shapes and textures, ready for downstream applications such as rigging and animation.</p></p class="citation"></blockquote><h3 id=4580--121341-sora-a-review-on-background-technology-limitations-and-opportunities-of-large-vision-models-yixin-liu-et-al-2024>(45/80 | 121/341) Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models (Yixin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, Lichao Sun. (2024)<br><strong>Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models</strong><br><button class=copy-to-clipboard title="Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Sora, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17177v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17177v2.pdf filename=2402.17177v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Sora</b> is a text-to-video <b>generative</b> <b>AI</b> model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model&rsquo;s background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace <b>Sora&rsquo;s</b> development and investigate the underlying technologies used to build this &ldquo;world simulator&rdquo;. Then, we describe in detail the applications and potential impact of <b>Sora</b> in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy <b>Sora,</b> such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of <b>Sora</b> and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></p class="citation"></blockquote><h3 id=4680--122341-few-shot-adaptation-for-morphology-independent-cell-instance-segmentation-ram-j-zaveri-et-al-2024>(46/80 | 122/341) Few-shot adaptation for morphology-independent cell instance segmentation (Ram J. Zaveri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ram J. Zaveri, Voke Brume, Gianfranco Doretto. (2024)<br><strong>Few-shot adaptation for morphology-independent cell instance segmentation</strong><br><button class=copy-to-clipboard title="Few-shot adaptation for morphology-independent cell instance segmentation" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Few-shot, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17165v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17165v1.pdf filename=2402.17165v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Microscopy data collections are becoming larger and more frequent. Accurate and precise quantitative analysis tools like cell instance segmentation are necessary to benefit from them. This is challenging due to the variability in the data, which requires retraining the segmentation model to maintain high accuracy on new collections. This is needed especially for segmenting cells with elongated and non-convex morphology like bacteria. We propose to reduce the amount of annotation and computing power needed for retraining the model by introducing a <b>few-shot</b> <b>domain</b> <b>adaptation</b> approach that requires annotating only one to five cells of the new data to process and that quickly adapts the model to maintain high accuracy. Our results show a significant boost in accuracy after adaptation to very challenging bacteria datasets.</p></p class="citation"></blockquote><h3 id=4780--123341-nocplace-nocturnal-visual-place-recognition-using-generative-and-inherited-knowledge-transfer-bingxi-liu-et-al-2024>(47/80 | 123/341) NocPlace: Nocturnal Visual Place Recognition Using Generative and Inherited Knowledge Transfer (Bingxi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingxi Liu, Yiqun Wang, Huaqi Tao, Tingjun Huang, Fulin Tang, Yihong Wu, Jinqiang Cui, Hong Zhang. (2024)<br><strong>NocPlace: Nocturnal Visual Place Recognition Using Generative and Inherited Knowledge Transfer</strong><br><button class=copy-to-clipboard title="NocPlace: Nocturnal Visual Place Recognition Using Generative and Inherited Knowledge Transfer" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17159v1.pdf filename=2402.17159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual Place Recognition (VPR) is crucial in computer vision, aiming to retrieve database images similar to a query image from an extensive collection of known images. However, like many vision-related tasks, learning-based VPR often experiences a decline in performance during nighttime due to the scarcity of nighttime images. Specifically, VPR needs to address the cross-domain problem of night-to-day rather than just the issue of a single nighttime domain. In response to these issues, we present NocPlace, which leverages a generated large-scale, multi-view, nighttime VPR dataset to embed resilience against dazzling lights and extreme darkness in the learned global descriptor. Firstly, we establish a day-night urban scene dataset called NightCities, capturing diverse nighttime scenarios and lighting variations across 60 cities globally. Following this, an unpaired image-to-image translation network is trained on this dataset. Using this trained translation network, we process an existing VPR dataset, thereby obtaining its nighttime version. The NocPlace is then <b>fine-tuned</b> using night-style images, the original labels, and descriptors inherited from the Daytime VPR model. Comprehensive experiments on various nighttime VPR test sets reveal that NocPlace considerably surpasses previous state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=4880--124341-adversarial-example-soups-averaging-multiple-adversarial-examples-improves-transferability-without-increasing-additional-generation-time-bo-yang-et-al-2024>(48/80 | 124/341) Adversarial example soups: averaging multiple adversarial examples improves transferability without increasing additional generation time (Bo Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Yang, Hengwei Zhang, Chenwei Li, Jindong Wang. (2024)<br><strong>Adversarial example soups: averaging multiple adversarial examples improves transferability without increasing additional generation time</strong><br><button class=copy-to-clipboard title="Adversarial example soups: averaging multiple adversarial examples improves transferability without increasing additional generation time" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18370v1.pdf filename=2402.18370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For transfer-based attacks, the adversarial examples are crafted on the surrogate model, which can be implemented to mislead the target model effectively. The conventional method for maximizing adversarial transferability involves: (1) <b>fine-tuning</b> hyperparameters to generate multiple batches of adversarial examples on the substitute model; (2) conserving the batch of adversarial examples that have the best comprehensive performance on substitute model and target model, and discarding the others. In this work, we revisit the second step of this process in the context of <b>fine-tuning</b> hyperparameters to craft adversarial examples, where multiple batches of <b>fine-tuned</b> adversarial examples often appear in a single high error hilltop. We demonstrate that averaging multiple batches of adversarial examples under different hyperparameter configurations, which refers to as &ldquo;adversarial example soups&rdquo;, can often enhance adversarial transferability. Compared with traditional methods, the proposed method incurs no additional generation time and computational cost. Besides, our method is orthogonal to existing transfer-based methods and can be combined with them seamlessly to generate more transferable adversarial examples. Extensive experiments on the ImageNet dataset show that our methods achieve a higher attack success rate than the state-of-the-art attacks.</p></p class="citation"></blockquote><h3 id=4980--125341-t-hitl-effectively-addresses-problematic-associations-in-image-generation-and-maintains-overall-visual-quality-susan-epstein-et-al-2024>(49/80 | 125/341) T-HITL Effectively Addresses Problematic Associations in Image Generation and Maintains Overall Visual Quality (Susan Epstein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Susan Epstein, Li Chen, Alessandro Vecchiato, Ankit Jain. (2024)<br><strong>T-HITL Effectively Addresses Problematic Associations in Image Generation and Maintains Overall Visual Quality</strong><br><button class=copy-to-clipboard title="T-HITL Effectively Addresses Problematic Associations in Image Generation and Maintains Overall Visual Quality" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2-1, I-I-2, cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17101v1.pdf filename=2402.17101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> image models may inadvertently generate problematic representations of people. Past research has noted that millions of users engage daily across the world with these models and that the models, including through problematic representations of people, have the potential to compound and accelerate real-world discrimination and other harms (Bianchi et al, 2023). In this paper, we focus on addressing the generation of problematic associations between demographic groups and semantic concepts that may reflect and reinforce negative narratives embedded in social data. Building on sociological literature (Blumer, 1958) and mapping representations to model behaviors, we have developed a taxonomy to study problematic associations in image generation models. We explore the effectiveness of fine tuning at the model level as a method to address these associations, identifying a potential reduction in visual quality as a limitation of traditional fine tuning. We also propose a new methodology with twice-human-in-the-loop (T-HITL) that promises improvements in both reducing problematic associations and also maintaining visual quality. We demonstrate the effectiveness of T-HITL by providing evidence of three problematic associations addressed by T-HITL at the model level. Our contributions to scholarship are two-fold. By defining problematic associations in the context of machine learning models and <b>generative</b> <b>AI,</b> we introduce a conceptual and technical taxonomy for addressing some of these associations. Finally, we provide a method, T-HITL, that addresses these associations and simultaneously maintains visual quality of image model generations. This mitigation need not be a tradeoff, but rather an enhancement.</p></p class="citation"></blockquote><h3 id=5080--126341-diffusion-model-based-image-editing-a-survey-yi-huang-et-al-2024>(50/80 | 126/341) Diffusion Model-Based Image Editing: A Survey (Yi Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, Liangliang Cao. (2024)<br><strong>Diffusion Model-Based Image Editing: A Survey</strong><br><button class=copy-to-clipboard title="Diffusion Model-Based Image Editing: A Survey" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 19<br>Keywords: Diffusion Model, Benchmarking, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17525v1.pdf filename=2402.17525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Denoising <b>diffusion</b> <b>models</b> have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using <b>diffusion</b> <b>models</b> for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current <b>multimodal</b> conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic <b>benchmark,</b> EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research. The accompanying repository is released at <a href=https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods>https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods</a>.</p></p class="citation"></blockquote><h3 id=5180--127341-plremix-combating-noisy-labels-with-pseudo-label-relaxed-contrastive-representation-learning-xiaoyu-liu-et-al-2024>(51/80 | 127/341) PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning (Xiaoyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Liu, Beitong Zhou, Cheng Cheng. (2024)<br><strong>PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning</strong><br><button class=copy-to-clipboard title="PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Representation Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17589v1.pdf filename=2402.17589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the application of Contrastive <b>Representation</b> <b>Learning</b> (CRL) in learning with noisy labels (LNL) has shown promising advancements due to its remarkable ability to learn well-distributed <b>representations</b> <b>for</b> better distinguishing noisy labels. However, CRL is mainly used as a pre-training technique, leading to a complicated multi-stage training pipeline. We also observed that trivially combining CRL with <b>supervised</b> LNL methods decreases performance. Using different images from the same class as negative pairs in CRL creates optimization conflicts between CRL and the <b>supervised</b> loss. To address these two issues, we propose an end-to-end PLReMix framework that avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR) contrastive loss to alleviate the conflicts between losses. This PLR loss constructs a reliable negative set of each sample by filtering out its inappropriate negative pairs that overlap at the top k indices of prediction probabilities, leading to more compact semantic clusters than vanilla CRL. Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to distinguish clean and noisy samples by leveraging semantic information and model outputs simultaneously, which is expanded on the previously widely used one-dimensional form. The PLR loss and a semi-supervised loss are simultaneously applied to train on the GMM divided clean and noisy samples. Experiments on multiple <b>benchmark</b> datasets demonstrate the effectiveness of the proposed method. Our proposed PLR loss is scalable, which can be easily integrated into other LNL methods and boost their performance. Codes will be available.</p></p class="citation"></blockquote><h3 id=5280--128341-black-box-adversarial-attacks-against-image-quality-assessment-models-yu-ran-et-al-2024>(52/80 | 128/341) Black-box Adversarial Attacks Against Image Quality Assessment Models (Yu Ran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Ran, Ao-Xiang Zhang, Mingjie Li, Weixuan Tang, Yuan-Gen Wang. (2024)<br><strong>Black-box Adversarial Attacks Against Image Quality Assessment Models</strong><br><button class=copy-to-clipboard title="Black-box Adversarial Attacks Against Image Quality Assessment Models" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 15<br>Keywords: Black Box, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17533v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17533v2.pdf filename=2402.17533v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The goal of No-Reference Image Quality Assessment (NR-IQA) is to predict the perceptual quality of an image in line with its subjective evaluation. To put the NR-IQA models into practice, it is essential to study their potential loopholes for model refinement. This paper makes the first attempt to explore the <b>black-box</b> <b>adversarial</b> <b>attacks</b> on NR-IQA models. Specifically, we first formulate the attack problem as maximizing the deviation between the estimated quality scores of original and perturbed images, while restricting the perturbed image distortions for visual quality preservation. Under such formulation, we then design a Bi-directional loss function to mislead the estimated quality scores of <b>adversarial</b> <b>examples</b> towards an opposite direction with maximum deviation. On this basis, we finally develop an efficient and effective <b>black-box</b> <b>attack</b> method against NR-IQA models. Extensive experiments reveal that all the evaluated NR-IQA models are vulnerable to the proposed attack method. And the generated perturbations are not transferable, enabling them to serve the investigation of specialities of disparate IQA models.</p></p class="citation"></blockquote><h3 id=5380--129341-sora-generates-videos-with-stunning-geometrical-consistency-xuanyi-li-et-al-2024>(53/80 | 129/341) Sora Generates Videos with Stunning Geometrical Consistency (Xuanyi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanyi Li, Daquan Zhou, Chenxu Zhang, Shaodong Wei, Qibin Hou, Ming-Ming Cheng. (2024)<br><strong>Sora Generates Videos with Stunning Geometrical Consistency</strong><br><button class=copy-to-clipboard title="Sora Generates Videos with Stunning Geometrical Consistency" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Sora, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17403v1.pdf filename=2402.17403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recently developed <b>Sora</b> model [1] has exhibited remarkable capabilities in video generation, sparking intense discussions regarding its ability to simulate real-world phenomena. Despite its growing popularity, there is a lack of established metrics to evaluate its fidelity to real-world physics quantitatively. In this paper, we introduce a new <b>benchmark</b> that assesses the quality of the generated videos based on their adherence to real-world physics principles. We employ a method that transforms the generated videos into 3D models, leveraging the premise that the accuracy of 3D reconstruction is heavily contingent on the video quality. From the perspective of 3D reconstruction, we use the fidelity of the geometric constraints satisfied by the constructed 3D models as a proxy to gauge the extent to which the generated videos conform to real-world physics rules. Project page: <a href=https://sora-geometrical-consistency.github.io/>https://sora-geometrical-consistency.github.io/</a></p></p class="citation"></blockquote><h3 id=5480--130341-deployment-prior-injection-for-run-time-calibratable-object-detection-mo-zhou-et-al-2024>(54/80 | 130/341) Deployment Prior Injection for Run-time Calibratable Object Detection (Mo Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mo Zhou, Yiding Yang, Haoxiang Li, Vishal M. Patel, Gang Hua. (2024)<br><strong>Deployment Prior Injection for Run-time Calibratable Object Detection</strong><br><button class=copy-to-clipboard title="Deployment Prior Injection for Run-time Calibratable Object Detection" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17207v1.pdf filename=2402.17207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With a strong alignment between the training and test distributions, <b>object</b> <b>relation</b> as a context prior facilitates <b>object</b> <b>detection.</b> Yet, it turns into a harmful but inevitable training set bias upon test distributions that shift differently across space and time. Nevertheless, the existing detectors cannot incorporate deployment context prior during the test phase without parameter update. Such kind of capability requires the model to explicitly learn disentangled representations with respect to context prior. To achieve this, we introduce an additional <b>graph</b> input to the detector, where the <b>graph</b> represents the deployment context prior, and its edge values represent <b>object</b> <b>relations.</b> Then, the detector behavior is trained to bound to the <b>graph</b> with a modified training objective. As a result, during the test phase, any suitable deployment context prior can be injected into the detector via <b>graph</b> edits, hence calibrating, or &ldquo;re-biasing&rdquo; the detector towards the given prior at run-time without parameter update. Even if the deployment prior is unknown, the detector can self-calibrate using deployment prior approximated using its own predictions. Comprehensive experimental results on the COCO dataset, as well as cross-dataset testing on the Objects365 dataset, demonstrate the effectiveness of the run-time calibratable detector.</p></p class="citation"></blockquote><h3 id=5580--131341-livehps-lidar-based-scene-level-human-pose-and-shape-estimation-in-free-environment-yiming-ren-et-al-2024>(55/80 | 131/341) LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment (Yiming Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Ren, Xiao Han, Chengfeng Zhao, Jingya Wang, Lan Xu, Jingyi Yu, Yuexin Ma. (2024)<br><strong>LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment</strong><br><button class=copy-to-clipboard title="LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Knowledge Distillation, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17171v1.pdf filename=2402.17171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For human-centric large-scale scenes, fine-grained modeling for 3D human global pose and shape is significant for scene understanding and can benefit many real-world applications. In this paper, we present LiveHPS, a novel single-LiDAR-based approach for scene-level human pose and shape estimation without any limitation of light conditions and wearable devices. In particular, we design a <b>distillation</b> mechanism to mitigate the distribution-varying effect of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic information existing in consecutive frames to solve the occlusion and noise disturbance. LiveHPS, with its efficient configuration and high-quality output, is well-suited for real-world applications. Moreover, we propose a huge human motion dataset, named FreeMotion, which is collected in various scenarios with diverse human poses, shapes and translations. It consists of <b>multi-modal</b> and multi-view acquisition data from calibrated and synchronized LiDARs, cameras, and IMUs. Extensive experiments on our new dataset and other public datasets demonstrate the SOTA performance and robustness of our approach. We will release our code and dataset soon.</p></p class="citation"></blockquote><h3 id=5680--132341-efficiently-leveraging-linguistic-priors-for-scene-text-spotting-nguyen-nguyen-et-al-2024>(56/80 | 132/341) Efficiently Leveraging Linguistic Priors for Scene Text Spotting (Nguyen Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nguyen Nguyen, Yapeng Tian, Chenliang Xu. (2024)<br><strong>Efficiently Leveraging Linguistic Priors for Scene Text Spotting</strong><br><button class=copy-to-clipboard title="Efficiently Leveraging Linguistic Priors for Scene Text Spotting" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17134v1.pdf filename=2402.17134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incorporating linguistic knowledge can improve scene text recognition, but it is questionable whether the same holds for scene text spotting, which typically involves text detection and recognition. This paper proposes a method that leverages linguistic knowledge from a large text corpus to replace the traditional one-hot encoding used in auto-regressive scene text spotting and recognition models. This allows the model to capture the relationship between characters in the same word. Additionally, we introduce a technique to generate text distributions that align well with scene text datasets, removing the need for in-domain <b>fine-tuning.</b> As a result, the newly created text distributions are more informative than pure one-hot encoding, leading to improved spotting and recognition performance. Our method is simple and efficient, and it can easily be integrated into existing auto-regressive-based approaches. Experimental results show that our method not only improves recognition accuracy but also enables more accurate localization of words. It significantly improves both state-of-the-art scene text spotting and recognition pipelines, achieving state-of-the-art results on several <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=5780--133341-alignmif-geometry-aligned-multimodal-implicit-field-for-lidar-camera-joint-synthesis-tao-tang-et-al-2024>(57/80 | 133/341) AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera Joint Synthesis (Tao Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Tang, Guangrun Wang, Yixing Lao, Peng Chen, Jie Liu, Liang Lin, Kaicheng Yu, Xiaodan Liang. (2024)<br><strong>AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera Joint Synthesis</strong><br><button class=copy-to-clipboard title="AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera Joint Synthesis" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 11<br>Keywords: Geometry, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17483v1.pdf filename=2402.17483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural implicit fields have been a de facto standard in novel view synthesis. Recently, there exist some methods exploring fusing multiple modalities within a single field, aiming to share implicit features from different modalities to enhance reconstruction performance. However, these modalities often exhibit misaligned behaviors: optimizing for one modality, such as LiDAR, can adversely affect another, like camera performance, and vice versa. In this work, we conduct comprehensive analyses on the <b>multimodal</b> implicit field of LiDAR-camera joint synthesis, revealing the underlying issue lies in the misalignment of different sensors. Furthermore, we introduce AlignMiF, a geometrically aligned <b>multimodal</b> implicit field with two proposed modules: <b>Geometry-Aware</b> Alignment (GAA) and Shared <b>Geometry</b> Initialization (SGI). These modules effectively align the coarse <b>geometry</b> across different modalities, significantly enhancing the fusion process between LiDAR and camera data. Through extensive experiments across various datasets and scenes, we demonstrate the effectiveness of our approach in facilitating better interaction between LiDAR and camera modalities within a unified neural field. Specifically, our proposed AlignMiF, achieves remarkable improvement over recent implicit fusion methods (+2.01 and +3.11 image PSNR on the KITTI-360 and Waymo datasets) and consistently surpasses single modality performance (13.8% and 14.2% reduction in LiDAR Chamfer Distance on the respective datasets).</p></p class="citation"></blockquote><h3 id=5880--134341-customsketching-sketch-concept-extraction-for-sketch-based-image-synthesis-and-editing-chufeng-xiao-et-al-2024>(58/80 | 134/341) CustomSketching: Sketch Concept Extraction for Sketch-based Image Synthesis and Editing (Chufeng Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chufeng Xiao, Hongbo Fu. (2024)<br><strong>CustomSketching: Sketch Concept Extraction for Sketch-based Image Synthesis and Editing</strong><br><button class=copy-to-clipboard title="CustomSketching: Sketch Concept Extraction for Sketch-based Image Synthesis and Editing" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17624v1.pdf filename=2402.17624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalization techniques for large <b>text-to-image</b> (T2I) models allow users to incorporate new concepts from reference images. However, existing methods primarily rely on textual descriptions, leading to limited control over customized images and failing to support fine-grained and local editing (e.g., shape, pose, and details). In this paper, we identify sketches as an intuitive and versatile representation that can facilitate such control, e.g., contour lines capturing shape information and flow lines representing texture. This motivates us to explore a novel task of sketch concept extraction: given one or more sketch-image pairs, we aim to extract a special sketch concept that bridges the correspondence between the images and sketches, thus enabling sketch-based image synthesis and editing at a fine-grained level. To accomplish this, we introduce CustomSketching, a two-stage framework for extracting novel sketch concepts. Considering that an object can often be depicted by a contour for general shapes and additional strokes for internal details, we introduce a dual-sketch representation to reduce the inherent ambiguity in sketch depiction. We employ a shape loss and a regularization loss to balance fidelity and editability during optimization. Through extensive experiments, a user study, and several applications, we show our method is effective and superior to the adapted baselines.</p></p class="citation"></blockquote><h3 id=5980--135341-scribble-hides-class-promoting-scribble-based-weakly-supervised-semantic-segmentation-with-its-class-label-xinliang-zhang-et-al-2024>(59/80 | 135/341) Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label (Xinliang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinliang Zhang, Lei Zhu, Hangzhou He, Lujia Jin, Yanye Lu. (2024)<br><strong>Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label</strong><br><button class=copy-to-clipboard title="Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17555v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17555v1.pdf filename=2402.17555v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scribble-based <b>weakly-supervised</b> semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label&rsquo;s boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.The code is available at <a href=https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network>https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network</a>.</p></p class="citation"></blockquote><h3 id=6080--136341-interactive-multi-head-self-attention-with-linear-complexity-hankyul-kang-et-al-2024>(60/80 | 136/341) Interactive Multi-Head Self-Attention with Linear Complexity (Hankyul Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hankyul Kang, Ming-Hsuan Yang, Jongbin Ryu. (2024)<br><strong>Interactive Multi-Head Self-Attention with Linear Complexity</strong><br><button class=copy-to-clipboard title="Interactive Multi-Head Self-Attention with Linear Complexity" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17507v1.pdf filename=2402.17507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose an efficient interactive method for multi-head <b>self-attention</b> via decomposition. For existing methods using multi-head <b>self-attention,</b> the attention operation of each head is computed independently. However, we show that the interactions between cross-heads of the attention matrix enhance the information flow of the attention operation. Considering that the attention matrix of each head can be seen as a feature of networks, it is beneficial to establish connectivity between them to capture interactions better. However, a straightforward approach to capture the interactions between the cross-heads is computationally prohibitive as the complexity grows substantially with the high dimension of an attention matrix. In this work, we propose an effective method to decompose the attention operation into query- and key-less components. This will result in a more manageable size for the attention matrix, specifically for the cross-head interactions. Expensive experimental results show that the proposed cross-head interaction approach performs favorably against existing efficient attention methods and state-of-the-art backbone models.</p></p class="citation"></blockquote><h3 id=6180--137341-mge-a-training-free-and-efficient-model-generation-and-enhancement-scheme-xuan-wang-et-al-2024>(61/80 | 137/341) MGE: A Training-Free and Efficient Model Generation and Enhancement Scheme (Xuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuan Wang, Zeshan Pang, Yuliang Lu, Xuehu Yan. (2024)<br><strong>MGE: A Training-Free and Efficient Model Generation and Enhancement Scheme</strong><br><button class=copy-to-clipboard title="MGE: A Training-Free and Efficient Model Generation and Enhancement Scheme" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17486v1.pdf filename=2402.17486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To provide a foundation for the research of deep learning models, the construction of model pool is an essential step. This paper proposes a Training-Free and Efficient Model Generation and Enhancement Scheme (MGE). This scheme primarily considers two aspects during the model generation process: the distribution of model parameters and model performance. Experiments result shows that generated models are comparable to models obtained through normal training, and even superior in some cases. Moreover, the time consumed in generating models accounts for only 1% of the time required for normal model training. More importantly, with the enhancement of Evolution-MGE, generated models exhibits competitive generalization ability in <b>few-shot</b> tasks. And the behavioral dissimilarity of generated models has the potential of adversarial defense.</p></p class="citation"></blockquote><h3 id=6280--138341-emo-emote-portrait-alive----generating-expressive-portrait-videos-with-audio2video-diffusion-model-under-weak-conditions-linrui-tian-et-al-2024>(62/80 | 138/341) EMO: Emote Portrait Alive &ndash; Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions (Linrui Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo. (2024)<br><strong>EMO: Emote Portrait Alive &ndash; Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions</strong><br><button class=copy-to-clipboard title="EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17485v1.pdf filename=2402.17485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism.</p></p class="citation"></blockquote><h3 id=6380--139341-neural-video-compression-with-feature-modulation-jiahao-li-et-al-2024>(63/80 | 139/341) Neural Video Compression with Feature Modulation (Jiahao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Li, Bin Li, Yan Lu. (2024)<br><strong>Neural Video Compression with Feature Modulation</strong><br><button class=copy-to-clipboard title="Neural Video Compression with Feature Modulation" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17414v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17414v2.pdf filename=2402.17414v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emerging conditional coding-based neural video codec (NVC) shows superiority over commonly-used residual coding-based codec and the latest NVC already claims to outperform the best traditional codec. However, there still exist critical problems blocking the practicality of NVC. In this paper, we propose a powerful conditional coding-based NVC that solves two critical problems via feature modulation. The first is how to support a wide quality range in a single model. Previous NVC with this capability only supports about 3.8 dB PSNR range on average. To tackle this limitation, we modulate the latent feature of the current frame via the learnable <b>quantization</b> scaler. During the training, we specially design the uniform <b>quantization</b> parameter sampling mechanism to improve the harmonization of encoding and <b>quantization.</b> This results in a better learning of the <b>quantization</b> scaler and helps our NVC support about 11.4 dB PSNR range. The second is how to make NVC still work under a long prediction chain. We expose that the previous SOTA NVC has an obvious quality degradation problem when using a large intra-period setting. To this end, we propose modulating the temporal feature with a periodically refreshing mechanism to boost the quality. %Besides solving the above two problems, we also design a single model that can support both RGB and YUV colorspaces. Notably, under single intra-frame setting, our codec can achieve 29.7% bitrate saving over previous SOTA NVC with 16% MACs reduction. Our codec serves as a notable landmark in the journey of NVC evolution. The codes are at <a href=https://github.com/microsoft/DCVC>https://github.com/microsoft/DCVC</a>.</p></p class="citation"></blockquote><h3 id=6480--140341-accelerating-diffusion-sampling-with-optimized-time-steps-shuchen-xue-et-al-2024>(64/80 | 140/341) Accelerating Diffusion Sampling with Optimized Time Steps (Shuchen Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, Zhenguo Li. (2024)<br><strong>Accelerating Diffusion Sampling with Optimized Time Steps</strong><br><button class=copy-to-clipboard title="Accelerating Diffusion Sampling with Optimized Time Steps" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17376v1.pdf filename=2402.17376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion <b>probabilistic</b> <b>models</b> (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than $15$ seconds. Our extensive experiments on both unconditional and conditional sampling using pixel- and latent-space DPMs demonstrate that, when combined with the state-of-the-art sampling method UniPC, our optimized time steps significantly improve image generation performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet, compared to using uniform time steps.</p></p class="citation"></blockquote><h3 id=6580--141341-capt-category-level-articulation-estimation-from-a-single-point-cloud-using-transformer-lian-fu-et-al-2024>(65/80 | 141/341) CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer (Lian Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lian Fu, Ryoichi Ishikawa, Yoshihiro Sato, Takeshi Oishi. (2024)<br><strong>CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer</strong><br><button class=copy-to-clipboard title="CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17360v1.pdf filename=2402.17360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to estimate joint parameters is essential for various applications in robotics and computer vision. In this paper, we propose CAPT: category-level articulation estimation from a point cloud using <b>Transformer.</b> CAPT uses an end-to-end <b>transformer-based</b> architecture for joint parameter and state estimation of articulated objects from a single point cloud. The proposed CAPT methods accurately estimate joint parameters and states for various articulated objects with high precision and robustness. The paper also introduces a motion loss approach, which improves articulation estimation performance by emphasizing the dynamic features of articulated objects. Additionally, the paper presents a double voting strategy to provide the framework with coarse-to-fine parameter estimation. Experimental results on several category datasets demonstrate that our methods outperform existing alternatives for articulation estimation. Our research provides a promising solution for applying <b>Transformer-based</b> architectures in articulated object analysis.</p></p class="citation"></blockquote><h3 id=6680--142341-icp-flow-lidar-scene-flow-estimation-with-icp-yancong-lin-et-al-2024>(66/80 | 142/341) ICP-Flow: LiDAR Scene Flow Estimation with ICP (Yancong Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yancong Lin, Holger Caesar. (2024)<br><strong>ICP-Flow: LiDAR Scene Flow Estimation with ICP</strong><br><button class=copy-to-clipboard title="ICP-Flow: LiDAR Scene Flow Estimation with ICP" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17351v1.pdf filename=2402.17351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene flow characterizes the 3D motion between two LiDAR scans captured by an autonomous vehicle at nearby timesteps. Prevalent methods consider scene flow as point-wise unconstrained flow vectors that can be learned by either large-scale training beforehand or time-consuming optimization at inference. However, these methods do not take into account that objects in autonomous driving often move rigidly. We incorporate this rigid-motion assumption into our design, where the goal is to associate objects over scans and then estimate the locally rigid transformations. We propose ICP-Flow, a learning-free flow estimator. The core of our design is the conventional Iterative Closest Point (ICP) algorithm, which aligns the objects over time and outputs the corresponding rigid transformations. Crucially, to aid ICP, we propose a histogram-based initialization that discovers the most likely translation, thus providing a good starting point for ICP. The complete scene flow is then recovered from the rigid transformations. We outperform state-of-the-art baselines, including <b>supervised</b> models, on the Waymo dataset and perform competitively on Argoverse-v2 and nuScenes. Further, we train a feedforward neural network, <b>supervised</b> by the pseudo labels from our model, and achieve top performance among all models capable of real-time inference. We validate the advantage of our model on scene flow estimation with longer temporal gaps, up to 0.5 seconds where other models fail to deliver meaningful results.</p></p class="citation"></blockquote><h3 id=6780--143341-enhancing-hyperspectral-images-via-diffusion-model-and-group-autoencoder-super-resolution-network-zhaoyang-wang-et-al-2024>(67/80 | 143/341) Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder Super-resolution Network (Zhaoyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoyang Wang, Dongyang Li, Mingyang Zhang, Hao Luo, Maoguo Gong. (2024)<br><strong>Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder Super-resolution Network</strong><br><button class=copy-to-clipboard title="Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder Super-resolution Network" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17285v1.pdf filename=2402.17285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while <b>diffusion</b> <b>models</b> represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features. The direct application of <b>diffusion</b> <b>models</b> to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time. In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the <b>diffusion</b> <b>model</b> to construct a highly effective HSI SR model (DMGASR). Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the <b>diffusion</b> <b>model</b> works, thereby alleviating the difficulty of training the <b>diffusion</b> <b>model</b> while maintaining band correlation and considerably reducing inference time. Experimental results on both natural and remote sensing hyperspectral datasets demonstrate that the proposed method is superior to other state-of-the-art methods both visually and metrically.</p></p class="citation"></blockquote><h3 id=6880--144341-image-text-matching-with-multi-view-attention-rui-cheng-et-al-2024>(68/80 | 144/341) Image-Text Matching with Multi-View Attention (Rui Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Cheng, Wanqing Cui. (2024)<br><strong>Image-Text Matching with Multi-View Attention</strong><br><button class=copy-to-clipboard title="Image-Text Matching with Multi-View Attention" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17237v1.pdf filename=2402.17237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing two-stream models for <b>image-text</b> matching show good performance while ensuring retrieval speed and have received extensive attention from industry and academia. These methods use a single representation to encode image and text separately and get a matching score with cosine similarity or the inner product of vectors. However, the performance of the two-stream model is often sub-optimal. On the one hand, a single representation is challenging to cover complex content comprehensively. On the other hand, in this framework of lack of interaction, it is challenging to match multiple meanings which leads to information being ignored. To address the problems mentioned above and facilitate the performance of the two-stream model, we propose a multi-view attention approach for two-stream <b>image-text</b> matching MVAM (\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{M}odel). It first learns multiple image and text representations by diverse attention heads with different view codes. And then concatenate these representations into one for matching. A diversity objective is also used to promote diversity between attention heads. With this method, models are able to encode images and text from different views and attend to more key points. So we can get representations that contain more information. When doing retrieval tasks, the matching scores between images and texts can be calculated from different aspects, leading to better matching performance. Experiment results on MSCOCO and Flickr30K show that our proposed model brings improvements over existing models. Further case studies show that different attention heads can focus on different contents and finally obtain a more comprehensive representation.</p></p class="citation"></blockquote><h3 id=6980--145341-preserving-fairness-generalization-in-deepfake-detection-li-lin-et-al-2024>(69/80 | 145/341) Preserving Fairness Generalization in Deepfake Detection (Li Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Lin, Xinan He, Yan Ju, Xin Wang, Feng Ding, Shu Hu. (2024)<br><strong>Preserving Fairness Generalization in Deepfake Detection</strong><br><button class=copy-to-clipboard title="Preserving Fairness Generalization in Deepfake Detection" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-CY, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17229v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17229v1.pdf filename=2402.17229v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although effective deepfake detection models have been developed in recent years, recent studies have revealed that these models can result in unfair performance disparities among demographic groups, such as race and gender. This can lead to particular groups facing unfair targeting or exclusion from detection, potentially allowing misclassified deepfakes to manipulate public opinion and undermine trust in the model. The existing method for addressing this problem is providing a fair loss function. It shows good <b>fairness</b> performance for intra-domain evaluation but does not maintain <b>fairness</b> for cross-domain testing. This highlights the significance of <b>fairness</b> generalization in the fight against deepfakes. In this work, we propose the first method to address the <b>fairness</b> generalization problem in deepfake detection by simultaneously considering features, loss, and optimization aspects. Our method employs disentanglement learning to extract demographic and domain-agnostic forgery features, fusing them to encourage fair learning across a flattened loss landscape. Extensive experiments on prominent deepfake datasets demonstrate our method&rsquo;s effectiveness, surpassing state-of-the-art approaches in preserving <b>fairness</b> during cross-domain deepfake detection. The code is available at <a href=https://github.com/Purdue-M2/Fairness-Generalization>https://github.com/Purdue-M2/Fairness-Generalization</a></p></p class="citation"></blockquote><h3 id=7080--146341-advancing-generative-model-evaluation-a-novel-algorithm-for-realistic-image-synthesis-and-comparison-in-ocr-system-majid-memari-et-al-2024>(70/80 | 146/341) Advancing Generative Model Evaluation: A Novel Algorithm for Realistic Image Synthesis and Comparison in OCR System (Majid Memari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Majid Memari, Khaled R. Ahmed, Shahram Rahimi, Noorbakhsh Amiri Golilarz. (2024)<br><strong>Advancing Generative Model Evaluation: A Novel Algorithm for Realistic Image Synthesis and Comparison in OCR System</strong><br><button class=copy-to-clipboard title="Advancing Generative Model Evaluation: A Novel Algorithm for Realistic Image Synthesis and Comparison in OCR System" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Optical Character Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17204v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17204v3.pdf filename=2402.17204v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research addresses a critical challenge in the field of generative models, particularly in the generation and evaluation of synthetic images. Given the inherent complexity of generative models and the absence of a standardized procedure for their comparison, our study introduces a pioneering algorithm to objectively assess the realism of synthetic images. This approach significantly enhances the evaluation methodology by refining the Fr'echet Inception Distance (FID) score, allowing for a more precise and subjective assessment of image quality. Our algorithm is particularly tailored to address the challenges in generating and evaluating realistic images of Arabic handwritten digits, a task that has traditionally been near-impossible due to the subjective nature of realism in image generation. By providing a systematic and objective framework, our method not only enables the comparison of different generative models but also paves the way for improvements in their design and output. This breakthrough in evaluation and comparison is crucial for advancing the field of <b>OCR,</b> especially for scripts that present unique complexities, and sets a new standard in the generation and assessment of high-quality synthetic images.</p></p class="citation"></blockquote><h3 id=7180--147341-deep-umbra-a-generative-approach-for-sunlight-access-computation-in-urban-spaces-kazi-shahrukh-omar-et-al-2024>(71/80 | 147/341) Deep Umbra: A Generative Approach for Sunlight Access Computation in Urban Spaces (Kazi Shahrukh Omar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kazi Shahrukh Omar, Gustavo Moreira, Daniel Hodczak, Maryam Hosseini, Nicola Colaninno, Marcos Lage, Fabio Miranda. (2024)<br><strong>Deep Umbra: A Generative Approach for Sunlight Access Computation in Urban Spaces</strong><br><button class=copy-to-clipboard title="Deep Umbra: A Generative Approach for Sunlight Access Computation in Urban Spaces" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-CY, cs.CV<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17169v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17169v1.pdf filename=2402.17169v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sunlight and shadow play critical roles in how urban spaces are utilized, thrive, and grow. While access to sunlight is essential to the success of urban environments, shadows can provide shaded places to stay during the hot seasons, mitigate heat island effect, and increase pedestrian comfort levels. Properly quantifying sunlight access and shadows in large urban environments is key in tackling some of the important challenges facing cities today. In this paper, we propose Deep Umbra, a novel computational framework that enables the quantification of sunlight access and shadows at a global scale. Our framework is based on a conditional <b>generative</b> <b>adversarial</b> <b>network</b> that considers the physical form of cities to compute high-resolution spatial information of accumulated sunlight access for the different seasons of the year. We use data from seven different cities to train our model, and show, through an extensive set of experiments, its low overall RMSE (below 0.1) as well as its extensibility to cities that were not part of the training set. Additionally, we contribute a set of case studies and a comprehensive dataset with sunlight access information for more than 100 cities across six continents of the world. Deep Umbra is available at <a href=https://urbantk.org/shadows>https://urbantk.org/shadows</a>.</p></p class="citation"></blockquote><h3 id=7280--148341-sam-diffsr-structure-modulated-diffusion-model-for-image-super-resolution-chengcheng-wang-et-al-2024>(72/80 | 148/341) SAM-DiffSR: Structure-Modulated Diffusion Model for Image Super-Resolution (Chengcheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengcheng Wang, Zhiwei Hao, Yehui Tang, Jianyuan Guo, Yujie Yang, Kai Han, Yunhe Wang. (2024)<br><strong>SAM-DiffSR: Structure-Modulated Diffusion Model for Image Super-Resolution</strong><br><button class=copy-to-clipboard title="SAM-DiffSR: Structure-Modulated Diffusion Model for Image Super-Resolution" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17133v1.pdf filename=2402.17133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion-based</b> <b>super-resolution</b> (SR) models have recently garnered significant attention due to their potent restoration capabilities. But conventional <b>diffusion</b> <b>models</b> perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions. With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of <b>diffusion-based</b> <b>SR</b> model. However, directly integrating SAM into SR models will result in much higher computational cost. In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference. In the process of training, we encode structural position information into the segmentation mask from SAM. Then the encoded mask is integrated into the forward <b>diffusion</b> <b>process</b> by modulating it to the sampled noise. This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area. The <b>diffusion</b> <b>model</b> is trained to estimate this modulated noise. Crucially, our proposed framework does NOT change the reverse <b>diffusion</b> <b>process</b> and does NOT require SAM at inference. Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing <b>diffusion-based</b> <b>methods</b> by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset. The code and dataset are available at <a href=https://github.com/lose4578/SAM-DiffSR>https://github.com/lose4578/SAM-DiffSR</a>.</p></p class="citation"></blockquote><h3 id=7380--149341-charnerf-3d-character-generation-from-concept-art-eddy-chu-et-al-2024>(73/80 | 149/341) CharNeRF: 3D Character Generation from Concept Art (Eddy Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eddy Chu, Yiyang Chen, Chedy Raissi, Anand Bhojan. (2024)<br><strong>CharNeRF: 3D Character Generation from Concept Art</strong><br><button class=copy-to-clipboard title="CharNeRF: 3D Character Generation from Concept Art" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17115v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17115v1.pdf filename=2402.17115v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D modeling holds significant importance in the realms of AR/VR and gaming, allowing for both artistic creativity and practical applications. However, the process is often time-consuming and demands a high level of skill. In this paper, we present a novel approach to create volumetric representations of 3D characters from consistent turnaround concept art, which serves as the standard input in the 3D modeling industry. While Neural Radiance Field (NeRF) has been a game-changer in image-based 3D reconstruction, to the best of our knowledge, there is no known research that optimizes the pipeline for concept art. To harness the potential of concept art, with its defined body poses and specific view angles, we propose encoding it as priors for our model. We train the network to make use of these priors for various 3D points through a learnable view-direction-attended multi-head <b>self-attention</b> layer. Additionally, we demonstrate that a combination of ray sampling and surface sampling enhances the inference capabilities of our network. Our model is able to generate high-quality 360-degree views of characters. Subsequently, we provide a simple guideline to better leverage our model to extract the 3D mesh. It is important to note that our model&rsquo;s inferencing capabilities are influenced by the training data&rsquo;s characteristics, primarily focusing on characters with a single head, two arms, and two legs. Nevertheless, our methodology remains versatile and adaptable to concept art from diverse subject matters, without imposing any specific assumptions on the data.</p></p class="citation"></blockquote><h3 id=7480--150341-coupled-laplacian-eigenmaps-for-locally-aware-3d-rigid-point-cloud-matching-matteo-bastico-et-al-2024>(74/80 | 150/341) Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching (Matteo Bastico et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Bastico, Etienne Decencière, Laurent Corté, Yannick Tillier, David Ryckelynck. (2024)<br><strong>Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching</strong><br><button class=copy-to-clipboard title="Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17372v1.pdf filename=2402.17372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point cloud matching, a crucial technique in computer vision, medical and robotics fields, is primarily concerned with finding correspondences between pairs of point clouds or voxels. In some practical scenarios, emphasizing local differences is crucial for accurately identifying a correct match, thereby enhancing the overall robustness and reliability of the matching process. Commonly used shape descriptors have several limitations and often fail to provide meaningful local insights on the paired geometries. In this work, we propose a new technique, based on <b>graph</b> Laplacian eigenmaps, to match point clouds by taking into account fine local structures. To deal with the order and sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called Coupled Laplacian, that allows to easily generate aligned eigenspaces for multiple rigidly-registered geometries. We show that the similarity between those aligned high-dimensional spaces provides a locally meaningful score to match shapes. We initially evaluate the performance of the proposed technique in a point-wise manner, specifically focusing on the task of object anomaly localization using the MVTec 3D-AD dataset. Additionally, we define a new medical task, called automatic Bone Side Estimation (BSE), which we address through a global similarity score derived from coupled eigenspaces. In order to test it, we propose a <b>benchmark</b> collecting bone surface structures from various public datasets. Our matching technique, based on Coupled Laplacian, outperforms other methods by reaching an impressive accuracy on both tasks. The code to reproduce our experiments is publicly available at <a href=https://github.com/matteo-bastico/CoupledLaplacian>https://github.com/matteo-bastico/CoupledLaplacian</a> and in the Supplementary Code.</p></p class="citation"></blockquote><h3 id=7580--151341-learning-dynamic-tetrahedra-for-high-quality-talking-head-synthesis-zicheng-zhang-et-al-2024>(75/80 | 151/341) Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis (Zicheng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang. (2024)<br><strong>Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</strong><br><button class=copy-to-clipboard title="Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17364v1.pdf filename=2402.17364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works in implicit representations, such as Neural Radiance Fields (NeRF), have advanced the generation of realistic and animatable head avatars from video sequences. These implicit methods are still confronted by visual artifacts and jitters, since the lack of explicit geometric constraints poses a fundamental challenge in accurately modeling complex facial deformations. In this paper, we introduce Dynamic Tetrahedra (DynTet), a novel hybrid representation that encodes explicit dynamic meshes by neural networks to ensure geometric consistency across various motions and viewpoints. DynTet is parameterized by the coordinate-based networks which learn signed distance, deformation, and material texture, anchoring the training data into a predefined tetrahedra grid. Leveraging Marching Tetrahedra, DynTet efficiently decodes textured meshes with a consistent topology, enabling fast rendering through a differentiable rasterizer and supervision via a pixel loss. To enhance training efficiency, we incorporate classical 3D Morphable Models to facilitate <b>geometry</b> learning and define a canonical space for simplifying texture learning. These advantages are readily achievable owing to the effective geometric representation employed in DynTet. Compared with prior works, DynTet demonstrates significant improvements in fidelity, lip synchronization, and real-time performance according to various metrics. Beyond producing stable and visually appealing synthesis videos, our method also outputs the dynamic meshes which is promising to enable many emerging applications.</p></p class="citation"></blockquote><h3 id=7680--152341-phnet-patch-based-normalization-for-portrait-harmonization-karen-efremyan-et-al-2024>(76/80 | 152/341) PHNet: Patch-based Normalization for Portrait Harmonization (Karen Efremyan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karen Efremyan, Elizaveta Petrova, Evgeny Kaskov, Alexander Kapitanov. (2024)<br><strong>PHNet: Patch-based Normalization for Portrait Harmonization</strong><br><button class=copy-to-clipboard title="PHNet: Patch-based Normalization for Portrait Harmonization" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17561v1.pdf filename=2402.17561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A common problem for composite images is the incompatibility of their foreground and background components. Image harmonization aims to solve this problem, making the whole image look more authentic and coherent. Most existing solutions predict lookup tables (LUTs) or reconstruct images, utilizing various attributes of composite images. Recent approaches have primarily focused on employing global transformations like normalization and color curve rendering to achieve visual consistency, and they often overlook the importance of local visual coherence. We present a patch-based harmonization network consisting of novel Patch-based normalization (PN) blocks and a feature extractor based on statistical color transfer. Extensive experiments demonstrate the network&rsquo;s high generalization capability for different domains. Our network achieves state-of-the-art results on the iHarmony4 dataset. Also, we created a new human portrait harmonization dataset based on FFHQ and checked the proposed method to show the generalization ability by achieving the best metrics on it. The <b>benchmark</b> experiments confirm that the suggested patch-based normalization block and feature extractor effectively improve the network&rsquo;s capability to harmonize portraits. Our code and model baselines are publicly available.</p></p class="citation"></blockquote><h3 id=7780--153341-avs-net-point-sampling-with-adaptive-voxel-size-for-3d-point-cloud-analysis-hongcheng-yang-et-al-2024>(77/80 | 153/341) AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud Analysis (Hongcheng Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongcheng Yang, Dingkang Liang, Dingyuan Zhang, Xingyu Jiang, Zhe Liu, Zhikang Zou, Yingying Zhu. (2024)<br><strong>AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud Analysis</strong><br><button class=copy-to-clipboard title="AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud Analysis" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17521v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17521v1.pdf filename=2402.17521v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient downsampling plays a crucial role in point cloud learning, particularly for large-scale 3D scenes. Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information. This paper presents an advanced sampler that achieves both high accuracy and efficiency. The proposed method utilizes voxel-based sampling as a foundation, but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues. Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio. This ensures the sampling results exhibit a favorable distribution for comprehending various 3D objects or scenes. Additionally, we introduce a network compatible with arbitrary voxel sizes for sampling and feature extraction while maintaining high efficiency. Our method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet <b>benchmarks</b> with promising efficiency. Code will be available at <a href=https://github.com/yhc2021/AVS-Net>https://github.com/yhc2021/AVS-Net</a>.</p></p class="citation"></blockquote><h3 id=7880--154341-pandas-prototype-based-novel-class-discovery-and-detection-tyler-l-hayes-et-al-2024>(78/80 | 154/341) PANDAS: Prototype-based Novel Class Discovery and Detection (Tyler L. Hayes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tyler L. Hayes, César R. de Souza, Namil Kim, Jiwon Kim, Riccardo Volpi, Diane Larlus. (2024)<br><strong>PANDAS: Prototype-based Novel Class Discovery and Detection</strong><br><button class=copy-to-clipboard title="PANDAS: Prototype-based Novel Class Discovery and Detection" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17420v1.pdf filename=2402.17420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Object detectors are typically trained once and for all on a fixed set of classes. However, this closed-world assumption is unrealistic in practice, as new classes will inevitably emerge after the detector is deployed in the wild. In this work, we look at ways to extend a detector trained for a set of base classes so it can i) spot the presence of novel classes, and ii) automatically enrich its repertoire to be able to detect those newly discovered classes together with the base ones. We propose PANDAS, a method for novel class discovery and detection. It discovers clusters representing novel classes from unlabeled data, and represents old and new classes with prototypes. During inference, a distance-based classifier uses these prototypes to assign a label to each detected object instance. The simplicity of our method makes it widely applicable. We experimentally demonstrate the effectiveness of PANDAS on the VOC 2012 and COCO-to-LVIS <b>benchmarks.</b> It performs favorably against the state of the art for this task while being computationally more affordable.</p></p class="citation"></blockquote><h3 id=7980--155341-learning-exposure-correction-in-dynamic-scenes-jin-liu-et-al-2024>(79/80 | 155/341) Learning Exposure Correction in Dynamic Scenes (Jin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin Liu, Bo Wang, Chuanming Wang, Huiyuan Fu, Huadong Ma. (2024)<br><strong>Learning Exposure Correction in Dynamic Scenes</strong><br><button class=copy-to-clipboard title="Learning Exposure Correction in Dynamic Scenes" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17296v1.pdf filename=2402.17296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Capturing videos with wrong exposure usually produces unsatisfactory visual effects. While image exposure correction is a popular topic, the video counterpart is less explored in the literature. Directly applying prior image-based methods to input videos often results in temporal incoherence with low visual quality. Existing research in this area is also limited by the lack of high-quality <b>benchmark</b> datasets. To address these issues, we construct the first real-world paired video dataset, including both underexposure and overexposure dynamic scenes. To achieve spatial alignment, we utilize two DSLR cameras and a beam splitter to simultaneously capture improper and normal exposure videos. In addition, we propose a Video Exposure Correction Network (VECNet) based on Retinex theory, which incorporates a two-stream illumination learning mechanism to enhance the overexposure and underexposure factors, respectively. The estimated multi-frame reflectance and dual-path illumination components are fused at both feature and image levels, leading to visually appealing results. Experimental results demonstrate that the proposed method outperforms existing image exposure correction and underexposed video enhancement methods. The code and dataset will be available soon.</p></p class="citation"></blockquote><h3 id=8080--156341-in-defense-and-revival-of-bayesian-filtering-for-thermal-infrared-object-tracking-peng-gao-et-al-2024>(80/80 | 156/341) In Defense and Revival of Bayesian Filtering for Thermal Infrared Object Tracking (Peng Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Gao, Shi-Min Li, Feng Gao, Fei Wang, Ru-Yue Yuan, Hamido Fujita. (2024)<br><strong>In Defense and Revival of Bayesian Filtering for Thermal Infrared Object Tracking</strong><br><button class=copy-to-clipboard title="In Defense and Revival of Bayesian Filtering for Thermal Infrared Object Tracking" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17098v1.pdf filename=2402.17098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning-based methods monopolize the latest research in the field of thermal infrared (TIR) object tracking. However, relying solely on deep learning models to obtain better tracking results requires carefully selecting feature information that is beneficial to representing the target object and designing a reasonable template update strategy, which undoubtedly increases the difficulty of model design. Thus, recent TIR tracking methods face many challenges in complex scenarios. This paper introduces a novel Deep Bayesian Filtering (DBF) method to enhance TIR tracking in these challenging situations. DBF is distinctive in its dual-model structure: the system and observation models. The system model leverages motion data to estimate the potential positions of the target object based on two-dimensional Brownian motion, thus generating a prior probability. Following this, the observation model comes into play upon capturing the TIR image. It serves as a classifier and employs infrared information to ascertain the likelihood of these estimated positions, creating a likelihood probability. According to the guidance of the two models, the position of the target object can be determined, and the template can be dynamically updated. Experimental analysis across several <b>benchmark</b> datasets reveals that DBF achieves competitive performance, surpassing most existing TIR tracking methods in complex scenarios.</p></p class="citation"></blockquote><h2 id=cssd-4>cs.SD (4)</h2><h3 id=14--157341-songcomposer-a-large-language-model-for-lyric-and-melody-composition-in-song-generation-shuangrui-ding-et-al-2024>(1/4 | 157/341) SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation (Shuangrui Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuangrui Ding, Zihan Liu, Xiaoyi Dong, Pan Zhang, Rui Qian, Conghui He, Dahua Lin, Jiaqi Wang. (2024)<br><strong>SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation</strong><br><button class=copy-to-clipboard title="SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-CL, cs-SD, cs.SD, eess-AS<br>Keyword Score: 70<br>Keywords: Quantization, GPT, GPT-4, Instruction Following, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17645v1.pdf filename=2402.17645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present SongComposer, an innovative <b>LLM</b> designed for song composition. It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of <b>LLM.</b> Existing music-related <b>LLM</b> treated the music as <b>quantized</b> audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility. In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable <b>LLM</b> to explicitly compose songs like humans. In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct <b>LLM</b> understanding of musical symbols and realizes precise alignment between lyrics and melody. To impart basic music understanding to <b>LLM,</b> we carefully collected SongCompose-PT, a <b>large-scale</b> <b>song</b> <b>pretraining</b> dataset that includes lyrics, melodies, and paired lyrics-melodies in either Chinese or English. After adequate pre-training, 10K carefully crafted <b>QA</b> pairs are used to empower the <b>LLM</b> with the <b>instruction-following</b> <b>capability</b> and solve diverse tasks. With extensive experiments, SongComposer demonstrates superior performance in lyric-to-melody generation, melody-to-lyric generation, song continuation, and text-to-song creation, outperforming advanced <b>LLMs</b> like <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=24--158341-emotional-voice-messages-emovome-database-emotion-recognition-in-spontaneous-voice-messages-lucía-gómez-zaragozá-et-al-2024>(2/4 | 158/341) Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages (Lucía Gómez Zaragozá et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucía Gómez Zaragozá, Rocío del Amor, Elena Parra Vargas, Valery Naranjo, Mariano Alcañiz Raya, Javier Marín-Morales. (2024)<br><strong>Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages</strong><br><button class=copy-to-clipboard title="Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: I-5-1; I-5-4; I-2-7, cs-AI, cs-CL, cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Fine-tuning, BERT, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17496v1.pdf filename=2402.17496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Emotional</b> <b>Voice</b> Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven <b>emotion</b> <b>categories.</b> To set a baseline for future investigations using EMOVOME, we implemented <b>emotion</b> <b>recognition</b> models using both speech and audio transcriptions. For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we <b>fine-tuned</b> a multilingual <b>BERT</b> model and achieved 61.15% and 47.43% unweighted accuracy for valence and arousal respectively. This database will significantly contribute to research on <b>emotion</b> <b>recognition</b> in the wild, while also providing a unique natural and freely accessible resource for Spanish.</p></p class="citation"></blockquote><h3 id=34--159341-experimental-study-enhancing-voice-spoofing-detection-models-with-wav2vec-20-taein-kang-et-al-2024>(3/4 | 159/341) Experimental Study: Enhancing Voice Spoofing Detection Models with wav2vec 2.0 (Taein Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taein Kang, Soyul Han, Sunmook Choi, Jaejin Seo, Sanghyeok Chung, Seungeun Lee, Seungsang Oh, Il-Youp Kwak. (2024)<br><strong>Experimental Study: Enhancing Voice Spoofing Detection Models with wav2vec 2.0</strong><br><button class=copy-to-clipboard title="Experimental Study: Enhancing Voice Spoofing Detection Models with wav2vec 2.0" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: 00A71, I-2-6, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17127v1.pdf filename=2402.17127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional spoofing detection systems have heavily relied on the use of handcrafted features derived from speech data. However, a notable shift has recently emerged towards the direct utilization of raw speech waveforms, as demonstrated by methods like SincNet filters. This shift underscores the demand for more sophisticated audio sample features. Moreover, the success of deep learning models, particularly those utilizing large pretrained wav2vec 2.0 as a featurization front-end, highlights the importance of refined feature encoders. In response, this research assessed the representational capability of wav2vec 2.0 as an audio feature extractor, modifying the size of its pretrained <b>Transformer</b> layers through two key adjustments: (1) selecting a subset of layers starting from the leftmost one and (2) <b>fine-tuning</b> a portion of the selected layers from the rightmost one. We complemented this analysis with five spoofing detection back-end models, with a primary focus on AASIST, enabling us to pinpoint the optimal configuration for the selection and <b>fine-tuning</b> process. In contrast to conventional handcrafted features, our investigation identified several spoofing detection systems that achieve state-of-the-art performance in the ASVspoof 2019 LA dataset. This comprehensive exploration offers valuable insights into feature selection strategies, advancing the field of spoofing detection.</p></p class="citation"></blockquote><h3 id=44--160341-edtc-enhance-depth-of-text-comprehension-in-automated-audio-captioning-liwen-tan-et-al-2024>(4/4 | 160/341) EDTC: enhance depth of text comprehension in automated audio captioning (Liwen Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liwen Tan, Yin Cao, Yi Zhou. (2024)<br><strong>EDTC: enhance depth of text comprehension in automated audio captioning</strong><br><button class=copy-to-clipboard title="EDTC: enhance depth of text comprehension in automated audio captioning" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 13<br>Keywords: Contrastive Learning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17259v1.pdf filename=2402.17259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modality discrepancies have perpetually posed significant challenges within the realm of Automated Audio Captioning (AAC) and across all <b>multi-modal</b> domains. Facilitating models in comprehending text information plays a pivotal role in establishing a seamless connection between the two modalities of text and audio. While recent research has focused on closing the gap between these two modalities through <b>contrastive</b> <b>learning,</b> it is challenging to bridge the difference between both modalities using only simple <b>contrastive</b> <b>loss.</b> This paper introduces Enhance Depth of Text Comprehension (EDTC), which enhances the model&rsquo;s understanding of text information from three different perspectives. First, we propose a novel fusion module, FUSER, which aims to extract shared semantic information from different audio features through feature fusion. We then introduced TRANSLATOR, a novel alignment module designed to align audio features and text features along the tensor level. Finally, the weights are updated by adding momentum to the twin structure so that the model can learn information about both modalities at the same time. The resulting method achieves state-of-the-art performance on AudioCaps datasets and demonstrates results comparable to the state-of-the-art on Clotho datasets.</p></p class="citation"></blockquote><h2 id=cslg-63>cs.LG (63)</h2><h3 id=163--161341-ds-agent-automated-data-science-by-empowering-large-language-models-with-case-based-reasoning-siyuan-guo-et-al-2024>(1/63 | 161/341) DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning (Siyuan Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang. (2024)<br><strong>DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning</strong><br><button class=copy-to-clipboard title="DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Low-Resource, GPT, GPT-4, Code Generation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17453v1.pdf filename=2402.17453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we investigate the potential of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing <b>LLM</b> agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses <b>LLM</b> agent and case-based <b>reasoning</b> (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a <b>low-resource</b> deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct <b>code</b> <b>generation,</b> significantly reducing the demand on foundational capabilities of <b>LLMs.</b> Empirically, DS-Agent with <b>GPT-4</b> achieves an unprecedented 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative <b>LLMs</b> in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing $1.60 and $0.13 per run with <b>GPT-4,</b> respectively.</p></p class="citation"></blockquote><h3 id=263--162341-sinkhorn-distance-minimization-for-knowledge-distillation-xiao-cui-et-al-2024>(2/63 | 162/341) Sinkhorn Distance Minimization for Knowledge Distillation (Xiao Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Cui, Yulei Qin, Yuting Gao, Enwei Zhang, Zihan Xu, Tong Wu, Ke Li, Xing Sun, Wengang Zhou, Houqiang Li. (2024)<br><strong>Sinkhorn Distance Minimization for Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Sinkhorn Distance Minimization for Knowledge Distillation" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, GLUE, Large Language Model, Large Language Model, SuperGLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17110v1.pdf filename=2402.17110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>distillation</b> <b>(KD)</b> has been widely adopted to compress <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Existing <b>KD</b> methods investigate various divergence measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL), and Jensen-Shannon (JS) divergences. However, due to limitations inherent in their assumptions and definitions, these measures fail to deliver effective supervision when few distribution overlap exists between the teacher and the student. In this paper, we show that the aforementioned KL, RKL, and JS divergences respectively suffer from issues of mode-averaging, mode-collapsing, and mode-underestimation, which deteriorates logits-based <b>KD</b> for diverse NLP tasks. We propose the Sinkhorn <b>Knowledge</b> <b>Distillation</b> (SinKD) that exploits the Sinkhorn distance to ensure a nuanced and precise assessment of the disparity between teacher and student distributions. Besides, profit by properties of the Sinkhorn metric, we can get rid of sample-wise <b>KD</b> that restricts the perception of divergence in each teacher-student sample pair. Instead, we propose a batch-wise reformulation to capture geometric intricacies of distributions across samples in the high-dimensional space. Comprehensive evaluation on <b>GLUE</b> and <b>SuperGLUE,</b> in terms of comparability, validity, and generalizability, highlights our superiority over state-of-the-art methods on all kinds of <b>LLMs</b> with encoder-only, encoder-decoder, and decoder-only architectures.</p></p class="citation"></blockquote><h3 id=363--163341-intensive-care-as-one-big-sequence-modeling-problem-vadim-liventsev-et-al-2024>(3/63 | 163/341) Intensive Care as One Big Sequence Modeling Problem (Vadim Liventsev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vadim Liventsev, Tobias Fritz. (2024)<br><strong>Intensive Care as One Big Sequence Modeling Problem</strong><br><button class=copy-to-clipboard title="Intensive Care as One Big Sequence Modeling Problem" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Benchmarking, Foundation Model, Reinforcement Learning, Transfer Learning, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17501v1.pdf filename=2402.17501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being <b>Large</b> <b>Language</b> <b>Models)</b> to outperform task-specific approaches due to their capability for implicit <b>transfer</b> <b>learning.</b> To enable training of <b>foundation</b> <b>models</b> for Healthcare as well as leverage the capabilities of state of the art <b>Transformer</b> architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling <b>benchmark</b> derived by translating heterogenous clinical records from MIMIC-IV dataset into a uniform event stream format, train a baseline model and explore its capabilities.</p></p class="citation"></blockquote><h3 id=463--164341-advancing-sleep-detection-by-modelling-weak-label-sets-a-novel-weakly-supervised-learning-approach-matthias-boeker-et-al-2024>(4/63 | 164/341) Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach (Matthias Boeker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthias Boeker, Vajira Thambawita, Michael Riegler, Pål Halvorsen, Hugo L. Hammer. (2024)<br><strong>Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach</strong><br><button class=copy-to-clipboard title="Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Supervised Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17601v1.pdf filename=2402.17601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding sleep and activity patterns plays a crucial role in physical and mental health. This study introduces a novel approach for sleep detection using <b>weakly</b> <b>supervised</b> <b>learning</b> for scenarios where reliable ground truth labels are unavailable. The proposed method relies on a set of weak labels, derived from the predictions generated by conventional sleep detection algorithms. Introducing a novel approach, we suggest a novel generalised non-linear statistical model in which the number of weak sleep labels is modelled as outcome of a binomial distribution. The probability of sleep in the binomial distribution is linked to the outcomes of neural networks trained to detect sleep based on actigraphy. We show that maximizing the likelihood function of the model, is equivalent to minimizing the soft cross-entropy loss. Additionally, we explored the use of the Brier score as a loss function for weak labels. The efficacy of the suggested modelling framework was demonstrated using the Multi-Ethnic Study of Atherosclerosis dataset. A \gls{lstm} trained on the soft cross-entropy outperformed conventional sleep detection algorithms, other neural network architectures and loss functions in accuracy and model calibration. This research not only advances sleep detection techniques in scenarios where ground truth data is scarce but also contributes to the broader field of <b>weakly</b> <b>supervised</b> <b>learning</b> by introducing innovative approach in modelling sets of weak labels.</p></p class="citation"></blockquote><h3 id=563--165341-localgcl-local-aware-contrastive-learning-for-graphs-haojun-jiang-et-al-2024>(5/63 | 165/341) LocalGCL: Local-aware Contrastive Learning for Graphs (Haojun Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haojun Jiang, Jiawei Sun, Jie Li, Chentao Wu. (2024)<br><strong>LocalGCL: Local-aware Contrastive Learning for Graphs</strong><br><button class=copy-to-clipboard title="LocalGCL: Local-aware Contrastive Learning for Graphs" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 48<br>Keywords: Graph, Contrastive Learning, Representation Learning, Self-supervised Learning, Self-supervised Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17345v1.pdf filename=2402.17345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>representation</b> <b>learning</b> (GRL) makes considerable progress recently, which encodes <b>graphs</b> with topological structures into low-dimensional embeddings. Meanwhile, the time-consuming and costly process of annotating <b>graph</b> labels manually <b>prompts</b> the growth of <b>self-supervised</b> <b>learning</b> (SSL) techniques. As a dominant approach of SSL, <b>Contrastive</b> <b>learning</b> (CL) learns discriminative <b>representations</b> <b>by</b> differentiating between positive and negative samples. However, when applied to <b>graph</b> data, it overemphasizes global patterns while neglecting local structures. To tackle the above issue, we propose \underline{Local}-aware \underline{G}raph \underline{C}ontrastive \underline{L}earning (\textbf{\methnametrim}), a <b>self-supervised</b> <b>learning</b> framework that supplementarily captures local <b>graph</b> information with masking-based modeling compared with vanilla <b>contrastive</b> <b>learning.</b> Extensive experiments validate the superiority of \methname against state-of-the-art methods, demonstrating its promise as a comprehensive <b>graph</b> <b>representation</b> <b>learner.</b></p></p class="citation"></blockquote><h3 id=663--166341-hybrid-square-neural-ode-causal-modeling-bob-junyi-zou-et-al-2024>(6/63 | 166/341) Hybrid Square Neural ODE Causal Modeling (Bob Junyi Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bob Junyi Zou, Matthew E. Levine, Dessi P. Zaharieva, Ramesh Johari, Emily B. Fox. (2024)<br><strong>Hybrid Square Neural ODE Causal Modeling</strong><br><button class=copy-to-clipboard title="Hybrid Square Neural ODE Causal Modeling" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-AP, stat-ME<br>Keyword Score: 45<br>Keywords: Black Box, Counter-factual, Counterfactual Reasoning, Grounding, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17233v1.pdf filename=2402.17233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hybrid models combine mechanistic ODE-based dynamics with flexible and expressive neural network components. Such models have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal <b>grounding</b> (e.g., for <b>counterfactual</b> <b>reasoning).</b> The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as hybrid models become more flexible, the causal <b>grounding</b> provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: ranking of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a causal loss that we combine with the standard predictive loss to arrive at a hybrid loss that biases our learning towards causally valid hybrid models. We demonstrate our ability to achieve a win-win &ndash; state-of-the-art predictive performance and causal validity &ndash; in the challenging task of modeling glucose dynamics during exercise.</p></p class="citation"></blockquote><h3 id=763--167341-temporal-logic-specification-conditioned-decision-transformer-for-offline-safe-reinforcement-learning-zijian-guo-et-al-2024>(7/63 | 167/341) Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning (Zijian Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Guo, Weichao Zhou, Wenchao Li. (2024)<br><strong>Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Reinforcement Learning, Supervised Learning, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17217v1.pdf filename=2402.17217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Offline safe <b>reinforcement</b> <b>learning</b> (RL) aims to train a constraint satisfaction policy from a fixed dataset. Current state-of-the-art approaches are based on <b>supervised</b> <b>learning</b> with a conditioned policy. However, these approaches fall short in real-world applications that involve complex tasks with rich temporal and logical structures. In this paper, we propose temporal logic Specification-conditioned Decision <b>Transformer</b> (SDT), a novel framework that harnesses the expressive power of signal temporal logic (STL) to specify complex temporal rules that an agent should follow and the sequential modeling capability of Decision <b>Transformer</b> (DT). Empirical evaluations on the DSRL <b>benchmarks</b> demonstrate the better capacity of SDT in learning safe and high-reward policies compared with existing approaches. In addition, SDT shows good alignment with respect to different desired degrees of satisfaction of the STL specification that it is conditioned on.</p></p class="citation"></blockquote><h3 id=863--168341-unsupervised-zero-shot-reinforcement-learning-via-functional-reward-encodings-kevin-frans-et-al-2024>(8/63 | 168/341) Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings (Kevin Frans et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Frans, Seohong Park, Pieter Abbeel, Sergey Levine. (2024)<br><strong>Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings</strong><br><button class=copy-to-clipboard title="Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Reinforcement Learning, Unsupervised Learning, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17135v1.pdf filename=2402.17135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a <b>zero-shot</b> manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this <b>zero-shot</b> RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a <b>transformer-based</b> variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general <b>unsupervised</b> reward functions, but also provides a way to solve any new downstream tasks in a <b>zero-shot</b> manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random <b>unsupervised</b> reward functions can generalize to solve novel tasks in a range of simulated robotic <b>benchmarks,</b> often outperforming previous <b>zero-shot</b> RL and offline RL methods. Code for this project is provided at: <a href=https://github.com/kvfrans/fre>https://github.com/kvfrans/fre</a></p></p class="citation"></blockquote><h3 id=963--169341-collaborative-learning-of-common-latent-representations-in-routinely-collected-multivariate-icu-physiological-signals-hollan-haule-et-al-2024>(9/63 | 169/341) Collaborative learning of common latent representations in routinely collected multivariate ICU physiological signals (Hollan Haule et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hollan Haule, Ian Piper, Patricia Jones, Tsz-Yan Milly Lo, Javier Escudero. (2024)<br><strong>Collaborative learning of common latent representations in routinely collected multivariate ICU physiological signals</strong><br><button class=copy-to-clipboard title="Collaborative learning of common latent representations in routinely collected multivariate ICU physiological signals" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Autoencoder, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17917v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17917v1.pdf filename=2402.17917v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Intensive Care Units (ICU), the abundance of multivariate time series presents an opportunity for machine learning (ML) to enhance patient phenotyping. In contrast to previous research focused on electronic health records (EHR), here we propose an ML approach for phenotyping using routinely collected physiological time series data. Our new algorithm integrates <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> networks with collaborative filtering concepts to identify common physiological states across patients. Tested on real-world ICU clinical data for intracranial hypertension (IH) detection in patients with brain injury, our method achieved an area under the curve (AUC) of 0.889 and average precision (AP) of 0.725. Moreover, our algorithm outperforms <b>autoencoders</b> in learning more structured latent representations of the physiological signals. These findings highlight the promise of our methodology for patient phenotyping, leveraging routinely collected multivariate time series to improve clinical care practices.</p></p class="citation"></blockquote><h3 id=1063--170341-meta-tasks-an-alternative-view-on-meta-learning-regularization-mohammad-rostami-et-al-2024>(10/63 | 170/341) Meta-Tasks: An alternative view on Meta-Learning Regularization (Mohammad Rostami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Rostami, Atik Faysal, Huaxia Wang, Avimanyu Sahoo, Ryan Antle. (2024)<br><strong>Meta-Tasks: An alternative view on Meta-Learning Regularization</strong><br><button class=copy-to-clipboard title="Meta-Tasks: An alternative view on Meta-Learning Regularization" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Few-shot, Few-shot Learning, Meta Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18599v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18599v1.pdf filename=2402.18599v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>learning</b> (FSL) is a challenging machine learning problem due to a scarcity of labeled data. The ability to generalize effectively on both novel and training tasks is a significant barrier to FSL. This paper proposes a novel solution that can generalize to both training and novel tasks while also utilizing unlabeled samples. The method refines the embedding model before updating the outer loop using <b>unsupervised</b> techniques as ``meta-tasks&rsquo;&rsquo;. The experimental results show that our proposed method performs well on novel and training tasks, with faster and better convergence, lower generalization, and standard deviation error, indicating its potential for practical applications in FSL. The experimental results show that the proposed method outperforms prototypical networks by 3.9%.</p></p class="citation"></blockquote><h3 id=1163--171341-independent-learning-in-constrained-markov-potential-games-philip-jordan-et-al-2024>(11/63 | 171/341) Independent Learning in Constrained Markov Potential Games (Philip Jordan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Jordan, Anas Barakat, Niao He. (2024)<br><strong>Independent Learning in Constrained Markov Potential Games</strong><br><button class=copy-to-clipboard title="Independent Learning in Constrained Markov Potential Games" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-GT, cs-LG, cs-MA, cs.LG<br>Keyword Score: 40<br>Keywords: Markov Game, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17885v1.pdf filename=2402.17885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Constrained <b>Markov</b> <b>games</b> offer a formal mathematical framework for modeling multi-agent <b>reinforcement</b> <b>learning</b> problems where the behavior of the agents is subject to constraints. In this work, we focus on the recently introduced class of constrained <b>Markov</b> <b>Potential</b> Games. While centralized algorithms have been proposed for solving such constrained games, the design of converging independent learning algorithms tailored for the constrained setting remains an open question. We propose an independent policy gradient algorithm for learning approximate constrained Nash equilibria: Each agent observes their own actions and rewards, along with a shared state. Inspired by the optimization literature, our algorithm performs proximal-point-like updates augmented with a regularized constraint set. Each proximal step is solved inexactly using a stochastic switching gradient algorithm. Notably, our algorithm can be implemented independently without a centralized coordination mechanism requiring turn-based agent updates. Under some technical constraint qualification conditions, we establish convergence guarantees towards constrained approximate Nash equilibria. We perform <b>simulations</b> to illustrate our results.</p></p class="citation"></blockquote><h3 id=1263--172341-understanding-neural-network-binarization-with-forward-and-backward-proximal-quantizers-yiwei-lu-et-al-2024>(12/63 | 172/341) Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers (Yiwei Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwei Lu, Yaoliang Yu, Xinlin Li, Vahid Partovi Nia. (2024)<br><strong>Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers</strong><br><button class=copy-to-clipboard title="Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Vision Transformer, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17710v1.pdf filename=2402.17710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In neural network binarization, BinaryConnect (BC) and its variants are considered the standard. These methods apply the sign function in their forward pass and their respective gradients are backpropagated to update the weights. However, the derivative of the sign function is zero whenever defined, which consequently freezes training. Therefore, implementations of BC (e.g., BNN) usually replace the derivative of sign in the backward computation with identity or other approximate gradient alternatives. Although such practice works well empirically, it is largely a heuristic or &lsquo;&rsquo;training trick.&rsquo;&rsquo; We aim at shedding some light on these training tricks from the optimization perspective. Building from existing theory on ProxConnect (PC, a generalization of BC), we (1) equip PC with different forward-backward quantizers and obtain ProxConnect++ (PC++) that includes existing binarization techniques as special cases; (2) derive a principled way to synthesize forward-backward quantizers with automatic theoretical guarantees; (3) illustrate our theory by proposing an enhanced binarization algorithm BNN++; (4) conduct image classification experiments on <b>CNNs</b> and <b>vision</b> <b>transformers,</b> and empirically verify that BNN++ generally achieves competitive results on binarizing these models.</p></p class="citation"></blockquote><h3 id=1363--173341-securing-reliability-a-brief-overview-on-enhancing-in-context-learning-for-foundation-models-yunpeng-huang-et-al-2024>(13/63 | 173/341) Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models (Yunpeng Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunpeng Huang, Yaonan Gu, Jingwei Xu, Zhihong Zhu, Zhaorun Chen, Xiaoxing Ma. (2024)<br><strong>Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models</strong><br><button class=copy-to-clipboard title="Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Foundation Model, In-context Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17671v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17671v1.pdf filename=2402.17671v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>foundation</b> <b>models</b> (FMs) continue to shape the landscape of AI, the <b>in-context</b> <b>learning</b> <b>(ICL)</b> paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency. Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem. In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within <b>ICL</b> frameworks, focusing on four key methodologies, each with its corresponding subgoals. We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent <b>ICL</b> environment, thereby unlocking their vast potential.</p></p class="citation"></blockquote><h3 id=1463--174341-variational-learning-is-effective-for-large-deep-networks-yuesong-shen-et-al-2024>(14/63 | 174/341) Variational Learning is Effective for Large Deep Networks (Yuesong Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi, Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz Khan, Thomas Möllenhoff. (2024)<br><strong>Variational Learning is Effective for Large Deep Networks</strong><br><button class=copy-to-clipboard title="Variational Learning is Effective for Large Deep Networks" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 40<br>Keywords: Fine-tuning, GPT, GPT-2, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17641v1.pdf filename=2402.17641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We give extensive empirical evidence against the common belief that variational learning is ineffective for <b>large</b> <b>neural</b> <b>networks.</b> We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training <b>large</b> <b>networks</b> <b>such</b> as <b>GPT-2</b> and ResNets from scratch. IVON&rsquo;s computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve <b>fine-tuning</b> and model merging in <b>Large</b> <b>Language</b> <b>Models,</b> accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.</p></p class="citation"></blockquote><h3 id=1563--175341-prioritizing-informative-features-and-examples-for-deep-learning-from-noisy-data-dongmin-park-2024>(15/63 | 175/341) Prioritizing Informative Features and Examples for Deep Learning from Noisy Data (Dongmin Park, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongmin Park. (2024)<br><strong>Prioritizing Informative Features and Examples for Deep Learning from Noisy Data</strong><br><button class=copy-to-clipboard title="Prioritizing Informative Features and Examples for Deep Learning from Noisy Data" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Active Learning, Out-of-distribution, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00013v1.pdf filename=2403.00013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this dissertation, we propose a systemic framework that prioritizes informative features and examples to enhance each stage of the development process. Specifically, we prioritize informative features and examples and improve the performance of feature learning, data labeling, and data selection. We first propose an approach to extract only informative features that are inherent to solving a target task by using auxiliary <b>out-of-distribution</b> data. We deactivate the noise features in the target distribution by using that in the <b>out-of-distribution</b> data. Next, we introduce an approach that prioritizes informative examples from unlabeled noisy data in order to reduce the labeling cost of <b>active</b> <b>learning.</b> In order to solve the purity-information dilemma, where an attempt to select informative examples induces the selection of many noisy examples, we propose a meta-model that finds the best balance between purity and informativeness. Lastly, we suggest an approach that prioritizes informative examples from labeled noisy data to preserve the performance of data selection. For labeled image noise data, we propose a data selection method that considers the confidence of neighboring samples to maintain the performance of the state-of-the-art Re-labeling models. For labeled text noise data, we present an instruction selection method that takes diversity into account for ranking the quality of instructions with <b>prompting,</b> thereby enhancing the performance of aligned <b>large</b> <b>language</b> <b>models.</b> Overall, our unified framework induces the deep learning development process robust to noisy data, thereby effectively mitigating noisy features and examples in real-world applications.</p></p class="citation"></blockquote><h3 id=1663--176341-representation-learning-in-multiplex-graphs-where-and-how-to-fuse-information-piotr-bielak-et-al-2024>(16/63 | 176/341) Representation learning in multiplex graphs: Where and how to fuse information? (Piotr Bielak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Piotr Bielak, Tomasz Kajdanowicz. (2024)<br><strong>Representation learning in multiplex graphs: Where and how to fuse information?</strong><br><button class=copy-to-clipboard title="Representation learning in multiplex graphs: Where and how to fuse information?" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 38<br>Keywords: Graph, Graph Neural Network, Representation Learning, Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17906v1.pdf filename=2402.17906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>unsupervised</b> and <b>self-supervised</b> <b>graph</b> <b>representation</b> <b>learning</b> has gained popularity in the research community. However, most proposed methods are focused on homogeneous networks, whereas real-world <b>graphs</b> often contain multiple node and edge types. Multiplex <b>graphs,</b> a special type of heterogeneous <b>graphs,</b> possess richer information, provide better modeling capabilities and integrate more detailed data from potentially different sources. The diverse edge types in multiplex <b>graphs</b> provide more context and insights into the underlying processes of <b>representation</b> <b>learning.</b> In this paper, we tackle the problem of learning <b>representations</b> <b>for</b> nodes in multiplex networks in an <b>unsupervised</b> or <b>self-supervised</b> manner. To that end, we explore diverse information fusion schemes performed at different levels of the <b>graph</b> processing pipeline. The detailed analysis and experimental evaluation of various scenarios inspired us to propose improvements in how to construct <b>GNN</b> architectures that deal with multiplex <b>graphs.</b></p></p class="citation"></blockquote><h3 id=1763--177341-principled-architecture-aware-scaling-of-hyperparameters-wuyang-chen-et-al-2024>(17/63 | 177/341) Principled Architecture-aware Scaling of Hyperparameters (Wuyang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wuyang Chen, Junru Wu, Zhangyang Wang, Boris Hanin. (2024)<br><strong>Principled Architecture-aware Scaling of Hyperparameters</strong><br><button class=copy-to-clipboard title="Principled Architecture-aware Scaling of Hyperparameters" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17440v1.pdf filename=2402.17440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process. Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios. However, most designs or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters. In this work, we precisely characterize the dependence of initializations and maximal learning rates on the network architecture, which includes the network depth, width, <b>convolutional</b> <b>kernel</b> <b>size,</b> and connectivity patterns. By pursuing every parameter to be maximally updated with the same mean squared change in pre-activations, we can generalize our initialization and learning rates across MLPs (multi-layer perception) and <b>CNNs</b> <b>(convolutional</b> <b>neural</b> <b>network)</b> with sophisticated <b>graph</b> topologies. We verify our principles with comprehensive experiments. More importantly, our strategy further sheds light on advancing current <b>benchmarks</b> for architecture design. A fair comparison of AutoML algorithms requires accurate network rankings. However, we demonstrate that network rankings can be easily changed by better training networks in <b>benchmarks</b> with our architecture-aware learning rates and initialization.</p></p class="citation"></blockquote><h3 id=1863--178341-material-microstructure-design-using-vae-regression-with-multimodal-prior-avadhut-sardeshmukh-et-al-2024>(18/63 | 178/341) Material Microstructure Design Using VAE-Regression with Multimodal Prior (Avadhut Sardeshmukh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avadhut Sardeshmukh, Sreedhar Reddy, BP Gautham, Pushpak Bhattacharyya. (2024)<br><strong>Material Microstructure Design Using VAE-Regression with Multimodal Prior</strong><br><button class=copy-to-clipboard title="Material Microstructure Design Using VAE-Regression with Multimodal Prior" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-mtrl-sci, cs-LG, cs.LG, stat-ML<br>Keyword Score: 36<br>Keywords: Autoencoder, Multi-modal, Multi-modal, Reconstruction Loss, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17806v1.pdf filename=2402.17806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a <b>variational</b> <b>autoencoder</b> (VAE)-based model for building forward and inverse structure-property linkages, a problem of paramount importance in computational materials science. Our model systematically combines VAE with regression, linking the two models through a two-level prior conditioned on the regression variables. The regression loss is optimized jointly with the <b>reconstruction</b> <b>loss</b> of the <b>variational</b> <b>autoencoder,</b> learning microstructure features relevant for property prediction and <b>reconstruction.</b> <b>The</b> resultant model can be used for both forward and inverse prediction i.e., for predicting the properties of a given microstructure as well as for predicting the microstructure required to obtain given properties. Since the inverse problem is ill-posed (one-to-many), we derive the objective function using a <b>multi-modal</b> Gaussian mixture prior enabling the model to infer multiple microstructures for a target set of properties. We show that for forward prediction, our model is as accurate as state-of-the-art forward-only models. Additionally, our method enables direct inverse inference. We show that the microstructures inferred using our model achieve desired properties reasonably accurately, avoiding the need for expensive optimization loops.</p></p class="citation"></blockquote><h3 id=1963--179341-reinforced-in-context-black-box-optimization-lei-song-et-al-2024>(19/63 | 179/341) Reinforced In-Context Black-Box Optimization (Lei Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Song, Chenxiao Gao, Ke Xue, Chenyang Wu, Dong Li, Jianye Hao, Zongzhang Zhang, Chao Qian. (2024)<br><strong>Reinforced In-Context Black-Box Optimization</strong><br><button class=copy-to-clipboard title="Reinforced In-Context Black-Box Optimization" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 35<br>Keywords: Black Box, Meta Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17423v1.pdf filename=2402.17423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Black-Box</b> <b>Optimization</b> (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in <b>meta-learning</b> <b>particular</b> components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the <b>in-context</b> <b>learning</b> ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumulative regret of the histories. The integration of regret-to-go tokens enables RIBBO to automatically generate sequences of query points that satisfy the user-desired regret, which is verified by its universally good empirical performance on diverse problems, including BBOB functions, hyper-parameter optimization and robot control problems.</p></p class="citation"></blockquote><h3 id=2063--180341-fraud-detection-with-binding-global-and-local-relational-interaction-haolin-li-et-al-2024>(20/63 | 180/341) Fraud Detection with Binding Global and Local Relational Interaction (Haolin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haolin Li, Shuyang Jiang, Lifeng Zhang, Siyuan Du, Guangnan Ye, Hongfeng Chai. (2024)<br><strong>Fraud Detection with Binding Global and Local Relational Interaction</strong><br><button class=copy-to-clipboard title="Fraud Detection with Binding Global and Local Relational Interaction" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17472v1.pdf filename=2402.17472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Network</b> has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view. Recently, <b>Transformer</b> network with great sequence encoding ability, has also outperformed other <b>GNN-based</b> methods in literatures. However, both <b>GNN-based</b> and <b>Transformer-based</b> networks only encode one perspective of the whole <b>graph,</b> <b>while</b> <b>GNN</b> encodes global features and <b>Transformer</b> network encodes local ones. Furthermore, previous works ignored encoding global interaction features of the heterogeneous <b>graph</b> <b>with</b> <b>separate</b> networks, thus leading to suboptimal performance. In this work, we present a novel framework called Relation-Aware <b>GNN</b> with <b>transFormer</b> (RAGFormer) which simultaneously embeds local and global features into a target node. The simple yet effective network applies a modified GAGA module where each <b>transformer</b> layer is followed by a cross-relation aggregation layer, to encode local embeddings and node interactions across different relations. Apart from the <b>Transformer-based</b> network, we further introduce a Relation-Aware <b>GNN</b> module to learn global embeddings, which is later merged into the local embeddings by an attention fusion module and a skip connection. Extensive experiments on two popular public datasets and an industrial dataset demonstrate that RAGFormer achieves the state-of-the-art performance. Substantial analysis experiments validate the effectiveness of each submodule of RAGFormer and its high efficiency in utilizing small-scale data and low hyper-parameter sensitivity.</p></p class="citation"></blockquote><h3 id=2163--181341-why-do-learning-rates-transfer-reconciling-optimization-and-scaling-limits-for-deep-learning-lorenzo-noci-et-al-2024>(21/63 | 181/341) Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning (Lorenzo Noci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Noci, Alexandru Meterez, Thomas Hofmann, Antonio Orvieto. (2024)<br><strong>Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning</strong><br><button class=copy-to-clipboard title="Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Vision Transformer, Benchmarking, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17457v1.pdf filename=2402.17457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes. In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time. On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at different scales, thus preventing learning rate transfer. But what causes these differences in the sharpness dynamics? Through a connection between the spectra of the Hessian and the NTK matrix, we argue that the cause lies in the presence (for $\mu$P) or progressive absence (for the NTK regime) of feature learning, which results in a different evolution of the NTK, and thus of the sharpness. We corroborate our claims with a substantial suite of experiments, covering a wide range of datasets and architectures: from ResNets and <b>Vision</b> <b>Transformers</b> trained on <b>benchmark</b> <b>vision</b> <b>datasets</b> to <b>Transformers-based</b> language models trained on WikiText</p></p class="citation"></blockquote><h3 id=2263--182341-inpainting-computational-fluid-dynamics-with-deep-learning-dule-shu-et-al-2024>(22/63 | 182/341) Inpainting Computational Fluid Dynamics with Deep Learning (Dule Shu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dule Shu, Wilson Zhen, Zijie Li, Amir Barati Farimani. (2024)<br><strong>Inpainting Computational Fluid Dynamics with Deep Learning</strong><br><button class=copy-to-clipboard title="Inpainting Computational Fluid Dynamics with Deep Learning" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 33<br>Keywords: Benchmarking, Quantization, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17185v1.pdf filename=2402.17185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fluid data completion is a research problem with high potential benefit for both experimental and computational fluid dynamics. An effective fluid data completion method reduces the required number of sensors in a fluid dynamics experiment, and allows a coarser and more adaptive mesh for a Computational Fluid Dynamics (CFD) <b>simulation.</b> However, the ill-posed nature of the fluid data completion problem makes it prohibitively difficult to obtain a theoretical solution and presents high numerical uncertainty and instability for a data-driven approach (e.g., a neural network model). To address these challenges, we leverage recent advancements in computer vision, employing the vector <b>quantization</b> technique to map both complete and incomplete fluid data spaces onto discrete-valued lower-dimensional representations via a two-stage learning procedure. We demonstrated the effectiveness of our approach on Kolmogorov flow data (Reynolds number: 1000) occluded by masks of different size and arrangement. Experimental results show that our proposed model consistently outperforms <b>benchmark</b> models under different occlusion settings in terms of point-wise reconstruction accuracy as well as turbulent energy spectrum and vorticity distribution.</p></p class="citation"></blockquote><h3 id=2363--183341-generative-learning-for-forecasting-the-dynamics-of-complex-systems-han-gao-et-al-2024>(23/63 | 183/341) Generative Learning for Forecasting the Dynamics of Complex Systems (Han Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Gao, Sebastian Kaltenbach, Petros Koumoutsakos. (2024)<br><strong>Generative Learning for Forecasting the Dynamics of Complex Systems</strong><br><button class=copy-to-clipboard title="Generative Learning for Forecasting the Dynamics of Complex Systems" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-comp-ph, physics-flu-dyn, stat-ML<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17157v1.pdf filename=2402.17157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce generative models for accelerating <b>simulations</b> of complex systems through learning and evolving their effective dynamics. In the proposed Generative Learning of Effective Dynamics (G-LED), instances of high dimensional data are down sampled to a lower dimensional manifold that is evolved through an auto-regressive attention mechanism. In turn, Bayesian <b>diffusion</b> <b>models,</b> that map this low-dimensional manifold onto its corresponding high-dimensional space, capture the statistics of the system dynamics. We demonstrate the capabilities and drawbacks of G-LED in <b>simulations</b> of several <b>benchmark</b> systems, including the Kuramoto-Sivashinsky (KS) equation, two-dimensional high Reynolds number flow over a backward-facing step, and <b>simulations</b> of three-dimensional turbulent channel flow. The results demonstrate that generative learning offers new frontiers for the accurate forecasting of the statistical properties of complex systems at a reduced computational cost.</p></p class="citation"></blockquote><h3 id=2463--184341-preroutgnn-for-timing-prediction-with-order-preserving-partition-global-circuit-pre-training-local-delay-learning-and-attentional-cell-modeling-ruizhe-zhong-et-al-2024>(24/63 | 184/341) PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling (Ruizhe Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruizhe Zhong, Junjie Ye, Zhentao Tang, Shixiong Kai, Mingxuan Yuan, Jianye Hao, Junchi Yan. (2024)<br><strong>PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling</strong><br><button class=copy-to-clipboard title="PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph Convolutional Network, Message-Passing, Graph, Graph Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00012v1.pdf filename=2403.00012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-routing timing prediction has been recently studied for evaluating the quality of a candidate cell placement in chip design. It involves directly estimating the timing metrics for both pin-level (slack, slew) and edge-level (net delay, cell delay), without time-consuming routing. However, it often suffers from signal decay and error accumulation due to the long timing paths in large-scale industrial circuits. To address these challenges, we propose a two-stage approach. First, we propose global circuit training to pre-train a <b>graph</b> <b>auto-encoder</b> that learns the global <b>graph</b> <b>embedding</b> from circuit netlist. Second, we use a novel node updating scheme for message passing on <b>GCN,</b> following the topological sorting sequence of the learned <b>graph</b> <b>embedding</b> and circuit <b>graph.</b> <b>This</b> scheme residually models the local time delay between two adjacent pins in the updating sequence, and extracts the lookup table information inside each cell via a new attention mechanism. To handle large-scale circuits efficiently, we introduce an order preserving partition scheme that reduces memory consumption while maintaining the topological dependencies. Experiments on 21 real world circuits achieve a new SOTA R2 of 0.93 for slack prediction, which is significantly surpasses 0.59 by previous SOTA method. Code will be available at: <a href=https://github.com/Thinklab-SJTU/EDA-AI>https://github.com/Thinklab-SJTU/EDA-AI</a>.</p></p class="citation"></blockquote><h3 id=2563--185341-latent-neural-pde-solver-a-reduced-order-modelling-framework-for-partial-differential-equations-zijie-li-et-al-2024>(25/63 | 185/341) Latent Neural PDE Solver: a reduced-order modelling framework for partial differential equations (Zijie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijie Li, Saurabh Patil, Francis Ogoke, Dule Shu, Wilson Zhen, Michael Schneier, John R. Buchanan, Jr., Amir Barati Farimani. (2024)<br><strong>Latent Neural PDE Solver: a reduced-order modelling framework for partial differential equations</strong><br><button class=copy-to-clipboard title="Latent Neural PDE Solver: a reduced-order modelling framework for partial differential equations" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-AP<br>Keyword Score: 30<br>Keywords: Autoencoder, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17853v1.pdf filename=2402.17853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks have shown promising potential in accelerating the numerical <b>simulation</b> of systems governed by partial differential equations (PDEs). Different from many existing neural network surrogates operating on high-dimensional discretized fields, we propose to learn the dynamics of the system in the latent space with much coarser discretizations. In our proposed framework - Latent Neural PDE Solver (LNS), a non-linear <b>autoencoder</b> is first trained to project the full-order representation of the system onto the mesh-reduced space, then a temporal model is trained to predict the future state in this mesh-reduced space. This reduction process simplifies the training of the temporal model by greatly reducing the computational cost accompanying a fine discretization. We study the capability of the proposed framework and several other popular neural PDE solvers on various types of systems including single-phase and multi-phase flows along with varying system parameters. We showcase that it has competitive accuracy and efficiency compared to the neural PDE solver that operates on full-order space.</p></p class="citation"></blockquote><h3 id=2663--186341-when-your-ais-deceive-you-challenges-with-partial-observability-of-human-evaluators-in-reward-learning-leon-lang-et-al-2024>(26/63 | 186/341) When Your AIs Deceive You: Challenges with Partial Observability of Human Evaluators in Reward Learning (Leon Lang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leon Lang, Davis Foote, Stuart Russell, Anca Dragan, Erik Jenner, Scott Emmons. (2024)<br><strong>When Your AIs Deceive You: Challenges with Partial Observability of Human Evaluators in Reward Learning</strong><br><button class=copy-to-clipboard title="When Your AIs Deceive You: Challenges with Partial Observability of Human Evaluators in Reward Learning" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17747v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17747v2.pdf filename=2402.17747v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Past analyses of <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF)</b> assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which <b>RLHF</b> is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying <b>RLHF</b> in partially observable settings and propose research directions to help tackle these challenges.</p></p class="citation"></blockquote><h3 id=2763--187341-towards-a-digital-twin-framework-in-additive-manufacturing-machine-learning-and-bayesian-optimization-for-time-series-process-optimization-vispi-karkaria-et-al-2024>(27/63 | 187/341) Towards a Digital Twin Framework in Additive Manufacturing: Machine Learning and Bayesian Optimization for Time Series Process Optimization (Vispi Karkaria et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vispi Karkaria, Anthony Goeckner, Rujing Zha, Jie Chen, Jianjing Zhang, Qi Zhu, Jian Cao, Robert X. Gao, Wei Chen. (2024)<br><strong>Towards a Digital Twin Framework in Additive Manufacturing: Machine Learning and Bayesian Optimization for Time Series Process Optimization</strong><br><button class=copy-to-clipboard title="Towards a Digital Twin Framework in Additive Manufacturing: Machine Learning and Bayesian Optimization for Time Series Process Optimization" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17718v1.pdf filename=2402.17718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Laser-directed-energy deposition (DED) offers advantages in additive manufacturing (AM) for creating intricate geometries and material grading. Yet, challenges like material inconsistency and part variability remain, mainly due to its layer-wise fabrication. A key issue is heat accumulation during DED, which affects the material microstructure and properties. While closed-loop control methods for heat management are common in DED research, few integrate real-time monitoring, physics-based modeling, and control in a unified framework. Our work presents a digital twin (DT) framework for real-time predictive control of DED process parameters to meet specific design objectives. We develop a surrogate model using <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)-based</b> machine learning with Bayesian Inference to predict temperatures in DED parts. This model predicts future temperature states in real time. We also introduce Bayesian Optimization (BO) for Time Series Process Optimization (BOTSPO), based on traditional BO but featuring a unique time series process profile generator with reduced dimensions. BOTSPO dynamically optimizes processes, identifying optimal laser power profiles to attain desired mechanical properties. The established process trajectory guides online optimizations, aiming to enhance performance. This paper outlines the digital twin framework&rsquo;s components, promoting its integration into a comprehensive system for AM.</p></p class="citation"></blockquote><h3 id=2863--188341-predicting-machine-failures-from-multivariate-time-series-an-industrial-case-study-nicolò-oreste-pinciroli-vago-et-al-2024>(28/63 | 188/341) Predicting machine failures from multivariate time series: an industrial case study (Nicolò Oreste Pinciroli Vago et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolò Oreste Pinciroli Vago, Francesca Forbicini, Piero Fraternali. (2024)<br><strong>Predicting machine failures from multivariate time series: an industrial case study</strong><br><button class=copy-to-clipboard title="Predicting machine failures from multivariate time series: an industrial case study" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Logistic Regression, LSTM, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17804v1.pdf filename=2402.17804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Non-neural Machine Learning (ML) and Deep Learning (DL) models are often used to predict system failures in the context of industrial maintenance. However, only a few researches jointly assess the effect of varying the amount of past data used to make a prediction and the extension in the future of the forecast. This study evaluates the impact of the size of the reading window and of the prediction window on the performances of models trained to forecast failures in three data sets concerning the operation of (1) an industrial wrapping machine working in discrete sessions, (2) an industrial blood refrigerator working continuously, and (3) a nitrogen generator working continuously. The problem is formulated as a binary classification task that assigns the positive label to the prediction window based on the probability of a failure to occur in such an interval. Six algorithms <b>(logistic</b> <b>regression,</b> random forest, support vector machine, <b>LSTM,</b> ConvLSTM, and <b>Transformers)</b> are compared using multivariate telemetry time series. The results indicate that, in the considered scenarios, the dimension of the prediction windows plays a crucial role and highlight the effectiveness of DL approaches at classifying data with diverse time-dependent patterns preceding a failure and the effectiveness of ML approaches at classifying similar and repetitive patterns preceding a failure.</p></p class="citation"></blockquote><h3 id=2963--189341-actions-speak-louder-than-words-trillion-parameter-sequential-transducers-for-generative-recommendations-jiaqi-zhai-et-al-2024>(29/63 | 189/341) Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations (Jiaqi Zhai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, Yu Shi. (2024)<br><strong>Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations</strong><br><button class=copy-to-clipboard title="Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Recommendation, GPT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17152v1.pdf filename=2402.17152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale <b>recommendation</b> systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning <b>Recommendation</b> Models (DLRMs) in industry fail to scale with compute. Inspired by success achieved by <b>Transformers</b> in language and vision domains, we revisit fundamental design choices in <b>recommendation</b> systems. We reformulate <b>recommendation</b> problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders&rsquo;&rsquo;), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming <b>recommendation</b> data. HSTU outperforms baselines over synthetic and public datasets by up to 65.8% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based <b>Transformers</b> on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to <b>GPT-3/LLaMa-2</b> scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundational models in <b>recommendations.</b></p></p class="citation"></blockquote><h3 id=3063--190341-predicting-o-glcnacylation-sites-in-mammalian-proteins-with-transformers-and-rnns-trained-with-a-new-loss-function-pedro-seber-2024>(30/63 | 190/341) Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function (Pedro Seber, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pedro Seber. (2024)<br><strong>Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function</strong><br><button class=copy-to-clipboard title="Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-MN<br>Keyword Score: 30<br>Keywords: Fine-tuning, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17131v1.pdf filename=2402.17131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Glycosylation, a protein modification, has multiple essential functional and structural roles. O-GlcNAcylation, a subtype of glycosylation, has the potential to be an important target for therapeutics, but methods to reliably predict O-GlcNAcylation sites had not been available until 2023; a 2021 review correctly noted that published models were insufficient and failed to generalize. Moreover, many are no longer usable. In 2023, a considerably better <b>RNN</b> model with an F$_1$ score of 36.17% and an MCC of 34.57% on a large dataset was published. This article first sought to improve these metrics using <b>transformer</b> encoders. While <b>transformers</b> displayed high performance on this dataset, their performance was inferior to that of the previously published <b>RNN.</b> We then created a new loss function, which we call the weighted focal differentiable MCC, to improve the performance of classification models. <b>RNN</b> models trained with this new function display superior performance to models trained using the weighted cross-entropy loss; this new function can also be used to <b>fine-tune</b> trained models. A two-cell <b>RNN</b> trained with this loss achieves state-of-the-art performance in O-GlcNAcylation site prediction with an F$_1$ score of 38.82% and an MCC of 38.21% on that large dataset.</p></p class="citation"></blockquote><h3 id=3163--191341-curriculum-learning-meets-directed-acyclic-graph-for-multimodal-emotion-recognition-cam-van-thi-nguyen-et-al-2024>(31/63 | 191/341) Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition (Cam-Van Thi Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cam-Van Thi Nguyen, Cao-Bach Nguyen, Quang-Thuy Ha, Duc-Trong Le. (2024)<br><strong>Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition</strong><br><button class=copy-to-clipboard title="Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 29<br>Keywords: Graph, Curriculum Learning, Multi-modal, Multi-modal, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17269v1.pdf filename=2402.17269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Emotion</b> <b>recognition</b> in conversation (ERC) is a crucial task in natural language processing and affective computing. This paper proposes MultiDAG+CL, a novel approach for <b>Multimodal</b> <b>Emotion</b> <b>Recognition</b> in Conversation (ERC) that employs Directed Acyclic <b>Graph</b> (DAG) to integrate textual, acoustic, and visual features within a unified framework. The model is enhanced by <b>Curriculum</b> <b>Learning</b> (CL) to address challenges related to <b>emotional</b> <b>shifts</b> and data imbalance. <b>Curriculum</b> <b>learning</b> facilitates the learning process by gradually presenting training samples in a meaningful order, thereby improving the model&rsquo;s performance in handling <b>emotional</b> <b>variations</b> and data imbalance. Experimental results on the IEMOCAP and MELD datasets demonstrate that the MultiDAG+CL models outperform baseline models.</p></p class="citation"></blockquote><h3 id=3263--192341-learning-topological-representations-with-bidirectional-graph-attention-network-for-solving-job-shop-scheduling-problem-cong-zhang-et-al-2024>(32/63 | 192/341) Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem (Cong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cong Zhang, Zhiguang Cao, Yaoxin Wu, Wen Song, Jing Sun. (2024)<br><strong>Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem</strong><br><button class=copy-to-clipboard title="Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Message-Passing, Graph, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17606v1.pdf filename=2402.17606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf <b>GNN</b> models tailored to undirected <b>graphs</b> and neglect the rich and meaningful topological structures of disjunctive <b>graphs</b> (DGs). This paper proposes the topology-aware bidirectional <b>graph</b> attention network (TBGAT), a novel <b>GNN</b> architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via <b>graph</b> attention. Then, we propose a novel operator based on the <b>message-passing</b> mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT has linear computational complexity to the number of jobs and machines, respectively, which strengthens the practical value of our method. Besides, extensive experiments on five synthetic datasets and seven classic <b>benchmarks</b> show that TBGAT achieves new SOTA results by outperforming a wide range of neural methods by a large margin.</p></p class="citation"></blockquote><h3 id=3363--193341-using-graph-neural-networks-to-predict-local-culture-thiago-h-silva-et-al-2024>(33/63 | 193/341) Using Graph Neural Networks to Predict Local Culture (Thiago H Silva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thiago H Silva, Daniel Silver. (2024)<br><strong>Using Graph Neural Networks to Predict Local Culture</strong><br><button class=copy-to-clipboard title="Using Graph Neural Networks to Predict Local Culture" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs-SI, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17905v1.pdf filename=2402.17905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Urban research has long recognized that neighbourhoods are dynamic and relational. However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics. To make progress on this issue, this study proposes a <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models. By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture. Results are promising from a substantive and methodologically point of view. Substantively, we find that either local area information (e.g. area demographics) or group profiles (tastes of Yelp reviewers) give the best results in predicting local culture, and they are nearly equivalent in all studied cases. Methodologically, exploring group profiles could be a helpful alternative where finding local information for specific areas is challenging, since they can be extracted automatically from many forms of online data. Thus, our approach could empower researchers and policy-makers to use a range of data sources when other local area information is lacking.</p></p class="citation"></blockquote><h3 id=3463--194341-graph-neural-networks-and-arithmetic-circuits-timon-barlag-et-al-2024>(34/63 | 194/341) Graph Neural Networks and Arithmetic Circuits (Timon Barlag et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timon Barlag, Vivian Holzapfel, Laura Strieker, Jonni Virtema, Heribert Vollmer. (2024)<br><strong>Graph Neural Networks and Arithmetic Circuits</strong><br><button class=copy-to-clipboard title="Graph Neural Networks and Arithmetic Circuits" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: F-1-1; F-1-3; I-2-m, cs-AI, cs-CC, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17805v1.pdf filename=2402.17805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We characterize the computational power of neural networks that follow the <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> architecture, not restricted to aggregate-combine <b>GNNs</b> or other particular types. We establish an exact correspondence between the expressivity of <b>GNNs</b> using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.</p></p class="citation"></blockquote><h3 id=3563--195341-data-efficient-learning-via-clustering-based-sensitivity-sampling-foundation-models-and-beyond-kyriakos-axiotis-et-al-2024>(35/63 | 195/341) Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond (Kyriakos Axiotis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyriakos Axiotis, Vincent Cohen-Addad, Monika Henzinger, Sammy Jerome, Vahab Mirrokni, David Saulpic, David Woodruff, Michael Wunder. (2024)<br><strong>Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond</strong><br><button class=copy-to-clipboard title="Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Clustering, Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17327v1.pdf filename=2402.17327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the data selection problem, whose aim is to select a small representative subset of data that can be used to efficiently train a machine learning model. We present a new data selection approach based on $k$-means <b>clustering</b> and sensitivity sampling. Assuming access to an embedding representation of the data with respect to which the model loss is H"older continuous, our approach provably allows selecting a set of ``typical&rsquo;&rsquo; $k + 1/\varepsilon^2$ elements whose average loss corresponds to the average loss of the whole dataset, up to a multiplicative $(1\pm\varepsilon)$ factor and an additive $\varepsilon \lambda \Phi_k$, where $\Phi_k$ represents the $k$-means cost for the input embeddings and $\lambda$ is the H"older constant. We furthermore demonstrate the performance and scalability of our approach on <b>fine-tuning</b> <b>foundation</b> <b>models</b> and show that it outperforms state-of-the-art methods. We also show how it can be applied on linear regression, leading to a new sampling strategy that surprisingly matches the performances of leverage score sampling, while being conceptually simpler and more scalable.</p></p class="citation"></blockquote><h3 id=3663--196341-dual-space-optimization-improved-molecule-sequence-design-by-latent-prompt-transformer-deqian-kong-et-al-2024>(36/63 | 196/341) Dual-Space Optimization: Improved Molecule Sequence Design by Latent Prompt Transformer (Deqian Kong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deqian Kong, Yuhao Huang, Jianwen Xie, Edouardo Honig, Ming Xu, Shuanghong Xue, Pei Lin, Sanping Zhou, Sheng Zhong, Nanning Zheng, Ying Nian Wu. (2024)<br><strong>Dual-Space Optimization: Improved Molecule Sequence Design by Latent Prompt Transformer</strong><br><button class=copy-to-clipboard title="Dual-Space Optimization: Improved Molecule Sequence Design by Latent Prompt Transformer" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17179v1.pdf filename=2402.17179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Designing molecules with desirable properties, such as drug-likeliness and high binding affinities towards protein targets, is a challenging problem. In this paper, we propose the Dual-Space Optimization (DSO) method that integrates latent space sampling and data space selection to solve this problem. DSO iteratively updates a latent space generative model and a synthetic dataset in an optimization process that gradually shifts the generative model and the synthetic data towards regions of desired property values. Our generative model takes the form of a Latent <b>Prompt</b> <b>Transformer</b> (LPT) where the latent vector serves as the <b>prompt</b> of a causal <b>transformer.</b> Our extensive experiments demonstrate effectiveness of the proposed method, which sets new performance <b>benchmarks</b> across single-objective, multi-objective and constrained molecule design tasks.</p></p class="citation"></blockquote><h3 id=3763--197341-taxdiff-taxonomic-guided-diffusion-model-for-protein-sequence-generation-lin-zongying-et-al-2024>(37/63 | 197/341) TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation (Lin Zongying et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Zongying, Li Hao, Lv Liuzhenghao, Lin Bin, Zhang Junwu, Chen Calvin Yu-Chian, Yuan Li, Tian Yonghong. (2024)<br><strong>TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation</strong><br><button class=copy-to-clipboard title="TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17156v1.pdf filename=2402.17156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Designing protein sequences with specific biological functions and structural stability is crucial in biology and chemistry. Generative models already demonstrated their capabilities for reliable protein design. However, previous models are limited to the unconditional generation of protein sequences and lack the controllable generation ability that is vital to biological tasks. In this work, we propose TaxDiff, a taxonomic-guided <b>diffusion</b> <b>model</b> for controllable protein sequence generation that combines biological species information with the generative capabilities of <b>diffusion</b> <b>models</b> to generate structurally stable proteins within the sequence space. Specifically, taxonomic control information is inserted into each layer of the <b>transformer</b> block to achieve fine-grained control. The combination of global and local attention ensures the sequence consistency and structural foldability of taxonomic-specific proteins. Extensive experiments demonstrate that TaxDiff can consistently achieve better performance on multiple protein sequence generation <b>benchmarks</b> in both taxonomic-guided controllable generation and unconditional generation. Remarkably, the sequences generated by TaxDiff even surpass those produced by direct-structure-generation models in terms of confidence based on predicted structures and require only a quarter of the time of models based on the <b>diffusion</b> <b>model.</b> The code for generating proteins and training new versions of TaxDiff is available at:https://github.com/Linzy19/TaxDiff.</p></p class="citation"></blockquote><h3 id=3863--198341-automated-statistical-model-discovery-with-language-models-michael-y-li-et-al-2024>(38/63 | 198/341) Automated Statistical Model Discovery with Language Models (Michael Y. Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Y. Li, Emily B. Fox, Noah D. Goodman. (2024)<br><strong>Automated Statistical Model Discovery with Language Models</strong><br><button class=copy-to-clipboard title="Automated Statistical Model Discovery with Language Models" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17879v1.pdf filename=2402.17879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Statistical model discovery involves a challenging search over a vast space of models subject to domain-specific modeling constraints. Efficiently searching over this space requires human expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of <b>large</b> <b>language</b> <b>models</b> (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the framework of Box&rsquo;s Loop: the LM iterates between proposing statistical models represented as <b>probabilistic</b> <b>programs,</b> acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, key restrictions of previous systems. We evaluate our method in three common settings in <b>probabilistic</b> <b>modeling:</b> searching within a restricted space of models, searching over an open-ended space, and improving classic models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method matches the performance of previous systems, identifies models on par with human expert designed models, and extends classic models in interpretable ways. Our results highlight the promise of LM driven model discovery.</p></p class="citation"></blockquote><h3 id=3963--199341-markovletics-methods-and-a-novel-application-for-learning-continuous-time-markov-chain-mixtures-fabian-spaeh-et-al-2024>(39/63 | 199/341) Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures (Fabian Spaeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabian Spaeh, Charalampos E. Tsourakakis. (2024)<br><strong>Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures</strong><br><button class=copy-to-clipboard title="Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17730v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17730v1.pdf filename=2402.17730v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential data naturally arises from user engagement on digital platforms like social media, music streaming services, and web navigation, encapsulating evolving user preferences and behaviors through <b>continuous</b> <b>information</b> streams. A notable unresolved query in stochastic processes is learning mixtures of <b>continuous-time</b> <b>Markov</b> chains (CTMCs). While there is progress in learning mixtures of <b>discrete-time</b> <b>Markov</b> chains with recovery guarantees [GKV16,ST23,KTT2023], the <b>continuous</b> <b>scenario</b> uncovers unique unexplored challenges. The intrigue in CTMC mixtures stems from their potential to model intricate <b>continuous-time</b> <b>stochastic</b> processes prevalent in various fields including social media, finance, and biology. In this study, we introduce a novel framework for exploring CTMCs, emphasizing the influence of observed trails&rsquo; length and mixture parameters on problem regimes, which demands specific algorithms. Through thorough experimentation, we examine the impact of discretizing <b>continuous-time</b> <b>trails</b> on the learnability of the <b>continuous-time</b> <b>mixture,</b> given that these processes are often observed via <b>discrete,</b> <b>resource-demanding</b> observations. Our comparative analysis with leading methods explores sample complexity and the trade-off between the number of trails and their lengths, offering crucial insights for method selection in different problem instances. We apply our algorithms on an extensive collection of Lastfm&rsquo;s user-generated trails spanning three years, demonstrating the capability of our algorithms to differentiate diverse user preferences. We pioneer the use of CTMC mixtures on a basketball passing dataset to unveil intricate offensive tactics of NBA teams. This underscores the pragmatic utility and versatility of our proposed framework. All results presented in this study are replicable, and we provide the implementations to facilitate reproducibility.</p></p class="citation"></blockquote><h3 id=4063--200341-federated-learning-for-estimating-heterogeneous-treatment-effects-disha-makhija-et-al-2024>(40/63 | 200/341) Federated Learning for Estimating Heterogeneous Treatment Effects (Disha Makhija et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Disha Makhija, Joydeep Ghosh, Yejin Kim. (2024)<br><strong>Federated Learning for Estimating Heterogeneous Treatment Effects</strong><br><button class=copy-to-clipboard title="Federated Learning for Estimating Heterogeneous Treatment Effects" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17705v1.pdf filename=2402.17705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning methods for estimating heterogeneous treatment effects (HTE) facilitate large-scale personalized decision-making across various domains such as healthcare, policy making, education, and more. Current machine learning approaches for HTE require access to substantial amounts of data per treatment, and the high costs associated with interventions makes centrally collecting so much data for each intervention a formidable challenge. To overcome this obstacle, in this work, we propose a novel framework for collaborative learning of HTE estimators across institutions via <b>Federated</b> <b>Learning.</b> We show that even under a diversity of interventions and subject populations across clients, one can jointly learn a common feature representation, while concurrently and privately learning the specific predictive functions for outcomes under distinct interventions across institutions. Our framework and the associated algorithm are based on this insight, and leverage tabular <b>transformers</b> to map multiple input data to feature representations which are then used for outcome prediction via multi-task learning. We also propose a novel way of <b>federated</b> <b>training</b> of personalised <b>transformers</b> that can work with heterogeneous input feature spaces. Experimental results on real-world clinical trial data demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=4163--201341-torchmd-net-20-fast-neural-network-potentials-for-molecular-simulations-raul-p-pelaez-et-al-2024>(41/63 | 201/341) TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations (Raul P. Pelaez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raul P. Pelaez, Guillem Simeon, Raimondas Galvelis, Antonio Mirarchi, Peter Eastman, Stefan Doerr, Philipp Thölke, Thomas E. Markland, Gianni De Fabritiis. (2024)<br><strong>TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations</strong><br><button class=copy-to-clipboard title="TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-bio-ph, physics-chem-ph, physics-comp-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17660v1.pdf filename=2402.17660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular <b>simulations</b> has been a persistent challenge. This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials. The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet. This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community. The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations. Other enhancements include highly optimized neighbor search algorithms that support periodic boundary conditions and the smooth integration with existing molecular dynamics frameworks. Additionally, the updated version introduces the capability to integrate physical priors, further enriching its application spectrum and utility in research. The software is available at <a href=https://github.com/torchmd/torchmd-net>https://github.com/torchmd/torchmd-net</a>.</p></p class="citation"></blockquote><h3 id=4263--202341-dropbp-accelerating-fine-tuning-of-large-language-models-by-dropping-backward-propagation-sunghyeon-woo-et-al-2024>(42/63 | 202/341) DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation (Sunghyeon Woo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Sejung Kwon, Dongsuk Jeon, Dongsoo Lee. (2024)<br><strong>DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation</strong><br><button class=copy-to-clipboard title="DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17812v1.pdf filename=2402.17812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full <b>fine-tuning</b> and parameter-efficient <b>fine-tuning</b> using backpropagation. Specifically, utilizing DropBP in QLoRA reduces training time by 44%, increases the convergence speed to the identical loss level by 1.5$\times$, and enables training with a 6.2$\times$ larger sequence length on a single NVIDIA-A100 80GiB GPU in LLaMA2-70B. The code is available at <a href=https://github.com/WooSunghyeon/dropbp>https://github.com/WooSunghyeon/dropbp</a>.</p></p class="citation"></blockquote><h3 id=4363--203341-evaluation-of-predictive-reliability-to-foster-trust-in-artificial-intelligence-a-case-study-in-multiple-sclerosis-lorenzo-peracchio-et-al-2024>(43/63 | 203/341) Evaluation of Predictive Reliability to Foster Trust in Artificial Intelligence. A case study in Multiple Sclerosis (Lorenzo Peracchio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Peracchio, Giovanna Nicora, Enea Parimbelli, Tommaso Mario Buonocore, Roberto Bergamaschi, Eleonora Tavazzi, Arianna Dagliati, Riccardo Bellazzi. (2024)<br><strong>Evaluation of Predictive Reliability to Foster Trust in Artificial Intelligence. A case study in Multiple Sclerosis</strong><br><button class=copy-to-clipboard title="Evaluation of Predictive Reliability to Foster Trust in Artificial Intelligence. A case study in Multiple Sclerosis" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Autoencoder, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17554v1.pdf filename=2402.17554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical contexts, such as medicine, requires the implementation of safety measures to reduce risks of harm in case of prediction errors. Spotting ML failures is of paramount importance when ML predictions are used to drive clinical decisions. ML predictive reliability measures the degree of trust of a ML prediction on a new instance, thus allowing decision-makers to accept or reject it based on its reliability. To assess reliability, we propose a method that implements two principles. First, our approach evaluates whether an instance to be classified is coming from the same distribution of the training set. To do this, we leverage <b>Autoencoders</b> (AEs) ability to reconstruct the training set with low error. An instance is considered <b>Out-of-Distribution</b> (OOD) if the AE reconstructs it with a high error. Second, it is evaluated whether the ML classifier has good performances on samples similar to the newly classified instance by using a proxy model. We show that this approach is able to assess reliability both in a simulated scenario and on a model trained to predict disease progression of Multiple Sclerosis patients. We also developed a Python package, named relAI, to embed reliability measures into ML pipelines. We propose a simple approach that can be used in the deployment phase of any ML model to suggest whether to trust predictions or not. Our method holds the promise to provide effective support to clinicians by spotting potential ML failures during deployment.</p></p class="citation"></blockquote><h3 id=4463--204341-quce-the-minimisation-and-quantification-of-path-based-uncertainty-for-generative-counterfactual-explanations-jamie-duell-et-al-2024>(44/63 | 204/341) QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations (Jamie Duell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jamie Duell, Hsuan Fu, Monika Seisenberger, Xiuyi Fan. (2024)<br><strong>QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations</strong><br><button class=copy-to-clipboard title="QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Counter-factual, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17516v1.pdf filename=2402.17516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during <b>out-of-distribution</b> path traversal. In this context, we introduce Quantified Uncertainty <b>Counterfactual</b> Explanations (QUCE), a method designed to mitigate <b>out-of-distribution</b> traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting explanations but also generates more certain <b>counterfactual</b> examples. We showcase the performance of the QUCE method by comparing it with competing methods for both path-based explanations and generative <b>counterfactual</b> examples. The code repository for the QUCE method is available at: <a href=https://github.com/jamie-duell/QUCE>https://github.com/jamie-duell/QUCE</a>.</p></p class="citation"></blockquote><h3 id=4563--205341-robustness-congruent-adversarial-training-for-secure-machine-learning-model-updates-daniele-angioni-et-al-2024>(45/63 | 205/341) Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates (Daniele Angioni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniele Angioni, Luca Demetrio, Maura Pintor, Luca Oneto, Davide Anguita, Battista Biggio, Fabio Roli. (2024)<br><strong>Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates</strong><br><button class=copy-to-clipboard title="Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17390v1.pdf filename=2402.17390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine-learning models demand for periodic updates to improve their average accuracy, exploiting novel architectures and additional data. However, a newly-updated model may commit mistakes that the previous model did not make. Such misclassifications are referred to as negative flips, and experienced by users as a regression of performance. In this work, we show that this problem also affects robustness to <b>adversarial</b> <b>examples,</b> thereby hindering the development of secure model update practices. In particular, when updating a model to improve its <b>adversarial</b> <b>robustness,</b> some previously-ineffective <b>adversarial</b> <b>examples</b> may become misclassified, causing a regression in the perceived security of the system. We propose a novel technique, named robustness-congruent <b>adversarial</b> <b>training,</b> to address this issue. It amounts to <b>fine-tuning</b> a model with <b>adversarial</b> <b>training,</b> while constraining it to retain higher robustness on the <b>adversarial</b> <b>examples</b> that were correctly classified before the update. We show that our algorithm and, more generally, learning with non-regression constraints, provides a theoretically-grounded framework to train consistent estimators. Our experiments on robust models for computer vision confirm that (i) both accuracy and robustness, even if improved after model update, can be affected by negative flips, and (ii) our robustness-congruent <b>adversarial</b> <b>training</b> can mitigate the problem, outperforming competing baseline methods.</p></p class="citation"></blockquote><h3 id=4663--206341-does-negative-sampling-matter-a-review-with-insights-into-its-theory-and-applications-zhen-yang-et-al-2024>(46/63 | 206/341) Does Negative Sampling Matter? A Review with Insights into its Theory and Applications (Zhen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Yang, Ming Ding, Tinglin Huang, Yukuo Cen, Junshuai Song, Bin Xu, Yuxiao Dong, Jie Tang. (2024)<br><strong>Does Negative Sampling Matter? A Review with Insights into its Theory and Applications</strong><br><button class=copy-to-clipboard title="Does Negative Sampling Matter? A Review with Insights into its Theory and Applications" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17238v1.pdf filename=2402.17238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Negative sampling has swiftly risen to prominence as a focal point of research, with wide-ranging applications spanning machine learning, computer vision, natural language processing, data mining, and <b>recommender</b> <b>systems.</b> This growing interest raises several critical questions: Does negative sampling really matter? Is there a general framework that can incorporate all existing negative sampling methods? In what fields is it applied? Addressing these questions, we propose a general framework that leverages negative sampling. Delving into the history of negative sampling, we trace the development of negative sampling through five evolutionary paths. We dissect and categorize the strategies used to select negative sample candidates, detailing global, local, mini-batch, hop, and memory-based approaches. Our review categorizes current negative sampling methods into five types: static, hard, <b>GAN-based,</b> Auxiliary-based, and In-batch methods, providing a clear structure for understanding negative sampling. Beyond detailed categorization, we highlight the application of negative sampling in various areas, offering insights into its practical benefits. Finally, we briefly discuss open problems and future directions for negative sampling.</p></p class="citation"></blockquote><h3 id=4763--207341-stochastic-gradient-succeeds-for-bandits-jincheng-mei-et-al-2024>(47/63 | 207/341) Stochastic Gradient Succeeds for Bandits (Jincheng Mei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jincheng Mei, Zixin Zhong, Bo Dai, Alekh Agarwal, Csaba Szepesvari, Dale Schuurmans. (2024)<br><strong>Stochastic Gradient Succeeds for Bandits</strong><br><button class=copy-to-clipboard title="Stochastic Gradient Succeeds for Bandits" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17235v1.pdf filename=2402.17235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that the \emph{stochastic gradient} <b>bandit</b> <b>algorithm</b> converges to a \emph{globally optimal} policy at an $O(1/t)$ rate, even with a \emph{constant} step size. Remarkably, global convergence of the stochastic gradient <b>bandit</b> <b>algorithm</b> has not been previously established, even though it is an old algorithm known to be applicable to <b>bandits.</b> <b>The</b> new result is achieved by establishing two novel technical findings: first, the noise of the stochastic updates in the gradient <b>bandit</b> <b>algorithm</b> satisfies a strong <code>growth condition'' property, where the variance diminishes whenever progress becomes small, implying that additional noise control via diminishing step sizes is unnecessary; second, a form of </code>weak exploration&rsquo;&rsquo; is automatically achieved through the stochastic gradient updates, since they prevent the action probabilities from decaying faster than $O(1/t)$, thus ensuring that every action is sampled infinitely often with probability $1$. These two findings can be used to show that the stochastic gradient update is already ``sufficient&rsquo;&rsquo; for <b>bandits</b> <b>in</b> the sense that exploration versus exploitation is automatically balanced in a manner that ensures almost sure convergence to a global optimum. These novel theoretical findings are further verified by experimental results.</p></p class="citation"></blockquote><h3 id=4863--208341-hyperdimensional-computing-a-fast-robust-and-interpretable-paradigm-for-biological-data-michiel-stock-et-al-2024>(48/63 | 208/341) Hyperdimensional computing: a fast, robust and interpretable paradigm for biological data (Michiel Stock et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michiel Stock, Dimitri Boeckaerts, Pieter Dewulf, Steff Taelman, Maxime Van Haeverbeke, Wim Van Criekinge, Bernard De Baets. (2024)<br><strong>Hyperdimensional computing: a fast, robust and interpretable paradigm for biological data</strong><br><button class=copy-to-clipboard title="Hyperdimensional computing: a fast, robust and interpretable paradigm for biological data" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-QM<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17572v1.pdf filename=2402.17572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in bioinformatics are primarily due to new algorithms for processing diverse biological data sources. While sophisticated alignment algorithms have been pivotal in analyzing biological sequences, deep learning has substantially transformed bioinformatics, addressing sequence, structure, and functional analyses. However, these methods are incredibly data-hungry, compute-intensive and hard to interpret. Hyperdimensional computing (HDC) has recently emerged as an intriguing alternative. The key idea is that random vectors of high dimensionality can represent concepts such as sequence identity or phylogeny. These vectors can then be combined using simple operators for learning, <b>reasoning</b> or querying by exploiting the peculiar properties of high-dimensional spaces. Our work reviews and explores the potential of HDC for bioinformatics, emphasizing its efficiency, interpretability, and adeptness in handling <b>multimodal</b> and structured data. HDC holds a lot of potential for various omics data searching, biosignal analysis and health applications.</p></p class="citation"></blockquote><h3 id=4963--209341-deepdrk-deep-dependency-regularized-knockoff-for-feature-selection-hongyu-shen-et-al-2024>(49/63 | 209/341) DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection (Hongyu Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyu Shen, Yici Yan, Zhizhen Zhao. (2024)<br><strong>DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection</strong><br><button class=copy-to-clipboard title="DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T07, I-5-1, cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Benchmarking, Sample Size, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17176v1.pdf filename=2402.17176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model-X knockoff, among various feature selection methods, received much attention recently due to its guarantee on false discovery rate (FDR) control. Subsequent to its introduction in parametric design, knockoff is advanced to handle arbitrary data distributions using deep learning-based generative modeling. However, we observed that current implementations of the deep Model-X knockoff framework exhibit limitations. Notably, the &ldquo;swap property&rdquo; that knockoffs necessitate frequently encounter challenges on <b>sample</b> <b>level,</b> leading to a diminished selection power. To overcome, we develop &ldquo;Deep Dependency Regularized Knockoff (DeepDRK)&rdquo;, a distribution-free deep learning method that strikes a balance between FDR and power. In DeepDRK, a generative model grounded in a <b>transformer</b> architecture is introduced to better achieve the &ldquo;swap property&rdquo;. Novel efficient regularization techniques are also proposed to reach higher power. Our model outperforms other <b>benchmarks</b> in synthetic, semi-synthetic, and real-world data, especially when <b>sample</b> <b>size</b> is small and data distribution is complex.</p></p class="citation"></blockquote><h3 id=5063--210341-conjnorm-tractable-density-estimation-for-out-of-distribution-detection-bo-peng-et-al-2024>(50/63 | 210/341) ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection (Bo Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Peng, Yadan Luo, Yonggang Zhang, Yixuan Li, Zhen Fang. (2024)<br><strong>ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection</strong><br><button class=copy-to-clipboard title="ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17888v1.pdf filename=2402.17888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Post-hoc <b>out-of-distribution</b> (OOD) detection has garnered intensive attention in reliable machine learning. Many efforts have been dedicated to deriving score functions based on logits, distances, or rigorous data distribution assumptions to identify low-scoring OOD samples. Nevertheless, these estimate scores may fail to accurately reflect the true data density or impose impractical constraints. To provide a unified perspective on density-based score design, we propose a novel theoretical framework grounded in Bregman divergence, which extends distribution considerations to encompass an exponential family of distributions. Leveraging the conjugation constraint revealed in our theorem, we introduce a \textsc{ConjNorm} method, reframing density function design as a search for the optimal norm coefficient $p$ against the given dataset. In light of the computational challenges of normalization, we devise an unbiased and analytically tractable estimator of the partition function using the Monte Carlo-based importance sampling technique. Extensive experiments across OOD detection <b>benchmarks</b> empirically demonstrate that our proposed \textsc{ConjNorm} has established a new state-of-the-art in a variety of OOD detection setups, outperforming the current best method by up to 13.25$%$ and 28.19$%$ (FPR95) on CIFAR-100 and ImageNet-1K, respectively.</p></p class="citation"></blockquote><h3 id=5163--211341-label-noise-robust-diffusion-models-byeonghu-na-et-al-2024>(51/63 | 211/341) Label-Noise Robust Diffusion Models (Byeonghu Na et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Byeonghu Na, Yeongmin Kim, HeeSun Bae, Jung Hyun Lee, Se Jung Kwon, Wanmo Kang, Il-Chul Moon. (2024)<br><strong>Label-Noise Robust Diffusion Models</strong><br><button class=copy-to-clipboard title="Label-Noise Robust Diffusion Models" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17517v1.pdf filename=2402.17517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conditional <b>diffusion</b> <b>models</b> have shown remarkable performance in various generative tasks, but training them requires large-scale datasets that often contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to condition mismatch and quality degradation of generated data. This paper proposes Transition-aware weighted Denoising Score Matching (TDSM) for training conditional <b>diffusion</b> <b>models</b> with noisy labels, which is the first study in the line of <b>diffusion</b> <b>models.</b> The TDSM objective contains a weighted sum of score networks, incorporating instance-wise and time-dependent label transition probabilities. We introduce a transition-aware weight estimator, which leverages a time-dependent noisy-label classifier distinctively customized to the <b>diffusion</b> <b>process.</b> Through experiments across various datasets and noisy label settings, TDSM improves the quality of generated samples aligned with given conditions. Furthermore, our method improves generation performance even on prevalent <b>benchmark</b> datasets, which implies the potential noisy labels and their risk of generative model learning. Finally, we show the improved performance of TDSM on top of conventional noisy label corrections, which empirically proving its contribution as a part of label-noise robust generative models. Our code is available at: <a href=https://github.com/byeonghu-na/tdsm>https://github.com/byeonghu-na/tdsm</a>.</p></p class="citation"></blockquote><h3 id=5263--212341-sequentialattention-for-block-sparsification-differentiable-pruning-meets-combinatorial-optimization-taisuke-yasuda-et-al-2024>(52/63 | 212/341) SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization (Taisuke Yasuda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taisuke Yasuda, Kyriakos Axiotis, Gang Fu, MohammadHossein Bateni, Vahab Mirrokni. (2024)<br><strong>SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization</strong><br><button class=copy-to-clipboard title="SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17902v1.pdf filename=2402.17902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural network <b>pruning</b> is a key technique towards engineering large yet scalable, interpretable, and generalizable models. Prior work on the subject has developed largely along two orthogonal directions: (1) differentiable <b>pruning</b> for efficiently and accurately scoring the importance of parameters, and (2) combinatorial optimization for efficiently searching over the space of sparse models. We unite the two approaches, both theoretically and empirically, to produce a coherent framework for structured neural network <b>pruning</b> in which differentiable <b>pruning</b> guides combinatorial optimization algorithms to select the most important sparse set of parameters. Theoretically, we show how many existing differentiable <b>pruning</b> techniques can be understood as nonconvex regularization for group sparse optimization, and prove that for a wide class of nonconvex regularizers, the global optimum is unique, group-sparse, and provably yields an approximate solution to a sparse convex optimization problem. The resulting algorithm that we propose, SequentialAttention++, advances the state of the art in large-scale neural network block-wise <b>pruning</b> tasks on the ImageNet and Criteo datasets.</p></p class="citation"></blockquote><h3 id=5363--213341-prediction-powered-ranking-of-large-language-models-ivi-chatzi-et-al-2024>(53/63 | 213/341) Prediction-Powered Ranking of Large Language Models (Ivi Chatzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ivi Chatzi, Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez. (2024)<br><strong>Prediction-Powered Ranking of Large Language Models</strong><br><button class=copy-to-clipboard title="Prediction-Powered Ranking of Large Language Models" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CY, cs-HC, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17826v1.pdf filename=2402.17826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> are often ranked according to their level of alignment with human preferences &ndash; a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong <b>large</b> <b>language</b> <b>model</b> &ndash; a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a <b>large</b> <b>set</b> <b>of</b> pairwise comparisons by a model, our framework provides a rank-set &ndash; a set of possible ranking positions &ndash; for each of the models under comparison. Moreover, it guarantees that, with a probability greater than or equal to a user-specified value, the rank-sets cover the true ranking consistent with (the distribution of) human pairwise preferences. Our framework is computationally efficient, easy to use, and does not make any assumption about the distribution of human preferences nor about the degree of alignment between the pairwise comparisons by the humans and the strong <b>large</b> <b>language</b> <b>model.</b></p></p class="citation"></blockquote><h3 id=5463--214341-multi-agent-deep-reinforcement-learning-for-distributed-satellite-routing-federico-lozano-cuadra-et-al-2024>(54/63 | 214/341) Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing (Federico Lozano-Cuadra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Federico Lozano-Cuadra, Beatriz Soret. (2024)<br><strong>Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing</strong><br><button class=copy-to-clipboard title="Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17666v1.pdf filename=2402.17666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a Multi-Agent Deep <b>Reinforcement</b> <b>Learning</b> (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents. Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online.</p></p class="citation"></blockquote><h3 id=5563--215341-feduv-uniformity-and-variance-for-heterogeneous-federated-learning-ha-min-son-et-al-2024>(55/63 | 215/341) FedUV: Uniformity and Variance for Heterogeneous Federated Learning (Ha Min Son et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ha Min Son, Moon Hyun Kim, Tai-Myoung Chung, Chao Huang, Xin Liu. (2024)<br><strong>FedUV: Uniformity and Variance for Heterogeneous Federated Learning</strong><br><button class=copy-to-clipboard title="FedUV: Uniformity and Variance for Heterogeneous Federated Learning" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18372v1.pdf filename=2402.18372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> is a promising framework to train neural networks with widely distributed data. However, performance degrades heavily with heterogeneously distributed data. Recent work has shown this is due to the final layer of the network being most prone to local bias, some finding success freezing the final layer as an orthogonal classifier. We investigate the training dynamics of the classifier by applying SVD to the weights motivated by the observation that freezing weights results in constant singular values. We find that there are differences when training in IID and non-IID settings. Based on this finding, we introduce two regularization terms for local training to continuously emulate IID settings: (1) variance in the dimension-wise probability distribution of the classifier and (2) hyperspherical uniformity of representations of the encoder. These regularizations promote local models to act as if it were in an IID setting regardless of the local data distribution, thus offsetting proneness to bias while being flexible to the data. On extensive experiments in both label-shift and feature-shift settings, we verify that our method achieves highest performance by a large margin especially in highly non-IID cases in addition to being scalable to larger models and datasets.</p></p class="citation"></blockquote><h3 id=5663--216341-sparse-variational-contaminated-noise-gaussian-process-regression-for-forecasting-geomagnetic-perturbations-daniel-iong-et-al-2024>(56/63 | 216/341) Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations (Daniel Iong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Iong, Matthew McAnear, Yuezhou Qu, Shasha Zou, Gabor Toth, Yang Chen. (2024)<br><strong>Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations</strong><br><button class=copy-to-clipboard title="Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-AP, stat-ME<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17570v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17570v2.pdf filename=2402.17570v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Gaussian</b> <b>Processes</b> (GP) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures. In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise. We propose a scalable inference algorithm based on the Sparse Variational <b>Gaussian</b> <b>Process</b> (SVGP) method for fitting sparse <b>Gaussian</b> <b>process</b> regression models with contaminated normal noise on large datasets. We examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks. We show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline.</p></p class="citation"></blockquote><h3 id=5763--217341-rime-robust-preference-based-reinforcement-learning-with-noisy-preferences-jie-cheng-et-al-2024>(57/63 | 217/341) RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences (Jie Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Cheng, Gang Xiong, Xingyuan Dai, Qinghai Miao, Yisheng Lv, Fei-Yue Wang. (2024)<br><strong>RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences</strong><br><button class=copy-to-clipboard title="RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17257v1.pdf filename=2402.17257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Preference-based <b>Reinforcement</b> <b>Learning</b> (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm start is crucial for both robustness and feedback-efficiency in limited-feedback cases.</p></p class="citation"></blockquote><h3 id=5863--218341-efficient-backpropagation-with-variance-controlled-adaptive-sampling-ziteng-wang-et-al-2024>(58/63 | 218/341) Efficient Backpropagation with Variance-Controlled Adaptive Sampling (Ziteng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziteng Wang, Jianfei Chen, Jun Zhu. (2024)<br><strong>Efficient Backpropagation with Variance-Controlled Adaptive Sampling</strong><br><button class=copy-to-clipboard title="Efficient Backpropagation with Variance-Controlled Adaptive Sampling" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17227v1.pdf filename=2402.17227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sampling-based algorithms, which eliminate &lsquo;&lsquo;unimportant&rsquo;&rsquo; computations during forward and/or back propagation (BP), offer potential solutions to accelerate neural network training. However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy across various tasks. In this work, we introduce a variance-controlled adaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an unbiased stochastic gradient with fine-grained layerwise importance sampling in data dimension for activation gradient calculation and leverage score sampling in token dimension for weight gradient calculation. To preserve accuracy, we control the additional variance by learning the sample ratio jointly with model parameters during training. We assessed VCAS on multiple <b>fine-tuning</b> and pre-training tasks in both vision and natural language domains. On all the tasks, VCAS can preserve the original training loss trajectory and validation accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction of the whole training process. The implementation is available at <a href=https://github.com/thu-ml/VCAS>https://github.com/thu-ml/VCAS</a> .</p></p class="citation"></blockquote><h3 id=5963--219341-fedbrb-an-effective-solution-to-the-small-to-large-scenario-in-device-heterogeneity-federated-learning-ziyue-xu-et-al-2024>(59/63 | 219/341) FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning (Ziyue Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyue Xu, Mingfeng Xu, Tianchi Liao, Zibin Zheng, Chuan Chen. (2024)<br><strong>FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning</strong><br><button class=copy-to-clipboard title="FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17202v1.pdf filename=2402.17202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the success of large models has demonstrated the importance of scaling up model size. This has spurred interest in exploring collaborative training of large-scale models from <b>federated</b> <b>learning</b> perspective. Due to computational constraints, many institutions struggle to train a large-scale model locally. Thus, training a larger global model using only smaller local models has become an important scenario (i.e., the \textbf{small-to-large scenario}). Although recent device-heterogeneity <b>federated</b> <b>learning</b> approaches have started to explore this area, they face limitations in fully covering the parameter space of the global model. In this paper, we propose a method called \textbf{FedBRB} (\underline{B}lock-wise \underline{R}olling and weighted \underline{B}roadcast) based on the block concept. FedBRB can uses small local models to train all blocks of the large global model, and broadcasts the trained parameters to the entire space for faster information interaction. Experiments demonstrate FedBRB yields substantial performance gains, achieving state-of-the-art results in this scenario. Moreover, FedBRB using only minimal local models can even surpass baselines using larger local models.</p></p class="citation"></blockquote><h3 id=6063--220341-gradient-based-discrete-sampling-with-automatic-cyclical-scheduling-patrick-pynadath-et-al-2024>(60/63 | 220/341) Gradient-based Discrete Sampling with Automatic Cyclical Scheduling (Patrick Pynadath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Pynadath, Riddhiman Bhattacharya, Arun Hariharan, Ruqi Zhang. (2024)<br><strong>Gradient-based Discrete Sampling with Automatic Cyclical Scheduling</strong><br><button class=copy-to-clipboard title="Gradient-based Discrete Sampling with Automatic Cyclical Scheduling" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17699v1.pdf filename=2402.17699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Discrete distributions, particularly in high-dimensional deep models, are often highly <b>multimodal</b> due to inherent discontinuities. While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information. To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in <b>multimodal</b> discrete distributions. Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning. We prove the non-asymptotic convergence and inference guarantee for our method in general discrete distributions. Extensive experiments demonstrate the superiority of our method in sampling complex <b>multimodal</b> discrete distributions.</p></p class="citation"></blockquote><h3 id=6163--221341-enhanced-bayesian-optimization-via-preferential-modeling-of-abstract-properties-arun-kumar-a-v-et-al-2024>(61/63 | 221/341) Enhanced Bayesian Optimization via Preferential Modeling of Abstract Properties (Arun Kumar A V et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arun Kumar A V, Alistair Shilton, Sunil Gupta, Santu Rana, Stewart Greenhill, Svetha Venkatesh. (2024)<br><strong>Enhanced Bayesian Optimization via Preferential Modeling of Abstract Properties</strong><br><button class=copy-to-clipboard title="Enhanced Bayesian Optimization via Preferential Modeling of Abstract Properties" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17343v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17343v1.pdf filename=2402.17343v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Experimental (design) optimization is a key driver in designing and discovering new products and processes. Bayesian Optimization (BO) is an effective tool for optimizing expensive and <b>black-box</b> <b>experimental</b> design processes. While Bayesian optimization is a principled data-driven approach to experimental optimization, it learns everything from scratch and could greatly benefit from the expertise of its human (domain) experts who often reason about systems at different abstraction levels using physical properties that are not necessarily directly measured (or measurable). In this paper, we propose a human-AI collaborative Bayesian framework to incorporate expert preferences about unmeasured abstract properties into the surrogate modeling to further boost the performance of BO. We provide an efficient strategy that can also handle any incorrect/misleading expert bias in preferential judgments. We discuss the convergence behavior of our proposed framework. Our experimental results involving synthetic functions and real-world datasets show the superiority of our method against the baselines.</p></p class="citation"></blockquote><h3 id=6263--222341-lcen-a-novel-feature-selection-algorithm-for-nonlinear-interpretable-machine-learning-models-pedro-seber-et-al-2024>(62/63 | 222/341) LCEN: A Novel Feature Selection Algorithm for Nonlinear, Interpretable Machine Learning Models (Pedro Seber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pedro Seber, Richard D. Braatz. (2024)<br><strong>LCEN: A Novel Feature Selection Algorithm for Nonlinear, Interpretable Machine Learning Models</strong><br><button class=copy-to-clipboard title="LCEN: A Novel Feature Selection Algorithm for Nonlinear, Interpretable Machine Learning Models" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17120v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17120v1.pdf filename=2402.17120v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interpretable architectures can have advantages over <b>black-box</b> <b>architectures,</b> and interpretability is essential for the application of machine learning in critical settings, such as aviation or medicine. However, the simplest, most commonly used interpretable architectures (such as LASSO or EN) are limited to linear predictions and have poor feature selection capabilities. In this work, we introduce the LASSO-Clip-EN (LCEN) algorithm for the creation of nonlinear, interpretable machine learning models. LCEN is tested on a wide variety of artificial and empirical datasets, creating more accurate, sparser models than other commonly used architectures. These experiments reveal that LCEN is robust against many issues typically present in datasets and modeling, including noise, multicollinearity, data scarcity, and hyperparameter variance. LCEN is also able to rediscover multiple physical laws from empirical data and, for processes with no known physical laws, LCEN achieves better results than many other dense and sparse methods &ndash; including using 10.8 times fewer features than dense methods and 8.1 times fewer features than EN on one dataset, and is comparable to an ANN on another dataset.</p></p class="citation"></blockquote><h3 id=6363--223341-an-interpretable-evaluation-of-entropy-based-novelty-of-generative-models-jingwei-zhang-et-al-2024>(63/63 | 223/341) An Interpretable Evaluation of Entropy-based Novelty of Generative Models (Jingwei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingwei Zhang, Cheuk Ting Li, Farzan Farnia. (2024)<br><strong>An Interpretable Evaluation of Entropy-based Novelty of Generative Models</strong><br><button class=copy-to-clipboard title="An Interpretable Evaluation of Entropy-based Novelty of Generative Models" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17287v1.pdf filename=2402.17287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The massive developments of generative model frameworks and architectures require principled methods for the evaluation of a model&rsquo;s novelty compared to a reference dataset or baseline generative models. While the recent literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model&rsquo;s novelty compared to a baseline model has not been adequately studied in the machine learning community. In this work, we focus on the novelty assessment under <b>multi-modal</b> generative models and attempt to answer the following question: Given the samples of a generative model $\mathcal{G}$ and a reference dataset $\mathcal{S}$, how can we discover and count the modes expressed by $\mathcal{G}$ more frequently than in $\mathcal{S}$. We introduce a spectral approach to the described task and propose the Kernel-based Entropic Novelty (KEN) score to quantify the mode-based novelty of distribution $P_\mathcal{G}$ with respect to distribution $P_\mathcal{S}$. We analytically interpret the behavior of the KEN score under mixture distributions with sub-Gaussian components. Next, we develop a method based on Cholesky decomposition to compute the KEN score from observed samples. We support the KEN-based quantification of novelty by presenting several numerical results on synthetic and real image distributions. Our numerical results indicate the success of the proposed approach in detecting the novel modes and the comparison of state-of-the-art generative models.</p></p class="citation"></blockquote><h2 id=csai-11>cs.AI (11)</h2><h3 id=111--224341-case-based-or-rule-based-how-do-transformers-do-the-math-yi-hu-et-al-2024>(1/11 | 224/341) Case-Based or Rule-Based: How Do Transformers Do the Math? (Yi Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Hu, Xiaojuan Tang, Haotong Yang, Muhan Zhang. (2024)<br><strong>Case-Based or Rule-Based: How Do Transformers Do the Math?</strong><br><button class=copy-to-clipboard title="Case-Based or Rule-Based: How Do Transformers Do the Math?" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Transformer, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17709v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17709v1.pdf filename=2402.17709v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the impressive performance in a variety of complex tasks, modern <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, <b>LLMs</b> struggle to do the same. Instead, they may rely on similar &ldquo;cases&rdquo; seen in the training corpus for help. We define these two different <b>reasoning</b> mechanisms as &ldquo;rule-based <b>reasoning&rdquo;</b> and &ldquo;case-based <b>reasoning&rdquo;.</b> Since rule-based <b>reasoning</b> is essential for acquiring the systematic generalization ability, we aim to explore exactly whether <b>transformers</b> use rule-based or case-based <b>reasoning</b> for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that <b>transformers</b> are performing case-based <b>reasoning,</b> no matter whether scratchpad is used, which aligns with the previous observations that <b>transformers</b> use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following <b>Fine-Tuning</b> (RFFT) technique to teach <b>transformers</b> to perform rule-based <b>reasoning.</b> Specifically, we provide explicit rules in the input and then instruct <b>transformers</b> to recite and follow the rules step by step. Through RFFT, we successfully enable <b>LLMs</b> <b>fine-tuned</b> on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad. The significant improvement demonstrates that teaching <b>LLMs</b> to explicitly use rules helps them learn rule-based <b>reasoning</b> and generalize better in length.</p></p class="citation"></blockquote><h3 id=211--225341-omniact-a-dataset-and-benchmark-for-enabling-multimodal-generalist-autonomous-agents-for-desktop-and-web-raghav-kapoor-et-al-2024>(2/11 | 225/341) OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web (Raghav Kapoor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, Ruslan Salakhutdinov. (2024)<br><strong>OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web</strong><br><button class=copy-to-clipboard title="OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CV, cs-HC, cs.AI<br>Keyword Score: 59<br>Keywords: Benchmarking, Human Intervention, Multi-modal, Multi-modal, GPT, GPT-4, Grounding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17553v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17553v2.pdf filename=2402.17553v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For decades, <b>human-computer</b> <b>interaction</b> has fundamentally been manual. Even today, almost all productive work done on the computer necessitates <b>human</b> <b>input</b> at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal <b>human</b> <b>intervention.</b> In this paper, we introduce OmniACT, the first-of-a-kind dataset and <b>benchmark</b> for assessing an agent&rsquo;s capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as &ldquo;Play the next song&rdquo;, as well as longer horizon tasks such as &ldquo;Send an email to John Doe mentioning the time and place to meet&rdquo;. Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our <b>benchmark.</b> The strongest baseline, <b>GPT-4,</b> performs the best on our <b>benchmark</b> However, its performance level still reaches only 15% of the <b>human</b> <b>proficiency</b> in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our <b>benchmark</b> provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building <b>multimodal</b> models that bridge <b>large</b> <b>language</b> <b>models</b> and the visual <b>grounding</b> of computer screens.</p></p class="citation"></blockquote><h3 id=311--226341-pragmatic-instruction-following-and-goal-assistance-via-cooperative-language-guided-inverse-planning-tan-zhi-xuan-et-al-2024>(3/11 | 226/341) Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning (Tan Zhi-Xuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tan Zhi-Xuan, Lance Ying, Vikash Mansinghka, Joshua B. Tenenbaum. (2024)<br><strong>Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning</strong><br><button class=copy-to-clipboard title="Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, GPT, Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17930v1.pdf filename=2402.17930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>People often give <b>instructions</b> <b>whose</b> meaning is ambiguous without further context, expecting that their actions or goals will disambiguate their intentions. How can we build assistive agents that follow such <b>instructions</b> <b>in</b> a flexible, context-sensitive manner? This paper introduces cooperative language-guided inverse plan search (CLIPS), a Bayesian agent architecture for pragmatic <b>instruction</b> <b>following</b> and goal assistance. Our agent assists a human by modeling them as a cooperative planner who communicates joint plans to the assistant, then performs <b>multimodal</b> Bayesian inference over the human&rsquo;s goal from actions and language, using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to evaluate the likelihood of an <b>instruction</b> <b>given</b> a hypothesized plan. Given this posterior, our assistant acts to minimize expected goal achievement cost, enabling it to pragmatically follow ambiguous <b>instructions</b> <b>and</b> provide effective assistance even when uncertain about the goal. We evaluate these capabilities in two cooperative planning domains (Doors, Keys & Gems and VirtualHome), finding that CLIPS significantly outperforms <b>GPT-4V,</b> <b>LLM-based</b> literal <b>instruction</b> <b>following</b> and unimodal inverse planning in both accuracy and helpfulness, while closely matching the inferences and assistive judgments provided by human raters.</p></p class="citation"></blockquote><h3 id=411--227341-rebandit-random-effects-based-online-rl-algorithm-for-reducing-cannabis-use-susobhan-ghosh-et-al-2024>(4/11 | 227/341) reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use (Susobhan Ghosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Susobhan Ghosh, Yongyi Guo, Pei-Yao Hung, Lara Coughlin, Erin Bonar, Inbal Nahum-Shani, Maureen Walton, Susan Murphy. (2024)<br><strong>reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use</strong><br><button class=copy-to-clipboard title="reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 40<br>Keywords: Online Reinforcement Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17739v1.pdf filename=2402.17739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an <b>online</b> <b>reinforcement</b> <b>learning</b> (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters <b>online.</b> <b>To</b> <b>evaluate</b> the performance of our algorithm, we construct a <b>simulation</b> testbed using data from a prior study, and compare against commonly used algorithms in mobile health studies. We show that reBandit performs equally well or better than all the baseline algorithms, and the performance gap widens as population heterogeneity increases in the <b>simulation</b> environment, proving its adeptness to adapt to diverse population of study participants.</p></p class="citation"></blockquote><h3 id=511--228341-agent-pro-learning-to-evolve-via-policy-level-reflection-and-optimization-wenqi-zhang-et-al-2024>(5/11 | 228/341) Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization (Wenqi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu. (2024)<br><strong>Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization</strong><br><button class=copy-to-clipboard title="Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17574v1.pdf filename=2402.17574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> exhibit robust problem-solving capabilities for diverse tasks. However, most <b>LLM-based</b> agents are designed as specific task solvers with sophisticated <b>prompt</b> engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted <b>prompts</b> to inform task rules and regulate <b>LLM</b> behaviors, inherently incapacitating to address complex dynamic scenarios e.g., <b>large</b> <b>interactive</b> <b>games.</b> In light of this, we propose Agent-Pro: an <b>LLM-based</b> Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, <b>fine-tuning</b> its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold&rsquo;em, outperforming vanilla <b>LLM</b> and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous <b>LLM-based</b> applications.</p></p class="citation"></blockquote><h3 id=611--229341-determinants-of-llm-assisted-decision-making-eva-eigner-et-al-2024>(6/11 | 229/341) Determinants of LLM-assisted Decision-Making (Eva Eigner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eva Eigner, Thorsten Händler. (2024)<br><strong>Determinants of LLM-assisted Decision-Making</strong><br><button class=copy-to-clipboard title="Determinants of LLM-assisted Decision-Making" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs.AI<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17385v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17385v1.pdf filename=2402.17385v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decision-making is a fundamental capability in everyday life. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> provide multifaceted support in enhancing human decision-making processes. However, understanding the influencing factors of <b>LLM-assisted</b> decision-making is crucial for enabling individuals to utilize <b>LLM-provided</b> advantages and minimize associated risks in order to make more informed and better decisions. This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with <b>LLM</b> support. In particular, we explore the effects of technological aspects of <b>LLMs,</b> including transparency and <b>prompt</b> engineering, psychological factors such as emotions and decision-making styles, as well as decision-specific determinants such as task difficulty and accountability. In addition, the impact of the determinants on the decision-making process is illustrated via multiple application scenarios. Drawing from our analysis, we develop a dependency framework that systematizes possible interactions in terms of reciprocal interdependencies between these determinants. Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on <b>LLMs,</b> the user&rsquo;s mental model, and the characteristics of information processing are identified as significant aspects influencing <b>LLM-assisted</b> decision-making processes. Our findings can be seen as crucial for improving decision quality in human-AI collaboration, empowering both users and organizations, and designing more effective <b>LLM</b> interfaces. Additionally, our work provides a foundation for future empirical investigations on the determinants of decision-making assisted by <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=711--230341-the-kandy-benchmark-incremental-neuro-symbolic-learning-and-reasoning-with-kandinsky-patterns-luca-salvatore-lorello-et-al-2024>(7/11 | 230/341) The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns (Luca Salvatore Lorello et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Salvatore Lorello, Marco Lippi, Stefano Melacci. (2024)<br><strong>The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns</strong><br><button class=copy-to-clipboard title="The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Semi-Supervised Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17431v1.pdf filename=2402.17431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial intelligence is continuously seeking novel challenges and <b>benchmarks</b> to effectively measure performance and to advance the state-of-the-art. In this paper we introduce KANDY, a <b>benchmarking</b> framework that can be used to generate a variety of learning and <b>reasoning</b> tasks inspired by Kandinsky patterns. By creating curricula of binary classification tasks with increasing complexity and with sparse supervisions, KANDY can be used to implement <b>benchmarks</b> for continual and <b>semi-supervised</b> <b>learning,</b> with a specific focus on symbol compositionality. Classification rules are also provided in the ground truth to enable analysis of interpretable solutions. Together with the <b>benchmark</b> generation pipeline, we release two curricula, an easier and a harder one, that we propose as new challenges for the research community. With a thorough experimental evaluation, we show how both state-of-the-art neural models and purely symbolic approaches struggle with solving most of the tasks, thus calling for the application of advanced neuro-symbolic methods trained over time.</p></p class="citation"></blockquote><h3 id=811--231341-benchmarking-data-science-agents-yuge-zhang-et-al-2024>(8/11 | 231/341) Benchmarking Data Science Agents (Yuge Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuge Zhang, Qiyang Jiang, Xingyu Han, Nan Chen, Yuqing Yang, Kan Ren. (2024)<br><strong>Benchmarking Data Science Agents</strong><br><button class=copy-to-clipboard title="Benchmarking Data Science Agents" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17168v1.pdf filename=2402.17168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval &ndash; a novel evaluation paradigm, as well as a series of innovative <b>benchmarks</b> tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand <b>benchmarking</b> comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.</p></p class="citation"></blockquote><h3 id=911--232341-cocoa-cbt-based-conversational-counseling-agent-using-memory-specialized-in-cognitive-distortions-and-dynamic-prompt-suyeon-lee-et-al-2024>(9/11 | 232/341) COCOA: CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions and Dynamic Prompt (Suyeon Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suyeon Lee, Jieun Kang, Harim Kim, Kyoung-Mee Chung, Dongha Lee, Jinyoung Yeo. (2024)<br><strong>COCOA: CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions and Dynamic Prompt</strong><br><button class=copy-to-clipboard title="COCOA: CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions and Dynamic Prompt" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: GPT, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17546v1.pdf filename=2402.17546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The demand for conversational agents that provide mental health care is consistently increasing. In this work, we develop a psychological counseling agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT) techniques to identify and address cognitive distortions inherent in the client&rsquo;s statements. Specifically, we construct a memory system to efficiently manage information necessary for counseling while extracting high-level insights about the client from their utterances. Additionally, to ensure that the counseling agent generates appropriate responses, we introduce dynamic <b>prompting</b> to flexibly apply CBT techniques and facilitate the appropriate retrieval of information. We conducted dialogues between CoCoA and characters from Character.ai, creating a dataset for evaluation. Then, we asked <b>GPT</b> to evaluate the constructed counseling dataset, and our model demonstrated a statistically significant difference from other models.</p></p class="citation"></blockquote><h3 id=1011--233341-large-language-model-for-participatory-urban-planning-zhilun-zhou-et-al-2024>(10/11 | 233/341) Large Language Model for Participatory Urban Planning (Zhilun Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhilun Zhou, Yuming Lin, Depeng Jin, Yong Li. (2024)<br><strong>Large Language Model for Participatory Urban Planning</strong><br><button class=copy-to-clipboard title="Large Language Model for Participatory Urban Planning" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-MA, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17161v1.pdf filename=2402.17161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Participatory urban planning is the mainstream of modern urban planning that involves the active engagement of residents. However, the traditional participatory paradigm requires experienced planning experts and is often time-consuming and costly. Fortunately, the emerging <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown considerable ability to simulate human-like agents, which can be used to emulate the participatory process easily. In this work, we introduce an <b>LLM-based</b> multi-agent collaboration framework for participatory urban planning, which can generate land-use plans for urban regions considering the diverse needs of residents. Specifically, we construct <b>LLM</b> agents to simulate a planner and thousands of residents with diverse profiles and backgrounds. We first ask the planner to carry out an initial land-use plan. To deal with the different facilities needs of residents, we initiate a discussion among the residents in each community about the plan, where residents provide feedback based on their profiles. Furthermore, to improve the efficiency of discussion, we adopt a fishbowl discussion mechanism, where part of the residents discuss and the rest of them act as listeners in each round. Finally, we let the planner modify the plan based on residents&rsquo; feedback. We deploy our method on two real-world regions in Beijing. Experiments show that our method achieves state-of-the-art performance in residents satisfaction and inclusion metrics, and also outperforms human experts in terms of service accessibility and ecology metrics.</p></p class="citation"></blockquote><h3 id=1111--234341-multi-agent-human-agent-and-beyond-a-survey-on-cooperation-in-social-dilemmas-hao-guo-et-al-2024>(11/11 | 234/341) Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas (Hao Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Guo, Chunjiang Mu, Yang Chen, Chen Shen, Shuyue Hu, Zhen Wang. (2024)<br><strong>Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas</strong><br><button class=copy-to-clipboard title="Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-GT, cs-HC, cs-LG, cs-MA, cs.AI<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17270v1.pdf filename=2402.17270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science. Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation. This survey examines three key areas at the intersection of AI and cooperation in social dilemmas. First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents. Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents. Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans. We conclude by discussing future research avenues, such as using <b>large</b> <b>language</b> <b>models,</b> establishing unified theoretical frameworks, revisiting existing theories of human cooperation, and exploring multiple real-world applications.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--235341-a-neural-rewriting-system-to-solve-algorithmic-problems-flavio-petruzzellis-et-al-2024>(1/2 | 235/341) A Neural Rewriting System to Solve Algorithmic Problems (Flavio Petruzzellis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti. (2024)<br><strong>A Neural Rewriting System to Solve Algorithmic Problems</strong><br><button class=copy-to-clipboard title="A Neural Rewriting System to Solve Algorithmic Problems" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-CL, cs-NE, cs.NE<br>Keyword Score: 53<br>Keywords: Benchmarking, Out-of-distribution, GPT, GPT-4, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17407v1.pdf filename=2402.17407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern neural network architectures still struggle to learn algorithmic procedures that require to systematically apply compositional rules to solve <b>out-of-distribution</b> problem instances. In this work, we propose an original approach to learn algorithmic tasks inspired by rewriting systems, a classic framework in symbolic artificial intelligence. We show that a rewriting system can be implemented as a neural architecture composed by specialized modules: the Selector identifies the target sub-expression to process, the Solver simplifies the sub-expression by computing the corresponding result, and the Combiner produces a new version of the original expression by replacing the sub-expression with the solution provided. We evaluate our model on three types of algorithmic tasks that require simplifying symbolic formulas involving lists, arithmetic, and algebraic expressions. We test the extrapolation capabilities of the proposed architecture using formulas involving a higher number of operands and nesting levels than those seen during training, and we <b>benchmark</b> its performance against the Neural Data Router, a recent model specialized for systematic generalization, and a state-of-the-art <b>large</b> <b>language</b> <b>model</b> <b>(GPT-4)</b> probed with advanced <b>prompting</b> strategies.</p></p class="citation"></blockquote><h3 id=22--236341-scaling-supervised-local-learning-with-augmented-auxiliary-networks-chenxiang-ma-et-al-2024>(2/2 | 236/341) Scaling Supervised Local Learning with Augmented Auxiliary Networks (Chenxiang Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenxiang Ma, Jibin Wu, Chenyang Si, Kay Chen Tan. (2024)<br><strong>Scaling Supervised Local Learning with Augmented Auxiliary Networks</strong><br><button class=copy-to-clipboard title="Scaling Supervised Local Learning with Augmented Auxiliary Networks" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-CV, cs-LG, cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17318v1.pdf filename=2402.17318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks are typically trained using global error signals that backpropagate (BP) end-to-end, which is not only biologically implausible but also suffers from the update locking problem and requires huge memory consumption. Local learning, which updates each layer independently with a gradient-isolated auxiliary network, offers a promising alternative to address the above problems. However, existing local learning methods are confronted with a large accuracy gap with the BP counterpart, particularly for large-scale networks. This is due to the weak coupling between local layers and their subsequent network layers, as there is no gradient communication across layers. To tackle this issue, we put forward an augmented local learning method, dubbed AugLocal. AugLocal constructs each hidden layer&rsquo;s auxiliary network by uniformly selecting a small subset of layers from its subsequent network layers to enhance their synergy. We also propose to linearly reduce the depth of auxiliary networks as the hidden layer goes deeper, ensuring sufficient network capacity while reducing the computational cost of auxiliary networks. Our extensive experiments on four image classification datasets (i.e., CIFAR-10, SVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up to tens of local layers with a comparable accuracy to BP-trained networks while reducing GPU memory usage by around 40%. The proposed AugLocal method, therefore, opens up a myriad of opportunities for training high-performance deep neural networks on resource-constrained platforms.Code is available at <a href=https://github.com/ChenxiangMA/AugLocal>https://github.com/ChenxiangMA/AugLocal</a>.</p></p class="citation"></blockquote><h2 id=quant-ph-3>quant-ph (3)</h2><h3 id=13--237341-learning-to-program-variational-quantum-circuits-with-fast-weights-samuel-yen-chi-chen-2024>(1/3 | 237/341) Learning to Program Variational Quantum Circuits with Fast Weights (Samuel Yen-Chi Chen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Yen-Chi Chen. (2024)<br><strong>Learning to Program Variational Quantum Circuits with Fast Weights</strong><br><button class=copy-to-clipboard title="Learning to Program Variational Quantum Circuits with Fast Weights" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, cs-ET, cs-LG, cs-NE, quant-ph, quant-ph<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Simulation, Simulator, Recurrent Neural Network, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17760v1.pdf filename=2402.17760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum Machine Learning (QML) has surfaced as a pioneering framework addressing sequential control tasks and time-series modeling. It has demonstrated empirical quantum advantages notably within domains such as <b>Reinforcement</b> <b>Learning</b> (RL) and time-series prediction. A significant advancement lies in Quantum <b>Recurrent</b> <b>Neural</b> <b>Networks</b> (QRNNs), specifically tailored for memory-intensive tasks encompassing partially observable environments and non-linear time-series prediction. Nevertheless, QRNN-based models encounter challenges, notably prolonged training duration <b>stemming</b> from the necessity to compute quantum gradients using backpropagation-through-time (BPTT). This predicament exacerbates when executing the complete model on quantum devices, primarily due to the substantial demand for circuit evaluation arising from the parameter-shift rule. This paper introduces the Quantum Fast Weight Programmers (QFWP) as a solution to the temporal or sequential learning challenge. The QFWP leverages a classical neural network (referred to as the &lsquo;slow programmer&rsquo;) functioning as a quantum programmer to swiftly modify the parameters of a variational quantum circuit (termed the &lsquo;fast programmer&rsquo;). Instead of completely overwriting the fast programmer at each time-step, the slow programmer generates parameter changes or updates for the quantum circuit parameters. This approach enables the fast programmer to incorporate past observations or information. Notably, the proposed QFWP model achieves learning of temporal dependencies without necessitating the use of quantum <b>recurrent</b> <b>neural</b> <b>networks.</b> Numerical <b>simulations</b> conducted in this study showcase the efficacy of the proposed QFWP model in both time-series prediction and RL tasks. The model exhibits performance levels either comparable to or surpassing those achieved by QLSTM-based models.</p></p class="citation"></blockquote><h3 id=23--238341-quantum-distance-approximation-for-persistence-diagrams-bernardo-ameneyro-et-al-2024>(2/3 | 238/341) Quantum Distance Approximation for Persistence Diagrams (Bernardo Ameneyro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bernardo Ameneyro, Rebekah Herrman, George Siopsis, Vasileios Maroulas. (2024)<br><strong>Quantum Distance Approximation for Persistence Diagrams</strong><br><button class=copy-to-clipboard title="Quantum Distance Approximation for Persistence Diagrams" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 13<br>Keywords: Clustering, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17295v1.pdf filename=2402.17295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Topological Data Analysis methods can be useful for classification and <b>clustering</b> tasks in many different fields as they can provide two dimensional persistence diagrams that <b>summarize</b> important information about the shape of potentially complex and high dimensional data sets. The space of persistence diagrams can be endowed with various metrics such as the Wasserstein distance which admit a statistical structure and allow to use these summaries for machine learning algorithms. However, computing the distance between two persistence diagrams involves finding an optimal way to match the points of the two diagrams and may not always be an easy task for classical computers. In this work we explore the potential of quantum computers to estimate the distance between persistence diagrams, in particular we propose variational quantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$ distance. Our implementation is a weighted version of the Quantum Approximate Optimization Algorithm that relies on control clauses to encode the constraints of the optimization problem.</p></p class="citation"></blockquote><h3 id=33--239341-a-quantum-approach-to-synthetic-minority-oversampling-technique-smote-nishikanta-mohanty-et-al-2024>(3/3 | 239/341) A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE) (Nishikanta Mohanty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nishikanta Mohanty, Bikash K. Behera, Christopher Ferrie, Pravat Dash. (2024)<br><strong>A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)</strong><br><button class=copy-to-clipboard title="A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, cs-LG, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17398v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17398v2.pdf filename=2402.17398v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation. The process varies from the conventional SMOTE algorithm&rsquo;s usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity. The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements. The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification algorithms, Random Forest and <b>Logistic</b> <b>Regression,</b> to determine its impact along with varying proportions of synthetic data.</p></p class="citation"></blockquote><h2 id=csse-4>cs.SE (4)</h2><h3 id=14--240341-the-emergence-of-large-language-models-in-static-analysis-a-first-look-through-micro-benchmarks-ashwin-prasad-shivarpatna-venkatesh-et-al-2024>(1/4 | 240/341) The Emergence of Large Language Models in Static Analysis: A First Look through Micro-Benchmarks (Ashwin Prasad Shivarpatna Venkatesh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashwin Prasad Shivarpatna Venkatesh, Samkutty Sabu, Amir M. Mir, Sofia Reis, Eric Bodden. (2024)<br><strong>The Emergence of Large Language Models in Static Analysis: A First Look through Micro-Benchmarks</strong><br><button class=copy-to-clipboard title="The Emergence of Large Language Models in Static Analysis: A First Look through Micro-Benchmarks" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Fine-tuning, GPT, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17679v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17679v1.pdf filename=2402.17679v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The application of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in software engineering, particularly in static analysis tasks, represents a paradigm shift in the field. In this paper, we investigate the role that current <b>LLMs</b> can play in improving callgraph analysis and type inference for Python programs. Using the PyCG, HeaderGen, and TypeEvalPy micro-benchmarks, we evaluate 26 <b>LLMs,</b> including OpenAI&rsquo;s <b>GPT</b> series and open-source models such as <b>LLaMA.</b> Our study reveals that <b>LLMs</b> show promising results in type inference, demonstrating higher accuracy than traditional methods, yet they exhibit limitations in callgraph analysis. This contrast emphasizes the need for specialized <b>fine-tuning</b> of <b>LLMs</b> to better suit specific static analysis tasks. Our findings provide a foundation for further research towards integrating <b>LLMs</b> for static analysis tasks.</p></p class="citation"></blockquote><h3 id=24--241341-nissist-an-incident-mitigation-copilot-based-on-troubleshooting-guides-kaikai-an-et-al-2024>(2/4 | 241/341) Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides (Kaikai An et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaikai An, Fangkai Yang, Liqun Li, Zhixing Ren, Hao Huang, Lu Wang, Pu Zhao, Yu Kang, Hua Ding, Qingwei Lin, Saravan Rajmohan, Qi Zhang. (2024)<br><strong>Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides</strong><br><button class=copy-to-clipboard title="Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Human Intervention, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17531v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17531v1.pdf filename=2402.17531v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective incident management is pivotal for the smooth operation of enterprises-level cloud services. In order to expedite incident mitigation, service teams compile troubleshooting knowledge into Troubleshooting Guides (TSGs) accessible to on-call engineers (OCEs). While automated pipelines are enabled to resolve the most frequent and easy incidents, there still exist complex incidents that require OCEs&rsquo; intervention. However, TSGs are often unstructured and incomplete, which requires manual interpretation by OCEs, leading to on-call fatigue and decreased productivity, especially among new-hire OCEs. In this work, we propose Nissist which leverages TSGs and incident mitigation histories to provide proactive suggestions, reducing <b>human</b> <b>intervention.</b> Leveraging <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM),</b> Nissist extracts insights from unstructured TSGs and historical incident mitigation discussions, forming a comprehensive knowledge base. Its multi-agent system design enhances proficiency in precisely discerning user queries, retrieving relevant information, and delivering systematic plans consecutively. Through our user case and experiment, we demonstrate that Nissist significant reduce Time to Mitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs and improving service reliability. Our demo is available at <a href=https://aka.ms/nissist_demo>https://aka.ms/nissist_demo</a>.</p></p class="citation"></blockquote><h3 id=34--242341-ansible-lightspeed-a-code-generation-service-for-it-automation-priyam-sahoo-et-al-2024>(3/4 | 242/341) Ansible Lightspeed: A Code Generation Service for IT Automation (Priyam Sahoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Priyam Sahoo, Saurabh Pujar, Ganesh Nalawade, Richard Gebhardt, Louis Mandel, Luca Buratti. (2024)<br><strong>Ansible Lightspeed: A Code Generation Service for IT Automation</strong><br><button class=copy-to-clipboard title="Ansible Lightspeed: A Code Generation Service for IT Automation" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-PL, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17442v1.pdf filename=2402.17442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The availability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> which can generate <b>code,</b> <b>has</b> made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with <b>LLMs.</b> Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for IT automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Red Hat Ansible Lightspeed with IBM Watson <b>Code</b> <b>Assistant,</b> further referred to as Ansible Lightspeed, is an <b>LLM-based</b> service designed explicitly for natural language to Ansible <b>code</b> <b>generation.</b> In this paper, we describe the design and implementation of the Ansible Lightspeed service and analyze feedback from thousands of real users. We examine diverse performance indicators, classified according to both immediate and extended utilization patterns along with user sentiments. The analysis shows that the user acceptance rate of Ansible Lightspeed suggestions is higher than comparable tools that are more general and not specific to a programming language. This remains true even after we use much more stringent criteria for what is considered an accepted model suggestion, discarding suggestions which were heavily edited after being accepted. The relatively high acceptance rate results in higher-than-expected user retention and generally positive user feedback. This paper provides insights on how a comparatively small, dedicated model performs on a domain-specific language and more importantly, how it is received by users.</p></p class="citation"></blockquote><h3 id=44--243341-faultprofit-hierarchical-fault-profiling-of-incident-tickets-in-large-scale-cloud-systems-junjie-huang-et-al-2024>(4/4 | 243/341) FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems (Junjie Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjie Huang, Jinyang Liu, Zhuangbin Chen, Zhihan Jiang, Yichen LI, Jiazhen Gu, Cong Feng, Zengyin Yang, Yongqiang Yang, Michael R. Lyu. (2024)<br><strong>FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems</strong><br><button class=copy-to-clipboard title="FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17583v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17583v1.pdf filename=2402.17583v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Postmortem analysis is essential in the management of incidents within cloud systems, which provides valuable insights to improve system&rsquo;s reliability and robustness. At CloudA, fault pattern profiling is performed during the postmortem phase, which involves the classification of incidents&rsquo; faults into unique categories, referred to as fault pattern. By aggregating and analyzing these fault patterns, engineers can discern common faults, vulnerable components and emerging fault trends. However, this process is currently conducted by manual labeling, which has inherent drawbacks. On the one hand, the sheer volume of incidents means only the most severe ones are analyzed, causing a skewed overview of fault patterns. On the other hand, the complexity of the task demands extensive domain knowledge, which leads to errors and inconsistencies. To address these limitations, we propose an automated approach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets. It leverages hierarchy-guided <b>contrastive</b> <b>learning</b> to train a hierarchy-aware incident encoder and predicts fault patterns with enhanced incident representations. We evaluate FaultProfIT using the production incidents from CloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art methods. Our ablation study and analysis also verify the effectiveness of hierarchy-guided <b>contrastive</b> <b>learning.</b> Additionally, we have deployed FaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+ incidents from 30+ cloud services, successfully revealing several fault trends that have informed system improvements.</p></p class="citation"></blockquote><h2 id=q-bioqm-2>q-bio.QM (2)</h2><h3 id=12--244341-biot5-towards-generalized-biological-understanding-with-iupac-integration-and-multi-task-tuning-qizhi-pei-et-al-2024>(1/2 | 244/341) BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning (Qizhi Pei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua Zhu, Shufang Xie, Tao Qin, Rui Yan. (2024)<br><strong>BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning</strong><br><button class=copy-to-clipboard title="BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, cs-CE, cs-LG, q-bio-BM, q-bio-QM, q-bio.QM<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Reasoning, Tokenization, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17810v1.pdf filename=2402.17810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins. However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery. BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task <b>instruction</b> <b>tuning</b> for generality across tasks, and a novel numerical <b>tokenization</b> technique for improved processing of numerical data. These enhancements allow BioT5+ to bridge the gap between molecular representations and their textual descriptions, providing a more holistic understanding of biological entities, and largely improving the grounded <b>reasoning</b> of bio-text and bio-sequences. The model is pre-trained and <b>fine-tuned</b> with a large number of experiments, including \emph{3 types of problems (classification, regression, generation), 15 kinds of tasks, and 21 total <b>benchmark</b> datasets}, demonstrating the remarkable performance and state-of-the-art results in most cases. BioT5+ stands out for its ability to capture intricate relationships in biological data, thereby contributing significantly to bioinformatics and computational biology. Our code is available at \url{https://github.com/QizhiPei/BioT5}.</p></p class="citation"></blockquote><h3 id=22--245341-transfer-learning-bayesian-optimization-to-design-competitor-dna-molecules-for-use-in-diagnostic-assays-ruby-sedgwick-et-al-2024>(2/2 | 245/341) Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays (Ruby Sedgwick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruby Sedgwick, John P. Goertz, Molly M. Stevens, Ruth Misener, Mark van der Wilk. (2024)<br><strong>Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays</strong><br><button class=copy-to-clipboard title="Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, q-bio-QM, q-bio.QM, stat-ML<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17704v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17704v1.pdf filename=2402.17704v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rise in engineered biomolecular devices, there is an increased need for tailor-made biological sequences. Often, many similar biological sequences need to be made for a specific application meaning numerous, sometimes prohibitively expensive, lab experiments are necessary for their optimization. This paper presents a <b>transfer</b> <b>learning</b> design of experiments workflow to make this development feasible. By combining a <b>transfer</b> <b>learning</b> surrogate model with Bayesian optimization, we show how the total number of experiments can be reduced by sharing information between optimization tasks. We demonstrate the reduction in the number of experiments using data from the development of DNA competitors for use in an amplification-based diagnostic assay. We use cross-validation to compare the predictive accuracy of different <b>transfer</b> <b>learning</b> models, and then compare the performance of the models for both single objective and penalized optimization tasks.</p></p class="citation"></blockquote><h2 id=csro-13>cs.RO (13)</h2><h3 id=113--246341-can-an-llm-powered-socially-assistive-robot-effectively-and-safely-deliver-cognitive-behavioral-therapy-a-study-with-university-students-mina-j-kian-et-al-2024>(1/13 | 246/341) Can an LLM-Powered Socially Assistive Robot Effectively and Safely Deliver Cognitive Behavioral Therapy? A Study With University Students (Mina J. Kian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mina J. Kian, Mingyu Zong, Katrin Fischer, Abhyuday Singh, Anna-Maria Velentza, Pau Sang, Shriya Upadhyay, Anika Gupta, Misha A. Faruki, Wallace Browning, Sebastien M. R. Arnold, Bhaskar Krishnamachari, Maja J. Mataric. (2024)<br><strong>Can an LLM-Powered Socially Assistive Robot Effectively and Safely Deliver Cognitive Behavioral Therapy? A Study With University Students</strong><br><button class=copy-to-clipboard title="Can an LLM-Powered Socially Assistive Robot Effectively and Safely Deliver Cognitive Behavioral Therapy? A Study With University Students" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17937v1.pdf filename=2402.17937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cognitive behavioral therapy (CBT) is a widely used therapeutic method for guiding individuals toward restructuring their thinking patterns as a means of addressing anxiety, depression, and other challenges. We developed a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)-powered</b> <b>prompt-engineered</b> socially assistive robot (SAR) that guides participants through interactive CBT at-home exercises. We evaluated the performance of the SAR through a 15-day study with 38 university students randomly assigned to interact daily with the robot or a <b>chatbot</b> (using the same <b>LLM),</b> or complete traditional CBT worksheets throughout the duration of the study. We measured weekly therapeutic outcomes, changes in pre-/post-session anxiety measures, and adherence to completing CBT exercises. We found that self-reported measures of general psychological distress significantly decreased over the study period in the robot and worksheet conditions but not the <b>chatbot</b> condition. Furthermore, the SAR enabled significant single-session improvements for more sessions than the other two conditions combined. Our findings suggest that SAR-guided <b>LLM-powered</b> CBT may be as effective as traditional worksheet methods in supporting therapeutic progress from the beginning to the end of the study and superior in decreasing user anxiety immediately after completing the CBT exercise.</p></p class="citation"></blockquote><h3 id=213--247341-rethinking-mutual-information-for-language-conditioned-skill-discovery-on-imitation-learning-zhaoxun-ju-et-al-2024>(2/13 | 247/341) Rethinking Mutual Information for Language Conditioned Skill Discovery on Imitation Learning (Zhaoxun Ju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoxun Ju, Chao Yang, Hongbo Wang, Yu Qiao, Fuchun Sun. (2024)<br><strong>Rethinking Mutual Information for Language Conditioned Skill Discovery on Imitation Learning</strong><br><button class=copy-to-clipboard title="Rethinking Mutual Information for Language Conditioned Skill Discovery on Imitation Learning" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-2-6, cs-AI, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Mutual Information, Quantization, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17511v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17511v1.pdf filename=2402.17511v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language-conditioned robot behavior plays a vital role in executing complex tasks by associating human commands or instructions with perception and actions. The ability to compose long-horizon tasks based on unconstrained language instructions necessitates the acquisition of a diverse set of general-purpose skills. However, acquiring inherent primitive skills in a coupled and long-horizon environment without external rewards or human supervision presents significant challenges. In this paper, we evaluate the relationship between skills and language instructions from a mathematical perspective, employing two forms of <b>mutual</b> <b>information</b> within the framework of language-conditioned policy learning. To maximize the <b>mutual</b> <b>information</b> between language and skills in an <b>unsupervised</b> manner, we propose an end-to-end imitation learning approach known as Language Conditioned Skill Discovery (LCSD). Specifically, we utilize vector <b>quantization</b> to learn discrete latent skills and leverage skill sequences of trajectories to reconstruct high-level semantic instructions. Through extensive experiments on language-conditioned robotic navigation and manipulation tasks, encompassing BabyAI, LORel, and CALVIN, we demonstrate the superiority of our method over prior works. Our approach exhibits enhanced generalization capabilities towards unseen tasks, improved skill interpretability, and notably higher rates of task completion success.</p></p class="citation"></blockquote><h3 id=313--248341-cggm-a-conditional-graph-generation-model-with-adaptive-sparsity-for-node-anomaly-detection-in-iot-networks-xianshi-su-et-al-2024>(3/13 | 248/341) CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks (Xianshi Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianshi Su, Munan Li, Tongbang Jiang, Hao Long. (2024)<br><strong>CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks</strong><br><button class=copy-to-clipboard title="CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Node Anomaly Detection, Graph, Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17363v1.pdf filename=2402.17363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic <b>graphs</b> are extensively employed for detecting anomalous behavior in <b>nodes</b> <b>within</b> <b>the</b> Internet of Things (IoT). Generative models are often used to address the issue of imbalanced <b>node</b> <b>categories</b> <b>in</b> dynamic <b>graphs.</b> Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for <b>nodes,</b> <b>and</b> <b>the</b> lack of a method for end-to-end generation of multiple categories of <b>nodes.</b> <b>This</b> <b>paper</b> presents a novel <b>graph</b> generation model, called CGGM, designed specifically to generate a larger number of <b>nodes</b> <b>belonging</b> <b>to</b> the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate <b>node</b> <b>features</b> <b>along</b> with topological information. Labels are transformed into embedding vectors, serving as conditional constraints to control the generation of synthetic data across multiple categories. Using a multi-stage loss, the distribution of synthetic data is adjusted to closely resemble that of real data. In extensive experiments, we show that CGGM&rsquo;s synthetic data outperforms state-of-the-art methods across various metrics. Our results demonstrate efficient generation of diverse data categories, robustly enhancing multi-category classification model performance.</p></p class="citation"></blockquote><h3 id=413--249341-icat-an-indoor-connected-and-autonomous-testbed-for-vehicle-computing-zhaofeng-tian-et-al-2024>(4/13 | 249/341) ICAT: An Indoor Connected and Autonomous Testbed for Vehicle Computing (Zhaofeng Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaofeng Tian, William He, Boyang Tian, Ren Zhong, Erfan Foorginejad, Weisong Shi. (2024)<br><strong>ICAT: An Indoor Connected and Autonomous Testbed for Vehicle Computing</strong><br><button class=copy-to-clipboard title="ICAT: An Indoor Connected and Autonomous Testbed for Vehicle Computing" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17933v1.pdf filename=2402.17933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Indoor autonomous driving testbeds have emerged to complement expensive outdoor testbeds and virtual <b>simulations,</b> offering scalable and cost-effective solutions for research in navigation, traffic optimization, and swarm intelligence. However, they often lack the robust sensing and computing infrastructure for advanced research. Addressing these limitations, we introduce the Indoor Connected Autonomous Testbed (ICAT), a platform that not only tackles the unique challenges of indoor autonomous driving but also innovates vehicle computing and V2X communication. Moreover, ICAT leverages digital twins through CARLA and SUMO <b>simulations,</b> facilitating both centralized and decentralized autonomy deployments.</p></p class="citation"></blockquote><h3 id=513--250341-diffusion-meets-dagger-supercharging-eye-in-hand-imitation-learning-xiaoyu-zhang-et-al-2024>(5/13 | 250/341) Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning (Xiaoyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Zhang, Matthew Chang, Pranav Kumar, Saurabh Gupta. (2024)<br><strong>Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning</strong><br><button class=copy-to-clipboard title="Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Diffusion Model, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17768v1.pdf filename=2402.17768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose <b>Diffusion</b> <b>Meets</b> DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems. Instead of collecting new samples to cover <b>out-of-distribution</b> states, DMD uses recent advances in <b>diffusion</b> <b>models</b> to create these samples with <b>diffusion</b> <b>models.</b> This leads to robust performance from few demonstrations. In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 expert demonstrations, where naive behavior cloning reaches only 20%. DMD also outperform competing NeRF-based augmentation schemes by 50%.</p></p class="citation"></blockquote><h3 id=613--251341-backpropagation-based-analytical-derivatives-of-ekf-covariance-for-active-sensing-jonas-benhamou-et-al-2024>(6/13 | 251/341) Backpropagation-Based Analytical Derivatives of EKF Covariance for Active Sensing (Jonas Benhamou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Benhamou, Silvère Bonnabel, Camille Chapdelaine. (2024)<br><strong>Backpropagation-Based Analytical Derivatives of EKF Covariance for Active Sensing</strong><br><button class=copy-to-clipboard title="Backpropagation-Based Analytical Derivatives of EKF Covariance for Active Sensing" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17569v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17569v2.pdf filename=2402.17569v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To enhance accuracy of robot state estimation, perception-aware (or active sensing) methods seek trajectories that minimize uncertainty. To this aim, one possibility is to seek trajectories that minimize the final covariance of an extended Kalman filter (EKF), w.r.t. its control inputs over a given horizon. However, this can be computationally demanding. In this article, we derive novel backpropagation analytical formulas for the derivatives of the final covariance of an EKF w.r.t. its inputs. We then leverage the obtained analytical gradients as an enabling technology to derive perception-aware optimal motion plans. <b>Simulations</b> validate the approach, showcasing improvements in both estimation accuracy and execution time. Experimental results on a real large ground vehicle also support the method.</p></p class="citation"></blockquote><h3 id=713--252341-underwater-acoustic-source-seeking-using-time-difference-of-arrival-measurements-filip-mandić-et-al-2024>(7/13 | 252/341) Underwater Acoustic Source Seeking Using Time-Difference-of-Arrival Measurements (Filip Mandić et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filip Mandić, Nikola Mišković, Ivan Lončar. (2024)<br><strong>Underwater Acoustic Source Seeking Using Time-Difference-of-Arrival Measurements</strong><br><button class=copy-to-clipboard title="Underwater Acoustic Source Seeking Using Time-Difference-of-Arrival Measurements" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17405v1.pdf filename=2402.17405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The research presented in this paper is aimed at developing a control algorithm for an autonomous surface system carrying a two-sensor array consisting of two acoustic receivers, capable of measuring the time-difference-of-arrival (TDOA) of a quasiperiodic underwater acoustic signal and utilizing this value to steer the system toward the acoustic source in the horizontal plane. Stability properties of the proposed algorithm are analyzed using the Lie bracket approximation technique. Furthermore, <b>simulation</b> results are presented, where particular attention is given to the relationship between the time difference of arrival measurement noise and the sensor baseline - the distance between the two acoustic receivers. Also, the influence of a constant disturbance caused by sea currents is considered. Finally, experimental results in which the algorithm was deployed on two autonomous surface vehicles, each equipped with a single acoustic receiver, are presented. The algorithm successfully steers the vehicle formation toward the acoustic source, despite the measurement noise and intermittent measurements, thus showing the feasibility of the proposed algorithm in real-life conditions.</p></p class="citation"></blockquote><h3 id=813--253341-active-propulsion-noise-shaping-for-multi-rotor-aircraft-localization-gabriele-serussi-et-al-2024>(8/13 | 253/341) Active propulsion noise shaping for multi-rotor aircraft localization (Gabriele Serussi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriele Serussi, Tamir Shor, Tom Hirshberg, Chaim Baskin, Alex Bronstein. (2024)<br><strong>Active propulsion noise shaping for multi-rotor aircraft localization</strong><br><button class=copy-to-clipboard title="Active propulsion noise shaping for multi-rotor aircraft localization" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17289v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17289v2.pdf filename=2402.17289v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision for navigation purposes. However, visual localization and odometry techniques suffer from poor performance in low or direct sunlight, a limited field of view, and vulnerability to occlusions. Acoustic sensing can serve as a complementary or even alternative modality for vision in many situations, and it also has the added benefits of lower system cost and energy footprint, which is especially important for micro aircraft. This paper proposes actively controlling and shaping the aircraft propulsion noise generated by the rotors to benefit localization tasks, rather than considering it a harmful nuisance. We present a neural network architecture for selfnoise-based localization in a known environment. We show that training it simultaneously with learning time-varying rotor phase modulation achieves accurate and robust localization. The proposed methods are evaluated using a computationally affordable <b>simulation</b> of MAV rotor noise in 2D acoustic environments that is fitted to real recordings of rotor pressure fields.</p></p class="citation"></blockquote><h3 id=913--254341-racp-risk-aware-contingency-planning-with-multi-modal-predictions-khaled-a-mustafa-et-al-2024>(9/13 | 254/341) RACP: Risk-Aware Contingency Planning with Multi-Modal Predictions (Khaled A. Mustafa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khaled A. Mustafa, Daniel Jarne Ornia, Jens Kober, Javier Alonso-Mora. (2024)<br><strong>RACP: Risk-Aware Contingency Planning with Multi-Modal Predictions</strong><br><button class=copy-to-clipboard title="RACP: Risk-Aware Contingency Planning with Multi-Modal Predictions" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Fine-tuning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17387v1.pdf filename=2402.17387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For an autonomous vehicle to operate reliably within real-world traffic scenarios, it is imperative to assess the repercussions of its prospective actions by anticipating the uncertain intentions exhibited by other participants in the traffic environment. Driven by the pronounced <b>multi-modal</b> nature of human driving behavior, this paper presents an approach that leverages Bayesian beliefs over the distribution of potential policies of other road users to construct a novel risk-aware probabilistic motion planning framework. In particular, we propose a novel contingency planner that outputs long-term contingent plans conditioned on multiple possible intents for other actors in the traffic scene. The Bayesian belief is incorporated into the optimization cost function to influence the behavior of the short-term plan based on the likelihood of other agents&rsquo; policies. Furthermore, a probabilistic risk metric is employed to <b>fine-tune</b> the balance between efficiency and robustness. Through a series of closed-loop safety-critical simulated traffic scenarios shared with human-driven vehicles, we demonstrate the practical efficacy of our proposed approach that can handle multi-vehicle scenarios.</p></p class="citation"></blockquote><h3 id=1013--255341-opening-cabinets-and-drawers-in-the-real-world-using-a-commodity-mobile-manipulator-arjun-gupta-et-al-2024>(10/13 | 255/341) Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator (Arjun Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta. (2024)<br><strong>Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator</strong><br><button class=copy-to-clipboard title="Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17767v1.pdf filename=2402.17767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments <b>zero-shot.</b> An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and build upon our system.</p></p class="citation"></blockquote><h3 id=1113--256341-real-time-estimation-of-relative-pose-for-uavs-using-a-dual-channel-feature-association-zhaoying-wang-et-al-2024>(11/13 | 256/341) Real-Time Estimation of Relative Pose for UAVs Using a Dual-Channel Feature Association (Zhaoying Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoying Wang, Wei Dong. (2024)<br><strong>Real-Time Estimation of Relative Pose for UAVs Using a Dual-Channel Feature Association</strong><br><button class=copy-to-clipboard title="Real-Time Estimation of Relative Pose for UAVs Using a Dual-Channel Feature Association" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: SuperGLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17504v1.pdf filename=2402.17504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leveraging multiple cameras on Unmanned Aerial Vehicles (UAVs) to form a variable-baseline stereo camera for collaborative perception is highly promising. The critical steps include high-rate cross-camera feature association and frame-rate relative pose estimation of multiple UAVs. To accelerate the feature association rate to match the frame rate, we propose a dual-channel structure to decouple the time-consuming feature detection and match from the high-rate image stream. The novel design of periodic guidance and fast prediction effectively utilizes each image frame to achieve a frame-rate feature association. Real-world experiments are executed using SuperPoint and <b>SuperGlue</b> on the NVIDIA NX 8G platform with a 30 Hz image stream. Using single-channel SuperPoint and <b>SuperGlue</b> can only achieve 13 Hz feature association. The proposed dual-channel method can improve the rate of feature association from 13 Hz to 30 Hz, supporting the frame-rate requirement. To accommodate the proposed feature association, we develop a Multi-State Constrained Kalman Filter (MSCKF)-based relative pose estimator in the back-end by fusing the local odometry from two UAVs together with the measurements of common features. Experiments show that the dual-channel feature association improves the rate of visual observation and enhances the real-time performance of back-end estimator compared to the existing methods. Video - <a href=https://youtu.be/UBAR1iP0GPk>https://youtu.be/UBAR1iP0GPk</a> Supplementary video - <a href=https://youtu.be/nPq8EpVzJZM>https://youtu.be/nPq8EpVzJZM</a></p></p class="citation"></blockquote><h3 id=1213--257341-using-programmable-drone-in-educational-projects-and-competitions-pavel-petrovič-et-al-2024>(12/13 | 257/341) Using Programmable Drone in Educational Projects and Competitions (Pavel Petrovič et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pavel Petrovič, Peter Verčimák. (2024)<br><strong>Using Programmable Drone in Educational Projects and Competitions</strong><br><button class=copy-to-clipboard title="Using Programmable Drone in Educational Projects and Competitions" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: K-3-2, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17409v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17409v1.pdf filename=2402.17409v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The mainstream of educational robotics platforms orbits the various versions of versatile robotics sets and kits, while interesting outliers add new opportunities and extend the possible learning situations. Examples of such are reconfigurable robots, rolling sphere robots, humanoids, swimming, or underwater robots. Another kind within this category are flying drones. While remotely controlled drones were a very attractive target for hobby model makers for quite a long time already, they were seldom used in educational scenarios as robots that are programmed by children to perform various simple tasks. A milestone was reached with the introduction of the educational drone Tello, which can be programmed even in Scratch, or some general-purpose languages such as Node.js or Python. The programs can even have access to the robot sensors that are used by the underlying layers of the controller. In addition, they have the option to acquire images from the drone camera and perform actions based on processing the frames applying computer vision algorithms. We have been using this drone in an educational robotics competition for three years without camera, and after our students have developed several successful projects that utilized a camera, we prepared a new competition challenge that requires the use of the camera. In the article, we <b>summarize</b> related efforts and our experiences with educational drones, and their use in the student projects and competition.</p></p class="citation"></blockquote><h3 id=1313--258341-swtrack-multiple-hypothesis-sliding-window-3d-multi-object-tracking-sandro-papais-et-al-2024>(13/13 | 258/341) SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking (Sandro Papais et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sandro Papais, Robert Ren, Steven Waslander. (2024)<br><strong>SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking</strong><br><button class=copy-to-clipboard title="SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17892v1.pdf filename=2402.17892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern robotic systems are required to operate in dense dynamic environments, requiring highly accurate real-time track identification and estimation. For 3D multi-object tracking, recent approaches process a single measurement frame recursively with greedy association and are prone to errors in ambiguous association decisions. Our method, Sliding Window Tracker (SWTrack), yields more accurate association and state estimation by batch processing many frames of sensor data while being capable of running online in real-time. The most probable track associations are identified by evaluating all possible track hypotheses across the temporal sliding window. A novel <b>graph</b> optimization approach is formulated to solve the multidimensional assignment problem with lifted <b>graph</b> edges introduced to account for missed detections and <b>graph</b> sparsity enforced to retain real-time efficiency. We evaluate our SWTrack implementation$^{2}$ on the NuScenes autonomous driving dataset to demonstrate improved tracking performance.</p></p class="citation"></blockquote><h2 id=eessiv-6>eess.IV (6)</h2><h3 id=16--259341-medcontext-learning-contextual-cues-for-efficient-volumetric-medical-segmentation-hanan-gani-et-al-2024>(1/6 | 259/341) MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation (Hanan Gani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanan Gani, Muzammal Naseer, Fahad Khan, Salman Khan. (2024)<br><strong>MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation</strong><br><button class=copy-to-clipboard title="MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Few-shot, Self-supervised Learning, Supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17725v1.pdf filename=2402.17725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Volumetric medical segmentation is a critical component of 3D medical image analysis that delineates different semantic regions. Deep neural networks have significantly improved volumetric medical segmentation, but they generally require large-scale annotated data to achieve better performance, which can be expensive and prohibitive to obtain. To address this limitation, existing works typically perform <b>transfer</b> <b>learning</b> or design dedicated pretraining-finetuning stages to learn representative features. However, the mismatch between the source and target domain can make it challenging to learn optimal representation for volumetric data, while the multi-stage training demands higher compute as well as careful selection of stage-specific design choices. In contrast, we propose a universal training framework called MedContext that is architecture-agnostic and can be incorporated into any existing training framework for 3D medical segmentation. Our approach effectively learns self <b>supervised</b> contextual cues jointly with the <b>supervised</b> voxel segmentation task without requiring large-scale annotated volumetric medical data or dedicated pretraining-finetuning stages. The proposed approach induces contextual knowledge in the network by learning to reconstruct the missing organ or parts of an organ in the output segmentation space. The effectiveness of MedContext is validated across multiple 3D medical datasets and four state-of-the-art model architectures. Our approach demonstrates consistent gains in segmentation performance across datasets and different architectures even in <b>few-shot</b> data scenarios. Our code and pretrained models are available at <a href=https://github.com/hananshafi/MedContext>https://github.com/hananshafi/MedContext</a></p></p class="citation"></blockquote><h3 id=26--260341-how-we-won-brats-2023-adult-glioma-challenge-just-faking-it-enhanced-synthetic-data-augmentation-and-model-ensemble-for-brain-tumour-segmentation-andré-ferreira-et-al-2024>(2/6 | 260/341) How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation (André Ferreira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>André Ferreira, Naida Solak, Jianning Li, Philipp Dammann, Jens Kleesiek, Victor Alves, Jan Egger. (2024)<br><strong>How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation</strong><br><button class=copy-to-clipboard title="How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Convolution, Data Augmentation, Generative Adversarial Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17317v1.pdf filename=2402.17317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning is the state-of-the-art technology for segmenting brain tumours. However, this requires a lot of high-quality <b>data,</b> <b>which</b> is difficult to obtain, especially in the medical field. Therefore, our solutions address this problem by using unconventional mechanisms for <b>data</b> <b>augmentation.</b> <b>Generative</b> <b>adversarial</b> <b>networks</b> and registration are used to massively increase the amount of available samples for training three different deep learning models for brain tumour segmentation, the first task of the BraTS2023 challenge. The first model is the standard nnU-Net, the second is the Swin UNETR and the third is the winning solution of the BraTS 2021 Challenge. The entire pipeline is built on the nnU-Net implementation, except for the generation of the synthetic <b>data.</b> <b>The</b> use of <b>convolutional</b> algorithms and <b>transformers</b> is able to fill each other&rsquo;s knowledge gaps. Using the new metric, our best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95 14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the validation set.</p></p class="citation"></blockquote><h3 id=36--261341-sdr-former-a-siamese-dual-resolution-transformer-for-liver-lesion-classification-using-3d-multi-phase-imaging-meng-lou-et-al-2024>(3/6 | 261/341) SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging (Meng Lou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Lou, Hanning Ying, Xiaoqing Liu, Hong-Yu Zhou, Yuqing Zhang, Yizhou Yu. (2024)<br><strong>SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging</strong><br><button class=copy-to-clipboard title="SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17246v1.pdf filename=2402.17246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated classification of liver lesions in multi-phase CT and MR scans is of clinical significance but challenging. This study proposes a novel Siamese Dual-Resolution <b>Transformer</b> (SDR-Former) framework, specifically designed for liver lesion classification in 3D multi-phase CT and MR imaging with varying phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural Network (SNN) to process multi-phase imaging inputs, possessing robust feature representations while maintaining computational efficiency. The weight-sharing feature of the SNN is further enriched by a hybrid Dual-Resolution <b>Transformer</b> (DR-Former), comprising a 3D <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> and a tailored 3D <b>Transformer</b> for processing high- and low-resolution images, respectively. This hybrid sub-architecture excels in capturing detailed local features and understanding global contextual information, thereby, boosting the SNN&rsquo;s feature extraction capabilities. Additionally, a novel Adaptive Phase Selection Module (APSM) is introduced, promoting phase-specific intercommunication and dynamically adjusting each phase&rsquo;s influence on the diagnostic outcome. The proposed SDR-Former framework has been validated through comprehensive experiments on two clinical datasets: a three-phase CT dataset and an eight-phase MR dataset. The experimental results affirm the efficacy of the proposed framework. To support the scientific community, we are releasing our extensive multi-phase MR dataset for liver lesion analysis to the public. This pioneering dataset, being the first publicly available multi-phase MR dataset in this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is accessible at:https://bit.ly/3IyYlgN.</p></p class="citation"></blockquote><h3 id=46--262341-denoising-diffusion-models-for-inpainting-of-healthy-brain-tissue-alicia-durrer-et-al-2024>(4/6 | 262/341) Denoising Diffusion Models for Inpainting of Healthy Brain Tissue (Alicia Durrer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alicia Durrer, Philippe C. Cattin, Julia Wolleb. (2024)<br><strong>Denoising Diffusion Models for Inpainting of Healthy Brain Tissue</strong><br><button class=copy-to-clipboard title="Denoising Diffusion Models for Inpainting of Healthy Brain Tissue" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17307v1.pdf filename=2402.17307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper is a contribution to the &ldquo;BraTS 2023 Local Synthesis of Healthy Brain Tissue via Inpainting Challenge&rdquo;. The task of this challenge is to transform tumor tissue into healthy tissue in brain magnetic resonance (MR) images. This idea originates from the problem that MR images can be evaluated using automatic processing tools, however, many of these tools are optimized for the analysis of healthy tissue. By solving the given inpainting task, we enable the automatic analysis of images featuring lesions, and further downstream tasks. Our approach builds on denoising <b>diffusion</b> <b>probabilistic</b> <b>models.</b> We use a 2D model that is trained using slices in which healthy tissue was cropped out and is learned to be inpainted again. This allows us to use the ground truth healthy tissue during training. In the sampling stage, we replace the slices containing diseased tissue in the original 3D volume with the slices containing the healthy tissue inpainting. With our approach, we achieve comparable results to the competing methods. On the validation set our model achieves a mean SSIM of 0.7804, a PSNR of 20.3525 and a MSE of 0.0113. In future we plan to extend our 2D model to a 3D model, allowing to inpaint the region of interest as a whole without losing context information of neighboring slices.</p></p class="citation"></blockquote><h3 id=56--263341-adapting-learned-image-codecs-to-screen-content-via-adjustable-transformations-h-burak-dogaroglu-et-al-2024>(5/6 | 263/341) Adapting Learned Image Codecs to Screen Content via Adjustable Transformations (H. Burak Dogaroglu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>H. Burak Dogaroglu, A. Burakhan Koyuncu, Atanas Boev, Elena Alshina, Eckehard Steinbach. (2024)<br><strong>Adapting Learned Image Codecs to Screen Content via Adjustable Transformations</strong><br><button class=copy-to-clipboard title="Adapting Learned Image Codecs to Screen Content via Adjustable Transformations" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17544v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17544v1.pdf filename=2402.17544v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As learned image codecs (LICs) become more prevalent, their low coding efficiency for <b>out-of-distribution</b> data becomes a bottleneck for some applications. To improve the performance of LICs for screen content (SC) images without breaking backwards compatibility, we propose to introduce parameterized and invertible linear transformations into the coding pipeline without changing the underlying baseline codec&rsquo;s operation flow. We design two neural networks to act as prefilters and postfilters in our setup to increase the coding efficiency and help with the recovery from coding artifacts. Our end-to-end trained solution achieves up to 10% bitrate savings on SC compression compared to the baseline LICs while introducing only 1% extra parameters.</p></p class="citation"></blockquote><h3 id=66--264341-pe-mvcnet-multi-view-and-cross-modal-fusion-network-for-pulmonary-embolism-prediction-zhaoxin-guo-et-al-2024>(6/6 | 264/341) PE-MVCNet: Multi-view and Cross-modal Fusion Network for Pulmonary Embolism Prediction (Zhaoxin Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoxin Guo, Zhipeng Wang, Ruiquan Ge, Jianxun Yu, Feiwei Qin, Yuan Tian, Yuqing Peng, Yonghong Li, Changmiao Wang. (2024)<br><strong>PE-MVCNet: Multi-view and Cross-modal Fusion Network for Pulmonary Embolism Prediction</strong><br><button class=copy-to-clipboard title="PE-MVCNet: Multi-view and Cross-modal Fusion Network for Pulmonary Embolism Prediction" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17187v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17187v2.pdf filename=2402.17187v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The early detection of a pulmonary embolism (PE) is critical for enhancing patient survival rates. Both image-based and non-image-based features are of utmost importance in medical classification tasks. In a clinical setting, physicians tend to rely on the contextual information provided by Electronic Medical Records (EMR) to interpret medical imaging. However, very few models effectively integrate clinical information with imaging data. To address this shortcoming, we suggest a <b>multimodal</b> fusion methodology, termed PE-MVCNet, which capitalizes on Computed Tomography Pulmonary Angiography imaging and EMR data. This method comprises the Image-only module with an integrated multi-view block, the EMR-only module, and the Cross-modal Attention Fusion (CMAF) module. These modules cooperate to extract comprehensive features that subsequently generate predictions for PE. We conducted experiments using the publicly accessible Stanford University Medical Center dataset, achieving an AUROC of 94.1%, an accuracy rate of 90.2%, and an F1 score of 90.6%. Our proposed model outperforms existing methodologies, corroborating that our <b>multimodal</b> fusion model excels compared to models that use a single data modality. Our source code is available at <a href=https://github.com/LeavingStarW/PE-MVCNET>https://github.com/LeavingStarW/PE-MVCNET</a>.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--265341-a-piece-of-theatre-investigating-how-teachers-design-llm-chatbots-to-assist-adolescent-cyberbullying-education-michael-a-hedderich-et-al-2024>(1/5 | 265/341) A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education (Michael A. Hedderich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael A. Hedderich, Natalie N. Bazarova, Wenting Zou, Ryun Shim, Xinda Ma, Qian Yang. (2024)<br><strong>A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education</strong><br><button class=copy-to-clipboard title="A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17456v1.pdf filename=2402.17456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cyberbullying harms teenagers&rsquo; mental health, and teaching them upstanding intervention is crucial. Wizard-of-Oz studies show <b>chatbots</b> can scale up personalized and interactive cyberbullying education, but implementing such <b>chatbots</b> is a challenging and delicate task. We created a no-code <b>chatbot</b> design tool for K-12 teachers. Using <b>large</b> <b>language</b> <b>models</b> and <b>prompt</b> chaining, our tool allows teachers to prototype bespoke dialogue flows and <b>chatbot</b> utterances. In offering this tool, we explore teachers&rsquo; distinctive needs when designing <b>chatbots</b> to assist their teaching, and how <b>chatbot</b> design tools might better support them. Our findings reveal that teachers welcome the tool enthusiastically. Moreover, they see themselves as playwrights guiding both the students&rsquo; and the <b>chatbot&rsquo;s</b> behaviors, while allowing for some improvisation. Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a safe environment. We discuss the design opportunities <b>LLM-Chains</b> offer for empowering teachers and the research opportunities this work opens up.</p></p class="citation"></blockquote><h3 id=25--266341-neuralsi-neural-design-of-semantic-interaction-for-interactive-deep-learning-yali-bian-et-al-2024>(2/5 | 266/341) NeuralSI: Neural Design of Semantic Interaction for Interactive Deep Learning (Yali Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yali Bian, Rebecca Faust, Chris North. (2024)<br><strong>NeuralSI: Neural Design of Semantic Interaction for Interactive Deep Learning</strong><br><button class=copy-to-clipboard title="NeuralSI: Neural Design of Semantic Interaction for Interactive Deep Learning" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17178v1.pdf filename=2402.17178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An increasing number of studies have utilized interactive deep learning as the analytic model of visual analytics systems for complex sensemaking tasks. In these systems, traditional interactive dimensionality reduction (DR) models are commonly utilized to build a bi-directional bridge between high-dimensional deep learning representations and low-dimensional visualizations. While these systems better capture analysts&rsquo; intents in the context of <b>human-in-the-loop</b> interactive deep learning, traditional DR cannot support several desired properties for visual analytics, including out-of-sample extensions, stability, and real-time inference. To avoid this issue, we propose the neural design framework of semantic interaction for interactive deep learning. In our framework, we replace the traditional DR with a neural projection network and append it to the deep learning model as the task-specific output layer. Therefore, the analytic model (deep learning) and visualization method (interactive DR) form one integrated end-to-end trainable deep neural network. In order to understand the performance of the neural design in comparison to the state-of-the-art, we systematically performed two complementary studies, a human-centered qualitative case study and an algorithm-centered <b>simulation-based</b> quantitative experiment. The results of these studies indicate that the neural design can give semantic interaction systems substantial advantages while still keeping comparable inference ability compared to the state-of-the-art model.</p></p class="citation"></blockquote><h3 id=35--267341-surgment-segmentation-enabled-semantic-search-and-creation-of-visual-question-and-feedback-to-support-video-based-surgery-learning-jingying-wang-et-al-2024>(3/5 | 267/341) Surgment: Segmentation-enabled Semantic Search and Creation of Visual Question and Feedback to Support Video-Based Surgery Learning (Jingying Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingying Wang, Haoran Tang, Taylor Kantor, Tandis Soltani, Vitaliy Popov, Xu Wang. (2024)<br><strong>Surgment: Segmentation-enabled Semantic Search and Creation of Visual Question and Feedback to Support Video-Based Surgery Learning</strong><br><button class=copy-to-clipboard title="Surgment: Segmentation-enabled Semantic Search and Creation of Visual Question and Feedback to Support Video-Based Surgery Learning" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CV, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17903v1.pdf filename=2402.17903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Videos are prominent learning materials to prepare surgical trainees before they enter the operating room (OR). In this work, we explore techniques to enrich the video-based surgery learning experience. We propose Surgment, a system that helps expert surgeons create exercises with feedback based on surgery recordings. Surgment is powered by a <b>few-shot-learning-based</b> <b>pipeline</b> (SegGPT+SAM) to segment surgery scenes, achieving an accuracy of 92%. The segmentation pipeline enables functionalities to create visual questions and feedback desired by surgeons from a formative study. Surgment enables surgeons to 1) retrieve frames of interest through sketches, and 2) design exercises that target specific anatomical components and offer visual feedback. In an evaluation study with 11 surgeons, participants applauded the search-by-sketch approach for identifying frames of interest and found the resulting image-based questions and feedback to be of high educational value.</p></p class="citation"></blockquote><h3 id=45--268341-content-centric-prototyping-of-generative-ai-applications-emerging-approaches-and-challenges-in-collaborative-software-teams-hari-subramonyam-et-al-2024>(4/5 | 268/341) Content-Centric Prototyping of Generative AI Applications: Emerging Approaches and Challenges in Collaborative Software Teams (Hari Subramonyam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hari Subramonyam, Divy Thakkar, Jürgen Dieber, Anoop Sinha. (2024)<br><strong>Content-Centric Prototyping of Generative AI Applications: Emerging Approaches and Challenges in Collaborative Software Teams</strong><br><button class=copy-to-clipboard title="Content-Centric Prototyping of Generative AI Applications: Emerging Approaches and Challenges in Collaborative Software Teams" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-SE, cs.HC<br>Keyword Score: 20<br>Keywords: Generative AI, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17721v1.pdf filename=2402.17721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> models are increasingly powering software applications, offering the capability to produce expressive content across varied contexts. However, unlike previous iterations of human-AI design, the emerging design process for <b>generative</b> <b>capabilities</b> primarily hinges on <b>prompt</b> engineering strategies. Given this fundamental shift in approach, our work aims to understand how collaborative software teams set up and apply design guidelines and values, iteratively prototype <b>prompts,</b> and evaluate <b>prompts</b> to achieve desired outcomes. We conducted design studies with 39 industry professionals, including designers, software engineers, and product managers. Our findings reveal a content-centric prototyping approach in which teams begin with the content they want to generate, then identify specific attributes, constraints, and values, and explore methods to give users the ability to influence and interact with those attributes. Based on associated challenges, such as the lack of model interpretability and overfitting the design to examples, we outline considerations for <b>generative</b> <b>AI</b> prototyping.</p></p class="citation"></blockquote><h3 id=55--269341-mitigating-barriers-to-public-social-interaction-with-meronymous-communication-nouran-soliman-et-al-2024>(5/5 | 269/341) Mitigating Barriers to Public Social Interaction with Meronymous Communication (Nouran Soliman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nouran Soliman, Hyeonsu B Kang, Matthew Latzke, Jonathan Bragg, Joseph Chee Chang, Amy X. Zhang, David R Karger. (2024)<br><strong>Mitigating Barriers to Public Social Interaction with Meronymous Communication</strong><br><button class=copy-to-clipboard title="Mitigating Barriers to Public Social Interaction with Meronymous Communication" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17847v1.pdf filename=2402.17847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In communities with social hierarchies, fear of judgment can discourage communication. While anonymity may alleviate some social pressure, fully anonymous spaces enable toxic behavior and hide the social context that motivates people to participate and helps them tailor their communication. We explore a design space of meronymous communication, where people can reveal carefully chosen aspects of their identity and also leverage trusted endorsers to gain credibility. We implemented these ideas in a system for scholars to meronymously seek and receive paper <b>recommendations</b> on Twitter and Mastodon. A formative study with 20 scholars confirmed that scholars see benefits to participating but are deterred due to social anxiety. From a month-long public deployment, we found that with meronymity, junior scholars could comfortably ask ``newbie&rsquo;&rsquo; questions and get responses from senior scholars who they normally found intimidating. Responses were also tailored to the aspects about themselves that junior scholars chose to reveal.</p></p class="citation"></blockquote><h2 id=physicsmed-ph-1>physics.med-ph (1)</h2><h3 id=11--270341-segment-anything-model-for-head-and-neck-tumor-segmentation-with-ct-pet-and-mri-multi-modality-images-jintao-ren-et-al-2024>(1/1 | 270/341) Segment anything model for head and neck tumor segmentation with CT, PET and MRI multi-modality images (Jintao Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jintao Ren, Mathis Rasmussen, Jasper Nijkamp, Jesper Grau Eriksen, Stine Korreman. (2024)<br><strong>Segment anything model for head and neck tumor segmentation with CT, PET and MRI multi-modality images</strong><br><button class=copy-to-clipboard title="Segment anything model for head and neck tumor segmentation with CT, PET and MRI multi-modality images" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.med-ph<br>Categories: cs-CV, physics-med-ph, physics.med-ph<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17454v1.pdf filename=2402.17454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning presents novel opportunities for the auto-segmentation of gross tumor volume (GTV) in head and neck cancer (HNC), yet fully automatic methods usually necessitate significant manual refinement. This study investigates the Segment Anything Model (SAM), recognized for requiring minimal human <b>prompting</b> and its <b>zero-shot</b> generalization ability across natural images. We specifically examine MedSAM, a version of SAM <b>fine-tuned</b> with large-scale public medical images. Despite its progress, the integration of multi-modality images (CT, PET, MRI) for effective GTV delineation remains a challenge. Focusing on SAM&rsquo;s application in HNC GTV segmentation, we assess its performance in both <b>zero-shot</b> and <b>fine-tuned</b> scenarios using single (CT-only) and fused multi-modality images. Our study demonstrates that <b>fine-tuning</b> SAM significantly enhances its segmentation accuracy, building upon the already effective <b>zero-shot</b> results achieved with bounding box <b>prompts.</b> These findings open a promising avenue for semi-automatic HNC GTV segmentation.</p></p class="citation"></blockquote><h2 id=csni-5>cs.NI (5)</h2><h3 id=15--271341-outdoor-environment-reconstruction-with-deep-learning-on-radio-propagation-paths-hrant-khachatrian-et-al-2024>(1/5 | 271/341) Outdoor Environment Reconstruction with Deep Learning on Radio Propagation Paths (Hrant Khachatrian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hrant Khachatrian, Rafayel Mkrtchyan, Theofanis P. Raptis. (2024)<br><strong>Outdoor Environment Reconstruction with Deep Learning on Radio Propagation Paths</strong><br><button class=copy-to-clipboard title="Outdoor Environment Reconstruction with Deep Learning on Radio Propagation Paths" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-LG, cs-NI, cs.NI, eess-SP<br>Keyword Score: 40<br>Keywords: Vision Transformer, Convolution, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17336v1.pdf filename=2402.17336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional methods for outdoor environment reconstruction rely predominantly on <b>vision-based</b> <b>techniques</b> like photogrammetry and LiDAR, facing limitations such as constrained coverage, susceptibility to environmental conditions, and high computational and energy demands. These challenges are particularly pronounced in applications like augmented reality navigation, especially when integrated with wearable devices featuring constrained computational resources and energy budgets. In response, this paper proposes a novel approach harnessing ambient wireless signals for outdoor environment reconstruction. By analyzing radio frequency (RF) data, the paper aims to deduce the environmental characteristics and digitally reconstruct the outdoor surroundings. Investigating the efficacy of selected deep learning (DL) techniques on the synthetic RF dataset WAIR-D, the study endeavors to address the research gap in this domain. Two DL-driven approaches are evaluated <b>(convolutional</b> U-Net and CLIP+ based on <b>vision</b> <b>transformers),</b> with performance assessed using metrics like intersection-over-union (IoU), Hausdorff distance, and Chamfer distance. The results demonstrate promising performance of the RF-based reconstruction method, paving the way towards lightweight and scalable reconstruction solutions.</p></p class="citation"></blockquote><h3 id=25--272341-emergency-caching-coded-caching-based-reliable-map-transmission-in-emergency-networks-zeyu-tian-et-al-2024>(2/5 | 272/341) Emergency Caching: Coded Caching-based Reliable Map Transmission in Emergency Networks (Zeyu Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Tian, Lianming Xu, Liang Li, Li Wang, Aiguo Fei. (2024)<br><strong>Emergency Caching: Coded Caching-based Reliable Map Transmission in Emergency Networks</strong><br><button class=copy-to-clipboard title="Emergency Caching: Coded Caching-based Reliable Map Transmission in Emergency Networks" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-NI, cs.NI, eess-SP<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17550v1.pdf filename=2402.17550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many rescue missions demand effective perception and real-time decision making, which highly rely on effective data collection and processing. In this study, we propose a three-layer architecture of emergency caching networks focusing on data collection and reliable transmission, by leveraging efficient perception and edge caching technologies. Based on this architecture, we propose a disaster map collection framework that integrates coded caching technologies. Our framework strategically caches coded fragments of maps across unmanned aerial vehicles (UAVs), fostering collaborative uploading for augmented transmission reliability. Additionally, we establish a comprehensive probability model to assess the effective recovery area of disaster maps. Towards the goal of utility maximization, we propose a deep <b>reinforcement</b> <b>learning</b> (DRL) based algorithm that jointly makes decisions about cooperative UAVs selection, bandwidth allocation and coded caching parameter adjustment, accommodating the real-time map updates in a dynamic disaster situation. Our proposed scheme is more effective than the non-coding caching scheme, as validated by <b>simulation.</b></p></p class="citation"></blockquote><h3 id=35--273341-reducing-unnecessary-alerts-in-pedestrian-protection-systems-based-on-p2v-communications-ignacio-soto-et-al-2024>(3/5 | 273/341) Reducing Unnecessary Alerts in Pedestrian Protection Systems Based on P2V Communications (Ignacio Soto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ignacio Soto, Felipe Jimenez, Maria Calderon, Jose E. Naranjo, Jose J. Anaya. (2024)<br><strong>Reducing Unnecessary Alerts in Pedestrian Protection Systems Based on P2V Communications</strong><br><button class=copy-to-clipboard title="Reducing Unnecessary Alerts in Pedestrian Protection Systems Based on P2V Communications" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17763v1.pdf filename=2402.17763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There are different proposals in the literature on how to protect pedestrians using warning systems to alert drivers of their presence. They can be based on onboard perception systems or wireless communications. The evaluation of these systems has been focused on testing their ability to detect pedestrians. A problem that has received much less attention is the possibility of generating too many alerts in the warning systems. In this paper, we propose and analyze four different algorithms to take the decision on generating alerts in a warning system that is based on direct wireless communications between vehicles and pedestrians. With the algorithms, we explore different strategies to reduce unnecessary alerts. The feasibility of the implementation of the algorithms was evaluated with a deployment using real equipment, and tests were carried out to verify their behavior in real scenarios. The ability of each algorithm to reduce unnecessary alerts was evaluated with realistic <b>simulations</b> in an urban scenario, using a traffic simulator with vehicular and pedestrian flows. The results show the importance of tackling the problem of driver overload in warning systems, and that it is not straightforward to predict the load of alerts generated by an algorithm in a large-scale deployment, in which there are multiple interactions between vehicles and pedestrians.</p></p class="citation"></blockquote><h3 id=45--274341-wykorzystanie-rekonfigurowalnych-matryc-antenowych-wraz-z-informacją-kontekstową-łukasz-kułacz-et-al-2024>(4/5 | 274/341) Wykorzystanie rekonfigurowalnych matryc antenowych wraz z informacją kontekstową (Łukasz Kułacz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Łukasz Kułacz, Adrian Kliks. (2024)<br><strong>Wykorzystanie rekonfigurowalnych matryc antenowych wraz z informacją kontekstową</strong><br><button class=copy-to-clipboard title="Wykorzystanie rekonfigurowalnych matryc antenowych wraz z informacją kontekstową" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17436v1.pdf filename=2402.17436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surfaces can be successfully used to control the radio environment. Simple control of the reflection angle of the signal from the surface allows maximization or minimization of the received power in specific places. The paper presents <b>simulations</b> where it is possible to receive a signal in a place where it was not possible, to detect the occupancy of the spectrum in a place where the sensor was unable to make correct detection or to minimize interference in a specific receiver.</p></p class="citation"></blockquote><h3 id=55--275341-a-survey-of-network-protocol-fuzzing-model-techniques-and-directions-shihao-jiang-et-al-2024>(5/5 | 275/341) A Survey of Network Protocol Fuzzing: Model, Techniques and Directions (Shihao Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shihao Jiang, Yu Zhang, Junqiang Li, Hongfang Yu, Long Luo, Gang Sun. (2024)<br><strong>A Survey of Network Protocol Fuzzing: Model, Techniques and Directions</strong><br><button class=copy-to-clipboard title="A Survey of Network Protocol Fuzzing: Model, Techniques and Directions" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17394v1.pdf filename=2402.17394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As one of the most successful and effective software testing techniques in recent years, fuzz testing has uncovered numerous bugs and vulnerabilities in modern software, including network protocol software. In contrast to other fuzzing targets, network protocol software exhibits its distinct characteristics and challenges, introducing a plethora of research questions that need to be addressed in the design and implementation of network protocol fuzzers. While some research work has evaluated and systematized the knowledge of general fuzzing techniques at a high level, there is a lack of similar analysis and <b>summarization</b> for fuzzing research specific to network protocols. This paper offers a comprehensive exposition of network protocol software&rsquo;s fuzzing-related features and conducts a systematic review of some representative advancements in network protocol fuzzing since its inception. We <b>summarize</b> state-of-the-art strategies and solutions in various aspects, propose a unified protocol fuzzing process model, and introduce the techniques involved in each stage of the model. At the same time, this paper also <b>summarizes</b> the promising research directions in the landscape of protocol fuzzing to foster exploration within the community for more efficient and intelligent modern network protocol fuzzing techniques.</p></p class="citation"></blockquote><h2 id=eesssy-7>eess.SY (7)</h2><h3 id=17--276341-reinforcement-learning-based-robust-voltvar-control-in-active-distribution-networks-with-imprecisely-known-delay-hong-cheng-et-al-2024>(1/7 | 276/341) Reinforcement Learning Based Robust Volt/Var Control in Active Distribution Networks With Imprecisely Known Delay (Hong Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hong Cheng, Huan Luo, Zhi Liu, Wei Sun, Weitao Li, Qiyue Li. (2024)<br><strong>Reinforcement Learning Based Robust Volt/Var Control in Active Distribution Networks With Imprecisely Known Delay</strong><br><button class=copy-to-clipboard title="Reinforcement Learning Based Robust Volt/Var Control in Active Distribution Networks With Imprecisely Known Delay" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Markov Decision Process, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17268v1.pdf filename=2402.17268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Active distribution networks (ADNs) incorporating massive photovoltaic (PV) devices encounter challenges of rapid voltage fluctuations and potential violations. Due to the fluctuation and intermittency of PV generation, the state gap, arising from time-inconsistent states and exacerbated by imprecisely known system delays, significantly impacts the accuracy of voltage control. This paper addresses this challenge by introducing a framework for delay adaptive Volt/Var control (VVC) in the presence of imprecisely known system delays to regulate the reactive power of PV inverters. The proposed approach formulates the voltage control, based on predicted system operation states, as a robust VVC problem. It employs sample selection from the state prediction interval to promptly identify the worst-performing system operation state. Furthermore, we leverage the decentralized partially observable <b>Markov</b> <b>decision</b> <b>process</b> (Dec-POMDP) to reformulate the robust VVC problem. We design Multiple Policy Networks and employ Multiple Policy Networks and Reward Shaping-based Multi-agent Twin Delayed Deep Deterministic Policy Gradient (MPNRS-MATD3) algorithm to efficiently address and solve the Dec-POMDP model-based problem. <b>Simulation</b> results show the delay adaption characteristic of our proposed framework, and the MPNRS-MATD3 outperforms other multi-agent <b>reinforcement</b> <b>learning</b> algorithms in robust voltage control.</p></p class="citation"></blockquote><h3 id=27--277341-distributed-estimation-and-control-for-lti-systems-under-finite-time-agreement-camilla-fioravanti-et-al-2024>(2/7 | 277/341) Distributed Estimation and Control for LTI Systems under Finite-Time Agreement (Camilla Fioravanti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Camilla Fioravanti, Evagoras Makridis, Gabriele Oliva, Maria Vrakopoulou, Themistoklis Charalambous. (2024)<br><strong>Distributed Estimation and Control for LTI Systems under Finite-Time Agreement</strong><br><button class=copy-to-clipboard title="Distributed Estimation and Control for LTI Systems under Finite-Time Agreement" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Discrete Time, Discrete Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17466v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17466v2.pdf filename=2402.17466v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers a strongly connected network of agents, each capable of partially observing and controlling a <b>discrete-time</b> <b>linear</b> time-invariant (LTI) system that is jointly observable and controllable. Additionally, agents collaborate to achieve a shared estimated state, computed as the average of their local state estimates. Recent studies suggest that increasing the number of average consensus steps between state estimation updates allows agents to choose from a wider range of state feedback controllers, thereby potentially enhancing control performance. However, such approaches require that agents know the input matrices of all other nodes, and the selection of control gains is, in general, centralized. Motivated by the limitations of such approaches, we propose a new technique where: (i) estimation and control gain design is fully distributed and finite-time, and (ii) agent coordination involves a finite-time exact average consensus algorithm, allowing arbitrary selection of estimation convergence rate despite the estimator&rsquo;s distributed nature. We verify our methodology&rsquo;s effectiveness using illustrative numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=37--278341-impact-of-computation-in-integral-reinforcement-learning-for-continuous-time-control-wenhan-cao-et-al-2024>(3/7 | 278/341) Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control (Wenhan Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhan Cao, Wei Pan. (2024)<br><strong>Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control</strong><br><button class=copy-to-clipboard title="Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17375v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17375v1.pdf filename=2402.17375v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integral <b>reinforcement</b> <b>learning</b> (IntRL) demands the precise computation of the utility function&rsquo;s integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in <b>discrete</b> <b>time.</b> Our research reveals a critical yet underexplored phenomenon: the choice of the computational method &ndash; in this case, the quadrature rule &ndash; can significantly impact control performance. This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration&rsquo;s convergence behavior, which in turn affects the learned controller. To elucidate how computation impacts control, we draw a parallel between IntRL&rsquo;s policy iteration and Newton&rsquo;s method applied to the Hamilton-Jacobi-Bellman equation. In this light, computational error in PEV manifests as an extra error term in each iteration of Newton&rsquo;s method, with its upper bound proportional to the computational error. Further, we demonstrate that when the utility function resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is achievable by employing Bayesian quadrature with the RKHS-inducing kernel function. We prove that the local convergence rates for IntRL using the trapezoidal rule and Bayesian quadrature with a Mat'ern kernel to be $O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples and $b$ is the Mat'ern kernel&rsquo;s smoothness parameter. These theoretical findings are finally validated by two canonical control tasks.</p></p class="citation"></blockquote><h3 id=47--279341-communication-constrained-stl-task-decomposition-through-convex-optimization-gregorio-marchesini-et-al-2024>(4/7 | 279/341) Communication-Constrained STL Task Decomposition through Convex Optimization (Gregorio Marchesini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gregorio Marchesini, Siyuan Liu, Lars Lindemann, Dimos V. Dimarogonas. (2024)<br><strong>Communication-Constrained STL Task Decomposition through Convex Optimization</strong><br><button class=copy-to-clipboard title="Communication-Constrained STL Task Decomposition through Convex Optimization" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17585v1.pdf filename=2402.17585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose a method to decompose signal temporal logic (STL) tasks for multi-agent systems subject to constraints imposed by the communication <b>graph.</b> Specifically, we propose to decompose tasks defined over multiple agents which require multi-hop communication, by a set of sub-tasks defined over the states of agents with 1-hop distance over the communication <b>graph.</b> To this end, we parameterize the predicates of the tasks to be decomposed as suitable hyper-rectangles. Then, we show that by solving a constrained convex optimization, optimal parameters maximising the volume of the predicate&rsquo;s super-level sets can be computed for the decomposed tasks. In addition, we provide a formal definition of conflicting conjunctions of tasks for the considered STL fragment and a formal procedure to exclude such conjunctions from the solution set of possible decompositions. The proposed approach is demonstrated through <b>simulations.</b></p></p class="citation"></blockquote><h3 id=57--280341-model-free-deep-deterministic-policy-gradient-controller-for-setpoint-tracking-of-non-minimum-phase-systems-fatemeh-tavakkoli-et-al-2024>(5/7 | 280/341) Model Free Deep Deterministic Policy Gradient Controller for Setpoint Tracking of Non-minimum Phase Systems (Fatemeh Tavakkoli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatemeh Tavakkoli, Pouria Sarhadi, Benoit Clement, Wasif Naeem. (2024)<br><strong>Model Free Deep Deterministic Policy Gradient Controller for Setpoint Tracking of Non-minimum Phase Systems</strong><br><button class=copy-to-clipboard title="Model Free Deep Deterministic Policy Gradient Controller for Setpoint Tracking of Non-minimum Phase Systems" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17703v1.pdf filename=2402.17703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>Reinforcement</b> <b>Learning</b> (DRL) techniques have received significant attention in control and decision-making algorithms. Most applications involve complex decision-making systems, justified by the algorithms&rsquo; computational power and cost. While model-based versions are emerging, model-free DRL approaches are intriguing for their independence from models, yet they remain relatively less explored in terms of performance, particularly in applied control. This study conducts a thorough performance analysis comparing the data-driven DRL paradigm with a classical state feedback controller, both designed based on the same cost (reward) function of the linear quadratic regulator (LQR) problem. Twelve additional performance criteria are introduced to assess the controllers&rsquo; performance, independent of the LQR problem for which they are designed. Two Deep Deterministic Policy Gradient (DDPG)-based controllers are developed, leveraging DDPG&rsquo;s widespread reputation. These controllers are aimed at addressing a challenging setpoint tracking problem in a Non-Minimum Phase (NMP) system. The performance and robustness of the controllers are assessed in the presence of operational challenges, including disturbance, noise, initial conditions, and model uncertainties. The findings suggest that the DDPG controller demonstrates promising behavior under rigorous test conditions. Nevertheless, further improvements are necessary for the DDPG controller to outperform classical methods in all criteria. While DRL algorithms may excel in complex environments owing to the flexibility in the reward function definition, this paper offers practical insights and a comparison framework specifically designed to evaluate these algorithms within the context of control engineering.</p></p class="citation"></blockquote><h3 id=67--281341-converse-barrier-certificates-for-finite-time-safety-verification-of-continuous-time-perturbed-deterministic-systems-yonghan-li-et-al-2024>(6/7 | 281/341) Converse Barrier Certificates for Finite-time Safety Verification of Continuous-time Perturbed Deterministic Systems (Yonghan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yonghan Li, Chenyu Wu, Taoran Wu, Shijie Wang, Bai Xue. (2024)<br><strong>Converse Barrier Certificates for Finite-time Safety Verification of Continuous-time Perturbed Deterministic Systems</strong><br><button class=copy-to-clipboard title="Converse Barrier Certificates for Finite-time Safety Verification of Continuous-time Perturbed Deterministic Systems" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17167v1.pdf filename=2402.17167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the problem of verifying the finite-time safety of <b>continuous-time</b> <b>perturbed</b> deterministic systems represented by ordinary differential equations in the presence of measurable disturbances. Given a finite time horizon, if the system is safe, it, starting from a compact initial set, will remain within an open and bounded safe region throughout the specified time horizon, regardless of the disturbances. The main contribution of this work is to uncover that there exists a time-dependent barrier certificate if and only if the system is safe. This barrier certificate satisfies the following conditions: negativity over the initial set at the initial time instant, non-negativity over the boundary of the safe set, and non-increasing behavior along the system dynamics over the specified finite time horizon. The existence problem is explored using a Hamilton-Jacobi differential equation, which has a unique Lipschitz viscosity solution.</p></p class="citation"></blockquote><h3 id=77--282341-exergetic-port-hamiltonian-systems-modeling-language-markus-lohmayer-et-al-2024>(7/7 | 282/341) Exergetic Port-Hamiltonian Systems Modeling Language (Markus Lohmayer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Markus Lohmayer, Owen Lynch, Sigrid Leyendecker. (2024)<br><strong>Exergetic Port-Hamiltonian Systems Modeling Language</strong><br><button class=copy-to-clipboard title="Exergetic Port-Hamiltonian Systems Modeling Language" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-MP, math-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17640v1.pdf filename=2402.17640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mathematical modeling of real-world physical systems requires the consistent combination of a multitude of physical laws and phenomenological models. This challenging task can be greatly simplified by hierarchically decomposing systems. Moreover, the use of diagrams for expressing such decompositions helps make the process more intuitive and facilitates communication, even with non-experts. As an important requirement, models have to respect fundamental physical laws such as the first and the second law of thermodynamics. While some existing modeling frameworks can make such guarantees based on structural properties of their models, they lack a formal graphical syntax. We present a compositional and thermodynamically consistent modeling language with a graphical syntax. As its semantics, port-Hamiltonian systems are endowed with further structural properties and a fixed physical interpretation such that thermodynamic consistency is ensured in a way that is closely related to the GENERIC framework. While port-Hamiltonian systems are inspired by graphical modeling with bond <b>graphs,</b> neither the link between the two, nor bond <b>graphs</b> themselves, can be easily formalized. In contrast, our syntax is based on a refinement of the well-studied operad of undirected wiring diagrams. The language effectively decouples the construction of complex models via the graphical syntax from physical concerns, which are dealt with only at the level of primitive subsystems that represent elementary physical behaviors. As a consequence, reuse of models and substitution of their parts becomes possible. Finally, by construction, systems interact by exchanging exergy, i.e. energy that is available for doing work, so the language is particularly well suited for thermodynamic analysis and optimization.</p></p class="citation"></blockquote><h2 id=cscr-8>cs.CR (8)</h2><h3 id=18--283341-chain-of-thought-prompting-of-large-language-models-for-discovering-and-fixing-software-vulnerabilities-yu-nong-et-al-2024>(1/8 | 283/341) Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities (Yu Nong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Nong, Mohammed Aldeen, Long Cheng, Hongxin Hu, Feng Chen, Haipeng Cai. (2024)<br><strong>Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities</strong><br><button class=copy-to-clipboard title="Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17230v1.pdf filename=2402.17230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Security vulnerabilities are increasingly prevalent in modern software and they are widely consequential to our society. Various approaches to defending against these vulnerabilities have been proposed, among which those leveraging deep learning (DL) avoid major barriers with other techniques hence attracting more attention in recent years. However, DL-based approaches face critical challenges including the lack of sizable and quality-labeled task-specific datasets and their inability to generalize well to unseen, real-world scenarios. Lately, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated impressive potential in various domains by overcoming those challenges, especially through <b>chain-of-thought</b> <b>(CoT)</b> <b>prompting.</b> In this paper, we explore how to leverage <b>LLMs</b> and CoT to address three key software vulnerability analysis tasks: identifying a given type of vulnerabilities, discovering vulnerabilities of any type, and patching detected vulnerabilities. We instantiate the general CoT methodology in the context of these tasks through VSP , our unified, vulnerability-semantics-guided <b>prompting</b> approach, and conduct extensive experiments assessing VSP versus five baselines for the three tasks against three <b>LLMs</b> and two datasets. Results show substantial superiority of our CoT-inspired <b>prompting</b> (553.3%, 36.5%, and 30.8% higher F1 accuracy for vulnerability identification, discovery, and patching, respectively, on CVE datasets) over the baselines. Through in-depth case studies analyzing VSP failures, we also reveal current gaps in LLM/CoT for challenging vulnerability cases, while proposing and validating respective improvements.</p></p class="citation"></blockquote><h3 id=28--284341-emmark-robust-watermarks-for-ip-protection-of-embedded-quantized-large-language-models-ruisi-zhang-et-al-2024>(2/8 | 284/341) EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models (Ruisi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruisi Zhang, Farinaz Koushanfar. (2024)<br><strong>EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models</strong><br><button class=copy-to-clipboard title="EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Quantization, LLaMA, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17938v1.pdf filename=2402.17938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces EmMark,a novel watermarking framework for protecting the intellectual property (IP) of embedded <b>large</b> <b>language</b> <b>models</b> deployed on resource-constrained edge devices. To address the IP theft risks posed by malicious end-users, EmMark enables proprietors to authenticate ownership by querying the watermarked model weights and matching the inserted signatures. EmMark&rsquo;s novelty lies in its strategic watermark weight parameters selection, nsuring robustness and maintaining model quality. Extensive proof-of-concept evaluations of models from OPT and <b>LLaMA-2</b> families demonstrate EmMark&rsquo;s fidelity, achieving 100% success in watermark extraction with model performance preservation. EmMark also showcased its resilience against watermark removal and forging attacks.</p></p class="citation"></blockquote><h3 id=38--285341-complexity-assessment-of-analog-security-primitives-using-the-disentropy-of-autocorrelation-paul-jimenez-et-al-2024>(3/8 | 285/341) Complexity Assessment of Analog Security Primitives Using the Disentropy of Autocorrelation (Paul Jimenez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Jimenez, Raphael Cardoso, Maurìcio Gomes de Queiroz, Mohab Abdalla, Cédric Marchand, Xavier Letartre, Fabio Pavanello. (2024)<br><strong>Complexity Assessment of Analog Security Primitives Using the Disentropy of Autocorrelation</strong><br><button class=copy-to-clipboard title="Complexity Assessment of Analog Security Primitives Using the Disentropy of Autocorrelation" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17488v1.pdf filename=2402.17488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of regularity in signals can be of great importance, typically in medicine to analyse electrocardiogram (ECG) or electromyography (EMG) signals, but also in climate studies, finance or security. In this work we focus on security primitives such as Physical Unclonable Functions (PUFs) or Pseudo-Random Number Generators (PRNGs). Such primitives must have a high level of complexity or entropy in their responses to guarantee enough security for their applications. There are several ways of assessing the complexity of their responses, especially in the binary domain. With the development of analog PUFs such as optical (photonic) PUFs, it would be useful to be able to assess their complexity in the analog domain when designing them, for example, before converting analog signals into binary. In this numerical study, we decided to explore the potential of the disentropy of autocorrelation as a measure of complexity for security primitives as PUFs or PRNGs with analog output or responses. We compare this metric to others used to assess regularities in analog signals such as Approximate Entropy (ApEn) and Fuzzy Entropy (FuzEn). We show that the disentropy of autocorrelation is able to differentiate between well-known PRNGs and non-optimised or bad PRNGs in the analog and binary domain with a better contrast than ApEn and FuzEn. Next, we show that the disentropy of autocorrelation is able to detect small patterns injected in PUFs responses and then we applied it to photonic PUFs <b>simulations.</b></p></p class="citation"></blockquote><h3 id=48--286341-ai-driven-anonymization-protecting-personal-data-privacy-while-leveraging-machine-learning-le-yang-et-al-2024>(4/8 | 286/341) AI-Driven Anonymization: Protecting Personal Data Privacy While Leveraging Machine Learning (Le Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Le Yang, Miao Tian, Duan Xin, Qishuo Cheng, Jiajian Zheng. (2024)<br><strong>AI-Driven Anonymization: Protecting Personal Data Privacy While Leveraging Machine Learning</strong><br><button class=copy-to-clipboard title="AI-Driven Anonymization: Protecting Personal Data Privacy While Leveraging Machine Learning" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17191v1.pdf filename=2402.17191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of artificial intelligence has significantly transformed people&rsquo;s lives. However, it has also posed a significant threat to privacy and security, with numerous instances of personal information being exposed online and reports of criminal attacks and theft. Consequently, the need to achieve intelligent protection of personal information through machine learning algorithms has become a paramount concern. Artificial intelligence leverages advanced algorithms and technologies to effectively encrypt and anonymize personal data, enabling valuable data analysis and utilization while safeguarding privacy. This paper focuses on personal data privacy protection and the promotion of anonymity as its core research objectives. It achieves personal data privacy protection and detection through the use of machine learning&rsquo;s <b>differential</b> <b>privacy</b> protection algorithm. The paper also addresses existing challenges in machine learning related to privacy and personal data protection, offers improvement suggestions, and analyzes factors impacting datasets to enable timely personal data privacy detection and protection.</p></p class="citation"></blockquote><h3 id=58--287341-the-seekers-dilemma-realistic-formulation-and-benchmarking-for-hardware-trojan-detection-amin-sarihi-et-al-2024>(5/8 | 287/341) The Seeker&rsquo;s Dilemma: Realistic Formulation and Benchmarking for Hardware Trojan Detection (Amin Sarihi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Sarihi, Ahmad Patooghy, Abdel-Hameed A. Badawy, Peter Jamieson. (2024)<br><strong>The Seeker&rsquo;s Dilemma: Realistic Formulation and Benchmarking for Hardware Trojan Detection</strong><br><button class=copy-to-clipboard title="The Seeker's Dilemma: Realistic Formulation and Benchmarking for Hardware Trojan Detection" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: B-8-1, cs-AR, cs-CR, cs-LG, cs.CR<br>Keyword Score: 9<br>Keywords: Graph, Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17918v1.pdf filename=2402.17918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work focuses on advancing security research in the hardware design space by formally defining the realistic problem of Hardware Trojan (HT) detection. The goal is to model HT detection more closely to the real world, i.e., describing the problem as &ldquo;The Seeker&rsquo;s Dilemma&rdquo; (an extension of Hide&amp;Seek on a <b>graph),</b> where a detecting agent is unaware of whether circuits are infected by HTs or not. Using this theoretical problem formulation, we create a <b>benchmark</b> that consists of a mixture of HT-free and HT-infected restructured circuits while preserving their original functionalities. The restructured circuits are randomly infected by HTs, causing a situation where the defender is uncertain if a circuit is infected or not. We believe that our innovative dataset will help the community better judge the detection quality of different methods by comparing their success rates in circuit classification. We use our developed <b>benchmark</b> to evaluate three state-of-the-art HT detection tools to show baseline results for this approach. We use Principal Component Analysis to assess the strength of our <b>benchmark,</b> where we observe that some restructured HT-infected circuits are mapped closely to HT-free circuits, leading to significant label misclassification by detectors.</p></p class="citation"></blockquote><h3 id=68--288341-on-central-primitives-for-quantum-cryptography-with-classical-communication-kai-min-chung-et-al-2024>(6/8 | 288/341) On Central Primitives for Quantum Cryptography with Classical Communication (Kai-Min Chung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai-Min Chung, Eli Goldin, Matthew Gray. (2024)<br><strong>On Central Primitives for Quantum Cryptography with Classical Communication</strong><br><button class=copy-to-clipboard title="On Central Primitives for Quantum Cryptography with Classical Communication" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17715v1.pdf filename=2402.17715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has introduced the &ldquo;Quantum-Computation Classical-Communication&rdquo; (QCCC) (Chung et. al.) setting for cryptography. There has been some evidence that One Way Puzzles (OWPuzz) are the natural central cryptographic primitive for this setting (Khurana and Tomer). For a primitive to be considered central it should have several characteristics. It should be well behaved (which for this paper we will think of as having amplification, combiners, and universal constructions); it should be implied by a wide variety of other primitives; and it should be equivalent to some class of useful primitives. We present combiners, correctness and security amplification, and a universal construction for OWPuzz. Our proof of security amplification uses a new and cleaner version construction of EFI from OWPuzz (in comparison to the result of Khurana and Tomer) that generalizes to weak OWPuzz and is the most technically involved section of the paper. It was previously known that OWPuzz are implied by other primitives of interest including commitments, symmetric key encryption, one way state generators (OWSG), and therefore pseudorandom states (PRS). However we are able to rule out OWPuzz&rsquo;s equivalence to many of these primitives by showing a <b>black</b> <b>box</b> separation between general OWPuzz and a restricted class of OWPuzz (those with efficient verification, which we call EV-OWPuzz). We then show that EV-OWPuzz are also implied by most of these primitives, which separates them from OWPuzz as well. This separation also separates extending PRS from highly compressing PRS answering an open question of Ananth et. al.</p></p class="citation"></blockquote><h3 id=78--289341-a-scalable-multi-layered-blockchain-architecture-for-enhanced-ehr-sharing-and-drug-supply-chain-management-reza-javan-et-al-2024>(7/8 | 289/341) A Scalable Multi-Layered Blockchain Architecture for Enhanced EHR Sharing and Drug Supply Chain Management (Reza Javan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reza Javan, Mehrzad Mohammadi, Mohammad Beheshti-Atashgah, Mohammad Reza Aref. (2024)<br><strong>A Scalable Multi-Layered Blockchain Architecture for Enhanced EHR Sharing and Drug Supply Chain Management</strong><br><button class=copy-to-clipboard title="A Scalable Multi-Layered Blockchain Architecture for Enhanced EHR Sharing and Drug Supply Chain Management" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DC, cs.CR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17342v1.pdf filename=2402.17342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the healthcare sector&rsquo;s shift to online platforms has spotlighted challenges concerning data security, privacy, and scalability. Blockchain technology, known for its decentralized, secure, and immutable nature, emerges as a viable solution for these pressing issues. This article presents an innovative Electronic Health Records (EHR) sharing and drug supply chain management framework tailored to address scalability, security, data integrity, traceability, and secure data sharing. The framework introduces five layers and transactions, prioritizing patient-centric healthcare by granting patients comprehensive access control over their health information. This access facilitates smoother processes, such as insurance claims, while maintaining robust security measures. Notably, our implementation of parallelism significantly bolsters scalability and transaction throughput while minimizing network traffic. Performance evaluations conducted through the Caliper <b>benchmark</b> indicate a slight increase in processor consumption during specific transactions, mitigated effectively by parallelization. RAM requirements remain largely stable. Additionally, our approach notably reduces network traffic while tripling transaction throughput. The framework ensures patient privacy, data integrity, access control, and interoperability, aligning with traditional healthcare systems. Moreover, it provides transparency and real-time drug supply monitoring, empowering decision-makers with actionable insights. As healthcare evolves, our framework sets a crucial precedent for innovative, scalable, and secure systems. Future enhancements could focus on scalability, real-world deployment, standardized data formats, reinforced security protocols, privacy preservation, and IoT integration to comply with regulations and meet evolving industry needs.</p></p class="citation"></blockquote><h3 id=88--290341-hardtaint-production-run-dynamic-taint-analysis-via-selective-hardware-tracing-yiyu-zhang-et-al-2024>(8/8 | 290/341) HardTaint: Production-Run Dynamic Taint Analysis via Selective Hardware Tracing (Yiyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyu Zhang, Tianyi Liu, Yueyang Wang, Yun Qi, Kai Ji, Jian Tang, Xiaoliang Wang, Xuandong Li, Zhiqiang Zuo. (2024)<br><strong>HardTaint: Production-Run Dynamic Taint Analysis via Selective Hardware Tracing</strong><br><button class=copy-to-clipboard title="HardTaint: Production-Run Dynamic Taint Analysis via Selective Hardware Tracing" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-PL, cs-SE, cs.CR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17241v1.pdf filename=2402.17241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic taint analysis (DTA), as a fundamental analysis technique, is widely used in security, privacy, and diagnosis, etc. As DTA demands to collect and analyze massive taint data online, it suffers extremely high runtime overhead. Over the past decades, numerous attempts have been made to lower the overhead of DTA. Unfortunately, the reductions they achieved are marginal, causing DTA only applicable to the debugging/testing scenarios. In this paper, we propose and implement HardTaint, a system that can realize production-run dynamic taint tracking. HardTaint adopts a hybrid and systematic design which combines static analysis, selective hardware tracing and parallel <b>graph</b> processing techniques. The comprehensive evaluations demonstrate that HardTaint introduces only around 9% runtime overhead which is an order of magnitude lower than the state-of-the-arts, while without sacrificing any taint detection capability.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--291341-prediction-of-the-sym-h-index-using-a-bayesian-deep-learning-method-with-uncertainty-quantification-yasser-abduallah-et-al-2024>(1/1 | 291/341) Prediction of the SYM-H Index Using a Bayesian Deep Learning Method with Uncertainty Quantification (Yasser Abduallah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasser Abduallah, Khalid A. Alobaid, Jason T. L. Wang, Haimin Wang, Vania K. Jordanova, Vasyl Yurchyshyn, Huseyin Cavus, Ju Jing. (2024)<br><strong>Prediction of the SYM-H Index Using a Bayesian Deep Learning Method with Uncertainty Quantification</strong><br><button class=copy-to-clipboard title="Prediction of the SYM-H Index Using a Bayesian Deep Learning Method with Uncertainty Quantification" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph.IM, cs-LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17196v1.pdf filename=2402.17196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel deep learning framework, named SYMHnet, which employs a <b>graph</b> <b>neural</b> <b>network</b> and a bidirectional <b>long</b> <b>short-term</b> <b>memory</b> <b>network</b> to cooperatively learn patterns from solar wind and interplanetary magnetic field parameters for short-term forecasts of the SYM-H index based on 1-minute and 5-minute resolution data. SYMHnet takes, as input, the time series of the parameters&rsquo; values provided by NASA&rsquo;s Space Science Data Coordinated Archive and predicts, as output, the SYM-H index value at time point t + w hours for a given time point t where w is 1 or 2. By incorporating Bayesian inference into the learning framework, SYMHnet can quantify both aleatoric (data) uncertainty and epistemic (model) uncertainty when predicting future SYM-H indices. Experimental results show that SYMHnet works well at quiet time and storm time, for both 1-minute and 5-minute resolution data. The results also show that SYMHnet generally performs better than related machine learning methods. For example, SYMHnet achieves a forecast skill score (FSS) of 0.343 compared to the FSS of 0.074 of a recent gradient boosting machine (GBM) method when predicting SYM-H indices (1 hour in advance) in a large storm (SYM-H = -393 nT) using 5-minute resolution data. When predicting the SYM-H indices (2 hours in advance) in the large storm, SYMHnet achieves an FSS of 0.553 compared to the FSS of 0.087 of the GBM method. In addition, SYMHnet can provide results for both data and model uncertainty quantification, whereas the related methods cannot.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--292341-using-ai-libraries-for-incompressible-computational-fluid-dynamics-boyang-chen-et-al-2024>(1/1 | 292/341) Using AI libraries for Incompressible Computational Fluid Dynamics (Boyang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyang Chen, Claire E. Heaney, Christopher C. Pain. (2024)<br><strong>Using AI libraries for Incompressible Computational Fluid Dynamics</strong><br><button class=copy-to-clipboard title="Using AI libraries for Incompressible Computational Fluid Dynamics" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-AI, cs-LG, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17913v1.pdf filename=2402.17913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been a huge effort focused on developing highly efficient open source libraries to perform Artificial Intelligence (AI) related computations on different computer architectures (for example, CPUs, GPUs and new AI processors). This has not only made the algorithms based on these libraries highly efficient and portable between different architectures, but also has substantially simplified the entry barrier to develop methods using AI. Here, we present a novel methodology to bring the power of both AI software and hardware into the field of numerical modelling by repurposing AI methods, such as <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> for the standard operations required in the field of the numerical solution of Partial Differential Equations (PDEs). The aim of this work is to bring the high performance, architecture agnosticism and ease of use into the field of the numerical solution of PDEs. We use the proposed methodology to solve the advection-diffusion equation, the non-linear Burgers equation and incompressible flow past a bluff body. For the latter, a <b>convolutional</b> <b>neural</b> <b>network</b> is used as a multigrid solver in order to enforce the incompressibility constraint. We show that the presented methodology can solve all these problems using repurposed AI libraries in an efficient way, and presents a new avenue to explore in the development of methods to solve PDEs and Computational Fluid Dynamics problems with implicit methods.</p></p class="citation"></blockquote><h2 id=cssy-1>cs.SY (1)</h2><h3 id=11--293341-corridor-mpc-for-multi-agent-inspection-of-orbiting-structures-gregorio-marchesini-et-al-2024>(1/1 | 293/341) Corridor MPC for Multi-Agent Inspection of Orbiting Structures (Gregorio Marchesini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gregorio Marchesini, Pedro Roque, Dimos V. Dimarogonas. (2024)<br><strong>Corridor MPC for Multi-Agent Inspection of Orbiting Structures</strong><br><button class=copy-to-clipboard title="Corridor MPC for Multi-Agent Inspection of Orbiting Structures" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SY<br>Categories: cs-SY, cs.SY, eess-SY<br>Keyword Score: 30<br>Keywords: Continuous Time, Continuous Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17596v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17596v1.pdf filename=2402.17596v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose an extension of the previously introduced Corridor Model Predictive Control scheme for high-order and distributed systems, with an application for on-orbit inspection. To this end, we leverage high order control barrier function (HOCBF) constraints as a suitable control approach to maintain each agent in the formation within a safe corridor from its reference trajectory. The recursive feasibility of the designed MPC scheme is tested numerically, while suitable modifications of the classical HOCBF constraint definition are introduced such that safety is guaranteed both in sampled and <b>continuous</b> <b>time.</b> The designed controller is validated through computer <b>simulation</b> in a realistic inspection scenario of the International Space Station.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--294341-note-evolutionary-game-theory-focus-informational-health-the-cocktail-party-effect-through-werewolfgame-under-incomplete-information-and-ess-search-method-using-expected-gains-of-repeated-dilemmas-yasuko-kawahata-2024>(1/1 | 294/341) Note: Evolutionary Game Theory Focus Informational Health: The Cocktail Party Effect Through Werewolfgame under Incomplete Information and ESS Search Method Using Expected Gains of Repeated Dilemmas (Yasuko Kawahata, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasuko Kawahata. (2024)<br><strong>Note: Evolutionary Game Theory Focus Informational Health: The Cocktail Party Effect Through Werewolfgame under Incomplete Information and ESS Search Method Using Expected Gains of Repeated Dilemmas</strong><br><button class=copy-to-clipboard title="Note: Evolutionary Game Theory Focus Informational Health: The Cocktail Party Effect Through Werewolfgame under Incomplete Information and ESS Search Method Using Expected Gains of Repeated Dilemmas" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-AI, physics-soc-ph, physics.soc-ph<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18598v1.pdf filename=2402.18598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore the state of information disruption caused by the cocktail party effect within the framework of non-perfect information games and evolutive games with multiple werewolves. In particular, we mathematically model and analyze the effects on the gain of each strategy choice and the formation process of evolutionary stable strategies (ESS) under the assumption that the pollution risk of <b>fake</b> <b>news</b> is randomly assigned in the context of repeated dilemmas. We will develop the computational process in detail, starting with the construction of the gain matrix, modeling the evolutionary dynamics using the replicator equation, and identifying the ESS. In addition, numerical <b>simulations</b> will be performed to observe system behavior under different initial conditions and parameter settings to better understand the impact of the spread of <b>fake</b> <b>news</b> on strategy evolution. This research will provide theoretical insights into the complex issues of contemporary society regarding the authenticity of information and expand the range of applications of evolutionary game theory.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--295341-designing-chatbots-to-support-victims-and-survivors-of-domestic-abuse-rahime-belen-saglam-et-al-2024>(1/2 | 295/341) Designing Chatbots to Support Victims and Survivors of Domestic Abuse (Rahime Belen Saglam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rahime Belen Saglam, Jason R. C. Nurse, Lisa Sugiura. (2024)<br><strong>Designing Chatbots to Support Victims and Survivors of Domestic Abuse</strong><br><button class=copy-to-clipboard title="Designing Chatbots to Support Victims and Survivors of Domestic Abuse" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 30<br>Keywords: ChatGPT, Gemini, Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17393v1.pdf filename=2402.17393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Objective: Domestic abuse cases have risen significantly over the last four years, in part due to the COVID-19 pandemic and the challenges for victims and survivors in accessing support. In this study, we investigate the role that <b>chatbots</b> - Artificial Intelligence (AI) and rule-based - may play in supporting victims/survivors in situations such as these or where direct access to help is limited. Methods: Interviews were conducted with experts working in domestic abuse support services and organizations (e.g., charities, law enforcement) and the content of websites of related support-service providers was collected. Thematic content analysis was then applied to assess and extract insights from the interview data and the content on victim-support websites. We also reviewed pertinent <b>chatbot</b> literature to reflect on studies that may inform design principles and interaction patterns for agents used to support victims/survivors. Results: From our analysis, we outlined a set of design considerations/practices for <b>chatbots</b> that consider potential use cases and target groups, dialog structure, personality traits that might be useful for <b>chatbots</b> to possess, and finally, safety and privacy issues that should be addressed. Of particular note are situations where AI systems (e.g., <b>ChatGPT,</b> CoPilot, <b>Gemini)</b> are not recommended for use, the value of conveying emotional support, the importance of transparency, and the need for a safe and confidential space. Conclusion: It is our hope that these considerations/practices will stimulate debate among <b>chatbots</b> and AI developers and service providers and - for situations where <b>chatbots</b> are deemed appropriate for use - inspire efficient use of <b>chatbots</b> in the support of survivors of domestic abuse.</p></p class="citation"></blockquote><h3 id=22--296341-a-review-of-data-mining-in-personalized-education-current-trends-and-future-prospects-zhang-xiong-et-al-2024>(2/2 | 296/341) A Review of Data Mining in Personalized Education: Current Trends and Future Prospects (Zhang Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhang Xiong, Haoxuan Li, Zhuang Liu, Zhuofan Chen, Hao Zhou, Wenge Rong, Yuanxin Ouyang. (2024)<br><strong>A Review of Data Mining in Personalized Education: Current Trends and Future Prospects</strong><br><button class=copy-to-clipboard title="A Review of Data Mining in Personalized Education: Current Trends and Future Prospects" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-LG, cs.CY<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17236v1.pdf filename=2402.17236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalized education, tailored to individual student needs, leverages educational technology and artificial intelligence (AI) in the digital age to enhance learning effectiveness. The integration of AI in educational platforms provides insights into academic performance, learning preferences, and behaviors, optimizing the personal learning process. Driven by data mining techniques, it not only benefits students but also provides educators and institutions with tools to craft customized learning experiences. To offer a comprehensive review of recent advancements in personalized educational data mining, this paper focuses on four primary scenarios: educational <b>recommendation,</b> cognitive diagnosis, knowledge tracing, and learning analysis. This paper presents a structured taxonomy for each area, compiles commonly used datasets, and identifies future research directions, emphasizing the role of data mining in enhancing personalized education and paving the way for future exploration and innovation.</p></p class="citation"></blockquote><h2 id=csgt-4>cs.GT (4)</h2><h3 id=14--297341-repeated-contracting-with-multiple-non-myopic-agents-policy-regret-and-limited-liability-natalie-collina-et-al-2024>(1/4 | 297/341) Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and Limited Liability (Natalie Collina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Natalie Collina, Varun Gupta, Aaron Roth. (2024)<br><strong>Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and Limited Liability</strong><br><button class=copy-to-clipboard title="Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and Limited Liability" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-DS, cs-GT, cs-LG, cs.GT<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Bandit Algorithm, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17108v1.pdf filename=2402.17108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a repeated contracting setting in which a Principal adaptively chooses amongst $k$ Agents at each of $T$ rounds. The Agents are non-myopic, and so a mechanism for the Principal induces a $T$-round extensive form game amongst the Agents. We give several results aimed at understanding an under-explored aspect of contract theory &ndash; the game induced when choosing an Agent to contract with. First, we show that this game admits a pure-strategy \emph{non-responsive} equilibrium amongst the Agents &ndash; informally an equilibrium in which the Agent&rsquo;s actions depend on the history of realized states of nature, but not on the history of each other&rsquo;s actions, and so avoids the complexities of collusion and threats. Next, we show that if the Principal selects Agents using a \emph{monotone} <b>bandit</b> <b>algorithm,</b> then for any concave contract, in any such equilibrium, the Principal obtains no regret to contracting with the best Agent in hindsight &ndash; not just given their realized actions, but also to the <b>counterfactual</b> world in which they had offered a guaranteed $T$-round contract to the best Agent in hindsight, which would have induced a different sequence of actions. Finally, we show that if the Principal selects Agents using a monotone <b>bandit</b> <b>algorithm</b> which guarantees no swap-regret, then the Principal can additionally offer only limited liability contracts (in which the Agent never needs to pay the Principal) while getting no-regret to the <b>counterfactual</b> world in which she offered a linear contract to the best Agent in hindsight &ndash; despite the fact that linear contracts are not limited liability. We instantiate this theorem by demonstrating the existence of a monotone no swap-regret <b>bandit</b> <b>algorithm,</b> which to our knowledge has not previously appeared in the literature.</p></p class="citation"></blockquote><h3 id=24--298341-replicating-electoral-success-kiran-tomlinson-et-al-2024>(2/4 | 298/341) Replicating Electoral Success (Kiran Tomlinson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kiran Tomlinson, Tanvi Namjoshi, Johan Ugander, Jon Kleinberg. (2024)<br><strong>Replicating Electoral Success</strong><br><button class=copy-to-clipboard title="Replicating Electoral Success" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-MA, cs.GT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17109v1.pdf filename=2402.17109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A core tension in the study of plurality elections is the clash between the classic Hotelling-Downs model, which predicts that two office-seeking candidates should position themselves at the median voter&rsquo;s policy, and the empirical observation that real-world democracies often have two major parties with divergent policies. Motivated by this tension and drawing from bounded rationality, we introduce a dynamic model of candidate positioning based on a simple behavioral heuristic: candidates imitate the policy of previous winners. The resulting model is closely connected to evolutionary replicator dynamics and exhibits complex behavior, despite its simplicity. For uniformly-distributed voters, we prove that when there are $k = 2$, $3$, or $4$ candidates per election, any symmetric candidate distribution converges over time to a concentration of candidates at the center. With $k \ge 5$, however, we prove that the candidate distribution does not converge to the center. For initial distributions without any extreme candidates, we prove a stronger statement than non-convergence, showing that the density in an interval around the center goes to zero when $k \ge 5$. As a matter of robustness, our conclusions are qualitatively unchanged if a small fraction of candidates are not winner-copiers and are instead positioned uniformly at random. Beyond our theoretical analysis, we illustrate our results in <b>simulation;</b> for five or more candidates, we find a tendency towards the emergence of two clusters, a mechanism suggestive of Duverger&rsquo;s Law, the empirical finding that plurality leads to two-party systems. Our <b>simulations</b> also explore several variations of the model, including non-uniform voter distributions and other forms of noise, which exhibit similar convergence patterns. Finally, we discuss the relationship between our model and prior work on strategic equilibria of candidate positioning games.</p></p class="citation"></blockquote><h3 id=34--299341-weighted-ef1-and-po-allocations-with-few-types-of-agents-or-chores-jugal-garg-et-al-2024>(3/4 | 299/341) Weighted EF1 and PO Allocations with Few Types of Agents or Chores (Jugal Garg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jugal Garg, Aniket Murhekar, John Qin. (2024)<br><strong>Weighted EF1 and PO Allocations with Few Types of Agents or Chores</strong><br><button class=copy-to-clipboard title="Weighted EF1 and PO Allocations with Few Types of Agents or Chores" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17173v1.pdf filename=2402.17173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the existence of fair and efficient allocations of indivisible chores to asymmetric agents who have unequal entitlements or weights. We consider the <b>fairness</b> notion of weighted envy-freeness up to one chore (wEF1) and the efficiency notion of Pareto-optimality (PO). The existence of EF1 and PO allocations of chores to symmetric agents is a major open problem in discrete fair division, and positive results are known only for certain structured instances. In this paper, we study this problem for a more general setting of asymmetric agents and show that an allocation that is wEF1 and PO exists and can be computed in polynomial time for instances with: - Three types of agents, where agents with the same type have identical preferences but can have different weights. - Two types of chores, where the chores can be partitioned into two sets, each containing copies of the same chore. For symmetric agents, our results establish that EF1 and PO allocations exist for three types of agents and also generalize known results for three agents, two types of agents, and two types of chores. Our algorithms use a weighted picking sequence algorithm as a subroutine; we expect this idea and our analysis to be of independent interest.</p></p class="citation"></blockquote><h3 id=44--300341-choosing-behind-the-veil-tight-bounds-for-identity-blind-online-algorithms-tomer-ezra-et-al-2024>(4/4 | 300/341) Choosing Behind the Veil: Tight Bounds for Identity-Blind Online Algorithms (Tomer Ezra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomer Ezra, Michal Feldman, Zhihao Gavin Tang. (2024)<br><strong>Choosing Behind the Veil: Tight Bounds for Identity-Blind Online Algorithms</strong><br><button class=copy-to-clipboard title="Choosing Behind the Veil: Tight Bounds for Identity-Blind Online Algorithms" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-DS, cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17160v1.pdf filename=2402.17160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Bayesian online settings, every element has a value that is drawn from a known underlying distribution, which we refer to as the element&rsquo;s identity. The elements arrive sequentially. Upon the arrival of an element, its value is revealed, and the decision maker needs to decide, immediately and irrevocably, whether to accept it or not. While most previous work has assumed that the decision maker, upon observing the element&rsquo;s value, also becomes aware of its identity &ndash; namely, its distribution &ndash; practical scenarios frequently demand that decisions be made based solely on the element&rsquo;s value, without considering its identity. This necessity arises either from the algorithm&rsquo;s ignorance of the element&rsquo;s identity or due to the pursuit of <b>fairness.</b> We call such algorithms identity-blind algorithms, and propose the identity-blindness gap as a metric to evaluate the performance loss caused by identity-blindness. This gap is defined as the maximum ratio between the expected performance of an identity-blind online algorithm and an optimal online algorithm that knows the arrival order, thus also the identities. We study the identity-blindness gap in the paradigmatic prophet inequality problem, under the two objectives of maximizing the expected value, and maximizing the probability to obtain the highest value. For the max-expectation objective, the celebrated prophet inequality establishes a single-threshold algorithm that gives at least 1/2 of the offline optimum, thus also an identity-blindness gap of at least 1/2. We show that this bound is tight. For the max-probability objective, while the competitive ratio is tightly 1/e, we provide a deterministic single-threshold algorithm that gives an identity-blindness gap of $\sim 0.562$ under the assumption that there are no large point masses. Moreover, we show that this bound is tight with respect to deterministic algorithms.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--301341-a-highly-efficient-computational-approach-for-part-scale-microstructure-predictions-in-ti-6al-4v-additive-manufacturing-sebastian-d-proell-et-al-2024>(1/1 | 301/341) A highly efficient computational approach for part-scale microstructure predictions in Ti-6Al-4V additive manufacturing (Sebastian D. Proell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian D. Proell, Julian Brotz, Martin Kronbichler, Wolfgang A. Wall, Christoph Meier. (2024)<br><strong>A highly efficient computational approach for part-scale microstructure predictions in Ti-6Al-4V additive manufacturing</strong><br><button class=copy-to-clipboard title="A highly efficient computational approach for part-scale microstructure predictions in Ti-6Al-4V additive manufacturing" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 28<br>Keywords: Benchmarking, Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17580v1.pdf filename=2402.17580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fast and efficient <b>simulations</b> of metal additive manufacturing (AM) processes are highly relevant to exploring the full potential of this promising manufacturing technique. The microstructure composition plays an important role in characterizing the part quality and deriving mechanical properties. When complete parts are simulated, one often needs to resort to strong simplifications such as layer-wise heating due to the large number of simulated time steps compared to the small time step sizes. This article proposes a scan-resolved approach to the coupled thermo-microstructural problem. Building on a highly efficient thermal model, we discuss the implementation of a phenomenological microstructure model for the evolution of the three main constituents of Ti-6Al-4V: stable $\alpha_s$-phase, martensite $\alpha_m$-phase and $\beta$-phase. The implementation is tailored to modern hardware features using vectorization and fast approximations of transcendental functions. A performance model and numerical examples verify the high degree of optimization. We demonstrate the applicability and predictive power of the approach and the influence of scan strategy and <b>geometry.</b> Depending on the specific example, results can be obtained with moderate computational resources in a few hours to days. The numerical examples include a prediction of the microstructure on the full NIST AM <b>Benchmark</b> cantilever specimen.</p></p class="citation"></blockquote><h2 id=hep-ph-1>hep-ph (1)</h2><h3 id=11--302341-a-case-study-of-sending-graph-neural-networks-back-to-the-test-bench-for-applications-in-high-energy-particle-physics-emanuel-pfeffer-et-al-2024>(1/1 | 302/341) A case study of sending graph neural networks back to the test bench for applications in high-energy particle physics (Emanuel Pfeffer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emanuel Pfeffer, Michael Waßmer, Yee-Ying Cung, Roger Wolf, Ulrich Husemann. (2024)<br><strong>A case study of sending graph neural networks back to the test bench for applications in high-energy particle physics</strong><br><button class=copy-to-clipboard title="A case study of sending graph neural networks back to the test bench for applications in high-energy particle physics" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ph<br>Categories: cs-AI, hep-ex, hep-ph, hep-ph<br>Keyword Score: 26<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17386v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17386v1.pdf filename=2402.17386v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In high-energy particle collisions, the primary collision products usually decay further resulting in tree-like, hierarchical structures with a priori unknown multiplicity. At the stable-particle level all decay products of a collision form permutation invariant sets of final state objects. The analogy to mathematical <b>graphs</b> <b>gives</b> <b>rise</b> to the idea that <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs),</b> which naturally resemble these properties, should be best-suited to address many tasks related to high-energy particle physics. In this paper we describe a <b>benchmark</b> test of a typical <b>GNN</b> against neural networks of the well-established deep fully-connected feed-forward architecture. We aim at performing this comparison maximally unbiased in terms of nodes, hidden layers, or trainable parameters of the neural networks under study. As physics case we use the classification of the final state X produced in association with top quark-antiquark pairs in proton-proton collisions at the Large Hadron Collider at CERN, where X stands for a bottom quark-antiquark pair produced either non-resonantly or through the decay of an intermediately produced Z or Higgs boson.</p></p class="citation"></blockquote><h2 id=astro-phco-1>astro-ph.CO (1)</h2><h3 id=11--303341-syren-halofit-a-fast-interpretable-high-precision-formula-for-the-λcdm-nonlinear-matter-power-spectrum-deaglan-j-bartlett-et-al-2024>(1/1 | 303/341) syren-halofit: A fast, interpretable, high-precision formula for the $Λ$CDM nonlinear matter power spectrum (Deaglan J. Bartlett et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deaglan J. Bartlett, Benjamin D. Wandelt, Matteo Zennaro, Pedro G. Ferreira, Harry Desmond. (2024)<br><strong>syren-halofit: A fast, interpretable, high-precision formula for the $Λ$CDM nonlinear matter power spectrum</strong><br><button class=copy-to-clipboard title="syren-halofit: A fast, interpretable, high-precision formula for the $Λ$CDM nonlinear matter power spectrum" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.CO<br>Categories: astro-ph-CO, astro-ph-IM, astro-ph.CO, cs-LG, cs-NE<br>Keyword Score: 25<br>Keywords: Black Box, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17492v1.pdf filename=2402.17492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$, as a function of cosmological parameters and redshift is of fundamental importance in cosmology. Analytic approximations provide an interpretable solution, yet current approximations are neither fast nor accurate relative to <b>black-box</b> <b>numerical</b> emulators. We use symbolic regression to obtain simple analytic approximations to the nonlinear scale, $k_\sigma$, the effective spectral index, $n_{\rm eff}$, and the curvature, $C$, which are required for the halofit model. We then re-optimise the coefficients of halofit to fit a wide range of cosmologies and redshifts. We then again exploit symbolic regression to explore the space of analytic expressions to fit the residuals between $P(k)$ and the optimised predictions of halofit. All methods are validated against $N$-body <b>simulations.</b> Our symbolic expressions for $k_\sigma$, $n_{\rm eff}$ and $C$ have root mean squared fractional errors of 0.8%, 0.2% and 0.3%, respectively, for redshifts below 3 and a wide range of cosmologies. The re-optimised halofit parameters reduce the root mean squared fractional error from 3% to below 2% for wavenumbers $k=9\times10^{-3}-9 , h{\rm Mpc^{-1}}$. We introduce syren-halofit (symbolic-regression-enhanced halofit), an extension to halofit containing a short symbolic correction which improves this error to 1%. Our method is 2350 and 3170 times faster than current halofit and hmcode implementations, respectively, and 2680 and 64 times faster than EuclidEmulator2 (which requires running class) and the BACCO emulator. We obtain comparable accuracy to EuclidEmulator2 and the BACCO emulator when tested on $N$-body <b>simulations.</b> Our work greatly increases the speed and accuracy of symbolic approximations to $P(k)$, making them significantly faster than their numerical counterparts without loss of accuracy.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--304341-sequential-transport-maps-using-sos-density-estimation-and-α-divergences-benjamin-zanger-et-al-2024>(1/3 | 304/341) Sequential transport maps using SoS density estimation and $α$-divergences (Benjamin Zanger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Zanger, Tiangang Cui, Martin Schreiber, Olivier Zahm. (2024)<br><strong>Sequential transport maps using SoS density estimation and $α$-divergences</strong><br><button class=copy-to-clipboard title="Sequential transport maps using SoS density estimation and $α$-divergences" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 23<br>Keywords: Benchmarking, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17943v1.pdf filename=2402.17943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Transport-based density estimation methods are receiving growing interest because of their ability to efficiently generate samples from the approximated density. We further invertigate the sequential transport maps framework proposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of composed Knothe-Rosenblatt (KR) maps. Each of those maps are built by first estimating an intermediate density of moderate complexity, and then by computing the exact KR map from a reference density to the precomputed approximate density. In our work, we explore the use of Sum-of-Squares (SoS) densities and $\alpha$-divergences for approximating the intermediate densities. Combining SoS densities with $\alpha$-divergence interestingly yields convex optimization problems which can be efficiently solved using semidefinite programming. The main advantage of $\alpha$-divergences is to enable working with unnormalized densities, which provides benefits both numerically and theoretically. In particular, we provide two new convergence analyses of the sequential transport maps: one based on a triangle-like inequality and the second on information geometric properties of $\alpha$-divergences for unnormalizied densities. The choice of intermediate densities is also crucial for the efficiency of the method. While tempered (or annealed) densities are the state-of-the-art, we introduce diffusion-based intermediate densities which permits to approximate densities known from samples only. Such intermediate densities are well-established in machine learning for generative modeling. Finally we propose and try different low-dimensional maps (or lazy maps) for dealing with high-dimensional problems and numerically demonstrate our methods on several <b>benchmarks,</b> including Bayesian inference problems and <b>unsupervised</b> <b>learning</b> task.</p></p class="citation"></blockquote><h3 id=23--305341-zeroth-order-sampling-methods-for-non-log-concave-distributions-alleviating-metastability-by-denoising-diffusion-ye-he-et-al-2024>(2/3 | 305/341) Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion (Ye He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye He, Kevin Rojas, Molei Tao. (2024)<br><strong>Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion</strong><br><button class=copy-to-clipboard title="Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-PR, math-ST, stat-ME, stat-ML, stat-TH, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17886v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17886v2.pdf filename=2402.17886v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the <b>simulation</b> of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit still suffering from the curse of dimensionality. Consequently, for low dimensional distributions, ZOD-MC is a very efficient sampler, with performance exceeding latest samplers, including also-denoising-diffusion-based RDMC and RS-DMC. Last, we experimentally demonstrate the insensitivity of ZOD-MC to increasingly higher barriers between modes or discontinuity in non-convex potential.</p></p class="citation"></blockquote><h3 id=33--306341-dataset-fairness-achievable-fairness-on-your-data-with-utility-guarantees-muhammad-faaiz-taufiq-et-al-2024>(3/3 | 306/341) Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees (Muhammad Faaiz Taufiq et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Faaiz Taufiq, Jean-Francois Ton, Yang Liu. (2024)<br><strong>Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees</strong><br><button class=copy-to-clipboard title="Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CY, cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17106v1.pdf filename=2402.17106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In machine learning <b>fairness,</b> training models which minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the <b>fairness-accuracy</b> trade-off. The severity of this trade-off fundamentally depends on dataset characteristics such as dataset imbalances or biases. Therefore using a uniform <b>fairness</b> requirement across datasets remains questionable and can often lead to models with substantially low utility. To address this, we present a computationally efficient approach to approximate the <b>fairness-accuracy</b> trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Moreover, we quantify the uncertainty in our approximation by introducing confidence intervals around this curve, offering a statistically grounded perspective on the acceptable range of <b>fairness</b> violations for any given accuracy threshold. Our empirical evaluation spanning tabular, image and language datasets underscores that our approach provides practitioners with a principled framework for dataset-specific <b>fairness</b> decisions across various data modalities.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--307341-rose-efficient-and-extensible-autodiff-on-the-web-sam-estep-et-al-2024>(1/1 | 307/341) Rose: Efficient and Extensible Autodiff on the Web (Sam Estep et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sam Estep, Raven Rothkopf, Wode Ni, Joshua Sunshine. (2024)<br><strong>Rose: Efficient and Extensible Autodiff on the Web</strong><br><button class=copy-to-clipboard title="Rose: Efficient and Extensible Autodiff on the Web" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17743v1.pdf filename=2402.17743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic differentiation (AD) has become the backbone for a new wave of optimization-driven domains such as computer graphics and machine learning over the past decade. However, existing AD systems face limitations, either lacking support for in-browser development or failing to harness more recent, compilerbased approaches to achieve both expressiveness and size-preserving differentiation. This work introduces Rose, a portable, extensible AD library that runs on the web. Rose allows users to write opaque functions with custom derivatives and supports dynamic construction of AD functions. We integrated Rose into two differentiable <b>simulations</b> and a diagram authoring tool to demonstrate the utility of Rose&rsquo;s design. Finally, we show that Rose is 173x as fast as TensorFlow.js in compiling and running a <b>benchmark</b> suite of optimized diagrams.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--308341-a-multi-agent-model-for-opinion-evolution-under-cognitive-biases-mário-s-alvim-et-al-2024>(1/1 | 308/341) A Multi-Agent Model for Opinion Evolution under Cognitive Biases (Mário S. Alvim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mário S. Alvim, Artur Gaspar da Silva, Sophia Knight, Frank Valencia. (2024)<br><strong>A Multi-Agent Model for Opinion Evolution under Cognitive Biases</strong><br><button class=copy-to-clipboard title="A Multi-Agent Model for Opinion Evolution under Cognitive Biases" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs-SI, cs.MA<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17615v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17615v1.pdf filename=2402.17615v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We generalize the DeGroot model for opinion dynamics to better capture realistic social scenarios. We introduce a model where each agent has their own individual cognitive biases. Society is represented as a directed <b>graph</b> whose edges indicate how much agents influence one another. Biases are represented as the functions in the square region $[-1,1]^2$ and categorized into four sub-regions based on the potential reactions they may elicit in an agent during instances of opinion disagreement. Under the assumption that each bias of every agent is a continuous function within the region of receptive but resistant reactions ($\mathbf{R}$), we show that the society converges to a consensus if the <b>graph</b> is strongly connected. Under the same assumption, we also establish that the entire society converges to a unanimous opinion if and only if the source components of the <b>graph-namely,</b> strongly connected components with no external influence-converge to that opinion. We illustrate that convergence is not guaranteed for strongly connected <b>graphs</b> when biases are either discontinuous functions in $\mathbf{R}$ or not included in $\mathbf{R}$. We showcase our model through a series of examples and <b>simulations,</b> offering insights into how opinions form in social networks under cognitive biases.</p></p class="citation"></blockquote><h2 id=nlinao-1>nlin.AO (1)</h2><h3 id=11--309341-predicting-instability-in-complex-oscillator-networks-limitations-and-potentials-of-network-measures-and-machine-learning-christian-nauck-et-al-2024>(1/1 | 309/341) Predicting Instability in Complex Oscillator Networks: Limitations and Potentials of Network Measures and Machine Learning (Christian Nauck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Nauck, Michael Lindner, Nora Molkenthin, Jürgen Kurths, Eckehard Schöll, Jörg Raisch, Frank Hellmann. (2024)<br><strong>Predicting Instability in Complex Oscillator Networks: Limitations and Potentials of Network Measures and Machine Learning</strong><br><button class=copy-to-clipboard title="Predicting Instability in Complex Oscillator Networks: Limitations and Potentials of Network Measures and Machine Learning" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: nlin.AO<br>Categories: cs-LG, cs-SI, nlin-AO, nlin.AO<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17500v1.pdf filename=2402.17500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A central question of network science is how functional properties of systems arise from their structure. For networked dynamical systems, structure is typically quantified with network measures. A functional property that is of theoretical and practical interest for oscillatory systems is the stability of synchrony to localized perturbations. Recently, <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have been shown to predict this stability successfully; at the same time, network measures have struggled to paint a clear picture. Here we collect 46 relevant network measures and find that no small subset can reliably predict stability. The performance of <b>GNNs</b> can only be matched by combining all network measures and nodewise machine learning. However, unlike <b>GNNs,</b> this approach fails to extrapolate from network ensembles to several real power grid topologies. This suggests that correlations of network measures and function may be misleading, and that <b>GNNs</b> capture the causal relationship between structure and stability substantially better.</p></p class="citation"></blockquote><h2 id=csit-1>cs.IT (1)</h2><h3 id=11--310341-weakly-private-information-retrieval-from-heterogeneously-trusted-servers-wenyuan-zhao-et-al-2024>(1/1 | 310/341) Weakly Private Information Retrieval from Heterogeneously Trusted Servers (Wenyuan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenyuan Zhao, Yu Shin Huang, Ruida Zhou, Chao Tian. (2024)<br><strong>Weakly Private Information Retrieval from Heterogeneously Trusted Servers</strong><br><button class=copy-to-clipboard title="Weakly Private Information Retrieval from Heterogeneously Trusted Servers" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Mutual Information, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17940v1.pdf filename=2402.17940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of weakly private <b>information</b> <b>retrieval</b> (PIR) when there is heterogeneity in servers&rsquo; trustfulness under the maximal leakage (Max-L) metric and <b>mutual</b> <b>information</b> <b>(MI)</b> metric. A user wishes to retrieve a desired message from N non-colluding servers efficiently, such that the identity of the desired message is not leaked in a significant manner; however, some servers can be more trustworthy than others. We propose a code construction for this setting and optimize the probability distribution for this construction. For the Max-L metric, it is shown that the optimal probability allocation for the proposed scheme essentially separates the delivery patterns into two parts: a completely private part that has the same download overhead as the capacity-achieving PIR code, and a non-private part that allows complete privacy leakage but has no download overhead by downloading only from the most trustful server. The optimal solution is established through a sophisticated analysis of the underlying convex optimization problem, and a reduction between the homogeneous setting and the heterogeneous setting. For the MI metric, the homogeneous case is studied first for which the code can be optimized with an explicit probability assignment, while a closed-form solution becomes intractable for the heterogeneous case. Numerical results are provided for both cases to corroborate the theoretical analysis.</p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--311341-pathwise-relaxed-optimal-control-of-rough-differential-equations-prakash-chakraborty-et-al-2024>(1/2 | 311/341) Pathwise Relaxed Optimal Control of Rough Differential Equations (Prakash Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prakash Chakraborty, Harsha Honnappa, Samy Tindel. (2024)<br><strong>Pathwise Relaxed Optimal Control of Rough Differential Equations</strong><br><button class=copy-to-clipboard title="Pathwise Relaxed Optimal Control of Rough Differential Equations" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math-PR, math.OC<br>Keyword Score: 20<br>Keywords: Continuous Time, Continuous Time, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17900v1.pdf filename=2402.17900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This note lays part of the theoretical ground for a definition of differential systems modeling <b>reinforcement</b> <b>learning</b> in <b>continuous</b> <b>time</b> non-Markovian rough environments. Specifically we focus on optimal relaxed control of rough equations (the term relaxed referring to the fact that controls have to be considered as measure valued objects). With <b>reinforcement</b> <b>learning</b> in view, our reward functions encompass forms that involve an entropy-type term favoring exploration. In this context, our contribution focuses on a careful definition of the corresponding relaxed Hamilton-Jacobi-Bellman (HJB)-type equation. A substantial part of our endeavor consists in a precise definition of the notion of test function and viscosity solution for the rough relaxed PDE obtained in this framework. Note that this task is often merely sketched in the rough viscosity literature, in spite of the fact that it gives a proper meaning to the differential system at stake. In the last part of the paper we prove that the natural value function in our context solves a relaxed rough HJB equation in the viscosity sense.</p></p class="citation"></blockquote><h3 id=22--312341-optimal-control-barrier-functions-maximizing-the-action-space-subject-to-control-bounds-logan-e-beaver-2024>(2/2 | 312/341) Optimal Control Barrier Functions: Maximizing the Action Space Subject to Control Bounds (Logan E. Beaver, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Logan E. Beaver. (2024)<br><strong>Optimal Control Barrier Functions: Maximizing the Action Space Subject to Control Bounds</strong><br><button class=copy-to-clipboard title="Optimal Control Barrier Functions: Maximizing the Action Space Subject to Control Bounds" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17694v1.pdf filename=2402.17694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This letter addresses the constraint compatibility problem of control barrier functions (CBFs), which occurs when a safety-critical CBF requires a system to apply more control effort than it is capable of generating. This inevitably leads to a safety violation, which transitions the system to an unsafe (and possibly dangerous) trajectory. We resolve the constraint compatibility problem by constructing a control barrier function that maximizes the feasible action space for first and second-order constraints, and we prove that the optimal CBF encodes a dynamical motion primitive. Furthermore, we show that this dynamical motion primitive contains an implicit model for the future trajectory for time-varying components of the system. We validate our optimal CBF in <b>simulation,</b> and compare its behavior with a linear CBF.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--313341-ssresf-sensitivity-aware-single-particle-radiation-effects-simulation-framework-in-soc-platforms-based-on-svm-algorithm-meng-liu-et-al-2024>(1/1 | 313/341) SSRESF: Sensitivity-aware Single-particle Radiation Effects Simulation Framework in SoC Platforms based on SVM Algorithm (Meng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Liu, Shuai Li, Fei Xiao, Ruijie Wang, Chunxue Liu, Liang Wang. (2024)<br><strong>SSRESF: Sensitivity-aware Single-particle Radiation Effects Simulation Framework in SoC Platforms based on SVM Algorithm</strong><br><button class=copy-to-clipboard title="SSRESF: Sensitivity-aware Single-particle Radiation Effects Simulation Framework in SoC Platforms based on SVM Algorithm" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17489v1.pdf filename=2402.17489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ever-expanding scale of integrated circuits has brought about a significant rise in the design risks associated with radiation-resistant integrated circuit chips. Traditional single-particle experimental methods, with their iterative design approach, are increasingly ill-suited for the challenges posed by large-scale integrated circuits. In response, this article introduces a novel sensitivity-aware single-particle radiation effects <b>simulation</b> framework tailored for System-on-Chip platforms. Based on SVM algorithm we have implemented fast finding and classification of sensitive circuit nodes. Additionally, the methodology automates soft error analysis across the entire software stack. The study includes practical experiments focusing on RISC-V architecture, encompassing core components, buses, and memory systems. It culminates in the establishment of databases for Single Event Upsets (SEU) and Single Event Transients (SET), showcasing the practical efficacy of the proposed methodology in addressing radiation-induced challenges at the scale of contemporary integrated circuits. Experimental results have shown up to 12.78X speed-up on the basis of achieving 94.58% accuracy.</p></p class="citation"></blockquote><h2 id=mathna-5>math.NA (5)</h2><h3 id=15--314341-numerical-schemes-for-3-wave-kinetic-equations-a-complete-treatment-of-the-collision-operator-steven-walton-et-al-2024>(1/5 | 314/341) Numerical Schemes for 3-Wave Kinetic Equations: A Complete Treatment of the Collision Operator (Steven Walton et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Steven Walton, Minh-Binh Tran. (2024)<br><strong>Numerical Schemes for 3-Wave Kinetic Equations: A Complete Treatment of the Collision Operator</strong><br><button class=copy-to-clipboard title="Numerical Schemes for 3-Wave Kinetic Equations: A Complete Treatment of the Collision Operator" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17481v1.pdf filename=2402.17481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In our previous work, numerical schemes for a simplified version of 3-wave kinetic equations, in which only the simple forward-cascade terms of the collision operators are kept, have been successfully designed, especially to capture the long time dynamics of the equation given the multiple blow-up time phenomenon. In this second work in the series, we propose numerical treatments for the complete 3-wave kinetic equations, in which the complete, much more complicated collision operators are fully considered based on a novel conservative form of the equation. We then derive an implicit finite volume scheme to solve the equation. The new discretization uses an adaptive time-stepping method which allows for the <b>simulations</b> to be carried to very long times. Our computed solutions are compared with previously derived long-time asymptotic estimates for the decay rate of total energy of time-dependent solutions of 3-wave kinetic equations and found to be in excellent agreement.</p></p class="citation"></blockquote><h3 id=25--315341-real-time-tracking-of-moving-objects-from-scattering-matrix-in-real-world-microwave-imaging-seong-ho-son-et-al-2024>(2/5 | 315/341) Real-time tracking of moving objects from scattering matrix in real-world microwave imaging (Seong-Ho Son et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seong-Ho Son, Kwang-Jae Lee, Won-Kwang Park. (2024)<br><strong>Real-time tracking of moving objects from scattering matrix in real-world microwave imaging</strong><br><button class=copy-to-clipboard title="Real-time tracking of moving objects from scattering matrix in real-world microwave imaging" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 78A46, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17273v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17273v2.pdf filename=2402.17273v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The problem of the real-time microwave imaging of small, moving objects from a scattering matrix, whose elements are measured scattering parameters, without diagonal elements is considered herein. An imaging algorithm based on a Kirchhoff migration operated at single frequency is designed, and its mathematical structure is investigated by establishing a relationship with an infinite series of Bessel functions of integer order and antenna configuration. This is based on the application of the Born approximation to the scattering parameters of small objects. The structure explains the reason for the detection of moving objects via a designed imaging function and supplies its some properties. To demonstrate the strengths and weaknesses of the proposed algorithm, various <b>simulations</b> with real-data are conducted.</p></p class="citation"></blockquote><h3 id=35--316341-performance-analysis-of-music-type-imaging-without-diagonal-elements-of-multi-static-response-matrix-won-kwang-park-2024>(3/5 | 316/341) Performance analysis of MUSIC-type imaging without diagonal elements of multi-static response matrix (Won-Kwang Park, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Won-Kwang Park. (2024)<br><strong>Performance analysis of MUSIC-type imaging without diagonal elements of multi-static response matrix</strong><br><button class=copy-to-clipboard title="Performance analysis of MUSIC-type imaging without diagonal elements of multi-static response matrix" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 78A46, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17180v1.pdf filename=2402.17180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generally, to apply the MUltiple SIgnal Classification (MUSIC) algorithm for the rapid imaging of small inhomogeneities, the complete elements of the multi-static response (MSR) matrix must be collected. However, in real-world applications such as microwave imaging or bistatic measurement configuration, diagonal elements of the MSR matrix are unknown. Nevertheless, it is possible to obtain imaging results using a traditional approach but theoretical reason of the applicability has not been investigated yet. In this paper, we establish mathematical structures of the imaging function of MUSIC from an MSR matrix without diagonal elements in both transverse magnetic (TM) and transverse electric (TE) polarizations. The established structures demonstrate why the shape of the location of small inhomogeneities can be retrieved via MUSIC without the diagonal elements of the MSR matrix. In addition, they reveal the intrinsic properties of imaging and the fundamental limitations. Results of numerical <b>simulations</b> are also provided to support the identified structures.</p></p class="citation"></blockquote><h3 id=45--317341-a-p-version-of-convolution-quadrature-in-wave-propagation-alexander-rieder-2024>(4/5 | 317/341) A $p$-version of convolution quadrature in wave propagation (Alexander Rieder, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Rieder. (2024)<br><strong>A $p$-version of convolution quadrature in wave propagation</strong><br><button class=copy-to-clipboard title="A $p$-version of convolution quadrature in wave propagation" index=317>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-317 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17712v1.pdf filename=2402.17712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a novel way of discretizing wave scattering problems using the general formalism of <b>convolution</b> quadrature, but instead of reducing the timestep size ($h$-method), we achieve accuracy by increasing the order of the method ($p$-method). We base this method on discontinuous Galerkin timestepping and use the Z-transform. We show that for a certain class of incident waves, the resulting schemes observes(root)-exponential convergence rate with respect to the number of boundary integral operators that need to be applied. Numerical experiments confirm the findings.</p></p class="citation"></blockquote><h3 id=55--318341-an-all-frequency-stable-surface-integral-equation-algorithm-for-electromagnetism-in-3-d-unbounded-penetrable-media-continuous-and-fully-discrete-model-analysis-mahadevan-ganesh-et-al-2024>(5/5 | 318/341) An all-frequency stable surface integral equation algorithm for electromagnetism in 3-D unbounded penetrable media: Continuous and fully-discrete model analysis (Mahadevan Ganesh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahadevan Ganesh, Stuart C. Hawkins, Darko Volkov. (2024)<br><strong>An all-frequency stable surface integral equation algorithm for electromagnetism in 3-D unbounded penetrable media: Continuous and fully-discrete model analysis</strong><br><button class=copy-to-clipboard title="An all-frequency stable surface integral equation algorithm for electromagnetism in 3-D unbounded penetrable media: Continuous and fully-discrete model analysis" index=318>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-318 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-AP, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17713v1.pdf filename=2402.17713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We use the time-harmonic Maxwell partial differential equations (PDEs) to model the wave propagation in 3-D space, which comprises a closed penetrable scatterer and its unbounded free-space complement. Surface integral equations (SIEs) that are equivalent to the time-harmonic Maxwell PDEs provide an efficient framework to directly model the surface electromagnetic fields and hence the RCS.The equivalent SIE system on the interface has the advantages that: (a) it avoids truncation of the unbounded region and the solution exactly satisfies the radiation condition; and (b) the surface-fields solution yields the unknowns in the Maxwell PDEs through surface potential representations of the interior and exterior fields. The Maxwell PDE system has been proven (several decades ago) to be stable for all frequencies, that is, (i) it does not possess eigenfrequencies (it is well-posed); and (ii) it does not suffer from low-frequency. However, weakly-singular SIE reformulations of the PDE satisfying these two properties, subject to a stabilization constraint, were derived and mathematically proven only about a decade ago (see {J. Math. Anal. Appl. 412 (2014) 277-300}). The aim of this article is two-fold: (I) To effect a robust coupling of the stabilization constraint to the weakly singular SIE and use mathematical analysis to establish that the resulting continuous weakly-singular second-kind self-adjoint SIE system (without constraints) retains all-frequency stability; and (II) To apply a fully-discrete spectral algorithm for the all-frequency-stable weakly-singular second-kind SIE, and prove spectral accuracy of the algorithm. We numerically demonstrate the high-order accuracy of the algorithm using several dielectric and absorbing <b>benchmark</b> scatterers with curved surfaces.</p></p class="citation"></blockquote><h2 id=csdc-3>cs.DC (3)</h2><h3 id=13--319341-massive-parallelization-and-performance-enhancement-of-an-immersed-boundary-method-based-unsteady-flow-solver-rahul-sundar-et-al-2024>(1/3 | 319/341) Massive parallelization and performance enhancement of an immersed boundary method based unsteady flow solver (Rahul Sundar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rahul Sundar, Dipanjan Majumdar, Chhote Lal Shah, Sunetra Sarkar. (2024)<br><strong>Massive parallelization and performance enhancement of an immersed boundary method based unsteady flow solver</strong><br><button class=copy-to-clipboard title="Massive parallelization and performance enhancement of an immersed boundary method based unsteady flow solver" index=319>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-319 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC, physics-comp-ph, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17337v1.pdf filename=2402.17337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-fidelity <b>simulations</b> of unsteady fluid flow are now possible with advancements in high-performance computing hardware and software frameworks. Since computational fluid dynamics (CFD) computations are dominated by linear algebraic routines, they can be significantly accelerated through massive parallelization on graphics processing units (GPUs). Thus, GPU implementation of high-fidelity CFD solvers is essential in reducing the turnaround time for quicker design space exploration. In the present work, an immersed boundary method (IBM) based in-house flow solver has been ported to the GPU using OpenACC, a compiler directive-based heterogeneous parallel programming framework. Out of various GPU porting pathways available, OpenACC was chosen because of its minimum code intrusion, low development time, and striking similarity with OpenMP, a similar directive-based shared memory programming framework. A detailed validation study and performance analysis of the parallel solver implementations on the CPU and GPU are presented. The GPU implementation shows a speedup up to the order $O(10)$ over the CPU parallel version and up to the order $O(10^2)$ over the serial code. The GPU implementation also scales well with increasing mesh size owing to the efficient utilization of the GPU processor cores.</p></p class="citation"></blockquote><h3 id=23--320341-application-of-machine-learning-optimization-in-cloud-computing-resource-scheduling-and-management-yifan-zhang-et-al-2024>(2/3 | 320/341) Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management (Yifan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Zhang, Bo Liu, Yulu Gong, Jiaxin Huang, Jingyu Xu, Weixiang Wan. (2024)<br><strong>Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management</strong><br><button class=copy-to-clipboard title="Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management" index=320>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-320 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AI, cs-DC, cs-LG, cs.DC<br>Keyword Score: 10<br>Keywords: Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17216v1.pdf filename=2402.17216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, cloud computing has been widely used. Cloud computing refers to the centralized computing resources, users through the access to the centralized resources to complete the calculation, the cloud computing center will return the results of the program processing to the user. Cloud computing is not only for individual users, but also for enterprise users. By purchasing a cloud server, users do not have to buy a large number of computers, saving computing costs. According to a report by China Economic News Network, the scale of cloud computing in China has reached 209.1 billion yuan. At present, the more mature cloud service providers in China are Ali Cloud, Baidu Cloud, Huawei Cloud and so on. Therefore, this paper proposes an innovative approach to solve complex problems in cloud computing resource scheduling and management using machine learning optimization techniques. Through in-depth study of challenges such as low resource utilization and unbalanced load in the cloud environment, this study proposes a comprehensive solution, including optimization methods such as deep learning and genetic algorithm, to improve system performance and efficiency, and thus bring new breakthroughs and progress in the field of cloud computing resource management.Rational allocation of resources plays a crucial role in cloud computing. In the resource allocation of cloud computing, the cloud computing center has limited cloud resources, and users arrive in sequence. Each user requests the cloud computing center to use a certain number of cloud resources at a specific time.</p></p class="citation"></blockquote><h3 id=33--321341-deep-reinforcement-learning-drl-based-methods-for-serverless-stream-processing-engines-a-vision-architectural-elements-and-future-directions-maria-r-read-et-al-2024>(3/3 | 321/341) Deep Reinforcement Learning (DRL)-based Methods for Serverless Stream Processing Engines: A Vision, Architectural Elements, and Future Directions (Maria R. Read et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maria R. Read, Chinmaya Dehury, Satish Narayana Srirama, Rajkumar Buyya. (2024)<br><strong>Deep Reinforcement Learning (DRL)-based Methods for Serverless Stream Processing Engines: A Vision, Architectural Elements, and Future Directions</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning (DRL)-based Methods for Serverless Stream Processing Engines: A Vision, Architectural Elements, and Future Directions" index=321>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-321 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17117v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17117v1.pdf filename=2402.17117v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Streaming applications are becoming widespread across an extensive range of business domains as an increasing number of sources continuously produce data that need to be processed and analysed in real time. Modern businesses are aggressively using streaming data to generate valuable knowledge that can be used to automate processes, help decision-making, optimize resource usage, and ultimately generate revenue for the organization. Despite their increased adoption and tangible benefits, support for the automated deployment and management of streaming applications is yet to emerge. Although a plethora of stream management systems have flooded the open source community in recent years, all of the existing frameworks demand a considerably challenging and lengthy effort from human operators to manually and continuously tune their configuration and deployment environment in order to reach and maintain the desired performance goals. To address these challenges, this article proposes a vision for creating Deep <b>Reinforcement</b> <b>Learning</b> (DRL)-based methods for transforming stream processing engines into self-managed serverless solutions. This will lead to an increase in productivity as engineers can focus on the actual development process, an increase in application performance potentially leading to reduced response times and more accurate and meaningful results, and a considerable decrease in operational costs for organizations.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--322341-image-to-mesh-conversion-for-biomedical-simulations-fotis-drakopoulos-et-al-2024>(1/1 | 322/341) Image-To-Mesh Conversion for Biomedical Simulations (Fotis Drakopoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fotis Drakopoulos, Kevin Garner, Christopher Rector, Nikos Chrisochoides. (2024)<br><strong>Image-To-Mesh Conversion for Biomedical Simulations</strong><br><button class=copy-to-clipboard title="Image-To-Mesh Conversion for Biomedical Simulations" index=322>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-322 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs-MS, cs-NA, cs.GR, math-NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18596v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18596v1.pdf filename=2402.18596v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Converting a three-dimensional medical image into a 3D mesh that satisfies both the quality and fidelity constraints of predictive <b>simulations</b> and image-guided surgical procedures remains a critical problem. Presented is an image-to-mesh conversion method called CBC3D. It first discretizes a segmented image by generating an adaptive Body-Centered Cubic (BCC) mesh of high-quality elements. Next, the tetrahedral mesh is converted into a mixed-element mesh of tetrahedra, pentahedra, and hexahedra to decrease element count while maintaining quality. Finally, the mesh surfaces are deformed to their corresponding physical image boundaries, improving the mesh&rsquo;s fidelity. The deformation scheme builds upon the ITK open-source library and is based on the concept of energy minimization, relying on a multi-material point-based registration. It uses non-connectivity patterns to implicitly control the number of extracted feature points needed for the registration and, thus, adjusts the trade-off between the achieved mesh fidelity and the deformation speed. We compare CBC3D with four widely used and state-of-the-art homegrown image-to-mesh conversion methods from industry and academia. Results indicate that the CBC3D meshes (i) achieve high fidelity, (ii) keep the element count reasonably low, and (iii) exhibit good element quality.</p></p class="citation"></blockquote><h2 id=physicscomp-ph-2>physics.comp-ph (2)</h2><h3 id=12--323341-beacon-a-lightweight-deep-reinforcement-learning-benchmark-library-for-flow-control-jonathan-viquerat-et-al-2024>(1/2 | 323/341) Beacon, a lightweight deep reinforcement learning benchmark library for flow control (Jonathan Viquerat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Viquerat, Philippe Meliga, Pablo Jeken, Elie Hachem. (2024)<br><strong>Beacon, a lightweight deep reinforcement learning benchmark library for flow control</strong><br><button class=copy-to-clipboard title="Beacon, a lightweight deep reinforcement learning benchmark library for flow control" index=323>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-323 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: cs-LG, cs-SY, eess-SY, physics-comp-ph, physics.comp-ph<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17402v1.pdf filename=2402.17402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the increasing use of deep <b>reinforcement</b> <b>learning</b> for flow control problems has led to a new area of research, focused on the coupling and the adaptation of the existing algorithms to the control of numerical fluid dynamics environments. Although still in its infancy, the field has seen multiple successes in a short time span, and its fast development pace can certainly be partly imparted to the open-source effort that drives the expansion of the community. Yet, this emerging domain still misses a common ground to (i) ensure the reproducibility of the results, and (ii) offer a proper ad-hoc <b>benchmarking</b> basis. To this end, we propose Beacon, an open-source <b>benchmark</b> library composed of seven lightweight 1D and 2D flow control problems with various characteristics, action and observation space characteristics, and CPU requirements. In this contribution, the seven considered problems are described, and reference control solutions are provided. The sources for the following work are available at <a href=https://github.com/jviquerat/beacon>https://github.com/jviquerat/beacon</a>.</p></p class="citation"></blockquote><h3 id=22--324341-thermodynamics-informed-super-resolution-of-scarce-temporal-dynamics-data-carlos-bermejo-barbanoj-et-al-2024>(2/2 | 324/341) Thermodynamics-informed super-resolution of scarce temporal dynamics data (Carlos Bermejo-Barbanoj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos Bermejo-Barbanoj, Beatriz Moya, Alberto Badías, Francisco Chinesta, Elías Cueto. (2024)<br><strong>Thermodynamics-informed super-resolution of scarce temporal dynamics data</strong><br><button class=copy-to-clipboard title="Thermodynamics-informed super-resolution of scarce temporal dynamics data" index=324>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-324 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: cs-LG, physics-comp-ph, physics.comp-ph<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17506v1.pdf filename=2402.17506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a method to increase the resolution of measurements of a physical system and subsequently predict its time evolution using thermodynamics-aware neural networks. Our method uses adversarial <b>autoencoders,</b> which reduce the dimensionality of the full order model to a set of latent variables that are enforced to match a prior, for example a normal distribution. Adversarial <b>autoencoders</b> are seen as generative models, and they can be trained to generate high-resolution samples from low-resoution inputs, meaning they can address the so-called super-resolution problem. Then, a second neural network is trained to learn the physical structure of the latent variables and predict their temporal evolution. This neural network is known as an structure-preserving neural network. It learns the metriplectic-structure of the system and applies a physical bias to ensure that the first and second principles of thermodynamics are fulfilled. The integrated trajectories are decoded to their original dimensionality, as well as to the higher dimensionality space produced by the adversarial <b>autoencoder</b> and they are compared to the ground truth solution. The method is tested with two examples of flow over a cylinder, where the fluid properties are varied between both examples.</p></p class="citation"></blockquote><h2 id=cslo-2>cs.LO (2)</h2><h3 id=12--325341-mcsat-based-finite-field-reasoning-in-the-yices2-smt-solver-thomas-hader-et-al-2024>(1/2 | 325/341) MCSat-based Finite Field Reasoning in the Yices2 SMT Solver (Thomas Hader et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Hader, Daniela Kaufmann, Ahmed Irfan, Stéphane Graham-Lengrand, Laura Kovács. (2024)<br><strong>MCSat-based Finite Field Reasoning in the Yices2 SMT Solver</strong><br><button class=copy-to-clipboard title="MCSat-based Finite Field Reasoning in the Yices2 SMT Solver" index=325>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-325 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17927v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17927v1.pdf filename=2402.17927v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This system description introduces an enhancement to the Yices2 SMT solver, enabling it to reason over non-linear polynomial systems over finite fields. Our <b>reasoning</b> approach fits into the model-constructing satisfiability (MCSat) framework and is based on zero decomposition techniques, which find finite basis explanations for theory conflicts over finite fields. As the MCSat solver within Yices2 can support (and combine) several theories via theory plugins, we implemented our <b>reasoning</b> approach as a new plugin for finite fields and extended Yices2&rsquo;s frontend to parse finite field problems, making our implementation the first MCSat-based <b>reasoning</b> engine for finite fields. We present its evaluation on finite field <b>benchmarks,</b> comparing it against cvc5. Additionally, our work leverages the modular architecture of the MCSat solver in Yices2 to provide a foundation for the rapid implementation of further <b>reasoning</b> techniques for this theory.</p></p class="citation"></blockquote><h3 id=22--326341-a-constraint-based-mathematical-modeling-library-in-prolog-with-answer-constraint-semantics-françois-fages-2024>(2/2 | 326/341) A Constraint-based Mathematical Modeling Library in Prolog with Answer Constraint Semantics (François Fages, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>François Fages. (2024)<br><strong>A Constraint-based Mathematical Modeling Library in Prolog with Answer Constraint Semantics</strong><br><button class=copy-to-clipboard title="A Constraint-based Mathematical Modeling Library in Prolog with Answer Constraint Semantics" index=326>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-326 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs-MS, cs-PL, cs.LO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17286v1.pdf filename=2402.17286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Constraint logic programming emerged in the late 80&rsquo;s as a highly declarative class of programming languages based on first-order logic and theories with decidable constraint languages, thereby subsuming Prolog restricted to equality constraints over the Herbrand&rsquo;s term domain. This approach has proven extremely successfull in solving combinatorial problems in the industry which quickly led to the development of a variety of constraint solving libraries in standard programming languages. Later came the design of a purely declarative front-end constraint-based modeling language, MiniZinc, independent of the constraint solvers, in order to compare their performances and create model <b>benchmarks.</b> Beyond that purpose, the use of a high-level modeling language such as MiniZinc to develop complete applications, or to teach constraint programming, is limited by the impossibility to program search strategies, or new constraint solvers, in a modeling language, as well as by the absence of an integrated development environment for both levels of constraint-based modeling and constraint solving. In this paper, we propose to solve those issues by taking Prolog with its constraint solving libraries, as a unified relation-based modeling and programming language. We present a Prolog library for high-level constraint-based mathematical modeling, inspired by MiniZinc, using subscripted variables (arrays) in addition to lists and terms, quantifiers and iterators in addition to recursion, together with a patch of constraint libraries in order to allow array functional notations in constraints. We show that this approach does not come with a significant computation time overhead, and presents several advantages in terms of the possibility of focussing on mathematical modeling, getting answer constraints in addition to ground solutions, programming search or constraint solvers if needed, and debugging models within a unique modeling and programming environment.</p></p class="citation"></blockquote><h2 id=cssi-3>cs.SI (3)</h2><h3 id=13--327341-gin-sd-source-detection-in-graphs-with-incomplete-nodes-via-positional-encoding-and-attentive-fusion-le-cheng-et-al-2024>(1/3 | 327/341) GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion (Le Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Le Cheng, Peican Zhu, Keke Tang, Chao Gao, Zhen Wang. (2024)<br><strong>GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion</strong><br><button class=copy-to-clipboard title="GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion" index=327>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-327 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-LG, cs-SI, cs.SI<br>Keyword Score: 13<br>Keywords: Graph, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.00014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.00014v1.pdf filename=2403.00014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Source detection in <b>graphs</b> has demonstrated robust efficacy in the domain of rumor source identification. Although recent solutions have enhanced performance by leveraging deep neural networks, they often require complete user data. In this paper, we address a more challenging task, rumor source detection with incomplete user data, and propose a novel framework, i.e., Source Detection in <b>Graphs</b> with Incomplete Nodes via Positional Encoding and Attentive Fusion (GIN-SD), to tackle this challenge. Specifically, our approach utilizes a positional embedding module to distinguish nodes that are incomplete and employs a <b>self-attention</b> mechanism to focus on nodes with greater information transmission capacity. To mitigate the prediction bias caused by the significant disparity between the numbers of source and non-source nodes, we also introduce a class-balancing mechanism. Extensive experiments validate the effectiveness of GIN-SD and its superiority to state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=23--328341-scalable-community-search-with-accuracy-guarantee-on-attributed-graphs-yuxiang-wang-et-al-2024>(2/3 | 328/341) Scalable Community Search with Accuracy Guarantee on Attributed Graphs (Yuxiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxiang Wang, Shuzhan Ye, Xiaoliang Xu, Yuxia Geng, Zhenghe Zhao, Xiangyu Ke, Tianxing Wu. (2024)<br><strong>Scalable Community Search with Accuracy Guarantee on Attributed Graphs</strong><br><button class=copy-to-clipboard title="Scalable Community Search with Accuracy Guarantee on Attributed Graphs" index=328>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-328 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-DB, cs-SI, cs.SI<br>Keyword Score: 13<br>Keywords: Graph, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17242v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17242v3.pdf filename=2402.17242v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given an attributed <b>graph</b> $G$ and a query node $q$, \underline{C}ommunity \underline{S}earch over \underline{A}ttributed \underline{G}raphs (CS-AG) aims to find a structure- and attribute-cohesive subgraph from $G$ that contains $q$. Although CS-AG has been widely studied, they still face three challenges. (1) Exact methods based on <b>graph</b> traversal are time-consuming, especially for large <b>graphs.</b> Some tailored indices can improve efficiency, but introduce nonnegligible storage and maintenance overhead. (2) Approximate methods with a loose approximation ratio only provide a coarse-grained evaluation of a community&rsquo;s quality, rather than a reliable evaluation with an accuracy guarantee in runtime. (3) Attribute cohesiveness metrics often ignores the important correlation with the query node $q$. We formally define our CS-AG problem atop a $q$-centric attribute cohesiveness metric considering both textual and numerical attributes, for $k$-core model on homogeneous <b>graphs.</b> We show the problem is NP-hard. To solve it, we first propose an exact baseline with three <b>pruning</b> strategies. Then, we propose an index-free sampling-estimation-based method to quickly return an approximate community with an accuracy guarantee, in the form of a confidence interval. Once a good result satisfying a user-desired error bound is reached, we terminate it early. We extend it to heterogeneous <b>graphs,</b> $k$-truss model, and size-bounded CS. Comprehensive experimental studies on ten real-world datasets show its superiority, e.g., at least 1.54$\times$ (41.1$\times$ on average) faster in response time and a reliable relative error (within a user-specific error bound) of attribute cohesiveness is achieved.</p></p class="citation"></blockquote><h3 id=33--329341-towards-spatiotemporal-integration-of-bus-transit-with-data-driven-approaches-júlio-borges-et-al-2024>(3/3 | 329/341) Towards spatiotemporal integration of bus transit with data-driven approaches (Júlio Borges et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Júlio Borges, Altieris M. Peixoto, Thiago H. Silva, Anelise Munaretto, Ricardo Luders. (2024)<br><strong>Towards spatiotemporal integration of bus transit with data-driven approaches</strong><br><button class=copy-to-clipboard title="Towards spatiotemporal integration of bus transit with data-driven approaches" index=329>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-329 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CE, cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17866v1.pdf filename=2402.17866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study aims to propose an approach for spatiotemporal integration of bus transit, which enables users to change bus lines by paying a single fare. This could increase bus transit efficiency and, consequently, help to make this mode of transportation more attractive. Usually, this strategy is allowed for a few hours in a non-restricted area; thus, certain walking distance areas behave like &ldquo;virtual terminals.&rdquo; For that, two data-driven algorithms are proposed in this work. First, a new algorithm for detecting itineraries based on bus GPS data and the bus stop location. The proposed algorithm&rsquo;s results show that 90% of the database detected valid itineraries by excluding invalid markings and adding times at missing bus stops through temporal interpolation. Second, this study proposes a bus stop <b>clustering</b> algorithm to define suitable areas for these virtual terminals where it would be possible to make bus transfers outside the physical terminals. Using real-world origin-destination trips, the bus network, including clusters, can reduce traveled distances by up to 50%, making twice as many connections on average.</p></p class="citation"></blockquote><h2 id=physicsoptics-1>physics.optics (1)</h2><h3 id=11--330341-scaling-on-chip-photonic-neural-processors-using-arbitrarily-programmable-wave-propagation-tatsuhiro-onodera-et-al-2024>(1/1 | 330/341) Scaling on-chip photonic neural processors using arbitrarily programmable wave propagation (Tatsuhiro Onodera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tatsuhiro Onodera, Martin M. Stein, Benjamin A. Ash, Mandar M. Sohoni, Melissa Bosch, Ryotatsu Yanagimoto, Marc Jankowski, Timothy P. McKenna, Tianyu Wang, Gennady Shvets, Maxim R. Shcherbakov, Logan G. Wright, Peter L. McMahon. (2024)<br><strong>Scaling on-chip photonic neural processors using arbitrarily programmable wave propagation</strong><br><button class=copy-to-clipboard title="Scaling on-chip photonic neural processors using arbitrarily programmable wave propagation" index=330>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-330 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.optics<br>Categories: cs-ET, cs-LG, physics-optics, physics.optics<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17750v1.pdf filename=2402.17750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>On-chip photonic processors for neural networks have potential benefits in both speed and energy efficiency but have not yet reached the scale at which they can outperform electronic processors. The dominant paradigm for designing on-chip photonics is to make networks of relatively bulky discrete components connected by one-dimensional waveguides. A far more compact alternative is to avoid explicitly defining any components and instead sculpt the continuous substrate of the photonic processor to directly perform the computation using waves freely propagating in two dimensions. We propose and demonstrate a device whose refractive index as a function of space, $n(x,z)$, can be rapidly reprogrammed, allowing arbitrary control over the wave propagation in the device. Our device, a 2D-programmable waveguide, combines photoconductive gain with the electro-optic effect to achieve massively parallel modulation of the refractive index of a slab waveguide, with an index modulation depth of $10^{-3}$ and approximately $10^4$ programmable degrees of freedom. We used a prototype device with a functional area of $12,\text{mm}^2$ to perform neural-network inference with up to 49-dimensional input vectors in a single pass, achieving 96% accuracy on vowel classification and 86% accuracy on $7 \times 7$-pixel <b>MNIST</b> handwritten-digit classification. This is a scale beyond that of previous photonic chips relying on discrete components, illustrating the benefit of the continuous-waves paradigm. In principle, with large enough chip area, the reprogrammability of the device&rsquo;s refractive index distribution enables the reconfigurable realization of any passive, linear photonic circuit or device. This promises the development of more compact and versatile photonic systems for a wide range of applications, including optical processing, smart sensing, spectroscopy, and optical communications.</p></p class="citation"></blockquote><h2 id=mathst-1>math.ST (1)</h2><h3 id=11--331341-batched-nonparametric-contextual-bandits-rong-jiang-et-al-2024>(1/1 | 331/341) Batched Nonparametric Contextual Bandits (Rong Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rong Jiang, Cong Ma. (2024)<br><strong>Batched Nonparametric Contextual Bandits</strong><br><button class=copy-to-clipboard title="Batched Nonparametric Contextual Bandits" index=331>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-331 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: cs-LG, math-ST, math.ST, stat-ML, stat-TH<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17732v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17732v1.pdf filename=2402.17732v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study nonparametric contextual <b>bandits</b> under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. We also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning. Additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting.</p></p class="citation"></blockquote><h2 id=physicschem-ph-1>physics.chem-ph (1)</h2><h3 id=11--332341-outlier-detection-for-reactive-machine-learned-potential-energy-surfaces-luis-itza-vazquez-salazar-et-al-2024>(1/1 | 332/341) Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces (Luis Itza Vazquez-Salazar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luis Itza Vazquez-Salazar, Silvan Käser, Markus Meuwly. (2024)<br><strong>Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces</strong><br><button class=copy-to-clipboard title="Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces" index=332>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-332 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.chem-ph<br>Categories: cs-LG, physics-chem-ph, physics.chem-ph<br>Keyword Score: 10<br>Keywords: Outlier Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17686v1.pdf filename=2402.17686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncertainty quantification (UQ) to detect samples with large expected errors <b>(outliers)</b> <b>is</b> applied to reactive molecular potential energy surfaces (PESs). Three methods - Ensembles, Deep Evidential Regression (DER), and Gaussian Mixture Models (GMM) - were applied to the H-transfer reaction between ${\it syn-}$Criegee and vinyl hydroxyperoxide. The results indicate that ensemble models provide the best results for detecting <b>outliers,</b> <b>followed</b> by GMM. For example, from a pool of 1000 structures with the largest uncertainty, the detection quality for <b>outliers</b> <b>is</b> $\sim 90$ % and $\sim 50$ %, respectively, if 25 or 1000 structures with large errors are sought. On the contrary, the limitations of the statistical assumptions of DER greatly impacted its prediction capabilities. Finally, a structure-based indicator was found to be correlated with large average error, which may help to rapidly classify new structures into those that provide an advantage for refining the neural network.</p></p class="citation"></blockquote><h2 id=q-biogn-1>q-bio.GN (1)</h2><h3 id=11--333341-supervised-machine-learning-for-microbiomics-bridging-the-gap-between-current-and-best-practices-natasha-k-dudek-et-al-2024>(1/1 | 333/341) Supervised machine learning for microbiomics: bridging the gap between current and best practices (Natasha K. Dudek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Natasha K. Dudek, Mariam Chakhvadze, Saba Kobakhidze, Omar Kantidze, Yuriy Gankin. (2024)<br><strong>Supervised machine learning for microbiomics: bridging the gap between current and best practices</strong><br><button class=copy-to-clipboard title="Supervised machine learning for microbiomics: bridging the gap between current and best practices" index=333>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-333 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-LG, q-bio-GN, q-bio.GN<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17621v1.pdf filename=2402.17621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning (ML) is set to accelerate innovations in clinical microbiomics, such as in disease diagnostics and prognostics. This will require high-quality, reproducible, interpretable workflows whose predictive capabilities meet or exceed the high thresholds set for clinical tools by regulatory agencies. Here, we capture a snapshot of current practices in the application of <b>supervised</b> ML to microbiomics data, through an in-depth analysis of 100 peer-reviewed journal articles published in 2021-2022. We apply a data-driven approach to steer discussion of the merits of varied approaches to experimental design, including key considerations such as how to mitigate the effects of small dataset size while avoiding data leakage. We further provide guidance on how to avoid common experimental design pitfalls that can hurt model performance, trustworthiness, and reproducibility. Discussion is accompanied by an interactive online tutorial that demonstrates foundational principles of ML experimental design, tailored to the microbiomics community. Formalizing community best practices for <b>supervised</b> ML in microbiomics is an important step towards improving the success and efficiency of clinical research, to the benefit of patients and other stakeholders.</p></p class="citation"></blockquote><h2 id=csds-3>cs.DS (3)</h2><h3 id=13--334341-purelottery-fair-and-bias-resistant-leader-election-with-a-novel-single-elimination-tournament-algorithm-jonas-ballweg-2024>(1/3 | 334/341) PureLottery: Fair and Bias-Resistant Leader Election with a Novel Single-Elimination Tournament Algorithm (Jonas Ballweg, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Ballweg. (2024)<br><strong>PureLottery: Fair and Bias-Resistant Leader Election with a Novel Single-Elimination Tournament Algorithm</strong><br><button class=copy-to-clipboard title="PureLottery: Fair and Bias-Resistant Leader Election with a Novel Single-Elimination Tournament Algorithm" index=334>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-334 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CR, cs-DC, cs-DS, cs-GT, cs.DS<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17459v1.pdf filename=2402.17459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leader Election (LE) is crucial in distributed systems and blockchain technology, ensuring one participant acts as the leader. Traditional LE methods often depend on distributed random number generation (RNG), facing issues like vulnerability to manipulation, lack of <b>fairness,</b> and the need for complex procedures such as verifiable delay functions (VDFs) and publicly-verifiable secret sharing (PVSS). This Bachelor&rsquo;s thesis presents a novel approach to randomized LE, leveraging a game-theoretic assumption that participants, aiming to be chosen as leaders, will naturally avoid actions that diminish their chances. This perspective simplifies LE by eliminating the need for decentralized RNG. Introducing PureLottery, inspired by single-elimination sports tournaments, this method offers a fair, bias-resistant, and efficient LE solution for blockchain environments. It operates on the principle of two participants competing in each match, rendering collusion efforts useless. PureLottery stands out for its low computational and communication complexity, suitable for smart contract implementation. It provides strong game-theoretic incentives for honesty and is robust against adversaries, ensuring no increase in election chances through dishonesty. The protocol guarantees that each honest player has at least a 1/n chance of winning, irrespective of adversary manipulation among the other n-1 participants. PureLottery can also address related problems like participant ranking, electing multiple leaders, and leader aversion, showcasing its versatility across various applications, including lotteries and blockchain protocols. An open-source implementation is made available for public use.</p></p class="citation"></blockquote><h3 id=23--335341-scalable-identification-of-minimum-undesignable-rna-motifs-on-loop-pair-graphs-tianshuo-zhou-et-al-2024>(2/3 | 335/341) Scalable Identification of Minimum Undesignable RNA Motifs on Loop-Pair Graphs (Tianshuo Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianshuo Zhou, Wei Yu Tang, David H. Mathews, Liang Huang. (2024)<br><strong>Scalable Identification of Minimum Undesignable RNA Motifs on Loop-Pair Graphs</strong><br><button class=copy-to-clipboard title="Scalable Identification of Minimum Undesignable RNA Motifs on Loop-Pair Graphs" index=335>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-335 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17206v1.pdf filename=2402.17206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivation: RNA design aims to find at least one sequence that folds with the highest probability into a designated target structure, but some structures are undesignable in the sense that no sequence folds into them. Identifying undesignable structures is useful in delineating and understanding the limit of RNA designability, but has received little attention until recently. In addition, existing methods on undesignability are not scalable and not interpretable. Results: We introduce a novel <b>graph</b> representation and a new general algorithmic framework to efficiently identify undesignable motifs in a secondary structure. The proposed algorithm enumerates minimal motifs based on the loop-pair <b>graph</b> representation of a structure and establishes the undesignability of a motif by proposing rival substructure(s). Our work can also identify unique minimum undesignable motifs across different structures. Our implemented algorithms successfully identify 26 unique minimum undesignable motifs among 18 undesignable puzzles from the <b>benchmark</b> Eterna100. Additionally, our algorithm is so efficient that it scales to natural structures of 16S and 23S Ribosomal RNAs (about 1,500 and 3,000 nucleotides, resp.), and finds all of those structures in the widely used ArchiveII database to be undesignable, with 73 unique minimum undesignable motifs, under the standard Turner energy model in ViennaRNA.</p></p class="citation"></blockquote><h3 id=33--336341-learning-based-algorithms-for-graph-searching-problems-adela-frances-depavia-et-al-2024>(3/3 | 336/341) Learning-Based Algorithms for Graph Searching Problems (Adela Frances DePavia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adela Frances DePavia, Erasmo Tani, Ali Vakilian. (2024)<br><strong>Learning-Based Algorithms for Graph Searching Problems</strong><br><button class=copy-to-clipboard title="Learning-Based Algorithms for Graph Searching Problems" index=336>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-336 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-AI, cs-DS, cs-LG, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17736v1.pdf filename=2402.17736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of <b>graph</b> searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) <b>graph</b> $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown <b>graphs.</b> We establish the first formal guarantees on unknown weighted <b>graphs</b> and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et al. (2022) for the case of searching on a known <b>graph,</b> and establish new lower bounds for this setting.</p></p class="citation"></blockquote><h2 id=econth-1>econ.TH (1)</h2><h3 id=11--337341-generative-ai-and-copyright-a-dynamic-perspective-s-alex-yang-et-al-2024>(1/1 | 337/341) Generative AI and Copyright: A Dynamic Perspective (S. Alex Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. Alex Yang, Angela Huyue Zhang. (2024)<br><strong>Generative AI and Copyright: A Dynamic Perspective</strong><br><button class=copy-to-clipboard title="Generative AI and Copyright: A Dynamic Perspective" index=337>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-337 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.TH<br>Categories: cs-AI, econ-TH, econ.TH<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17801v1.pdf filename=2402.17801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of <b>generative</b> <b>AI</b> is poised to disrupt the creative industry. Amidst the immense excitement for this new technology, its future development and applications in the creative industry hinge crucially upon two copyright issues: 1) the compensation to creators whose content has been used to train <b>generative</b> <b>AI</b> models (the fair use standard); and 2) the eligibility of AI-generated content for copyright protection (AI-copyrightability). While both issues have ignited heated debates among academics and practitioners, most analysis has focused on their challenges posed to existing copyright doctrines. In this paper, we aim to better understand the economic implications of these two regulatory issues and their interactions. By constructing a dynamic model with endogenous content creation and AI model development, we unravel the impacts of the fair use standard and AI-copyrightability on AI development, AI company profit, creators income, and consumer welfare, and how these impacts are influenced by various economic and operational factors. For example, while generous fair use (use data for AI training without compensating the creator) benefits all parties when abundant training data exists, it can hurt creators and consumers when such data is scarce. Similarly, stronger AI-copyrightability (AI content enjoys more copyright protection) could hinder AI development and reduce social welfare. Our analysis also highlights the complex interplay between these two copyright issues. For instance, when existing training data is scarce, generous fair use may be preferred only when AI-copyrightability is weak. Our findings underscore the need for policymakers to embrace a dynamic, context-specific approach in making regulatory decisions and provide insights for business leaders navigating the complexities of the global regulatory environment.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--338341-geometric-deep-learning-for-computer-aided-design-a-survey-negar-heidari-et-al-2024>(1/1 | 338/341) Geometric Deep Learning for Computer-Aided Design: A Survey (Negar Heidari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Negar Heidari, Alexandros Iosifidis. (2024)<br><strong>Geometric Deep Learning for Computer-Aided Design: A Survey</strong><br><button class=copy-to-clipboard title="Geometric Deep Learning for Computer-Aided Design: A Survey" index=338>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-338 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs-LG, cs.CG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17695v1.pdf filename=2402.17695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Geometric Deep Learning techniques have become a transformative force in the field of Computer-Aided Design (CAD), and have the potential to revolutionize how designers and engineers approach and enhance the design process. By harnessing the power of machine learning-based methods, CAD designers can optimize their workflows, save time and effort while making better informed decisions, and create designs that are both innovative and practical. The ability to process the CAD designs represented by geometric data and to analyze their encoded features enables the identification of similarities among diverse CAD models, the proposition of alternative designs and enhancements, and even the generation of novel design alternatives. This survey offers a comprehensive overview of learning-based methods in computer-aided design across various categories, including similarity analysis and retrieval, 2D and 3D CAD model synthesis, and CAD generation from point clouds. Additionally, it provides a complete list of <b>benchmark</b> datasets and their characteristics, along with open-source codes that have propelled research in this domain. The final discussion delves into the challenges prevalent in this field, followed by potential future research directions in this rapidly evolving field.</p></p class="citation"></blockquote><h2 id=csdb-2>cs.DB (2)</h2><h3 id=12--339341-graphmatch-subgraph-query-processing-on-fpgas-jonas-dann-et-al-2024>(1/2 | 339/341) GraphMatch: Subgraph Query Processing on FPGAs (Jonas Dann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, Holger Fröning. (2024)<br><strong>GraphMatch: Subgraph Query Processing on FPGAs</strong><br><button class=copy-to-clipboard title="GraphMatch: Subgraph Query Processing on FPGAs" index=339>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-339 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AR, cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17559v1.pdf filename=2402.17559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficiently finding subgraph embeddings in large <b>graphs</b> is crucial for many application areas like biology and social network analysis. Set intersections are the predominant and most challenging aspect of current join-based subgraph query processing systems for CPUs. Previous work has shown the viability of utilizing FPGAs for acceleration of <b>graph</b> and join processing. In this work, we propose GraphMatch, the first genearl-purpose stand-alone subgraph query processing accelerator based on worst-case optimal joins (WCOJ) that is fully designed for modern, field programmable gate array (FPGA) hardware. For efficient processing of various <b>graph</b> data sets and query <b>graph</b> patterns, it leverages a novel set intersection approach, called AllCompare, tailor-made for FPGAs. We show that this set intersection approach efficiently solves multi-set intersections in subgraph query processing, superior to CPU-based approaches. Overall, GraphMatch achieves a speedup of over 2.68x and 5.16x, compared to the state-of-the-art systems GraphFlow and RapidMatch, respectively.</p></p class="citation"></blockquote><h3 id=22--340341-metasql-a-generate-then-rank-framework-for-natural-language-to-sql-translation-yuankai-fan-et-al-2024>(2/2 | 340/341) Metasql: A Generate-then-Rank Framework for Natural Language to SQL Translation (Yuankai Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuankai Fan, Zhenying He, Tonghui Ren, Can Huang, Yinan Jing, Kai Zhang, X. Sean Wang. (2024)<br><strong>Metasql: A Generate-then-Rank Framework for Natural Language to SQL Translation</strong><br><button class=copy-to-clipboard title="Metasql: A Generate-then-Rank Framework for Natural Language to SQL Translation" index=340>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-340 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17144v1.pdf filename=2402.17144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Natural Language Interface to Databases (NLIDB) empowers non-technical users with database access through intuitive natural language (NL) interactions. Advanced approaches, utilizing neural sequence-to-sequence models or large-scale language models, typically employ auto-regressive decoding to generate unique SQL queries sequentially. While these translation models have greatly improved the overall translation accuracy, surpassing 70% on NLIDB <b>benchmarks,</b> the use of auto-regressive decoding to generate single SQL queries may result in sub-optimal outputs, potentially leading to erroneous translations. In this paper, we propose Metasql, a unified generate-then-rank framework that can be flexibly incorporated with existing NLIDBs to consistently improve their translation accuracy. Metasql introduces query metadata to control the generation of better SQL query candidates and uses learning-to-rank algorithms to retrieve globally optimized queries. Specifically, Metasql first breaks down the meaning of the given NL query into a set of possible query metadata, representing the basic concepts of the semantics. These metadata are then used as language constraints to steer the underlying translation model toward generating a set of candidate SQL queries. Finally, Metasql ranks the candidates to identify the best matching one for the given NL query. Extensive experiments are performed to study Metasql on two public NLIDB <b>benchmarks.</b> The results show that the performance of the translation models can be effectively improved using Metasql.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--341341-minimum-length-word-representants-of-graph-products-eshwar-srinivasan-et-al-2024>(1/1 | 341/341) Minimum length word-representants of graph products (Eshwar Srinivasan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eshwar Srinivasan, Ramesh Hariharasubramanian. (2024)<br><strong>Minimum length word-representants of graph products</strong><br><button class=copy-to-clipboard title="Minimum length word-representants of graph products" index=341>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-341 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17105v1.pdf filename=2402.17105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A <b>graph</b> $G = (V, E)$ is said to be word-representable if a word $w$ can be formed using the letters of the alphabet $V$ such that for every pair of vertices $x$ and $y$, $xy \in E$ if and only if $x$ and $y$ alternate in $w$. Gaetz and Ji have recently introduced the notion of minimum length word-representants for word-representable <b>graphs.</b> They have also determined the minimum possible length of the word-representants for certain classes of <b>graphs,</b> such as trees and cycles. It is know that Cartesian and Rooted products preserve word-representability. Moreover, Broere constructed a uniform word representing the Cartesian product of $G$ and $K_n$ using occurrence based functions. In this paper, we study the minimum length of word-representants for Cartesian and Rooted products using morphism and occurrence based function, respectively. Also, we solve an open problem posed by Broere in his master thesis. This problem asks to construct a word for the Cartesian product of two arbitrary word-representable <b>graphs.</b></p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.28</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.01</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-68>cs.CL (68)</a><ul><li><a href=#168--1341-a-language-model-based-framework-for-new-concept-placement-in-ontologies-hang-dong-et-al-2024>(1/68 | 1/341) A Language Model based Framework for New Concept Placement in Ontologies (Hang Dong et al., 2024)</a></li><li><a href=#268--2341-reasoning-in-conversation-solving-subjective-tasks-through-dialogue-simulation-for-large-language-models-xiaolong-wang-et-al-2024>(2/68 | 2/341) Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models (Xiaolong Wang et al., 2024)</a></li><li><a href=#368--3341-beyond-the-known-investigating-llms-performance-on-out-of-domain-intent-detection-pei-wang-et-al-2024>(3/68 | 3/341) Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection (Pei Wang et al., 2024)</a></li><li><a href=#468--4341-deep-learning-detection-method-for-large-language-models-generated-scientific-content-bushra-alhijawi-et-al-2024>(4/68 | 4/341) Deep Learning Detection Method for Large Language Models-Generated Scientific Content (Bushra Alhijawi et al., 2024)</a></li><li><a href=#568--5341-evaluating-very-long-term-conversational-memory-of-llm-agents-adyasha-maharana-et-al-2024>(5/68 | 5/341) Evaluating Very Long-Term Conversational Memory of LLM Agents (Adyasha Maharana et al., 2024)</a></li><li><a href=#668--6341-deep-learning-based-named-entity-recognition-models-for-recipes-mansi-goel-et-al-2024>(6/68 | 6/341) Deep Learning Based Named Entity Recognition Models for Recipes (Mansi Goel et al., 2024)</a></li><li><a href=#768--7341-mathsensei-a-tool-augmented-large-language-model-for-mathematical-reasoning-debrup-das-et-al-2024>(7/68 | 7/341) MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning (Debrup Das et al., 2024)</a></li><li><a href=#868--8341-benchmarking-gpt-4-on-algorithmic-problems-a-systematic-evaluation-of-prompting-strategies-flavio-petruzzellis-et-al-2024>(8/68 | 8/341) Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies (Flavio Petruzzellis et al., 2024)</a></li><li><a href=#968--9341-can-llm-generate-culturally-relevant-commonsense-qa-data-case-study-in-indonesian-and-sundanese-rifki-afina-putri-et-al-2024>(9/68 | 9/341) Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese (Rifki Afina Putri et al., 2024)</a></li><li><a href=#1068--10341-self-refinement-of-language-models-from-external-proxy-metrics-feedback-keshav-ramji-et-al-2024>(10/68 | 10/341) Self-Refinement of Language Models from External Proxy Metrics Feedback (Keshav Ramji et al., 2024)</a></li><li><a href=#1168--11341-rear-a-relevance-aware-retrieval-augmented-framework-for-open-domain-question-answering-yuhao-wang-et-al-2024>(11/68 | 11/341) REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering (Yuhao Wang et al., 2024)</a></li><li><a href=#1268--12341-enhancing-eeg-to-text-decoding-through-transferable-representations-from-pre-trained-contrastive-eeg-text-masked-autoencoder-jiaqi-wang-et-al-2024>(12/68 | 12/341) Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder (Jiaqi Wang et al., 2024)</a></li><li><a href=#1368--13341-when-scaling-meets-llm-finetuning-the-effect-of-data-model-and-finetuning-method-biao-zhang-et-al-2024>(13/68 | 13/341) When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method (Biao Zhang et al., 2024)</a></li><li><a href=#1468--14341-blendsql-a-scalable-dialect-for-unifying-hybrid-question-answering-in-relational-algebra-parker-glenn-et-al-2024>(14/68 | 14/341) BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra (Parker Glenn et al., 2024)</a></li><li><a href=#1568--15341-training-free-long-context-scaling-of-large-language-models-chenxin-an-et-al-2024>(15/68 | 15/341) Training-Free Long-Context Scaling of Large Language Models (Chenxin An et al., 2024)</a></li><li><a href=#1668--16341-sofa-shielded-on-the-fly-alignment-via-priority-rule-following-xinyu-lu-et-al-2024>(16/68 | 16/341) SoFA: Shielded On-the-fly Alignment via Priority Rule Following (Xinyu Lu et al., 2024)</a></li><li><a href=#1768--17341-comparing-effectiveness-of-regularization-methods-on-text-classification-simple-and-complex-model-in-data-shortage-situation-jongga-lee-et-al-2024>(17/68 | 17/341) Comparing effectiveness of regularization methods on text classification: Simple and complex model in data shortage situation (Jongga Lee et al., 2024)</a></li><li><a href=#1868--18341-can-gpt-4-identify-propaganda-annotation-and-detection-of-propaganda-spans-in-news-articles-maram-hasanain-et-al-2024>(18/68 | 18/341) Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles (Maram Hasanain et al., 2024)</a></li><li><a href=#1968--19341-consistency-matters-explore-llms-consistency-from-a-black-box-perspective-fufangchen-zhao-et-al-2024>(19/68 | 19/341) Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective (Fufangchen Zhao et al., 2024)</a></li><li><a href=#2068--20341-researchy-questions-a-dataset-of-multi-perspective-decompositional-questions-for-llm-web-agents-corby-rosset-et-al-2024>(20/68 | 20/341) Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents (Corby Rosset et al., 2024)</a></li><li><a href=#2168--21341-multitask-multilingual-model-adaptation-with-featurized-low-rank-mixtures-chu-cheng-lin-et-al-2024>(21/68 | 21/341) Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures (Chu-Cheng Lin et al., 2024)</a></li><li><a href=#2268--22341-jmlr-joint-medical-llm-and-retrieval-training-for-enhancing-reasoning-and-professional-question-answering-capability-junda-wang-et-al-2024>(22/68 | 22/341) JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability (Junda Wang et al., 2024)</a></li><li><a href=#2368--23341-follow-my-instruction-and-spill-the-beans-scalable-data-extraction-from-retrieval-augmented-generation-systems-zhenting-qi-et-al-2024>(23/68 | 23/341) Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems (Zhenting Qi et al., 2024)</a></li><li><a href=#2468--24341-massive-activations-in-large-language-models-mingjie-sun-et-al-2024>(24/68 | 24/341) Massive Activations in Large Language Models (Mingjie Sun et al., 2024)</a></li><li><a href=#2568--25341-ambignlg-addressing-task-ambiguity-in-instruction-for-nlg-ayana-niwa-et-al-2024>(25/68 | 25/341) AmbigNLG: Addressing Task Ambiguity in Instruction for NLG (Ayana Niwa et al., 2024)</a></li><li><a href=#2668--26341-nextlevelbert-investigating-masked-language-modeling-with-higher-level-representations-for-long-documents-tamara-czinczoll-et-al-2024>(26/68 | 26/341) NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents (Tamara Czinczoll et al., 2024)</a></li><li><a href=#2768--27341-prescribing-large-language-models-for-perioperative-care-whats-the-right-dose-for-pre-trained-models-bing-xue-et-al-2024>(27/68 | 27/341) Prescribing Large Language Models for Perioperative Care: What&rsquo;s The Right Dose for Pre-trained Models? (Bing Xue et al., 2024)</a></li><li><a href=#2868--28341-fact-and-reflection-far-improves-confidence-calibration-of-large-language-models-xinran-zhao-et-al-2024>(28/68 | 28/341) Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models (Xinran Zhao et al., 2024)</a></li><li><a href=#2968--29341-measuring-vision-language-stem-skills-of-neural-models-jianhao-shen-et-al-2024>(29/68 | 29/341) Measuring Vision-Language STEM Skills of Neural Models (Jianhao Shen et al., 2024)</a></li><li><a href=#3068--30341-are-llms-capable-of-data-based-statistical-and-causal-reasoning-benchmarking-advanced-quantitative-reasoning-with-data-xiao-liu-et-al-2024>(30/68 | 30/341) Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data (Xiao Liu et al., 2024)</a></li><li><a href=#3168--31341-investigating-continual-pretraining-in-large-language-models-insights-and-implications-çağatay-yıldız-et-al-2024>(31/68 | 31/341) Investigating Continual Pretraining in Large Language Models: Insights and Implications (Çağatay Yıldız et al., 2024)</a></li><li><a href=#3268--32341-the-era-of-1-bit-llms-all-large-language-models-are-in-158-bits-shuming-ma-et-al-2024>(32/68 | 32/341) The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits (Shuming Ma et al., 2024)</a></li><li><a href=#3368--33341-recost-external-knowledge-guided-data-efficient-instruction-tuning-qi-zhang-et-al-2024>(33/68 | 33/341) RECOST: External Knowledge Guided Data-efficient Instruction Tuning (Qi Zhang et al., 2024)</a></li><li><a href=#3468--34341-mini-ensemble-low-rank-adapters-for-parameter-efficient-fine-tuning-pengjie-ren-et-al-2024>(34/68 | 34/341) Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning (Pengjie Ren et al., 2024)</a></li><li><a href=#3568--35341-creating-suspenseful-stories-iterative-planning-with-large-language-models-kaige-xie-et-al-2024>(35/68 | 35/341) Creating Suspenseful Stories: Iterative Planning with Large Language Models (Kaige Xie et al., 2024)</a></li><li><a href=#3668--36341-large-language-modelsllms-on-tabular-data-prediction-generation-and-understanding----a-survey-xi-fang-et-al-2024>(36/68 | 36/341) Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding &ndash; A Survey (Xi Fang et al., 2024)</a></li><li><a href=#3768--37341-unleashing-the-potential-of-large-language-models-as-prompt-optimizers-an-analogical-analysis-with-gradient-based-model-optimizers-xinyu-tang-et-al-2024>(37/68 | 37/341) Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers (Xinyu Tang et al., 2024)</a></li><li><a href=#3868--38341-predict-the-next-word-humans-exhibit-uncertainty-in-this-task-and-language-models-_____-evgenia-ilia-et-al-2024>(38/68 | 38/341) Predict the Next Word: <humans exhibit uncertainty in this task and language models _____>(Evgenia Ilia et al., 2024)</a></li><li><a href=#3968--39341-acquiring-linguistic-knowledge-from-multimodal-input-theodor-amariucai-et-al-2024>(39/68 | 39/341) Acquiring Linguistic Knowledge from Multimodal Input (Theodor Amariucai et al., 2024)</a></li><li><a href=#4068--40341-probing-multimodal-large-language-models-for-global-and-local-semantic-representation-mingxu-tao-et-al-2024>(40/68 | 40/341) Probing Multimodal Large Language Models for Global and Local Semantic Representation (Mingxu Tao et al., 2024)</a></li><li><a href=#4168--41341-stable-lm-2-16b-technical-report-marco-bellagente-et-al-2024>(41/68 | 41/341) Stable LM 2 1.6B Technical Report (Marco Bellagente et al., 2024)</a></li><li><a href=#4268--42341-tower-an-open-multilingual-large-language-model-for-translation-related-tasks-duarte-m-alves-et-al-2024>(42/68 | 42/341) Tower: An Open Multilingual Large Language Model for Translation-Related Tasks (Duarte M. Alves et al., 2024)</a></li><li><a href=#4368--43341-truthx-alleviating-hallucinations-by-editing-large-language-models-in-truthful-space-shaolei-zhang-et-al-2024>(43/68 | 43/341) TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space (Shaolei Zhang et al., 2024)</a></li><li><a href=#4468--44341-exploiting-emotion-semantic-correlations-for-empathetic-response-generation-zhou-yang-et-al-2024>(44/68 | 44/341) Exploiting Emotion-Semantic Correlations for Empathetic Response Generation (Zhou Yang et al., 2024)</a></li><li><a href=#4568--45341-skt5scisumm----a-hybrid-generative-approach-for-multi-document-scientific-summarization-huy-quoc-to-et-al-2024>(45/68 | 45/341) SKT5SciSumm &ndash; A Hybrid Generative Approach for Multi-Document Scientific Summarization (Huy Quoc To et al., 2024)</a></li><li><a href=#4668--46341-llm-resistant-math-word-problem-generation-via-adversarial-attacks-roy-xie-et-al-2024>(46/68 | 46/341) LLM-Resistant Math Word Problem Generation via Adversarial Attacks (Roy Xie et al., 2024)</a></li><li><a href=#4768--47341-beyond-prompt-brittleness-evaluating-the-reliability-and-consistency-of-political-worldviews-in-llms-tanise-ceron-et-al-2024>(47/68 | 47/341) Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs (Tanise Ceron et al., 2024)</a></li><li><a href=#4868--48341-an-effective-mixture-of-experts-approach-for-code-switching-speech-recognition-leveraging-encoder-disentanglement-tzu-ting-yang-et-al-2024>(48/68 | 48/341) An Effective Mixture-Of-Experts Approach For Code-Switching Speech Recognition Leveraging Encoder Disentanglement (Tzu-Ting Yang et al., 2024)</a></li><li><a href=#4968--49341-extreme-encoder-output-frame-rate-reduction-improving-computational-latencies-of-large-end-to-end-models-rohit-prabhavalkar-et-al-2024>(49/68 | 49/341) Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models (Rohit Prabhavalkar et al., 2024)</a></li><li><a href=#5068--50341-unsupervised-multiple-choices-question-answering-via-universal-corpus-qin-zhang-et-al-2024>(50/68 | 50/341) Unsupervised multiple choices question answering via universal corpus (Qin Zhang et al., 2024)</a></li><li><a href=#5168--51341-fine-grained-natural-language-inference-based-faithfulness-evaluation-for-diverse-summarisation-tasks-huajian-zhang-et-al-2024>(51/68 | 51/341) Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks (Huajian Zhang et al., 2024)</a></li><li><a href=#5268--52341-kodialogbench-evaluating-conversational-understanding-of-language-models-with-korean-dialogue-benchmark-seongbo-jang-et-al-2024>(52/68 | 52/341) KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark (Seongbo Jang et al., 2024)</a></li><li><a href=#5368--53341-re-ex-revising-after-explanation-reduces-the-factual-errors-in-llm-responses-juyeon-kim-et-al-2024>(53/68 | 53/341) Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses (Juyeon Kim et al., 2024)</a></li><li><a href=#5468--54341-neural-automated-writing-evaluation-with-corrective-feedback-izia-xiaoxiao-wang-et-al-2024>(54/68 | 54/341) Neural Automated Writing Evaluation with Corrective Feedback (Izia Xiaoxiao Wang et al., 2024)</a></li><li><a href=#5568--55341-linguistic-knowledge-can-enhance-encoder-decoder-models-if-you-let-it-alessio-miaschi-et-al-2024>(55/68 | 55/341) Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It) (Alessio Miaschi et al., 2024)</a></li><li><a href=#5668--56341-latent-attention-for-linear-time-transformers-rares-dolga-et-al-2024>(56/68 | 56/341) Latent Attention for Linear Time Transformers (Rares Dolga et al., 2024)</a></li><li><a href=#5768--57341-extreme-miscalibration-and-the-illusion-of-adversarial-robustness-vyas-raina-et-al-2024>(57/68 | 57/341) Extreme Miscalibration and the Illusion of Adversarial Robustness (Vyas Raina et al., 2024)</a></li><li><a href=#5868--58341-fairbelief----assessing-harmful-beliefs-in-language-models-mattia-setzu-et-al-2024>(58/68 | 58/341) FairBelief &ndash; Assessing Harmful Beliefs in Language Models (Mattia Setzu et al., 2024)</a></li><li><a href=#5968--59341-llmguard-guarding-against-unsafe-llm-behavior-shubh-goyal-et-al-2024>(59/68 | 59/341) LLMGuard: Guarding Against Unsafe LLM Behavior (Shubh Goyal et al., 2024)</a></li><li><a href=#6068--60341-speak-out-of-turn-safety-vulnerability-of-large-language-models-in-multi-turn-dialogue-zhenhong-zhou-et-al-2024>(60/68 | 60/341) Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue (Zhenhong Zhou et al., 2024)</a></li><li><a href=#6168--61341-from-text-segmentation-to-smart-chaptering-a-novel-benchmark-for-structuring-video-transcriptions-fabian-retkowski-et-al-2024>(61/68 | 61/341) From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions (Fabian Retkowski et al., 2024)</a></li><li><a href=#6268--62341-clustering-document-parts-detecting-and-characterizing-influence-campaigns-from-documents-zhengxiang-wang-et-al-2024>(62/68 | 62/341) Clustering Document Parts: Detecting and Characterizing Influence Campaigns From Documents (Zhengxiang Wang et al., 2024)</a></li><li><a href=#6368--63341-information-flow-routes-automatically-interpreting-language-models-at-scale-javier-ferrando-et-al-2024>(63/68 | 63/341) Information Flow Routes: Automatically Interpreting Language Models at Scale (Javier Ferrando et al., 2024)</a></li><li><a href=#6468--64341-towards-optimal-learning-of-language-models-yuxian-gu-et-al-2024>(64/68 | 64/341) Towards Optimal Learning of Language Models (Yuxian Gu et al., 2024)</a></li><li><a href=#6568--65341-retrieval-is-accurate-generation-bowen-cao-et-al-2024>(65/68 | 65/341) Retrieval is Accurate Generation (Bowen Cao et al., 2024)</a></li><li><a href=#6668--66341-spot-the-bot-coarse-grained-partition-of-semantic-paths-for-bots-and-humans-vasilii-a-gromov-et-al-2024>(66/68 | 66/341) Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and Humans (Vasilii A. Gromov et al., 2024)</a></li><li><a href=#6768--67341-a-dataset-for-metaphor-detection-in-early-medieval-hebrew-poetry-michael-toker-et-al-2024>(67/68 | 67/341) A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry (Michael Toker et al., 2024)</a></li><li><a href=#6868--68341-ravel-evaluating-interpretability-methods-on-disentangling-language-model-representations-jing-huang-et-al-2024>(68/68 | 68/341) RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations (Jing Huang et al., 2024)</a></li></ul></li><li><a href=#csir-8>cs.IR (8)</a><ul><li><a href=#18--69341-promptmm-multi-modal-knowledge-distillation-for-recommendation-with-prompt-tuning-wei-wei-et-al-2024>(1/8 | 69/341) PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning (Wei Wei et al., 2024)</a></li><li><a href=#28--70341-bases-large-scale-web-search-user-simulation-with-large-language-model-based-agents-ruiyang-ren-et-al-2024>(2/8 | 70/341) BASES: Large-scale Web Search User Simulation with Large Language Model based Agents (Ruiyang Ren et al., 2024)</a></li><li><a href=#38--71341-bivrec-bidirectional-view-based-multimodal-sequential-recommendation-jiaxi-hu-et-al-2024>(3/8 | 71/341) BiVRec: Bidirectional View-based Multimodal Sequential Recommendation (Jiaxi Hu et al., 2024)</a></li><li><a href=#48--72341-re-modeling-personalized-item-frequency-information-for-next-basket-recommendation-sławomir-garcarz-et-al-2024>(4/8 | 72/341) [RE] Modeling Personalized Item Frequency Information for Next-basket Recommendation (Sławomir Garcarz et al., 2024)</a></li><li><a href=#58--73341-natural-language-processing-methods-for-symbolic-music-generation-and-information-retrieval-a-survey-dinh-viet-toan-le-et-al-2024>(5/8 | 73/341) Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey (Dinh-Viet-Toan Le et al., 2024)</a></li><li><a href=#68--74341-difashion-towards-personalized-outfit-generation-and-recommendation-yiyan-xu-et-al-2024>(6/8 | 74/341) DiFashion: Towards Personalized Outfit Generation and Recommendation (Yiyan Xu et al., 2024)</a></li><li><a href=#78--75341-multimodal-learned-sparse-retrieval-with-probabilistic-expansion-control-thong-nguyen-et-al-2024>(7/8 | 75/341) Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control (Thong Nguyen et al., 2024)</a></li><li><a href=#88--76341-side-information-driven-session-based-recommendation-a-survey-xiaokun-zhang-et-al-2024>(8/8 | 76/341) Side Information-Driven Session-based Recommendation: A Survey (Xiaokun Zhang et al., 2024)</a></li></ul></li><li><a href=#cscv-80>cs.CV (80)</a><ul><li><a href=#180--77341-vision-transformers-with-natural-language-semantics-young-kyung-kim-et-al-2024>(1/80 | 77/341) Vision Transformers with Natural Language Semantics (Young Kyung Kim et al., 2024)</a></li><li><a href=#280--78341-fedlppa-learning-personalized-prompt-and-aggregation-for-federated-weakly-supervised-medical-image-segmentation-li-lin-et-al-2024>(2/80 | 78/341) FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation (Li Lin et al., 2024)</a></li><li><a href=#380--79341-lspt-long-term-spatial-prompt-tuning-for-visual-representation-learning-shentong-mo-et-al-2024>(3/80 | 79/341) LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning (Shentong Mo et al., 2024)</a></li><li><a href=#480--80341-a-large-scale-evaluation-of-pretraining-paradigms-for-the-detection-of-defects-in-electroluminescence-solar-cell-images-david-torpey-et-al-2024>(4/80 | 80/341) A Large-scale Evaluation of Pretraining Paradigms for the Detection of Defects in Electroluminescence Solar Cell Images (David Torpey et al., 2024)</a></li><li><a href=#580--81341-shapellm-universal-3d-object-understanding-for-embodied-interaction-zekun-qi-et-al-2024>(5/80 | 81/341) ShapeLLM: Universal 3D Object Understanding for Embodied Interaction (Zekun Qi et al., 2024)</a></li><li><a href=#680--82341-vital-an-advanced-framework-for-automated-plant-disease-identification-in-leaf-images-using-vision-transformers-and-linear-projection-for-feature-reduction-abhishek-sebastian-et-al-2024>(6/80 | 82/341) ViTaL: An Advanced Framework for Automated Plant Disease Identification in Leaf Images Using Vision Transformers and Linear Projection For Feature Reduction (Abhishek Sebastian et al., 2024)</a></li><li><a href=#780--83341-vcd-knowledge-base-guided-visual-commonsense-discovery-in-images-xiangqing-shen-et-al-2024>(7/80 | 83/341) VCD: Knowledge Base Guided Visual Commonsense Discovery in Images (Xiangqing Shen et al., 2024)</a></li><li><a href=#880--84341-a-novel-image-space-formalism-of-fourier-domain-interpolation-neural-networks-for-noise-propagation-analysis-peter-dawood-et-al-2024>(8/80 | 84/341) A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis (Peter Dawood et al., 2024)</a></li><li><a href=#980--85341-carzero-cross-attention-alignment-for-radiology-zero-shot-classification-haoran-lai-et-al-2024>(9/80 | 85/341) CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification (Haoran Lai et al., 2024)</a></li><li><a href=#1080--86341-video-as-the-new-language-for-real-world-decision-making-sherry-yang-et-al-2024>(10/80 | 86/341) Video as the New Language for Real-World Decision Making (Sherry Yang et al., 2024)</a></li><li><a href=#1180--87341-an-empirical-study-of-the-generalization-ability-of-lidar-3d-object-detectors-to-unseen-domains-george-eskandar-et-al-2024>(11/80 | 87/341) An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains (George Eskandar et al., 2024)</a></li><li><a href=#1280--88341-demonstrating-and-reducing-shortcuts-in-vision-language-representation-learning-maurits-bleeker-et-al-2024>(12/80 | 88/341) Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning (Maurits Bleeker et al., 2024)</a></li><li><a href=#1380--89341-divavatar-diverse-3d-avatar-generation-with-a-single-prompt-weijing-tao-et-al-2024>(13/80 | 89/341) DivAvatar: Diverse 3D Avatar Generation with a Single Prompt (Weijing Tao et al., 2024)</a></li><li><a href=#1480--90341-reprune-channel-pruning-via-kernel-representative-selection-mincheol-park-et-al-2024>(14/80 | 90/341) REPrune: Channel Pruning via Kernel Representative Selection (Mincheol Park et al., 2024)</a></li><li><a href=#1580--91341-arcsin-adaptive-ranged-cosine-similarity-injected-noise-for-language-driven-visual-tasks-yang-liu-et-al-2024>(15/80 | 91/341) ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks (Yang Liu et al., 2024)</a></li><li><a href=#1680--92341-towards-fairness-aware-adversarial-learning-yanghao-zhang-et-al-2024>(16/80 | 92/341) Towards Fairness-Aware Adversarial Learning (Yanghao Zhang et al., 2024)</a></li><li><a href=#1780--93341-structure-guided-adversarial-training-of-diffusion-models-ling-yang-et-al-2024>(17/80 | 93/341) Structure-Guided Adversarial Training of Diffusion Models (Ling Yang et al., 2024)</a></li><li><a href=#1880--94341-diffusekrona-a-parameter-efficient-fine-tuning-method-for-personalized-diffusion-models-shyam-marjit-et-al-2024>(18/80 | 94/341) DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Models (Shyam Marjit et al., 2024)</a></li><li><a href=#1980--95341-towards-robust-and-efficient-cloud-edge-elastic-model-adaptation-via-selective-entropy-distillation-yaofo-chen-et-al-2024>(19/80 | 95/341) Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation (Yaofo Chen et al., 2024)</a></li><li><a href=#2080--96341-feature-re-embedding-towards-foundation-model-level-performance-in-computational-pathology-wenhao-tang-et-al-2024>(20/80 | 96/341) Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology (Wenhao Tang et al., 2024)</a></li><li><a href=#2180--97341-box-it-to-bind-it-unified-layout-control-and-attribute-binding-in-t2i-diffusion-models-ashkan-taghipour-et-al-2024>(21/80 | 97/341) Box It to Bind It: Unified Layout Control and Attribute Binding in T2I Diffusion Models (Ashkan Taghipour et al., 2024)</a></li><li><a href=#2280--98341-lane2seq-towards-unified-lane-detection-via-sequence-generation-kunyang-zhou-2024>(22/80 | 98/341) Lane2Seq: Towards Unified Lane Detection via Sequence Generation (Kunyang Zhou, 2024)</a></li><li><a href=#2380--99341-weakly-supervised-co-training-with-swapping-assignments-for-semantic-segmentation-xinyu-yang-et-al-2024>(23/80 | 99/341) Weakly Supervised Co-training with Swapping Assignments for Semantic Segmentation (Xinyu Yang et al., 2024)</a></li><li><a href=#2480--100341-adaptive-quantization-with-mixed-precision-based-on-low-cost-proxy-junzhe-chen-et-al-2024>(24/80 | 100/341) Adaptive quantization with mixed-precision based on low-cost proxy (Junzhe Chen et al., 2024)</a></li><li><a href=#2580--101341-sdf2net-shallow-to-deep-feature-fusion-network-for-polsar-image-classification-mohammed-q-alkhatib-et-al-2024>(25/80 | 101/341) SDF2Net: Shallow to Deep Feature Fusion Network for PolSAR Image Classification (Mohammed Q. Alkhatib et al., 2024)</a></li><li><a href=#2680--102341-adapt-before-comparison-a-new-perspective-on-cross-domain-few-shot-segmentation-jonas-herzog-2024>(26/80 | 102/341) Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation (Jonas Herzog, 2024)</a></li><li><a href=#2780--103341-sddgr-stable-diffusion-based-deep-generative-replay-for-class-incremental-object-detection-junsu-kim-et-al-2024>(27/80 | 103/341) SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection (Junsu Kim et al., 2024)</a></li><li><a href=#2880--104341-one-shot-structure-aware-stylized-image-synthesis-hansam-cho-et-al-2024>(28/80 | 104/341) One-Shot Structure-Aware Stylized Image Synthesis (Hansam Cho et al., 2024)</a></li><li><a href=#2980--105341-transparent-image-layer-diffusion-using-latent-transparency-lvmin-zhang-et-al-2024>(29/80 | 105/341) Transparent Image Layer Diffusion using Latent Transparency (Lvmin Zhang et al., 2024)</a></li><li><a href=#3080--106341-structural-teacher-student-normality-learning-for-multi-class-anomaly-detection-and-localization-hanqiu-deng-et-al-2024>(30/80 | 106/341) Structural Teacher-Student Normality Learning for Multi-Class Anomaly Detection and Localization (Hanqiu Deng et al., 2024)</a></li><li><a href=#3180--107341-oscar-object-state-captioning-and-state-change-representation-nguyen-nguyen-et-al-2024>(31/80 | 107/341) OSCaR: Object State Captioning and State Change Representation (Nguyen Nguyen et al., 2024)</a></li><li><a href=#3280--108341-mcf-vc-mitigate-catastrophic-forgetting-in-class-incremental-learning-for-multimodal-video-captioning-huiyu-xiong-et-al-2024>(32/80 | 108/341) MCF-VC: Mitigate Catastrophic Forgetting in Class-Incremental Learning for Multimodal Video Captioning (Huiyu Xiong et al., 2024)</a></li><li><a href=#3380--109341-explicit-interaction-for-fusion-based-place-recognition-jingyi-xu-et-al-2024>(33/80 | 109/341) Explicit Interaction for Fusion-Based Place Recognition (Jingyi Xu et al., 2024)</a></li><li><a href=#3480--110341-generative-3d-part-assembly-via-part-whole-hierarchy-message-passing-bian-du-et-al-2024>(34/80 | 110/341) Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing (Bi&rsquo;an Du et al., 2024)</a></li><li><a href=#3580--111341-mitigating-distributional-shift-in-semantic-segmentation-via-uncertainty-estimation-from-unlabelled-data-david-s-w-williams-et-al-2024>(35/80 | 111/341) Mitigating Distributional Shift in Semantic Segmentation via Uncertainty Estimation from Unlabelled Data (David S. W. Williams et al., 2024)</a></li><li><a href=#3680--112341-masked-gamma-ssl-learning-uncertainty-estimation-via-masked-image-modeling-david-s-w-williams-et-al-2024>(36/80 | 112/341) Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image Modeling (David S. W. Williams et al., 2024)</a></li><li><a href=#3780--113341-socialcvae-predicting-pedestrian-trajectory-via-interaction-conditioned-latents-wei-xiang-et-al-2024>(37/80 | 113/341) SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents (Wei Xiang et al., 2024)</a></li><li><a href=#3880--114341-analyzing-regional-organization-of-the-human-hippocampus-in-3d-pli-using-contrastive-learning-and-geometric-unfolding-alexander-oberstrass-et-al-2024>(38/80 | 114/341) Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using Contrastive Learning and Geometric Unfolding (Alexander Oberstrass et al., 2024)</a></li><li><a href=#3980--115341-vrp-sam-sam-with-visual-reference-prompt-yanpeng-sun-et-al-2024>(39/80 | 115/341) VRP-SAM: SAM with Visual Reference Prompt (Yanpeng Sun et al., 2024)</a></li><li><a href=#4080--116341-robust-unsupervised-crowd-counting-and-localization-with-adaptive-resolution-sam-jia-wan-et-al-2024>(40/80 | 116/341) Robust Unsupervised Crowd Counting and Localization with Adaptive Resolution SAM (Jia Wan et al., 2024)</a></li><li><a href=#4180--117341-a-vanilla-multi-task-framework-for-dense-visual-prediction-solution-to-1st-vcl-challenge----multi-task-robustness-track-zehui-chen-et-al-2024>(41/80 | 117/341) A Vanilla Multi-Task Framework for Dense Visual Prediction Solution to 1st VCL Challenge &ndash; Multi-Task Robustness Track (Zehui Chen et al., 2024)</a></li><li><a href=#4280--118341-context-based-and-diversity-driven-specificity-in-compositional-zero-shot-learning-yun-li-et-al-2024>(42/80 | 118/341) Context-based and Diversity-driven Specificity in Compositional Zero-Shot Learning (Yun Li et al., 2024)</a></li><li><a href=#4380--119341-playground-v25-three-insights-towards-enhancing-aesthetic-quality-in-text-to-image-generation-daiqing-li-et-al-2024>(43/80 | 119/341) Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation (Daiqing Li et al., 2024)</a></li><li><a href=#4480--120341-charactergen-efficient-3d-character-generation-from-single-images-with-multi-view-pose-canonicalization-hao-yang-peng-et-al-2024>(44/80 | 120/341) CharacterGen: Efficient 3D Character Generation from Single Images with Multi-View Pose Canonicalization (Hao-Yang Peng et al., 2024)</a></li><li><a href=#4580--121341-sora-a-review-on-background-technology-limitations-and-opportunities-of-large-vision-models-yixin-liu-et-al-2024>(45/80 | 121/341) Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models (Yixin Liu et al., 2024)</a></li><li><a href=#4680--122341-few-shot-adaptation-for-morphology-independent-cell-instance-segmentation-ram-j-zaveri-et-al-2024>(46/80 | 122/341) Few-shot adaptation for morphology-independent cell instance segmentation (Ram J. Zaveri et al., 2024)</a></li><li><a href=#4780--123341-nocplace-nocturnal-visual-place-recognition-using-generative-and-inherited-knowledge-transfer-bingxi-liu-et-al-2024>(47/80 | 123/341) NocPlace: Nocturnal Visual Place Recognition Using Generative and Inherited Knowledge Transfer (Bingxi Liu et al., 2024)</a></li><li><a href=#4880--124341-adversarial-example-soups-averaging-multiple-adversarial-examples-improves-transferability-without-increasing-additional-generation-time-bo-yang-et-al-2024>(48/80 | 124/341) Adversarial example soups: averaging multiple adversarial examples improves transferability without increasing additional generation time (Bo Yang et al., 2024)</a></li><li><a href=#4980--125341-t-hitl-effectively-addresses-problematic-associations-in-image-generation-and-maintains-overall-visual-quality-susan-epstein-et-al-2024>(49/80 | 125/341) T-HITL Effectively Addresses Problematic Associations in Image Generation and Maintains Overall Visual Quality (Susan Epstein et al., 2024)</a></li><li><a href=#5080--126341-diffusion-model-based-image-editing-a-survey-yi-huang-et-al-2024>(50/80 | 126/341) Diffusion Model-Based Image Editing: A Survey (Yi Huang et al., 2024)</a></li><li><a href=#5180--127341-plremix-combating-noisy-labels-with-pseudo-label-relaxed-contrastive-representation-learning-xiaoyu-liu-et-al-2024>(51/80 | 127/341) PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning (Xiaoyu Liu et al., 2024)</a></li><li><a href=#5280--128341-black-box-adversarial-attacks-against-image-quality-assessment-models-yu-ran-et-al-2024>(52/80 | 128/341) Black-box Adversarial Attacks Against Image Quality Assessment Models (Yu Ran et al., 2024)</a></li><li><a href=#5380--129341-sora-generates-videos-with-stunning-geometrical-consistency-xuanyi-li-et-al-2024>(53/80 | 129/341) Sora Generates Videos with Stunning Geometrical Consistency (Xuanyi Li et al., 2024)</a></li><li><a href=#5480--130341-deployment-prior-injection-for-run-time-calibratable-object-detection-mo-zhou-et-al-2024>(54/80 | 130/341) Deployment Prior Injection for Run-time Calibratable Object Detection (Mo Zhou et al., 2024)</a></li><li><a href=#5580--131341-livehps-lidar-based-scene-level-human-pose-and-shape-estimation-in-free-environment-yiming-ren-et-al-2024>(55/80 | 131/341) LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment (Yiming Ren et al., 2024)</a></li><li><a href=#5680--132341-efficiently-leveraging-linguistic-priors-for-scene-text-spotting-nguyen-nguyen-et-al-2024>(56/80 | 132/341) Efficiently Leveraging Linguistic Priors for Scene Text Spotting (Nguyen Nguyen et al., 2024)</a></li><li><a href=#5780--133341-alignmif-geometry-aligned-multimodal-implicit-field-for-lidar-camera-joint-synthesis-tao-tang-et-al-2024>(57/80 | 133/341) AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-Camera Joint Synthesis (Tao Tang et al., 2024)</a></li><li><a href=#5880--134341-customsketching-sketch-concept-extraction-for-sketch-based-image-synthesis-and-editing-chufeng-xiao-et-al-2024>(58/80 | 134/341) CustomSketching: Sketch Concept Extraction for Sketch-based Image Synthesis and Editing (Chufeng Xiao et al., 2024)</a></li><li><a href=#5980--135341-scribble-hides-class-promoting-scribble-based-weakly-supervised-semantic-segmentation-with-its-class-label-xinliang-zhang-et-al-2024>(59/80 | 135/341) Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label (Xinliang Zhang et al., 2024)</a></li><li><a href=#6080--136341-interactive-multi-head-self-attention-with-linear-complexity-hankyul-kang-et-al-2024>(60/80 | 136/341) Interactive Multi-Head Self-Attention with Linear Complexity (Hankyul Kang et al., 2024)</a></li><li><a href=#6180--137341-mge-a-training-free-and-efficient-model-generation-and-enhancement-scheme-xuan-wang-et-al-2024>(61/80 | 137/341) MGE: A Training-Free and Efficient Model Generation and Enhancement Scheme (Xuan Wang et al., 2024)</a></li><li><a href=#6280--138341-emo-emote-portrait-alive----generating-expressive-portrait-videos-with-audio2video-diffusion-model-under-weak-conditions-linrui-tian-et-al-2024>(62/80 | 138/341) EMO: Emote Portrait Alive &ndash; Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions (Linrui Tian et al., 2024)</a></li><li><a href=#6380--139341-neural-video-compression-with-feature-modulation-jiahao-li-et-al-2024>(63/80 | 139/341) Neural Video Compression with Feature Modulation (Jiahao Li et al., 2024)</a></li><li><a href=#6480--140341-accelerating-diffusion-sampling-with-optimized-time-steps-shuchen-xue-et-al-2024>(64/80 | 140/341) Accelerating Diffusion Sampling with Optimized Time Steps (Shuchen Xue et al., 2024)</a></li><li><a href=#6580--141341-capt-category-level-articulation-estimation-from-a-single-point-cloud-using-transformer-lian-fu-et-al-2024>(65/80 | 141/341) CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer (Lian Fu et al., 2024)</a></li><li><a href=#6680--142341-icp-flow-lidar-scene-flow-estimation-with-icp-yancong-lin-et-al-2024>(66/80 | 142/341) ICP-Flow: LiDAR Scene Flow Estimation with ICP (Yancong Lin et al., 2024)</a></li><li><a href=#6780--143341-enhancing-hyperspectral-images-via-diffusion-model-and-group-autoencoder-super-resolution-network-zhaoyang-wang-et-al-2024>(67/80 | 143/341) Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder Super-resolution Network (Zhaoyang Wang et al., 2024)</a></li><li><a href=#6880--144341-image-text-matching-with-multi-view-attention-rui-cheng-et-al-2024>(68/80 | 144/341) Image-Text Matching with Multi-View Attention (Rui Cheng et al., 2024)</a></li><li><a href=#6980--145341-preserving-fairness-generalization-in-deepfake-detection-li-lin-et-al-2024>(69/80 | 145/341) Preserving Fairness Generalization in Deepfake Detection (Li Lin et al., 2024)</a></li><li><a href=#7080--146341-advancing-generative-model-evaluation-a-novel-algorithm-for-realistic-image-synthesis-and-comparison-in-ocr-system-majid-memari-et-al-2024>(70/80 | 146/341) Advancing Generative Model Evaluation: A Novel Algorithm for Realistic Image Synthesis and Comparison in OCR System (Majid Memari et al., 2024)</a></li><li><a href=#7180--147341-deep-umbra-a-generative-approach-for-sunlight-access-computation-in-urban-spaces-kazi-shahrukh-omar-et-al-2024>(71/80 | 147/341) Deep Umbra: A Generative Approach for Sunlight Access Computation in Urban Spaces (Kazi Shahrukh Omar et al., 2024)</a></li><li><a href=#7280--148341-sam-diffsr-structure-modulated-diffusion-model-for-image-super-resolution-chengcheng-wang-et-al-2024>(72/80 | 148/341) SAM-DiffSR: Structure-Modulated Diffusion Model for Image Super-Resolution (Chengcheng Wang et al., 2024)</a></li><li><a href=#7380--149341-charnerf-3d-character-generation-from-concept-art-eddy-chu-et-al-2024>(73/80 | 149/341) CharNeRF: 3D Character Generation from Concept Art (Eddy Chu et al., 2024)</a></li><li><a href=#7480--150341-coupled-laplacian-eigenmaps-for-locally-aware-3d-rigid-point-cloud-matching-matteo-bastico-et-al-2024>(74/80 | 150/341) Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching (Matteo Bastico et al., 2024)</a></li><li><a href=#7580--151341-learning-dynamic-tetrahedra-for-high-quality-talking-head-synthesis-zicheng-zhang-et-al-2024>(75/80 | 151/341) Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis (Zicheng Zhang et al., 2024)</a></li><li><a href=#7680--152341-phnet-patch-based-normalization-for-portrait-harmonization-karen-efremyan-et-al-2024>(76/80 | 152/341) PHNet: Patch-based Normalization for Portrait Harmonization (Karen Efremyan et al., 2024)</a></li><li><a href=#7780--153341-avs-net-point-sampling-with-adaptive-voxel-size-for-3d-point-cloud-analysis-hongcheng-yang-et-al-2024>(77/80 | 153/341) AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud Analysis (Hongcheng Yang et al., 2024)</a></li><li><a href=#7880--154341-pandas-prototype-based-novel-class-discovery-and-detection-tyler-l-hayes-et-al-2024>(78/80 | 154/341) PANDAS: Prototype-based Novel Class Discovery and Detection (Tyler L. Hayes et al., 2024)</a></li><li><a href=#7980--155341-learning-exposure-correction-in-dynamic-scenes-jin-liu-et-al-2024>(79/80 | 155/341) Learning Exposure Correction in Dynamic Scenes (Jin Liu et al., 2024)</a></li><li><a href=#8080--156341-in-defense-and-revival-of-bayesian-filtering-for-thermal-infrared-object-tracking-peng-gao-et-al-2024>(80/80 | 156/341) In Defense and Revival of Bayesian Filtering for Thermal Infrared Object Tracking (Peng Gao et al., 2024)</a></li></ul></li><li><a href=#cssd-4>cs.SD (4)</a><ul><li><a href=#14--157341-songcomposer-a-large-language-model-for-lyric-and-melody-composition-in-song-generation-shuangrui-ding-et-al-2024>(1/4 | 157/341) SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation (Shuangrui Ding et al., 2024)</a></li><li><a href=#24--158341-emotional-voice-messages-emovome-database-emotion-recognition-in-spontaneous-voice-messages-lucía-gómez-zaragozá-et-al-2024>(2/4 | 158/341) Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages (Lucía Gómez Zaragozá et al., 2024)</a></li><li><a href=#34--159341-experimental-study-enhancing-voice-spoofing-detection-models-with-wav2vec-20-taein-kang-et-al-2024>(3/4 | 159/341) Experimental Study: Enhancing Voice Spoofing Detection Models with wav2vec 2.0 (Taein Kang et al., 2024)</a></li><li><a href=#44--160341-edtc-enhance-depth-of-text-comprehension-in-automated-audio-captioning-liwen-tan-et-al-2024>(4/4 | 160/341) EDTC: enhance depth of text comprehension in automated audio captioning (Liwen Tan et al., 2024)</a></li></ul></li><li><a href=#cslg-63>cs.LG (63)</a><ul><li><a href=#163--161341-ds-agent-automated-data-science-by-empowering-large-language-models-with-case-based-reasoning-siyuan-guo-et-al-2024>(1/63 | 161/341) DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning (Siyuan Guo et al., 2024)</a></li><li><a href=#263--162341-sinkhorn-distance-minimization-for-knowledge-distillation-xiao-cui-et-al-2024>(2/63 | 162/341) Sinkhorn Distance Minimization for Knowledge Distillation (Xiao Cui et al., 2024)</a></li><li><a href=#363--163341-intensive-care-as-one-big-sequence-modeling-problem-vadim-liventsev-et-al-2024>(3/63 | 163/341) Intensive Care as One Big Sequence Modeling Problem (Vadim Liventsev et al., 2024)</a></li><li><a href=#463--164341-advancing-sleep-detection-by-modelling-weak-label-sets-a-novel-weakly-supervised-learning-approach-matthias-boeker-et-al-2024>(4/63 | 164/341) Advancing sleep detection by modelling weak label sets: A novel weakly supervised learning approach (Matthias Boeker et al., 2024)</a></li><li><a href=#563--165341-localgcl-local-aware-contrastive-learning-for-graphs-haojun-jiang-et-al-2024>(5/63 | 165/341) LocalGCL: Local-aware Contrastive Learning for Graphs (Haojun Jiang et al., 2024)</a></li><li><a href=#663--166341-hybrid-square-neural-ode-causal-modeling-bob-junyi-zou-et-al-2024>(6/63 | 166/341) Hybrid Square Neural ODE Causal Modeling (Bob Junyi Zou et al., 2024)</a></li><li><a href=#763--167341-temporal-logic-specification-conditioned-decision-transformer-for-offline-safe-reinforcement-learning-zijian-guo-et-al-2024>(7/63 | 167/341) Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning (Zijian Guo et al., 2024)</a></li><li><a href=#863--168341-unsupervised-zero-shot-reinforcement-learning-via-functional-reward-encodings-kevin-frans-et-al-2024>(8/63 | 168/341) Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings (Kevin Frans et al., 2024)</a></li><li><a href=#963--169341-collaborative-learning-of-common-latent-representations-in-routinely-collected-multivariate-icu-physiological-signals-hollan-haule-et-al-2024>(9/63 | 169/341) Collaborative learning of common latent representations in routinely collected multivariate ICU physiological signals (Hollan Haule et al., 2024)</a></li><li><a href=#1063--170341-meta-tasks-an-alternative-view-on-meta-learning-regularization-mohammad-rostami-et-al-2024>(10/63 | 170/341) Meta-Tasks: An alternative view on Meta-Learning Regularization (Mohammad Rostami et al., 2024)</a></li><li><a href=#1163--171341-independent-learning-in-constrained-markov-potential-games-philip-jordan-et-al-2024>(11/63 | 171/341) Independent Learning in Constrained Markov Potential Games (Philip Jordan et al., 2024)</a></li><li><a href=#1263--172341-understanding-neural-network-binarization-with-forward-and-backward-proximal-quantizers-yiwei-lu-et-al-2024>(12/63 | 172/341) Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers (Yiwei Lu et al., 2024)</a></li><li><a href=#1363--173341-securing-reliability-a-brief-overview-on-enhancing-in-context-learning-for-foundation-models-yunpeng-huang-et-al-2024>(13/63 | 173/341) Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models (Yunpeng Huang et al., 2024)</a></li><li><a href=#1463--174341-variational-learning-is-effective-for-large-deep-networks-yuesong-shen-et-al-2024>(14/63 | 174/341) Variational Learning is Effective for Large Deep Networks (Yuesong Shen et al., 2024)</a></li><li><a href=#1563--175341-prioritizing-informative-features-and-examples-for-deep-learning-from-noisy-data-dongmin-park-2024>(15/63 | 175/341) Prioritizing Informative Features and Examples for Deep Learning from Noisy Data (Dongmin Park, 2024)</a></li><li><a href=#1663--176341-representation-learning-in-multiplex-graphs-where-and-how-to-fuse-information-piotr-bielak-et-al-2024>(16/63 | 176/341) Representation learning in multiplex graphs: Where and how to fuse information? (Piotr Bielak et al., 2024)</a></li><li><a href=#1763--177341-principled-architecture-aware-scaling-of-hyperparameters-wuyang-chen-et-al-2024>(17/63 | 177/341) Principled Architecture-aware Scaling of Hyperparameters (Wuyang Chen et al., 2024)</a></li><li><a href=#1863--178341-material-microstructure-design-using-vae-regression-with-multimodal-prior-avadhut-sardeshmukh-et-al-2024>(18/63 | 178/341) Material Microstructure Design Using VAE-Regression with Multimodal Prior (Avadhut Sardeshmukh et al., 2024)</a></li><li><a href=#1963--179341-reinforced-in-context-black-box-optimization-lei-song-et-al-2024>(19/63 | 179/341) Reinforced In-Context Black-Box Optimization (Lei Song et al., 2024)</a></li><li><a href=#2063--180341-fraud-detection-with-binding-global-and-local-relational-interaction-haolin-li-et-al-2024>(20/63 | 180/341) Fraud Detection with Binding Global and Local Relational Interaction (Haolin Li et al., 2024)</a></li><li><a href=#2163--181341-why-do-learning-rates-transfer-reconciling-optimization-and-scaling-limits-for-deep-learning-lorenzo-noci-et-al-2024>(21/63 | 181/341) Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning (Lorenzo Noci et al., 2024)</a></li><li><a href=#2263--182341-inpainting-computational-fluid-dynamics-with-deep-learning-dule-shu-et-al-2024>(22/63 | 182/341) Inpainting Computational Fluid Dynamics with Deep Learning (Dule Shu et al., 2024)</a></li><li><a href=#2363--183341-generative-learning-for-forecasting-the-dynamics-of-complex-systems-han-gao-et-al-2024>(23/63 | 183/341) Generative Learning for Forecasting the Dynamics of Complex Systems (Han Gao et al., 2024)</a></li><li><a href=#2463--184341-preroutgnn-for-timing-prediction-with-order-preserving-partition-global-circuit-pre-training-local-delay-learning-and-attentional-cell-modeling-ruizhe-zhong-et-al-2024>(24/63 | 184/341) PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling (Ruizhe Zhong et al., 2024)</a></li><li><a href=#2563--185341-latent-neural-pde-solver-a-reduced-order-modelling-framework-for-partial-differential-equations-zijie-li-et-al-2024>(25/63 | 185/341) Latent Neural PDE Solver: a reduced-order modelling framework for partial differential equations (Zijie Li et al., 2024)</a></li><li><a href=#2663--186341-when-your-ais-deceive-you-challenges-with-partial-observability-of-human-evaluators-in-reward-learning-leon-lang-et-al-2024>(26/63 | 186/341) When Your AIs Deceive You: Challenges with Partial Observability of Human Evaluators in Reward Learning (Leon Lang et al., 2024)</a></li><li><a href=#2763--187341-towards-a-digital-twin-framework-in-additive-manufacturing-machine-learning-and-bayesian-optimization-for-time-series-process-optimization-vispi-karkaria-et-al-2024>(27/63 | 187/341) Towards a Digital Twin Framework in Additive Manufacturing: Machine Learning and Bayesian Optimization for Time Series Process Optimization (Vispi Karkaria et al., 2024)</a></li><li><a href=#2863--188341-predicting-machine-failures-from-multivariate-time-series-an-industrial-case-study-nicolò-oreste-pinciroli-vago-et-al-2024>(28/63 | 188/341) Predicting machine failures from multivariate time series: an industrial case study (Nicolò Oreste Pinciroli Vago et al., 2024)</a></li><li><a href=#2963--189341-actions-speak-louder-than-words-trillion-parameter-sequential-transducers-for-generative-recommendations-jiaqi-zhai-et-al-2024>(29/63 | 189/341) Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations (Jiaqi Zhai et al., 2024)</a></li><li><a href=#3063--190341-predicting-o-glcnacylation-sites-in-mammalian-proteins-with-transformers-and-rnns-trained-with-a-new-loss-function-pedro-seber-2024>(30/63 | 190/341) Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function (Pedro Seber, 2024)</a></li><li><a href=#3163--191341-curriculum-learning-meets-directed-acyclic-graph-for-multimodal-emotion-recognition-cam-van-thi-nguyen-et-al-2024>(31/63 | 191/341) Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition (Cam-Van Thi Nguyen et al., 2024)</a></li><li><a href=#3263--192341-learning-topological-representations-with-bidirectional-graph-attention-network-for-solving-job-shop-scheduling-problem-cong-zhang-et-al-2024>(32/63 | 192/341) Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem (Cong Zhang et al., 2024)</a></li><li><a href=#3363--193341-using-graph-neural-networks-to-predict-local-culture-thiago-h-silva-et-al-2024>(33/63 | 193/341) Using Graph Neural Networks to Predict Local Culture (Thiago H Silva et al., 2024)</a></li><li><a href=#3463--194341-graph-neural-networks-and-arithmetic-circuits-timon-barlag-et-al-2024>(34/63 | 194/341) Graph Neural Networks and Arithmetic Circuits (Timon Barlag et al., 2024)</a></li><li><a href=#3563--195341-data-efficient-learning-via-clustering-based-sensitivity-sampling-foundation-models-and-beyond-kyriakos-axiotis-et-al-2024>(35/63 | 195/341) Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond (Kyriakos Axiotis et al., 2024)</a></li><li><a href=#3663--196341-dual-space-optimization-improved-molecule-sequence-design-by-latent-prompt-transformer-deqian-kong-et-al-2024>(36/63 | 196/341) Dual-Space Optimization: Improved Molecule Sequence Design by Latent Prompt Transformer (Deqian Kong et al., 2024)</a></li><li><a href=#3763--197341-taxdiff-taxonomic-guided-diffusion-model-for-protein-sequence-generation-lin-zongying-et-al-2024>(37/63 | 197/341) TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation (Lin Zongying et al., 2024)</a></li><li><a href=#3863--198341-automated-statistical-model-discovery-with-language-models-michael-y-li-et-al-2024>(38/63 | 198/341) Automated Statistical Model Discovery with Language Models (Michael Y. Li et al., 2024)</a></li><li><a href=#3963--199341-markovletics-methods-and-a-novel-application-for-learning-continuous-time-markov-chain-mixtures-fabian-spaeh-et-al-2024>(39/63 | 199/341) Markovletics: Methods and A Novel Application for Learning Continuous-Time Markov Chain Mixtures (Fabian Spaeh et al., 2024)</a></li><li><a href=#4063--200341-federated-learning-for-estimating-heterogeneous-treatment-effects-disha-makhija-et-al-2024>(40/63 | 200/341) Federated Learning for Estimating Heterogeneous Treatment Effects (Disha Makhija et al., 2024)</a></li><li><a href=#4163--201341-torchmd-net-20-fast-neural-network-potentials-for-molecular-simulations-raul-p-pelaez-et-al-2024>(41/63 | 201/341) TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations (Raul P. Pelaez et al., 2024)</a></li><li><a href=#4263--202341-dropbp-accelerating-fine-tuning-of-large-language-models-by-dropping-backward-propagation-sunghyeon-woo-et-al-2024>(42/63 | 202/341) DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation (Sunghyeon Woo et al., 2024)</a></li><li><a href=#4363--203341-evaluation-of-predictive-reliability-to-foster-trust-in-artificial-intelligence-a-case-study-in-multiple-sclerosis-lorenzo-peracchio-et-al-2024>(43/63 | 203/341) Evaluation of Predictive Reliability to Foster Trust in Artificial Intelligence. A case study in Multiple Sclerosis (Lorenzo Peracchio et al., 2024)</a></li><li><a href=#4463--204341-quce-the-minimisation-and-quantification-of-path-based-uncertainty-for-generative-counterfactual-explanations-jamie-duell-et-al-2024>(44/63 | 204/341) QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations (Jamie Duell et al., 2024)</a></li><li><a href=#4563--205341-robustness-congruent-adversarial-training-for-secure-machine-learning-model-updates-daniele-angioni-et-al-2024>(45/63 | 205/341) Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates (Daniele Angioni et al., 2024)</a></li><li><a href=#4663--206341-does-negative-sampling-matter-a-review-with-insights-into-its-theory-and-applications-zhen-yang-et-al-2024>(46/63 | 206/341) Does Negative Sampling Matter? A Review with Insights into its Theory and Applications (Zhen Yang et al., 2024)</a></li><li><a href=#4763--207341-stochastic-gradient-succeeds-for-bandits-jincheng-mei-et-al-2024>(47/63 | 207/341) Stochastic Gradient Succeeds for Bandits (Jincheng Mei et al., 2024)</a></li><li><a href=#4863--208341-hyperdimensional-computing-a-fast-robust-and-interpretable-paradigm-for-biological-data-michiel-stock-et-al-2024>(48/63 | 208/341) Hyperdimensional computing: a fast, robust and interpretable paradigm for biological data (Michiel Stock et al., 2024)</a></li><li><a href=#4963--209341-deepdrk-deep-dependency-regularized-knockoff-for-feature-selection-hongyu-shen-et-al-2024>(49/63 | 209/341) DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection (Hongyu Shen et al., 2024)</a></li><li><a href=#5063--210341-conjnorm-tractable-density-estimation-for-out-of-distribution-detection-bo-peng-et-al-2024>(50/63 | 210/341) ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection (Bo Peng et al., 2024)</a></li><li><a href=#5163--211341-label-noise-robust-diffusion-models-byeonghu-na-et-al-2024>(51/63 | 211/341) Label-Noise Robust Diffusion Models (Byeonghu Na et al., 2024)</a></li><li><a href=#5263--212341-sequentialattention-for-block-sparsification-differentiable-pruning-meets-combinatorial-optimization-taisuke-yasuda-et-al-2024>(52/63 | 212/341) SequentialAttention++ for Block Sparsification: Differentiable Pruning Meets Combinatorial Optimization (Taisuke Yasuda et al., 2024)</a></li><li><a href=#5363--213341-prediction-powered-ranking-of-large-language-models-ivi-chatzi-et-al-2024>(53/63 | 213/341) Prediction-Powered Ranking of Large Language Models (Ivi Chatzi et al., 2024)</a></li><li><a href=#5463--214341-multi-agent-deep-reinforcement-learning-for-distributed-satellite-routing-federico-lozano-cuadra-et-al-2024>(54/63 | 214/341) Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing (Federico Lozano-Cuadra et al., 2024)</a></li><li><a href=#5563--215341-feduv-uniformity-and-variance-for-heterogeneous-federated-learning-ha-min-son-et-al-2024>(55/63 | 215/341) FedUV: Uniformity and Variance for Heterogeneous Federated Learning (Ha Min Son et al., 2024)</a></li><li><a href=#5663--216341-sparse-variational-contaminated-noise-gaussian-process-regression-for-forecasting-geomagnetic-perturbations-daniel-iong-et-al-2024>(56/63 | 216/341) Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations (Daniel Iong et al., 2024)</a></li><li><a href=#5763--217341-rime-robust-preference-based-reinforcement-learning-with-noisy-preferences-jie-cheng-et-al-2024>(57/63 | 217/341) RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences (Jie Cheng et al., 2024)</a></li><li><a href=#5863--218341-efficient-backpropagation-with-variance-controlled-adaptive-sampling-ziteng-wang-et-al-2024>(58/63 | 218/341) Efficient Backpropagation with Variance-Controlled Adaptive Sampling (Ziteng Wang et al., 2024)</a></li><li><a href=#5963--219341-fedbrb-an-effective-solution-to-the-small-to-large-scenario-in-device-heterogeneity-federated-learning-ziyue-xu-et-al-2024>(59/63 | 219/341) FedBRB: An Effective Solution to the Small-to-Large Scenario in Device-Heterogeneity Federated Learning (Ziyue Xu et al., 2024)</a></li><li><a href=#6063--220341-gradient-based-discrete-sampling-with-automatic-cyclical-scheduling-patrick-pynadath-et-al-2024>(60/63 | 220/341) Gradient-based Discrete Sampling with Automatic Cyclical Scheduling (Patrick Pynadath et al., 2024)</a></li><li><a href=#6163--221341-enhanced-bayesian-optimization-via-preferential-modeling-of-abstract-properties-arun-kumar-a-v-et-al-2024>(61/63 | 221/341) Enhanced Bayesian Optimization via Preferential Modeling of Abstract Properties (Arun Kumar A V et al., 2024)</a></li><li><a href=#6263--222341-lcen-a-novel-feature-selection-algorithm-for-nonlinear-interpretable-machine-learning-models-pedro-seber-et-al-2024>(62/63 | 222/341) LCEN: A Novel Feature Selection Algorithm for Nonlinear, Interpretable Machine Learning Models (Pedro Seber et al., 2024)</a></li><li><a href=#6363--223341-an-interpretable-evaluation-of-entropy-based-novelty-of-generative-models-jingwei-zhang-et-al-2024>(63/63 | 223/341) An Interpretable Evaluation of Entropy-based Novelty of Generative Models (Jingwei Zhang et al., 2024)</a></li></ul></li><li><a href=#csai-11>cs.AI (11)</a><ul><li><a href=#111--224341-case-based-or-rule-based-how-do-transformers-do-the-math-yi-hu-et-al-2024>(1/11 | 224/341) Case-Based or Rule-Based: How Do Transformers Do the Math? (Yi Hu et al., 2024)</a></li><li><a href=#211--225341-omniact-a-dataset-and-benchmark-for-enabling-multimodal-generalist-autonomous-agents-for-desktop-and-web-raghav-kapoor-et-al-2024>(2/11 | 225/341) OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web (Raghav Kapoor et al., 2024)</a></li><li><a href=#311--226341-pragmatic-instruction-following-and-goal-assistance-via-cooperative-language-guided-inverse-planning-tan-zhi-xuan-et-al-2024>(3/11 | 226/341) Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning (Tan Zhi-Xuan et al., 2024)</a></li><li><a href=#411--227341-rebandit-random-effects-based-online-rl-algorithm-for-reducing-cannabis-use-susobhan-ghosh-et-al-2024>(4/11 | 227/341) reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use (Susobhan Ghosh et al., 2024)</a></li><li><a href=#511--228341-agent-pro-learning-to-evolve-via-policy-level-reflection-and-optimization-wenqi-zhang-et-al-2024>(5/11 | 228/341) Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization (Wenqi Zhang et al., 2024)</a></li><li><a href=#611--229341-determinants-of-llm-assisted-decision-making-eva-eigner-et-al-2024>(6/11 | 229/341) Determinants of LLM-assisted Decision-Making (Eva Eigner et al., 2024)</a></li><li><a href=#711--230341-the-kandy-benchmark-incremental-neuro-symbolic-learning-and-reasoning-with-kandinsky-patterns-luca-salvatore-lorello-et-al-2024>(7/11 | 230/341) The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns (Luca Salvatore Lorello et al., 2024)</a></li><li><a href=#811--231341-benchmarking-data-science-agents-yuge-zhang-et-al-2024>(8/11 | 231/341) Benchmarking Data Science Agents (Yuge Zhang et al., 2024)</a></li><li><a href=#911--232341-cocoa-cbt-based-conversational-counseling-agent-using-memory-specialized-in-cognitive-distortions-and-dynamic-prompt-suyeon-lee-et-al-2024>(9/11 | 232/341) COCOA: CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions and Dynamic Prompt (Suyeon Lee et al., 2024)</a></li><li><a href=#1011--233341-large-language-model-for-participatory-urban-planning-zhilun-zhou-et-al-2024>(10/11 | 233/341) Large Language Model for Participatory Urban Planning (Zhilun Zhou et al., 2024)</a></li><li><a href=#1111--234341-multi-agent-human-agent-and-beyond-a-survey-on-cooperation-in-social-dilemmas-hao-guo-et-al-2024>(11/11 | 234/341) Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas (Hao Guo et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--235341-a-neural-rewriting-system-to-solve-algorithmic-problems-flavio-petruzzellis-et-al-2024>(1/2 | 235/341) A Neural Rewriting System to Solve Algorithmic Problems (Flavio Petruzzellis et al., 2024)</a></li><li><a href=#22--236341-scaling-supervised-local-learning-with-augmented-auxiliary-networks-chenxiang-ma-et-al-2024>(2/2 | 236/341) Scaling Supervised Local Learning with Augmented Auxiliary Networks (Chenxiang Ma et al., 2024)</a></li></ul></li><li><a href=#quant-ph-3>quant-ph (3)</a><ul><li><a href=#13--237341-learning-to-program-variational-quantum-circuits-with-fast-weights-samuel-yen-chi-chen-2024>(1/3 | 237/341) Learning to Program Variational Quantum Circuits with Fast Weights (Samuel Yen-Chi Chen, 2024)</a></li><li><a href=#23--238341-quantum-distance-approximation-for-persistence-diagrams-bernardo-ameneyro-et-al-2024>(2/3 | 238/341) Quantum Distance Approximation for Persistence Diagrams (Bernardo Ameneyro et al., 2024)</a></li><li><a href=#33--239341-a-quantum-approach-to-synthetic-minority-oversampling-technique-smote-nishikanta-mohanty-et-al-2024>(3/3 | 239/341) A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE) (Nishikanta Mohanty et al., 2024)</a></li></ul></li><li><a href=#csse-4>cs.SE (4)</a><ul><li><a href=#14--240341-the-emergence-of-large-language-models-in-static-analysis-a-first-look-through-micro-benchmarks-ashwin-prasad-shivarpatna-venkatesh-et-al-2024>(1/4 | 240/341) The Emergence of Large Language Models in Static Analysis: A First Look through Micro-Benchmarks (Ashwin Prasad Shivarpatna Venkatesh et al., 2024)</a></li><li><a href=#24--241341-nissist-an-incident-mitigation-copilot-based-on-troubleshooting-guides-kaikai-an-et-al-2024>(2/4 | 241/341) Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides (Kaikai An et al., 2024)</a></li><li><a href=#34--242341-ansible-lightspeed-a-code-generation-service-for-it-automation-priyam-sahoo-et-al-2024>(3/4 | 242/341) Ansible Lightspeed: A Code Generation Service for IT Automation (Priyam Sahoo et al., 2024)</a></li><li><a href=#44--243341-faultprofit-hierarchical-fault-profiling-of-incident-tickets-in-large-scale-cloud-systems-junjie-huang-et-al-2024>(4/4 | 243/341) FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems (Junjie Huang et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-2>q-bio.QM (2)</a><ul><li><a href=#12--244341-biot5-towards-generalized-biological-understanding-with-iupac-integration-and-multi-task-tuning-qizhi-pei-et-al-2024>(1/2 | 244/341) BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning (Qizhi Pei et al., 2024)</a></li><li><a href=#22--245341-transfer-learning-bayesian-optimization-to-design-competitor-dna-molecules-for-use-in-diagnostic-assays-ruby-sedgwick-et-al-2024>(2/2 | 245/341) Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays (Ruby Sedgwick et al., 2024)</a></li></ul></li><li><a href=#csro-13>cs.RO (13)</a><ul><li><a href=#113--246341-can-an-llm-powered-socially-assistive-robot-effectively-and-safely-deliver-cognitive-behavioral-therapy-a-study-with-university-students-mina-j-kian-et-al-2024>(1/13 | 246/341) Can an LLM-Powered Socially Assistive Robot Effectively and Safely Deliver Cognitive Behavioral Therapy? A Study With University Students (Mina J. Kian et al., 2024)</a></li><li><a href=#213--247341-rethinking-mutual-information-for-language-conditioned-skill-discovery-on-imitation-learning-zhaoxun-ju-et-al-2024>(2/13 | 247/341) Rethinking Mutual Information for Language Conditioned Skill Discovery on Imitation Learning (Zhaoxun Ju et al., 2024)</a></li><li><a href=#313--248341-cggm-a-conditional-graph-generation-model-with-adaptive-sparsity-for-node-anomaly-detection-in-iot-networks-xianshi-su-et-al-2024>(3/13 | 248/341) CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks (Xianshi Su et al., 2024)</a></li><li><a href=#413--249341-icat-an-indoor-connected-and-autonomous-testbed-for-vehicle-computing-zhaofeng-tian-et-al-2024>(4/13 | 249/341) ICAT: An Indoor Connected and Autonomous Testbed for Vehicle Computing (Zhaofeng Tian et al., 2024)</a></li><li><a href=#513--250341-diffusion-meets-dagger-supercharging-eye-in-hand-imitation-learning-xiaoyu-zhang-et-al-2024>(5/13 | 250/341) Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning (Xiaoyu Zhang et al., 2024)</a></li><li><a href=#613--251341-backpropagation-based-analytical-derivatives-of-ekf-covariance-for-active-sensing-jonas-benhamou-et-al-2024>(6/13 | 251/341) Backpropagation-Based Analytical Derivatives of EKF Covariance for Active Sensing (Jonas Benhamou et al., 2024)</a></li><li><a href=#713--252341-underwater-acoustic-source-seeking-using-time-difference-of-arrival-measurements-filip-mandić-et-al-2024>(7/13 | 252/341) Underwater Acoustic Source Seeking Using Time-Difference-of-Arrival Measurements (Filip Mandić et al., 2024)</a></li><li><a href=#813--253341-active-propulsion-noise-shaping-for-multi-rotor-aircraft-localization-gabriele-serussi-et-al-2024>(8/13 | 253/341) Active propulsion noise shaping for multi-rotor aircraft localization (Gabriele Serussi et al., 2024)</a></li><li><a href=#913--254341-racp-risk-aware-contingency-planning-with-multi-modal-predictions-khaled-a-mustafa-et-al-2024>(9/13 | 254/341) RACP: Risk-Aware Contingency Planning with Multi-Modal Predictions (Khaled A. Mustafa et al., 2024)</a></li><li><a href=#1013--255341-opening-cabinets-and-drawers-in-the-real-world-using-a-commodity-mobile-manipulator-arjun-gupta-et-al-2024>(10/13 | 255/341) Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator (Arjun Gupta et al., 2024)</a></li><li><a href=#1113--256341-real-time-estimation-of-relative-pose-for-uavs-using-a-dual-channel-feature-association-zhaoying-wang-et-al-2024>(11/13 | 256/341) Real-Time Estimation of Relative Pose for UAVs Using a Dual-Channel Feature Association (Zhaoying Wang et al., 2024)</a></li><li><a href=#1213--257341-using-programmable-drone-in-educational-projects-and-competitions-pavel-petrovič-et-al-2024>(12/13 | 257/341) Using Programmable Drone in Educational Projects and Competitions (Pavel Petrovič et al., 2024)</a></li><li><a href=#1313--258341-swtrack-multiple-hypothesis-sliding-window-3d-multi-object-tracking-sandro-papais-et-al-2024>(13/13 | 258/341) SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking (Sandro Papais et al., 2024)</a></li></ul></li><li><a href=#eessiv-6>eess.IV (6)</a><ul><li><a href=#16--259341-medcontext-learning-contextual-cues-for-efficient-volumetric-medical-segmentation-hanan-gani-et-al-2024>(1/6 | 259/341) MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation (Hanan Gani et al., 2024)</a></li><li><a href=#26--260341-how-we-won-brats-2023-adult-glioma-challenge-just-faking-it-enhanced-synthetic-data-augmentation-and-model-ensemble-for-brain-tumour-segmentation-andré-ferreira-et-al-2024>(2/6 | 260/341) How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation (André Ferreira et al., 2024)</a></li><li><a href=#36--261341-sdr-former-a-siamese-dual-resolution-transformer-for-liver-lesion-classification-using-3d-multi-phase-imaging-meng-lou-et-al-2024>(3/6 | 261/341) SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging (Meng Lou et al., 2024)</a></li><li><a href=#46--262341-denoising-diffusion-models-for-inpainting-of-healthy-brain-tissue-alicia-durrer-et-al-2024>(4/6 | 262/341) Denoising Diffusion Models for Inpainting of Healthy Brain Tissue (Alicia Durrer et al., 2024)</a></li><li><a href=#56--263341-adapting-learned-image-codecs-to-screen-content-via-adjustable-transformations-h-burak-dogaroglu-et-al-2024>(5/6 | 263/341) Adapting Learned Image Codecs to Screen Content via Adjustable Transformations (H. Burak Dogaroglu et al., 2024)</a></li><li><a href=#66--264341-pe-mvcnet-multi-view-and-cross-modal-fusion-network-for-pulmonary-embolism-prediction-zhaoxin-guo-et-al-2024>(6/6 | 264/341) PE-MVCNet: Multi-view and Cross-modal Fusion Network for Pulmonary Embolism Prediction (Zhaoxin Guo et al., 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--265341-a-piece-of-theatre-investigating-how-teachers-design-llm-chatbots-to-assist-adolescent-cyberbullying-education-michael-a-hedderich-et-al-2024>(1/5 | 265/341) A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education (Michael A. Hedderich et al., 2024)</a></li><li><a href=#25--266341-neuralsi-neural-design-of-semantic-interaction-for-interactive-deep-learning-yali-bian-et-al-2024>(2/5 | 266/341) NeuralSI: Neural Design of Semantic Interaction for Interactive Deep Learning (Yali Bian et al., 2024)</a></li><li><a href=#35--267341-surgment-segmentation-enabled-semantic-search-and-creation-of-visual-question-and-feedback-to-support-video-based-surgery-learning-jingying-wang-et-al-2024>(3/5 | 267/341) Surgment: Segmentation-enabled Semantic Search and Creation of Visual Question and Feedback to Support Video-Based Surgery Learning (Jingying Wang et al., 2024)</a></li><li><a href=#45--268341-content-centric-prototyping-of-generative-ai-applications-emerging-approaches-and-challenges-in-collaborative-software-teams-hari-subramonyam-et-al-2024>(4/5 | 268/341) Content-Centric Prototyping of Generative AI Applications: Emerging Approaches and Challenges in Collaborative Software Teams (Hari Subramonyam et al., 2024)</a></li><li><a href=#55--269341-mitigating-barriers-to-public-social-interaction-with-meronymous-communication-nouran-soliman-et-al-2024>(5/5 | 269/341) Mitigating Barriers to Public Social Interaction with Meronymous Communication (Nouran Soliman et al., 2024)</a></li></ul></li><li><a href=#physicsmed-ph-1>physics.med-ph (1)</a><ul><li><a href=#11--270341-segment-anything-model-for-head-and-neck-tumor-segmentation-with-ct-pet-and-mri-multi-modality-images-jintao-ren-et-al-2024>(1/1 | 270/341) Segment anything model for head and neck tumor segmentation with CT, PET and MRI multi-modality images (Jintao Ren et al., 2024)</a></li></ul></li><li><a href=#csni-5>cs.NI (5)</a><ul><li><a href=#15--271341-outdoor-environment-reconstruction-with-deep-learning-on-radio-propagation-paths-hrant-khachatrian-et-al-2024>(1/5 | 271/341) Outdoor Environment Reconstruction with Deep Learning on Radio Propagation Paths (Hrant Khachatrian et al., 2024)</a></li><li><a href=#25--272341-emergency-caching-coded-caching-based-reliable-map-transmission-in-emergency-networks-zeyu-tian-et-al-2024>(2/5 | 272/341) Emergency Caching: Coded Caching-based Reliable Map Transmission in Emergency Networks (Zeyu Tian et al., 2024)</a></li><li><a href=#35--273341-reducing-unnecessary-alerts-in-pedestrian-protection-systems-based-on-p2v-communications-ignacio-soto-et-al-2024>(3/5 | 273/341) Reducing Unnecessary Alerts in Pedestrian Protection Systems Based on P2V Communications (Ignacio Soto et al., 2024)</a></li><li><a href=#45--274341-wykorzystanie-rekonfigurowalnych-matryc-antenowych-wraz-z-informacją-kontekstową-łukasz-kułacz-et-al-2024>(4/5 | 274/341) Wykorzystanie rekonfigurowalnych matryc antenowych wraz z informacją kontekstową (Łukasz Kułacz et al., 2024)</a></li><li><a href=#55--275341-a-survey-of-network-protocol-fuzzing-model-techniques-and-directions-shihao-jiang-et-al-2024>(5/5 | 275/341) A Survey of Network Protocol Fuzzing: Model, Techniques and Directions (Shihao Jiang et al., 2024)</a></li></ul></li><li><a href=#eesssy-7>eess.SY (7)</a><ul><li><a href=#17--276341-reinforcement-learning-based-robust-voltvar-control-in-active-distribution-networks-with-imprecisely-known-delay-hong-cheng-et-al-2024>(1/7 | 276/341) Reinforcement Learning Based Robust Volt/Var Control in Active Distribution Networks With Imprecisely Known Delay (Hong Cheng et al., 2024)</a></li><li><a href=#27--277341-distributed-estimation-and-control-for-lti-systems-under-finite-time-agreement-camilla-fioravanti-et-al-2024>(2/7 | 277/341) Distributed Estimation and Control for LTI Systems under Finite-Time Agreement (Camilla Fioravanti et al., 2024)</a></li><li><a href=#37--278341-impact-of-computation-in-integral-reinforcement-learning-for-continuous-time-control-wenhan-cao-et-al-2024>(3/7 | 278/341) Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control (Wenhan Cao et al., 2024)</a></li><li><a href=#47--279341-communication-constrained-stl-task-decomposition-through-convex-optimization-gregorio-marchesini-et-al-2024>(4/7 | 279/341) Communication-Constrained STL Task Decomposition through Convex Optimization (Gregorio Marchesini et al., 2024)</a></li><li><a href=#57--280341-model-free-deep-deterministic-policy-gradient-controller-for-setpoint-tracking-of-non-minimum-phase-systems-fatemeh-tavakkoli-et-al-2024>(5/7 | 280/341) Model Free Deep Deterministic Policy Gradient Controller for Setpoint Tracking of Non-minimum Phase Systems (Fatemeh Tavakkoli et al., 2024)</a></li><li><a href=#67--281341-converse-barrier-certificates-for-finite-time-safety-verification-of-continuous-time-perturbed-deterministic-systems-yonghan-li-et-al-2024>(6/7 | 281/341) Converse Barrier Certificates for Finite-time Safety Verification of Continuous-time Perturbed Deterministic Systems (Yonghan Li et al., 2024)</a></li><li><a href=#77--282341-exergetic-port-hamiltonian-systems-modeling-language-markus-lohmayer-et-al-2024>(7/7 | 282/341) Exergetic Port-Hamiltonian Systems Modeling Language (Markus Lohmayer et al., 2024)</a></li></ul></li><li><a href=#cscr-8>cs.CR (8)</a><ul><li><a href=#18--283341-chain-of-thought-prompting-of-large-language-models-for-discovering-and-fixing-software-vulnerabilities-yu-nong-et-al-2024>(1/8 | 283/341) Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities (Yu Nong et al., 2024)</a></li><li><a href=#28--284341-emmark-robust-watermarks-for-ip-protection-of-embedded-quantized-large-language-models-ruisi-zhang-et-al-2024>(2/8 | 284/341) EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models (Ruisi Zhang et al., 2024)</a></li><li><a href=#38--285341-complexity-assessment-of-analog-security-primitives-using-the-disentropy-of-autocorrelation-paul-jimenez-et-al-2024>(3/8 | 285/341) Complexity Assessment of Analog Security Primitives Using the Disentropy of Autocorrelation (Paul Jimenez et al., 2024)</a></li><li><a href=#48--286341-ai-driven-anonymization-protecting-personal-data-privacy-while-leveraging-machine-learning-le-yang-et-al-2024>(4/8 | 286/341) AI-Driven Anonymization: Protecting Personal Data Privacy While Leveraging Machine Learning (Le Yang et al., 2024)</a></li><li><a href=#58--287341-the-seekers-dilemma-realistic-formulation-and-benchmarking-for-hardware-trojan-detection-amin-sarihi-et-al-2024>(5/8 | 287/341) The Seeker&rsquo;s Dilemma: Realistic Formulation and Benchmarking for Hardware Trojan Detection (Amin Sarihi et al., 2024)</a></li><li><a href=#68--288341-on-central-primitives-for-quantum-cryptography-with-classical-communication-kai-min-chung-et-al-2024>(6/8 | 288/341) On Central Primitives for Quantum Cryptography with Classical Communication (Kai-Min Chung et al., 2024)</a></li><li><a href=#78--289341-a-scalable-multi-layered-blockchain-architecture-for-enhanced-ehr-sharing-and-drug-supply-chain-management-reza-javan-et-al-2024>(7/8 | 289/341) A Scalable Multi-Layered Blockchain Architecture for Enhanced EHR Sharing and Drug Supply Chain Management (Reza Javan et al., 2024)</a></li><li><a href=#88--290341-hardtaint-production-run-dynamic-taint-analysis-via-selective-hardware-tracing-yiyu-zhang-et-al-2024>(8/8 | 290/341) HardTaint: Production-Run Dynamic Taint Analysis via Selective Hardware Tracing (Yiyu Zhang et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--291341-prediction-of-the-sym-h-index-using-a-bayesian-deep-learning-method-with-uncertainty-quantification-yasser-abduallah-et-al-2024>(1/1 | 291/341) Prediction of the SYM-H Index Using a Bayesian Deep Learning Method with Uncertainty Quantification (Yasser Abduallah et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--292341-using-ai-libraries-for-incompressible-computational-fluid-dynamics-boyang-chen-et-al-2024>(1/1 | 292/341) Using AI libraries for Incompressible Computational Fluid Dynamics (Boyang Chen et al., 2024)</a></li></ul></li><li><a href=#cssy-1>cs.SY (1)</a><ul><li><a href=#11--293341-corridor-mpc-for-multi-agent-inspection-of-orbiting-structures-gregorio-marchesini-et-al-2024>(1/1 | 293/341) Corridor MPC for Multi-Agent Inspection of Orbiting Structures (Gregorio Marchesini et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--294341-note-evolutionary-game-theory-focus-informational-health-the-cocktail-party-effect-through-werewolfgame-under-incomplete-information-and-ess-search-method-using-expected-gains-of-repeated-dilemmas-yasuko-kawahata-2024>(1/1 | 294/341) Note: Evolutionary Game Theory Focus Informational Health: The Cocktail Party Effect Through Werewolfgame under Incomplete Information and ESS Search Method Using Expected Gains of Repeated Dilemmas (Yasuko Kawahata, 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--295341-designing-chatbots-to-support-victims-and-survivors-of-domestic-abuse-rahime-belen-saglam-et-al-2024>(1/2 | 295/341) Designing Chatbots to Support Victims and Survivors of Domestic Abuse (Rahime Belen Saglam et al., 2024)</a></li><li><a href=#22--296341-a-review-of-data-mining-in-personalized-education-current-trends-and-future-prospects-zhang-xiong-et-al-2024>(2/2 | 296/341) A Review of Data Mining in Personalized Education: Current Trends and Future Prospects (Zhang Xiong et al., 2024)</a></li></ul></li><li><a href=#csgt-4>cs.GT (4)</a><ul><li><a href=#14--297341-repeated-contracting-with-multiple-non-myopic-agents-policy-regret-and-limited-liability-natalie-collina-et-al-2024>(1/4 | 297/341) Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and Limited Liability (Natalie Collina et al., 2024)</a></li><li><a href=#24--298341-replicating-electoral-success-kiran-tomlinson-et-al-2024>(2/4 | 298/341) Replicating Electoral Success (Kiran Tomlinson et al., 2024)</a></li><li><a href=#34--299341-weighted-ef1-and-po-allocations-with-few-types-of-agents-or-chores-jugal-garg-et-al-2024>(3/4 | 299/341) Weighted EF1 and PO Allocations with Few Types of Agents or Chores (Jugal Garg et al., 2024)</a></li><li><a href=#44--300341-choosing-behind-the-veil-tight-bounds-for-identity-blind-online-algorithms-tomer-ezra-et-al-2024>(4/4 | 300/341) Choosing Behind the Veil: Tight Bounds for Identity-Blind Online Algorithms (Tomer Ezra et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--301341-a-highly-efficient-computational-approach-for-part-scale-microstructure-predictions-in-ti-6al-4v-additive-manufacturing-sebastian-d-proell-et-al-2024>(1/1 | 301/341) A highly efficient computational approach for part-scale microstructure predictions in Ti-6Al-4V additive manufacturing (Sebastian D. Proell et al., 2024)</a></li></ul></li><li><a href=#hep-ph-1>hep-ph (1)</a><ul><li><a href=#11--302341-a-case-study-of-sending-graph-neural-networks-back-to-the-test-bench-for-applications-in-high-energy-particle-physics-emanuel-pfeffer-et-al-2024>(1/1 | 302/341) A case study of sending graph neural networks back to the test bench for applications in high-energy particle physics (Emanuel Pfeffer et al., 2024)</a></li></ul></li><li><a href=#astro-phco-1>astro-ph.CO (1)</a><ul><li><a href=#11--303341-syren-halofit-a-fast-interpretable-high-precision-formula-for-the-λcdm-nonlinear-matter-power-spectrum-deaglan-j-bartlett-et-al-2024>(1/1 | 303/341) syren-halofit: A fast, interpretable, high-precision formula for the $Λ$CDM nonlinear matter power spectrum (Deaglan J. Bartlett et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--304341-sequential-transport-maps-using-sos-density-estimation-and-α-divergences-benjamin-zanger-et-al-2024>(1/3 | 304/341) Sequential transport maps using SoS density estimation and $α$-divergences (Benjamin Zanger et al., 2024)</a></li><li><a href=#23--305341-zeroth-order-sampling-methods-for-non-log-concave-distributions-alleviating-metastability-by-denoising-diffusion-ye-he-et-al-2024>(2/3 | 305/341) Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion (Ye He et al., 2024)</a></li><li><a href=#33--306341-dataset-fairness-achievable-fairness-on-your-data-with-utility-guarantees-muhammad-faaiz-taufiq-et-al-2024>(3/3 | 306/341) Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees (Muhammad Faaiz Taufiq et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--307341-rose-efficient-and-extensible-autodiff-on-the-web-sam-estep-et-al-2024>(1/1 | 307/341) Rose: Efficient and Extensible Autodiff on the Web (Sam Estep et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--308341-a-multi-agent-model-for-opinion-evolution-under-cognitive-biases-mário-s-alvim-et-al-2024>(1/1 | 308/341) A Multi-Agent Model for Opinion Evolution under Cognitive Biases (Mário S. Alvim et al., 2024)</a></li></ul></li><li><a href=#nlinao-1>nlin.AO (1)</a><ul><li><a href=#11--309341-predicting-instability-in-complex-oscillator-networks-limitations-and-potentials-of-network-measures-and-machine-learning-christian-nauck-et-al-2024>(1/1 | 309/341) Predicting Instability in Complex Oscillator Networks: Limitations and Potentials of Network Measures and Machine Learning (Christian Nauck et al., 2024)</a></li></ul></li><li><a href=#csit-1>cs.IT (1)</a><ul><li><a href=#11--310341-weakly-private-information-retrieval-from-heterogeneously-trusted-servers-wenyuan-zhao-et-al-2024>(1/1 | 310/341) Weakly Private Information Retrieval from Heterogeneously Trusted Servers (Wenyuan Zhao et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--311341-pathwise-relaxed-optimal-control-of-rough-differential-equations-prakash-chakraborty-et-al-2024>(1/2 | 311/341) Pathwise Relaxed Optimal Control of Rough Differential Equations (Prakash Chakraborty et al., 2024)</a></li><li><a href=#22--312341-optimal-control-barrier-functions-maximizing-the-action-space-subject-to-control-bounds-logan-e-beaver-2024>(2/2 | 312/341) Optimal Control Barrier Functions: Maximizing the Action Space Subject to Control Bounds (Logan E. Beaver, 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--313341-ssresf-sensitivity-aware-single-particle-radiation-effects-simulation-framework-in-soc-platforms-based-on-svm-algorithm-meng-liu-et-al-2024>(1/1 | 313/341) SSRESF: Sensitivity-aware Single-particle Radiation Effects Simulation Framework in SoC Platforms based on SVM Algorithm (Meng Liu et al., 2024)</a></li></ul></li><li><a href=#mathna-5>math.NA (5)</a><ul><li><a href=#15--314341-numerical-schemes-for-3-wave-kinetic-equations-a-complete-treatment-of-the-collision-operator-steven-walton-et-al-2024>(1/5 | 314/341) Numerical Schemes for 3-Wave Kinetic Equations: A Complete Treatment of the Collision Operator (Steven Walton et al., 2024)</a></li><li><a href=#25--315341-real-time-tracking-of-moving-objects-from-scattering-matrix-in-real-world-microwave-imaging-seong-ho-son-et-al-2024>(2/5 | 315/341) Real-time tracking of moving objects from scattering matrix in real-world microwave imaging (Seong-Ho Son et al., 2024)</a></li><li><a href=#35--316341-performance-analysis-of-music-type-imaging-without-diagonal-elements-of-multi-static-response-matrix-won-kwang-park-2024>(3/5 | 316/341) Performance analysis of MUSIC-type imaging without diagonal elements of multi-static response matrix (Won-Kwang Park, 2024)</a></li><li><a href=#45--317341-a-p-version-of-convolution-quadrature-in-wave-propagation-alexander-rieder-2024>(4/5 | 317/341) A $p$-version of convolution quadrature in wave propagation (Alexander Rieder, 2024)</a></li><li><a href=#55--318341-an-all-frequency-stable-surface-integral-equation-algorithm-for-electromagnetism-in-3-d-unbounded-penetrable-media-continuous-and-fully-discrete-model-analysis-mahadevan-ganesh-et-al-2024>(5/5 | 318/341) An all-frequency stable surface integral equation algorithm for electromagnetism in 3-D unbounded penetrable media: Continuous and fully-discrete model analysis (Mahadevan Ganesh et al., 2024)</a></li></ul></li><li><a href=#csdc-3>cs.DC (3)</a><ul><li><a href=#13--319341-massive-parallelization-and-performance-enhancement-of-an-immersed-boundary-method-based-unsteady-flow-solver-rahul-sundar-et-al-2024>(1/3 | 319/341) Massive parallelization and performance enhancement of an immersed boundary method based unsteady flow solver (Rahul Sundar et al., 2024)</a></li><li><a href=#23--320341-application-of-machine-learning-optimization-in-cloud-computing-resource-scheduling-and-management-yifan-zhang-et-al-2024>(2/3 | 320/341) Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management (Yifan Zhang et al., 2024)</a></li><li><a href=#33--321341-deep-reinforcement-learning-drl-based-methods-for-serverless-stream-processing-engines-a-vision-architectural-elements-and-future-directions-maria-r-read-et-al-2024>(3/3 | 321/341) Deep Reinforcement Learning (DRL)-based Methods for Serverless Stream Processing Engines: A Vision, Architectural Elements, and Future Directions (Maria R. Read et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--322341-image-to-mesh-conversion-for-biomedical-simulations-fotis-drakopoulos-et-al-2024>(1/1 | 322/341) Image-To-Mesh Conversion for Biomedical Simulations (Fotis Drakopoulos et al., 2024)</a></li></ul></li><li><a href=#physicscomp-ph-2>physics.comp-ph (2)</a><ul><li><a href=#12--323341-beacon-a-lightweight-deep-reinforcement-learning-benchmark-library-for-flow-control-jonathan-viquerat-et-al-2024>(1/2 | 323/341) Beacon, a lightweight deep reinforcement learning benchmark library for flow control (Jonathan Viquerat et al., 2024)</a></li><li><a href=#22--324341-thermodynamics-informed-super-resolution-of-scarce-temporal-dynamics-data-carlos-bermejo-barbanoj-et-al-2024>(2/2 | 324/341) Thermodynamics-informed super-resolution of scarce temporal dynamics data (Carlos Bermejo-Barbanoj et al., 2024)</a></li></ul></li><li><a href=#cslo-2>cs.LO (2)</a><ul><li><a href=#12--325341-mcsat-based-finite-field-reasoning-in-the-yices2-smt-solver-thomas-hader-et-al-2024>(1/2 | 325/341) MCSat-based Finite Field Reasoning in the Yices2 SMT Solver (Thomas Hader et al., 2024)</a></li><li><a href=#22--326341-a-constraint-based-mathematical-modeling-library-in-prolog-with-answer-constraint-semantics-françois-fages-2024>(2/2 | 326/341) A Constraint-based Mathematical Modeling Library in Prolog with Answer Constraint Semantics (François Fages, 2024)</a></li></ul></li><li><a href=#cssi-3>cs.SI (3)</a><ul><li><a href=#13--327341-gin-sd-source-detection-in-graphs-with-incomplete-nodes-via-positional-encoding-and-attentive-fusion-le-cheng-et-al-2024>(1/3 | 327/341) GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion (Le Cheng et al., 2024)</a></li><li><a href=#23--328341-scalable-community-search-with-accuracy-guarantee-on-attributed-graphs-yuxiang-wang-et-al-2024>(2/3 | 328/341) Scalable Community Search with Accuracy Guarantee on Attributed Graphs (Yuxiang Wang et al., 2024)</a></li><li><a href=#33--329341-towards-spatiotemporal-integration-of-bus-transit-with-data-driven-approaches-júlio-borges-et-al-2024>(3/3 | 329/341) Towards spatiotemporal integration of bus transit with data-driven approaches (Júlio Borges et al., 2024)</a></li></ul></li><li><a href=#physicsoptics-1>physics.optics (1)</a><ul><li><a href=#11--330341-scaling-on-chip-photonic-neural-processors-using-arbitrarily-programmable-wave-propagation-tatsuhiro-onodera-et-al-2024>(1/1 | 330/341) Scaling on-chip photonic neural processors using arbitrarily programmable wave propagation (Tatsuhiro Onodera et al., 2024)</a></li></ul></li><li><a href=#mathst-1>math.ST (1)</a><ul><li><a href=#11--331341-batched-nonparametric-contextual-bandits-rong-jiang-et-al-2024>(1/1 | 331/341) Batched Nonparametric Contextual Bandits (Rong Jiang et al., 2024)</a></li></ul></li><li><a href=#physicschem-ph-1>physics.chem-ph (1)</a><ul><li><a href=#11--332341-outlier-detection-for-reactive-machine-learned-potential-energy-surfaces-luis-itza-vazquez-salazar-et-al-2024>(1/1 | 332/341) Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces (Luis Itza Vazquez-Salazar et al., 2024)</a></li></ul></li><li><a href=#q-biogn-1>q-bio.GN (1)</a><ul><li><a href=#11--333341-supervised-machine-learning-for-microbiomics-bridging-the-gap-between-current-and-best-practices-natasha-k-dudek-et-al-2024>(1/1 | 333/341) Supervised machine learning for microbiomics: bridging the gap between current and best practices (Natasha K. Dudek et al., 2024)</a></li></ul></li><li><a href=#csds-3>cs.DS (3)</a><ul><li><a href=#13--334341-purelottery-fair-and-bias-resistant-leader-election-with-a-novel-single-elimination-tournament-algorithm-jonas-ballweg-2024>(1/3 | 334/341) PureLottery: Fair and Bias-Resistant Leader Election with a Novel Single-Elimination Tournament Algorithm (Jonas Ballweg, 2024)</a></li><li><a href=#23--335341-scalable-identification-of-minimum-undesignable-rna-motifs-on-loop-pair-graphs-tianshuo-zhou-et-al-2024>(2/3 | 335/341) Scalable Identification of Minimum Undesignable RNA Motifs on Loop-Pair Graphs (Tianshuo Zhou et al., 2024)</a></li><li><a href=#33--336341-learning-based-algorithms-for-graph-searching-problems-adela-frances-depavia-et-al-2024>(3/3 | 336/341) Learning-Based Algorithms for Graph Searching Problems (Adela Frances DePavia et al., 2024)</a></li></ul></li><li><a href=#econth-1>econ.TH (1)</a><ul><li><a href=#11--337341-generative-ai-and-copyright-a-dynamic-perspective-s-alex-yang-et-al-2024>(1/1 | 337/341) Generative AI and Copyright: A Dynamic Perspective (S. Alex Yang et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--338341-geometric-deep-learning-for-computer-aided-design-a-survey-negar-heidari-et-al-2024>(1/1 | 338/341) Geometric Deep Learning for Computer-Aided Design: A Survey (Negar Heidari et al., 2024)</a></li></ul></li><li><a href=#csdb-2>cs.DB (2)</a><ul><li><a href=#12--339341-graphmatch-subgraph-query-processing-on-fpgas-jonas-dann-et-al-2024>(1/2 | 339/341) GraphMatch: Subgraph Query Processing on FPGAs (Jonas Dann et al., 2024)</a></li><li><a href=#22--340341-metasql-a-generate-then-rank-framework-for-natural-language-to-sql-translation-yuankai-fan-et-al-2024>(2/2 | 340/341) Metasql: A Generate-then-Rank Framework for Natural Language to SQL Translation (Yuankai Fan et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--341341-minimum-length-word-representants-of-graph-products-eshwar-srinivasan-et-al-2024>(1/1 | 341/341) Minimum length word-representants of graph products (Eshwar Srinivasan et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>